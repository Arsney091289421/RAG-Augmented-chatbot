[
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ExecuTorch\n\n[ExecuTorch](https://pytorch.org/executorch/stable/index.html) is a platform that enables PyTorch training and inference programs to be run on mobile and edge devices. It is powered by [torch.compile](https://pytorch.org/docs/stable/torch.compiler.html) and [torch.export](https://pytorch.org/docs/main/export.html) for performance and deployment.",
  "You can use ExecuTorch with Transformers with [torch.export](https://pytorch.org/docs/main/export.html). The [`~transformers.convert_and_export_with_cache`] method converts a [`PreTrainedModel`] into an exportable module. Under the hood, it uses [torch.export](https://pytorch.org/docs/main/export.html) to export the model, ensuring compatibility with ExecuTorch.\n\n```py\nimport torch\nfrom transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig\nfrom transformers.integrations.executorch import(\nTorchExportableModuleWithStaticCache,\nconvert_and_export_with_cache\n)\n\ngeneration_config = GenerationConfig(\nuse_cache=True,\ncache_implementation=\"static\",\ncache_config={\n\"batch_size\": 1,\n\"max_cache_len\": 20,\n}\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", pad_token=\"</s>\", padding_side=\"right\")\nmodel = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", generation_config=generation_config)\n\nexported_program = convert_and_export_with_cache(model)\n```",
  "The exported PyTorch model is now ready to be used with ExecuTorch. Wrap the model with [`~transformers.TorchExportableModuleWithStaticCache`] to generate text.\n\n```py\nprompts = [\"Simply put, the theory of relativity states that \"]\nprompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\nprompt_token_ids = prompt_tokens[\"input_ids\"]\n\ngenerated_ids = TorchExportableModuleWithStaticCache.generate(\nexported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=20,\n)\ngenerated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(generated_text)\n['Simply put, the theory of relativity states that 1) the speed of light is the']\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Template writing\n\nA chat template is a [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) template stored in the tokenizers [chat_template](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.chat_template) attribute. Jinja is a templating language that allows you to write Python-like code and syntax. A chat template performs the following three roles.",
  "1. Print the role enclosed in `<|` and `|>` (`<|user|>`, `<|assistant|>`, etc.).\n2. Print the message followed by an end-of-sequence (`EOS`) token.\n3. Print the assistant token if [add_generation_prompt=True](./chat_templating#add_generation_prompt) so the model generates an assistant response.\n\nAn example template is shown below.\n\n```jinja\n{%- for message in messages %}\n{{- '<|' + message['role'] + |>\\n' }}\n{{- message['content'] + eos_token }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n{{- '<|assistant|>\\n' }}\n{%- endif %}\n```\n\nThe template can be customized to handle more complex use cases. This guide will show you how to add and edit templates and includes template writing tips.\n\n## Create a template\n\nCreate a template by writing a Jinja template and then setting it as the chat template in the tokenizer. For example, the template below adds `[ASST]` and `[/ASST]` tags to the assistant messages.\n\n```jinja\n{%- for message in messages %}\n{%- if message['role'] == 'user' %}\n{{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n{%- elif message['role'] == 'system' %}\n{{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}",
  "{%- elif message['role'] == 'assistant' %}\n{{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n{%- endif %}\n{%- endfor %}\n```\n\nSet the template in the tokenizer, and the next time you use [`~PreTrainedTokenizerBase.apply_chat_template`], the new template is used.\n\n```py\ntemplate = tokenizer.chat_template\ntemplate = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\ntokenizer.chat_template = template  # Set the new template\n```\n\nThe template is saved in the `tokenizer_config.json` file. Upload it to the Hub with [`~PreTrainedTokenizer.push_to_hub`] so you can reuse it later and make sure everyone is using the right template for your model.\n\n```py\ntokenizer.push_to_hub(\"model_name\")\n```\n\n## Template writing tips\n\nThe easiest way to start writing Jinja templates is to refer to existing templates. Use `print(tokenizer.chat_template)` on any chat model to see what template it's using. Try starting with simple models that don't call any tools or support RAG. Finally, take a look at the [Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for more details about formatting and syntax.",
  "This section curates some best practices for writing clean and efficient Jinja templates.\n\n### Trimming whitespace\n\nJinja prints any whitespace before or after a block of text. This can be an issue for chat templates because whitespace usage should be intentional. Add `-` to strip any whitespace before a block.\n\n```jinja\n{%- for message in messages %}\n{{- message['role'] + message['content'] }}\n{%- endfor %}\n```\n\nThe incorrect whitespace usage example below may introduce a newline and indentation in the output.\n\n```jinja\n{% for message in messages %}\n{{ message['role'] + message['content'] }}\n{% endfor %}\n```\n\n### Special variables\n\nThere are five special variables available inside a template. You can pass virtually any additional arguments to [`~PreTrainedTokenizerBase.apply_chat_template`] and it will be available inside the template as a variable. However, you should try to keep the number of variables to the five below to make it easier for users to use the chat model without writing custom code to handle model-specific arguments.\n\n- `messages` contains the chat history as a list of message dicts.\n- `tools` contains a list of tools in JSON schema format.",
  "- `documents` contains a list of documents with the format `{\"title\": Title, \"contents\": \"Contents\"}` (designed for RAG models).\n- `add_generation_prompt` is a boolean that determines whether to add an assistant header at the end of the conversation.\n- `bos_token` and `eos_token` are special tokens extracted from a tokenizers `special_tokens_map`.\n\n### Callable functions\n\nThere are two callable functions available inside a template.\n\n- `raise_exception(msg)` raises a `TemplateException`. This is useful for debugging or warning users about incorrect template usage.\n- `strftime_now(format_str)` retrieves the current date and time in a specific format which could be useful to include in system messages. It is equivalent to [datetime.now().strftime(format_str)](https://docs.python.org/3/library/datetime.html#datetime.datetime.now) in Python.\n\n### Compatibility with non-Python Jinja",
  "Jinja is implemented in multiple languages and they generally have the same syntax. Writing a template in Python allows you to use Python methods such as [lower](https://docs.python.org/3/library/stdtypes.html#str.lower) on strings or [items](https://docs.python.org/3/library/stdtypes.html#dict.items) on dicts. But this won't work if the template is used in a non-Python implementation, for example, when deploying with Javascript or Rust.\n\nMake the changes below to ensure compatibility across all Jinja implementations.\n\n- Replace Python methods with Jinja filters. For example, replace `string.lower()` with `string|lower` or `dict.items()` with `dict|dictitems`. Most of the changes follow the same pattern except `string.strip()`, which is replaced with `string|trim`. Refer to the list of [built-in filters](https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters) for a complete list of filters.\n- Replace `True`, `False`, and `None` (these are Python specific) with `true`, `false`, and `none` respectively.",
  "- Directly rendering a dict or list may return different results in other implementations. For example, string entries may change from single-quote to double-quote. To avoid this, add the [tojson](https://jinja.palletsprojects.com/en/3.1.x/templates/#jinja-filters.tojson) filter to maintain consistency.\n\n### Big templates\n\nNewer models or models with features like [tool-calling](./chat_extras#tools) and [RAG](./chat_extras#retrieval-augmented-generation-rag) require larger templates that can be longer than 100 lines. It may be easier to write larger templates in a separate file. The line numbers in the separate file corresponds exactly to the line numbers in template parsing or execution errors, making it easier to debug any potential issues.\n\nWrite the template in a separate file and extract it to the chat template.\n\n```py\nopen(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n```\n\nYou could also load an edited template back into the tokenizer.\n\n```py\ntokenizer.chat_template = open(\"template.jinja\").read()\n```\n\n## Templates for tools",
  "There isn't a specific format for writing templates for tools but it is best to follow the standard API. This ensures the template is widely accessible across models without requiring users to write custom code to use tools with your model.\n\n> [!WARNING]\n> Formatting such as whitespace and special tokens are model-specific. Make sure everything exactly matches the format a model was trained with.\n\nThe following section lists elements of the standard API for writing templates for tools.\n\n### Tool definitions\n\nTransformers chat template methods allow a user to pass tools as Python functions or a JSON schema. When functions are passed, a JSON schema is automatically generated and passed to the template. The `tools` variable in a template always takes a list of JSON schemas.",
  "The specific tokens and tool descriptions should match the ones your model was trained with. Your model doesn't need to understand the JSON schema input because your template can translate the JSON schema into your models format. For example, [Command-R](./model_doc/cohere) was trained with tools defined with Python function headers, but the Command-R tool template accepts JSON schemas. The template internally converts types and renders the input tools as Python headers.\n\n```json\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"multiply\",\n\"description\": \"A function that multiplies two numbers\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"a\": {\n\"type\": \"number\",\n\"description\": \"The first number to multiply\"\n},\n\"b\": {\n\"type\": \"number\",\n\"description\": \"The second number to multiply\"\n}\n},\n\"required\": [\"a\", \"b\"]\n}\n}\n}\n```\n\nAn example for handling tool definitions in a chat template is shown below. The specific tokens and tool descriptions should be changed to match the ones a model was trained with.\n\n```\n{%- if tools %}\n{%- for tool in tools %}\n{{- '<tool>' + tool['function']['name'] + '\\n' }}\n{%- for argument in tool['function']['parameters']['properties'] %}",
  "{{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }}\n{%- endfor %}\n{{- '\\n</tool>' }}\n{%- endif %}\n{%- endif %}\n```\n\n### Tool calls\n\nTool calls, if present, is a list with the `\"assistant”` role. This is always a list even though most tool-calling models only support single tool calls, which means the list usually only contains a single element.\n\n```json\n{\n\"role\": \"assistant\",\n\"tool_calls\": [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"multiply\",\n\"arguments\": {\n\"a\": 5,\n\"b\": 6\n}\n}\n}\n]\n}\n```\n\nA common pattern for handling tool calls is shown below.\n\n```\n{%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n{%- for tool_call in message['tool_calls'] %}\n{{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n{%- endif %}\n{%- endfor %}\n{%- endif %}\n```\n\n### Tool responses\n\nTool responses are a message dict with the `role`, `name` (name of the function) and `content` (result of the tool call) keys.\n\n```json\n{\n\"role\": \"tool\",\n\"name\": \"multiply\",\n\"content\": \"30\"\n}\n```",
  "Not all the keys need to be used in the tool response. For example, if a model doesn’t expect the function name to be included in the tool response, then you can just include the `role` and `content`.\n\n```\n{%- if message['role'] == 'tool' %}\n{{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n{%- endif %}\n```\n\n## Contribute\n\nAdd a chat template by setting the `chat_template` attribute in the tokenizer and testing it with [`~PreTrainedTokenizerBase.apply_chat_template`]. If it works as expected, then you can upload it to the Hub with with [`~PreTrainedTokenizer.push_to_hub`].\n\nEven if you're not the model owner, it is still helpful to add a template for a model with an empty chat template or a model that is using a default class template. Open a [pull request](https://hf.co/docs/hub/repositories-pull-requests-discussions) on the model repository to add the template.\n\n```py\ntokenizer.chat_template = template\ntokenizer.push_to_hub(\"model_name\")\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Sharing\n\nThe Hugging Face [Hub](https://hf.co/models) is a platform for sharing, discovering, and consuming models of all different types and sizes. We highly recommend sharing your model on the Hub to push open-source machine learning forward for everyone!\n\nThis guide will show you how to share a model to the Hub from Transformers.\n\n## Set up",
  "To share a model to the Hub, you need a Hugging Face [account](https://hf.co/join). Create a [User Access Token](https://hf.co/docs/hub/security-tokens#user-access-tokens) (stored in the [cache](./installation#cache-directory) by default) and login to your account from either the command line or notebook.\n\n<hfoptions id=\"share\">\n<hfoption id=\"huggingface-CLI\">\n\n```bash\nhuggingface-cli login\n```\n\n</hfoption>\n<hfoption id=\"notebook\">\n\n```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n</hfoption>\n</hfoptions>\n\n## Repository features\n\n<Youtube id=\"XvSGPZFEjDY\"/>\n\nEach model repository features versioning, commit history, and diff visualization.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png\"/>\n</div>\n\nVersioning is based on [Git](https://git-scm.com/) and [Git Large File Storage (LFS)](https://git-lfs.github.com/), and it enables revisions, a way to specify a model version with a commit hash, tag or branch.\n\nFor example, use the `revision` parameter in [`~PreTrainedModel.from_pretrained`] to load a specific model version from a commit hash.\n\n```py",
  "model = AutoModel.from_pretrained(\n\"julien-c/EsperBERTo-small\", revision=\"4c77982\"\n)\n```\n\nModel repositories also support [gating](https://hf.co/docs/hub/models-gated) to control who can access a model. Gating is common for allowing a select group of users to preview a research model before it's made public.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/gated-model.png\"/>\n</div>\n\nA model repository also includes an inference [widget](https://hf.co/docs/hub/models-widgets) for users to directly interact with a model on the Hub.\n\nCheck out the Hub [Models](https://hf.co/docs/hub/models) documentation to for more information.\n\n## Model framework conversion\n\nReach a wider audience by making a model available in PyTorch, TensorFlow, and Flax. While users can still load a model if they're using a different framework, it is slower because Transformers needs to convert the checkpoint on the fly. It is faster to convert the checkpoint first.\n\n<hfoptions id=\"convert\">\n<hfoption id=\"PyTorch\">\n\nSet `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch and then save it.\n\n```py",
  "from transformers import DistilBertForSequenceClassification\n\npt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\npt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n```\n\n</hfoption>\n<hfoption id=\"TensorFlow\">\n\nSet `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow and then save it.\n\n```py\nfrom transformers import TFDistilBertForSequenceClassification\n\ntf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\ntf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\nSet `from_pt=True` to convert a checkpoint from PyTorch to Flax and then save it.\n\n```py\nfrom transformers import FlaxDistilBertForSequenceClassification\nflax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n\"path/to/awesome-name-you-picked\", from_pt=True\n)\nflax_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Uploading a model",
  "There are several ways to upload a model to the Hub depending on your workflow preference. You can push a model with [`Trainer`], a callback for TensorFlow models, call [`~PreTrainedModel.push_to_hub`] directly on a model, or use the Hub web interface.\n\n<Youtube id=\"Z1-XMy-GNLQ\"/>\n\n### Trainer\n\n[`Trainer`] can push a model directly to the Hub after training. Set `push_to_hub=True` in [`TrainingArguments`] and pass it to [`Trainer`]. Once training is complete, call [`~transformers.Trainer.push_to_hub`] to upload the model.\n\n[`~transformers.Trainer.push_to_hub`] automatically adds useful information like training hyperparameters and results to the model card.\n\n```py\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=small_train_dataset,\neval_dataset=small_eval_dataset,\ncompute_metrics=compute_metrics,\n)\ntrainer.push_to_hub()\n```\n\n### PushToHubCallback\n\nFor TensorFlow models, add the [`PushToHubCallback`] to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n\n```py\nfrom transformers import PushToHubCallback",
  "push_to_hub_callback = PushToHubCallback(\noutput_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n)\nmodel.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n```\n\n### PushToHubMixin\n\nThe [`~utils.PushToHubMixin`] provides functionality for pushing a model or tokenizer to the Hub.\n\nCall [`~utils.PushToHubMixin.push_to_hub`] directly on a model to upload it to the Hub. It creates a repository under your namespace with the model name specified in [`~utils.PushToHubMixin.push_to_hub`].\n\n```py\nmodel.push_to_hub(\"my-awesome-model\")\n```\n\nOther objects like a tokenizer or TensorFlow model are also pushed to the Hub in the same way.\n\n```py\ntokenizer.push_to_hub(\"my-awesome-model\")\n```\n\nYour Hugging Face profile should now display the newly created model repository. Navigate to the **Files** tab to see all the uploaded files.\n\nRefer to the [Upload files to the Hub](https://hf.co/docs/hub/how-to-upstream) guide for more information about pushing files to the Hub.\n\n### Hub web interface\n\nThe Hub web interface is a no-code approach for uploading a model.",
  "1. Create a new repository by selecting [**New Model**](https://huggingface.co/new).\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png\"/>\n</div>\n\nAdd some information about your model:\n\n- Select the **owner** of the repository. This can be yourself or any of the organizations you belong to.\n- Pick a name for your model, which will also be the repository name.\n- Choose whether your model is public or private.\n- Set the license usage.\n\n2. Click on **Create model** to create the model repository.\n\n3. Select the **Files** tab and click on the **Add file** button to drag-and-drop a file to your repository. Add a commit message and click on **Commit changes to main** to commit the file.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png\"/>\n</div>\n\n## Model card\n\n[Model cards](https://hf.co/docs/hub/model-cards#model-cards) inform users about a models performance, limitations, potential biases, and ethical considerations. It is highly recommended to add a model card to your repository!",
  "A model card is a `README.md` file in your repository. Add this file by:\n\n- manually creating and uploading a `README.md` file\n- clicking on the **Edit model card** button in the repository\n\nTake a look at the Llama 3.1 [model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) for an example of what to include on a model card.\n\nLearn more about other model card metadata (carbon emissions, license, link to paper, etc.) available in the [Model Cards](https://hf.co/docs/hub/model-cards#model-cards) guide.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPU\n\nGPUs are the standard hardware for machine learning because they're optimized for memory bandwidth and parallelism. With the increasing sizes of modern models, it's more important than ever to make sure GPUs are capable of efficiently handling and delivering the best possible performance.\n\nThis guide will demonstrate a few ways to optimize inference on a GPU. The optimization methods shown below can be combined with each other to achieve even better performance, and they also work for distributed GPUs.",
  "## bitsandbytes\n\n[bitsandbytes](https://hf.co/docs/bitsandbytes/index) is a quantization library that supports 8-bit and 4-bit quantization. Quantization represents weights in a lower precision compared to the original full precision format. It reduces memory requirements and makes it easier to fit large model into memory.\n\nMake sure bitsandbytes and Accelerate are installed first.\n\n```bash\npip install bitsandbytes accelerate\n```\n\n<hfoptions id=\"bnb\">\n<hfoption id=\"8-bit\">\n\nFor text generation with 8-bit quantization, you should use [`~GenerationMixin.generate`] instead of the high-level [`Pipeline`] API. The [`Pipeline`] returns slower performance because it isn't optimized for 8-bit models, and some sampling strategies (nucleus sampling) also aren't supported.\n\nSet up a [`BitsAndBytesConfig`] and set `load_in_8bit=True` to load a model in 8-bit precision. The [`BitsAndBytesConfig`] is passed to the `quantization_config` parameter in [`~PreTrainedModel.from_pretrained`].\n\nAllow Accelerate to automatically distribute the model across your available hardware by setting [device_map=\"auto\"](https://hf.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map).",
  "Place all inputs on the same device as the model.\n\n```py\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer(\"meta-llama/Llama-3.1-8B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\n\nprompt = \"Hello, my llama is cute\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ngenerated_ids = model.generate(**inputs)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n```\n\nFor distributed setups, use the `max_memory` parameter to create a mapping of the amount of memory to allocate to each GPU. The example below distributes 16GB of memory to the first GPU and 16GB of memory to the second GPU.\n\n```py\nmax_memory_mapping = {0: \"16GB\", 1: \"16GB\"}\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config, max_memory=max_memory_mapping\n)\n```",
  "Learn in more detail the concepts underlying 8-bit quantization in the [Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://hf.co/blog/hf-bitsandbytes-integration) blog post.\n\n</hfoption>\n<hfoption id=\"4-bit\">\n\nSet up a [`BitsAndBytesConfig`] and set `load_in_4bit=True` to load a model in 4-bit precision. The [`BitsAndBytesConfig`] is passed to the `quantization_config` parameter in [`~PreTrainedModel.from_pretrained`].\n\nAllow Accelerate to automatically distribute the model across your available hardware by setting `device_map=“auto”`.\n\nPlace all inputs on the same device as the model.\n\n```py\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer(\"meta-llama/Llama-3.1-8B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\n\nprompt = \"Hello, my llama is cute\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model_8bit.device)\ngenerated_ids = model_8bit.generate(**inputs)",
  "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n```\n\nFor distributed setups, use the `max_memory` parameter to create a mapping of the amount of memory to allocate to each GPU. The example below distributes 16GB of memory to the first GPU and 16GB of memory to the second GPU.\n\n```py\nmax_memory_mapping = {0: \"16GB\", 1: \"16GB\"}\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config, max_memory=max_memory_mapping\n)\n```\n\n</hfoption>\n</hfoptions>\n\n## Optimum\n\n[Optimum](https://hf.co/docs/optimum/en/index) is a Hugging Face library focused on optimizing model performance across various hardware. It supports [ONNX Runtime](https://onnxruntime.ai/docs/) (ORT), a model accelerator, for a wide range of hardware and frameworks including NVIDIA GPUs and AMD GPUs that use the [ROCm](https://www.amd.com/en/products/software/rocm.html) stack.",
  "ORT uses optimization techniques that fuse common operations into a single node and constant folding to reduce the number of computations. ORT also places the most computationally intensive operations on the GPU and the rest on the CPU to intelligently distribute the workload between the two devices.\n\nOptimum provides the [`~optimum.onnxruntime.ORTModel`] class for loading ONNX models. Set the `provider` parameter according to the table below.\n\n| provider | hardware |\n|---|---|\n| [CUDAExecutionProvider](https://hf.co/docs/optimum/main/en/onnxruntime/usage_guides/gpu#cudaexecutionprovider) | CUDA-enabled GPUs |\n| [ROCMExecutionProvider](https://hf.co/docs/optimum/onnxruntime/usage_guides/amdgpu) | AMD Instinct, Radeon Pro, Radeon GPUs |\n| [TensorrtExecutionProvider](https://hf.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider) | TensorRT |",
  "For example, load the [distilbert/distilbert-base-uncased-finetuned-sst-2-english](https://hf.co/optimum/roberta-base-squad2) checkpoint for sequence classification. This checkpoint contains a [model.onnx](https://hf.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/blob/main/onnx/model.onnx) file. If a checkpoint doesn't have a `model.onnx` file, set `export=True` to convert a checkpoint on the fly to the ONNX format.\n\n```py\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\n\nort_model = ORTModelForSequenceClassification.from_pretrained(\n\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n#export=True,\nprovider=\"CUDAExecutionProvider\",\n)\n```\n\nNow you can use the model for inference in a [`Pipeline`].\n\n```py\nfrom optimum.pipelines import pipeline\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\npipeline = pipeline(task=\"text-classification\", model=ort_model, tokenizer=tokenizer, device=\"cuda:0\")\nresult = pipeline(\"Both the music and visual were astounding, not to mention the actors performance.\")\n```",
  "Learn more details about using ORT with Optimum in the [Accelerated inference on NVIDIA GPUs](https://hf.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) and [Accelerated inference on AMD GPUs](https://hf.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus) guides.\n\n### BetterTransformer\n\n[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) is a *fastpath* execution of specialized Transformers functions directly on the hardware level such as a GPU. There are two main components of the fastpath execution.\n\n- fusing multiple operations into a single kernel for faster and more efficient execution\n- skipping unnecessary computation of padding tokens with nested tensors\n\n> [!WARNING]",
  "> Some BetterTransformer features are being upstreamed to Transformers with default support for native [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA). BetterTransformer has a wider coverage than the Transformers SDPA integration, but you can expect more and more architectures to natively support SDPA in Transformers.\n\nBetterTransformer is available through Optimum with [`~PreTrainedModel.to_bettertransformer`].\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\")\nmodel = model.to_bettertransformer()\n```\n\nCall [`~PreTrainedModel.reverse_bettertransformer`] and save it first to return the model to the original Transformers model.\n\n```py\nmodel = model.reverse_bettertransformer()\nmodel.save_pretrained(\"saved_model\")\n```",
  "Refer to the benchmarks in [Out of the box acceleration and memory savings of 🤗 decoder models with PyTorch 2.0](https://pytorch.org/blog/out-of-the-box-acceleration/) for BetterTransformer and scaled dot product attention performance. The [BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) blog post also discusses fastpath execution in greater detail if you're interested in learning more.\n\n## Scaled dot product attention (SDPA)\n\nPyTorch's [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) is a native implementation of the scaled dot product attention mechanism. SDPA is a more efficient and optimized version of the attention mechanism used in transformer models.\n\nThere are three supported implementations available.\n\n- [FlashAttention2](https://github.com/Dao-AILab/flash-attention) only supports models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate type first.",
  "- [xFormers](https://github.com/facebookresearch/xformers) or Memory-Efficient Attention is able to support models with the fp32 torch type.\n- C++ implementation of scaled dot product attention\n\nSDPA is used by default for PyTorch v2.1.1. and greater when an implementation is available. You could explicitly enable SDPA by setting `attn_implementation=\"sdpa\"` in [`~PreTrainedModel.from_pretrained`] though. Certain attention parameters, such as `head_mask` and `output_attentions=True`, are unsupported and returns a warning that Transformers will fall back to the (slower) eager implementation.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", attn_implementation=\"sdpa\")\n```\n\nSDPA selects the most performant implementation available, but you can also explicitly select an implementation with [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) as a context manager. The example below shows how to enable the FlashAttention2 implementation with `enable_flash=True`.\n\n```py\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel",
  "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\").to(\"cuda\")\n\ninput_text = \"Hello, my llama is cute\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION)::\noutputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nIf you encounter the following `RuntimeError`, try installing the nightly version of PyTorch which has broader coverage for FlashAttention.\n\n```bash\nRuntimeError: No available kernel. Aborting execution.\n\npip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n```\n\n## FlashAttention\n\n[FlashAttention](https://github.com/Dao-AILab/flash-attention) is also available as a standalone package. It can significantly speed up inference by:\n\n1. additionally parallelizing the attention computation over sequence length\n2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them",
  "Install FlashAttention first for the hardware you're using.\n\n<hfoptions id=\"install\">\n<hfoption id=\"NVIDIA\">\n\n```bash\npip install flash-attn --no-build-isolation\n```\n\n</hfoption>\n<hfoption id=\"AMD\">\n\nFlashAttention2 support is currently limited to Instinct MI210, Instinct MI250 and Instinct MI300. We strongly suggest running this [Dockerfile](https://github.com/huggingface/optimum-amd/tree/main/docker/transformers-pytorch-amd-gpu-flash/Dockerfile) for FlashAttention2 on AMD GPUs.\n\n</hfoption>\n</hfoptions>\n\nEnable FlashAttention2 by setting `attn_implementation=\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`]. FlashAttention2 is only supported for models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate data type first.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n```\n\n### Benchmarks",
  "FlashAttention2 speeds up inference considerably especially for inputs with long sequences. However, since FlashAttention2 doesn't support computing attention scores with padding tokens, you must manually pad and unpad the attention scores for batched inference if a sequence contains padding tokens. The downside is batched generation is slower with padding tokens.\n\n<hfoptions id=\"padded\">\n<hfoption id=\"short sequence length\">\n\nWith a relatively small sequence length, a single forward pass creates overhead leading to a small speed up. The graph below shows the expected speed up for a single forward pass with [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) with padding.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-small-seqlen-padding.png\"/>\n</div>\n\n</hfoption>\n<hfoption id=\"long sequence length\">",
  "You can train on much longer sequence lengths without running into out-of-memory issues with FlashAttention2, and potentially reduce memory usage up to 20x. The speed up benefits are even better. The graph below shows the expected speed up for a single forward pass with [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) with padding on a longer sequence length.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png\"/>\n</div>\n\n</hfoption>\n</hfoptions>\n\nTo avoid this slowdown, use FlashAttention2 without padding tokens in the sequence during training. Pack the dataset or concatenate sequences until reaching the maximum sequence length.\n\n<hfoptions id=\"not-padded\">\n<hfoption id=\"tiiuae/falcon-7b\">\n\nThe graph below shows the expected speed up for a single forward pass with [tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b) with a sequence length of 4096 and various batch sizes without padding tokens.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png\"/>\n</div>\n\n</hfoption>",
  "<hfoption id=\"meta-llama/Llama-7b-hf\">\n\nThe graph below shows the expected speed up for a single forward pass with [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) with a sequence length of 4096 and various batch sizes without padding tokens.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png\"/>\n</div>\n\n</hfoption>\n</hfoptions>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Optimizers\n\nTransformers offers two native optimizers, AdamW and AdaFactor. It also provides integrations for more specialized optimizers. Install the library that offers the optimizer and drop it in the `optim` parameter in [`TrainingArguments`].\n\nThis guide will show you how to use these optimizers with [`Trainer`] using [`TrainingArguments`] shown below.\n\n```py\nimport torch",
  "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer\n\nargs = TrainingArguments(\noutput_dir=\"./test-optimizer\",\nmax_steps=1000,\nper_device_train_batch_size=4,\nlogging_strategy=\"steps\",\nlogging_steps=1,\nlearning_rate=2e-5,\nsave_strategy=\"no\",\nrun_name=\"optimizer-name\",\n)\n```\n\n## APOLLO\n\n```bash\npip install apollo-torch\n```\n\n[Approximated Gradient Scaling for Memory Efficient LLM Optimization (APOLLO)](https://github.com/zhuhanqing/APOLLO) is a memory-efficient optimizer that allows full parameter learning for both pretraining and fine-tuning. It maintains AdamW-level performance with SGD-like memory efficiency. For extreme memory efficiency, you can use APOLLO-Mini, a rank 1 variant of APOLLO. APOLLO optimizers support:\n\n* Ultra-low rank efficiency. You can use a much lower rank than [GaLoRE](./trainer#galore), rank 1 is sufficient.\n* Avoid expensive SVD computations. APOLLO leverages random projections to avoid training stalls.\n\nUse the `optim_target_modules` parameter to specify which layers to train.\n\n```diff\nimport torch\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\noutput_dir=\"./test-apollo\",\nmax_steps=100,",
  "per_device_train_batch_size=2,\n+   optim=\"apollo_adamw\",\n+   optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\nlogging_strategy=\"steps\",\nlogging_steps=1,\nlearning_rate=2e-5,\nsave_strategy=\"no\",\nrun_name=\"apollo_adamw\",\n)\n```\n\nFor additional training options, use `optim_args` to define hyperparameters like `rank`, `scale`, and more. Refer to the table below for a complete list of available hyperparameters.\n\n> [!TIP]\n> The `scale` parameter can be set to `n/r`, where `n` is the original space dimension and `r` is the low-rank space dimension. You could achieve a similar effect by adjusting the learning rate while keeping `scale` at its default value.\n\n| parameter | description | APOLLO | APOLLO-Mini |\n|---|---|---|---|\n| rank | rank of the auxiliary sub-space for gradient scaling | 256 | 1 |\n| scale_type | how scaling factors are applied | `channel` (per-channel scaling) | `tensor` (per-tensor scaling) |\n| scale | adjusts gradient updates to stabilize training | 1.0 | 128 |\n| update_proj_gap | steps before updating projection matrices | 200 | 200 |\n| proj | projection type | `random` | `random` |\n\nThe example below enables the APOLLO-Mini optimizer.\n\n```py",
  "from transformers import TrainingArguments\n\nargs = TrainingArguments(\noutput_dir=\"./test-apollo_mini\",\nmax_steps=100,\nper_device_train_batch_size=2,\noptim=\"apollo_adamw\",\noptim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\noptim_args=\"proj=random,rank=1,scale=128.0,scale_type=tensor,update_proj_gap=200\",\n)\n```\n\n## GrokAdamW\n\n```bash\npip install grokadamw\n```\n\n[GrokAdamW](https://github.com/cognitivecomputations/grokadamw) is an optimizer designed to help models that benefit from *grokking*, a term used to describe delayed generalization because of slow-varying gradients. It is particularly useful for models requiring more advanced optimization techniques to achieve better performance and stability.\n\n```diff\nimport torch\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\noutput_dir=\"./test-grokadamw\",\nmax_steps=1000,\nper_device_train_batch_size=4,\n+   optim=\"grokadamw\",\nlogging_strategy=\"steps\",\nlogging_steps=1,\nlearning_rate=2e-5,\nsave_strategy=\"no\",\nrun_name=\"grokadamw\",\n)\n```\n\n## LOMO\n\n```bash\npip install lomo-optim\n```",
  "[Low-Memory Optimization (LOMO)](https://github.com/OpenLMLab/LOMO) is a family of optimizers, [LOMO](https://huggingface.co/papers/2306.09782) and [AdaLomo](https://hf.co/papers/2310.10195), designed for low-memory full-parameter finetuning of LLMs. Both LOMO optimizers fuse the gradient computation and parameter update in one step to reduce memory usage. AdaLomo builds on top of LOMO by incorporating an adaptive learning rate for each parameter like the Adam optimizer.\n\n> [!TIP]\n> It is recommended to use AdaLomo without `grad_norm` for better performance and higher throughput.\n\n```diff\nargs = TrainingArguments(\noutput_dir=\"./test-lomo\",\nmax_steps=1000,\nper_device_train_batch_size=4,\n+   optim=\"adalomo\",\ngradient_checkpointing=True,\ngradient_checkpointing=True,\nlogging_strategy=\"steps\",\nlogging_steps=1,\nlearning_rate=2e-6,\nsave_strategy=\"no\",\nrun_name=\"adalomo\",\n)\n```\n\n## Schedule Free\n\n```bash\npip install schedulefree\n```\n\n[Schedule Free optimizer (SFO)](https://hf.co/papers/2405.15682) replaces the base optimizers momentum with a combination of averaging and interpolation. Unlike a traditional scheduler, SFO completely removes the need to anneal the learning rate.",
  "SFO supports the RAdam (`schedule_free_radam`), AdamW (`schedule_free_adamw`) and SGD (`schedule_free_sgd`) optimizers. The RAdam scheduler doesn't require `warmup_steps` or `warmup_ratio`.\n\nBy default, it is recommended to set `lr_scheduler_type=\"constant\"`. Other `lr_scheduler_type` values may also work, but combining SFO optimizers with other learning rate schedules could affect SFOs intended behavior and performance.\n\n```diff\nargs = TrainingArguments(\noutput_dir=\"./test-schedulefree\",\nmax_steps=1000,\nper_device_train_batch_size=4,\n+   optim=\"schedule_free_radamw,\n+   lr_scheduler_type=\"constant\",\ngradient_checkpointing=True,\nlogging_strategy=\"steps\",\nlogging_steps=1,\nlearning_rate=2e-6,\nsave_strategy=\"no\",\nrun_name=\"sfo\",\n)\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Distributed CPUs\n\nCPUs are commonly available and can be a cost-effective training option when GPUs are unavailable. When training large models or if a single CPU is too slow, distributed training with CPUs can help speed up training.",
  "This guide demonstrates how to perform distributed training with multiple CPUs using a [DistributedDataParallel (DDP)](./perf_train_gpu_many#distributeddataparallel) strategy on bare metal with [`Trainer`] and a Kubernetes cluster. All examples shown in this guide depend on the [Intel oneAPI HPC Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit.html).\n\nThere are two toolkits you'll need from Intel oneAPI.\n\n1. [oneCCL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html) includes efficient implementations of collectives commonly used in deep learning such as all-gather, all-reduce, and reduce-scatter. To install from a prebuilt wheel, make sure you always use the latest release. Refer to the table [here](https://github.com/intel/torch-ccl#install-prebuilt-wheel) to check if a version of oneCCL is supported for a Python and PyTorch version.\n\n```bash\n# installs oneCCL for PyTorch 2.4.0\npip install oneccl_bind_pt==2.4.0 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n> [!TIP]\n> Refer to the oneCCL [installation](https://github.com/intel/torch-ccl#installation) for more details.",
  "1. [MPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html) is a message-passing interface for communications between hardware and networks. The oneCCL toolkit is installed along with MPI, but you need to source the environment as shown below before using it.\n\n```bash\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\n```\n\nLastly, install the [Intex Extension for PyTorch (IPEX)](https://intel.github.io/intel-extension-for-pytorch/index.html) which enables additional performance optimizations for Intel hardware such as weight sharing and better thread runtime control.\n\n```bash\npip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n> [!TIP]\n> Refer to the IPEX [installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation) for more details.\n\n## Trainer\n\n[`Trainer`] supports distributed training with CPUs with the oneCCL backend. Add the `--ddp_backend ccl` parameter in the command arguments to enable it.\n\n<hfoptions id=\"distrib-cpu\">\n<hfoption id=\"single node\">",
  "The example below demonstrates the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) script. It enables training with two processes on one Xeon CPU, with one process running per socket.\n\n> [!TIP]\n> Tune the variable `OMP_NUM_THREADS/CCL_WORKER_COUNT` for optimal performance.\n\n```bash\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=127.0.0.1\nmpirun -n 2 -genv OMP_NUM_THREADS=23 \\\npython3 run_qa.py \\\n--model_name_or_path google-bert/bert-large-uncased \\\n--dataset_name squad \\\n--do_train \\\n--do_eval \\\n--per_device_train_batch_size 12  \\\n--learning_rate 3e-5  \\\n--num_train_epochs 2  \\\n--max_seq_length 384 \\\n--doc_stride 128  \\\n--output_dir /tmp/debug_squad/ \\\n--no_cuda \\\n--ddp_backend ccl \\\n--use_ipex\n```\n\n</hfoption>\n<hfoption id=\"multiple nodes\">\n\nScale the training script to four processes on two Xeon CPUs (`node0` and `node1`) by setting `-n 4` and `ppn 2`. The `ppn` parameter specifies the number of processes per node, with one process running per socket.",
  "Assume `node0` is the main process and create a configuration file containing the IP addresses of each node (for example, hostfile) and pass the configuration file path as an argument.\n\n```bash\ncat hostfile\nxxx.xxx.xxx.xxx #node0 ip\nxxx.xxx.xxx.xxx #node1 ip\n```\n\nRun the script below on `node0` to enable DDP on `node0` and `node1` and train with bf16 auto mixed precision.\n\n> [!TIP]\n> Tune the variable `OMP_NUM_THREADS/CCL_WORKER_COUNT` for optimal performance.\n\n```bash\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\nmpirun -f hostfile -n 4 -ppn 2 \\\n-genv OMP_NUM_THREADS=23 \\\npython3 run_qa.py \\\n--model_name_or_path google-bert/bert-large-uncased \\\n--dataset_name squad \\\n--do_train \\\n--do_eval \\\n--per_device_train_batch_size 12  \\\n--learning_rate 3e-5  \\\n--num_train_epochs 2  \\\n--max_seq_length 384 \\\n--doc_stride 128  \\\n--output_dir /tmp/debug_squad/ \\\n--no_cuda \\\n--ddp_backend ccl \\\n--use_ipex \\\n--bf16\n```\n\n</hfoption>\n</hfoptions>\n\n## Kubernetes",
  "Distributed training with CPUs can also be deployed to a Kubernetes cluster with [PyTorchJob](https://www.kubeflow.org/docs/components/training/user-guides/pytorch/). Before you get started, you should perform the following setup steps.\n\n1. Ensure you have access to a Kubernetes cluster with [Kubeflow](https://www.kubeflow.org/docs/started/installing-kubeflow/) installed.\n1. Install and configure [kubectl](https://kubernetes.io/docs/tasks/tools) to interact with the cluster.\n1. Set up a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) to store datasets and model files. There are multiple options to choose from, including a [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/) or a cloud storage bucket.\n1. Set up a Docker container for the training script and all required dependencies such as PyTorch, Transformers, IPEX, oneCCL, and OpenSSH to facilitate communicattion between containers.",
  "The example Dockerfile below uses a base image that supports distributed training with CPUs, and extracts Transformers to the `/workspace` directory to include the training scripts in the image. The image needs to be built and copied to the clusters nodes or pushed to a container registry prior to deployment.\n\n```dockerfile\nFROM intel/intel-optimized-pytorch:2.4.0-pip-multinode\n\nRUN apt-get update -y && \\\napt-get install -y --no-install-recommends --fix-missing \\\ngoogle-perftools \\\nlibomp-dev\n\nWORKDIR /workspace\n\n# Download and extract the transformers code\nARG HF_TRANSFORMERS_VER=\"4.46.0\"\nRUN pip install --no-cache-dir \\\ntransformers==${HF_TRANSFORMERS_VER} && \\\nmkdir transformers && \\\ncurl -sSL --retry 5 https://github.com/huggingface/transformers/archive/refs/tags/v${HF_TRANSFORMERS_VER}.tar.gz | tar -C transformers --strip-components=1 -xzf -\n```\n\n### PyTorchJob",
  "[PyTorchJob](https://www.kubeflow.org/docs/components/training/user-guides/pytorch/) is an extension of the Kubernetes API for running PyTorch training jobs on Kubernetes. It includes a yaml file that defines the training jobs parameters such as the name of the PyTorchJob, number of workers, types of resources for each worker, and more.\n\nThe volume mount parameter is a path to where the PVC is mounted in the container for each worker pod. The PVC is typically used to hold the dataset, checkpoint files, and the model after it has finished training.\n\nThe example yaml file below sets up four workers on the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) script. Adapt the yaml file based on your training script and number of nodes in your cluster.",
  "The CPU resource limits and requests are defined in [CPU units](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu). One CPU unit is equivalent to one physical CPU core or virtual core. The CPU units defined in the yaml file should be less than the amount of available CPU and memory capacity of a single machine in order to leave some resources for kubelet and the system. For a `Guaranteed` [quality of service](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod), set the same CPU and memory amounts for both the resource limits and requests.\n\n```yaml\napiVersion: \"kubeflow.org/v1\"\nkind: PyTorchJob\nmetadata:\nname: transformers-pytorchjob\nspec:\nelasticPolicy:\nrdzvBackend: c10d\nminReplicas: 1\nmaxReplicas: 4\nmaxRestarts: 10\npytorchReplicaSpecs:\nWorker:\nreplicas: 4  # The number of worker pods\nrestartPolicy: OnFailure\ntemplate:\nspec:\ncontainers:\n- name: pytorch\nimage: <image name>:<tag>  # Specify the docker image to use for the worker pods\nimagePullPolicy: IfNotPresent\ncommand: [\"/bin/bash\", \"-c\"]\nargs:\n- >-\ncd /workspace/transformers;",
  "pip install -r /workspace/transformers/examples/pytorch/question-answering/requirements.txt;\nsource /usr/local/lib/python3.10/dist-packages/oneccl_bindings_for_pytorch/env/setvars.sh;\ntorchrun /workspace/transformers/examples/pytorch/question-answering/run_qa.py \\\n--model_name_or_path distilbert/distilbert-base-uncased \\\n--dataset_name squad \\\n--do_train \\\n--do_eval \\\n--per_device_train_batch_size 12 \\\n--learning_rate 3e-5 \\\n--num_train_epochs 2 \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/pvc-mount/output_$(date +%Y%m%d_%H%M%S) \\\n--no_cuda \\\n--ddp_backend ccl \\\n--bf16 \\\n--use_ipex;\nenv:\n- name: LD_PRELOAD\nvalue: \"/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9:/usr/local/lib/libiomp5.so\"\n- name: TRANSFORMERS_CACHE\nvalue: \"/tmp/pvc-mount/transformers_cache\"\n- name: HF_DATASETS_CACHE\nvalue: \"/tmp/pvc-mount/hf_datasets_cache\"\n- name: LOGLEVEL\nvalue: \"INFO\"\n- name: CCL_WORKER_COUNT\nvalue: \"1\"\n- name: OMP_NUM_THREADS  # Can be tuned for optimal performance\nvalue: \"240\"\nresources:\nlimits:\ncpu: 240  # Update the CPU and memory limit values based on your nodes\nmemory: 128Gi\nrequests:",
  "cpu: 240  # Update the CPU and memory request values based on your nodes\nmemory: 128Gi\nvolumeMounts:\n- name: pvc-volume\nmountPath: /tmp/pvc-mount\n- mountPath: /dev/shm\nname: dshm\nrestartPolicy: Never\nnodeSelector:  # Optionally use nodeSelector to match a certain node label for the worker pods\nnode-type: gnr\nvolumes:\n- name: pvc-volume\npersistentVolumeClaim:\nclaimName: transformers-pvc\n- name: dshm\nemptyDir:\nmedium: Memory\n```\n\n### Deploy\n\nAfter you've setup the PyTorchJob yaml file with the appropriate settings for your cluster and training job, deploy it to the cluster with the command below.\n\n```bash\nexport NAMESPACE=<specify your namespace>\n\nkubectl create -f pytorchjob.yaml -n ${NAMESPACE}\n```\n\nList the pods in the namespace with `kubectl get pods -n ${NAMESPACE}`. At first, the status may be \"Pending\" but it should change to \"Running\" once the containers are pulled and created.\n\n```bash\nkubectl get pods -n ${NAMESPACE}\n\nNAME                                                     READY   STATUS                  RESTARTS          AGE\n...\ntransformers-pytorchjob-worker-0                         1/1     Running                 0                 7m37s",
  "transformers-pytorchjob-worker-1                         1/1     Running                 0                 7m37s\ntransformers-pytorchjob-worker-2                         1/1     Running                 0                 7m37s\ntransformers-pytorchjob-worker-3                         1/1     Running                 0                 7m37s\n...\n```\n\nInspect the logs for each worker with the following command. Add `-f` to stream the logs.\n\n```bash\nkubectl logs transformers-pytorchjob-worker-0 -n ${NAMESPACE} -f\n```\n\nOnce training is complete, the trained model can be copied from the PVC or storage location. Delete the PyTorchJob resource from the cluster with the command below.\n\n```bash\nkubectl delete -f pytorchjob.yaml -n ${NAMESPACE}\n```",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Padding and truncation",
  "Batched inputs are often different lengths, so they can't be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a special **padding token** to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences.\n\nIn most cases, padding your batch to the length of the longest sequence and truncating to the maximum length a model can accept works pretty well. However, the API supports more strategies if you need them. The three arguments you need to know are: `padding`, `truncation` and `max_length`.\n\nThe `padding` argument controls padding. It can be a boolean or a string:\n\n- `True` or `'longest'`: pad to the longest sequence in the batch (no padding is applied if you only provide\na single sequence).\n- `'max_length'`: pad to a length specified by the `max_length` argument or the maximum length accepted",
  "by the model if no `max_length` is provided (`max_length=None`). Padding will still be applied if you only provide a single sequence.\n- `False` or `'do_not_pad'`: no padding is applied. This is the default behavior.\n\nThe `truncation` argument controls truncation. It can be a boolean or a string:\n\n- `True` or `'longest_first'`: truncate to a maximum length specified by the `max_length` argument or\nthe maximum length accepted by the model if no `max_length` is provided (`max_length=None`). This will\ntruncate token by token, removing a token from the longest sequence in the pair until the proper length is\nreached.\n- `'only_second'`: truncate to a maximum length specified by the `max_length` argument or the maximum\nlength accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate\nthe second sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.\n- `'only_first'`: truncate to a maximum length specified by the `max_length` argument or the maximum\nlength accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate",
  "the first sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.\n- `False` or `'do_not_truncate'`: no truncation is applied. This is the default behavior.\n\nThe `max_length` argument controls the length of the padding and truncation. It can be an integer or `None`, in which case it will default to the maximum length the model can accept. If the model has no specific maximum input length, truncation or padding to `max_length` is deactivated.\n\nThe following table summarizes the recommended way to setup padding and truncation. If you use pairs of input sequences in any of the following examples, you can replace `truncation=True` by a `STRATEGY` selected in\n`['only_first', 'only_second', 'longest_first']`, i.e. `truncation='only_second'` or `truncation='longest_first'` to control how both sequences in the pair are truncated as detailed before.\n\n| Truncation                           | Padding                           | Instruction                                                                                 |",
  "|--------------------------------------|-----------------------------------|---------------------------------------------------------------------------------------------|\n| no truncation                        | no padding                        | `tokenizer(batch_sentences)`                                                           |\n|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True)` or                                          |\n|                                      |                                   | `tokenizer(batch_sentences, padding='longest')`                                        |\n|                                      | padding to max model input length | `tokenizer(batch_sentences, padding='max_length')`                                     |\n|                                      | padding to specific length        | `tokenizer(batch_sentences, padding='max_length', max_length=42)`                      |\n|                                      | padding to a multiple of a value  | `tokenizer(batch_sentences, padding=True, pad_to_multiple_of=8)`                        |",
  "| truncation to max model input length | no padding                        | `tokenizer(batch_sentences, truncation=True)` or                                       |\n|                                      |                                   | `tokenizer(batch_sentences, truncation=STRATEGY)`                                      |\n|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True, truncation=True)` or                         |\n|                                      |                                   | `tokenizer(batch_sentences, padding=True, truncation=STRATEGY)`                        |\n|                                      | padding to max model input length | `tokenizer(batch_sentences, padding='max_length', truncation=True)` or                 |\n|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)`                |\n|                                      | padding to specific length        | Not possible                                                                                |",
  "| truncation to specific length        | no padding                        | `tokenizer(batch_sentences, truncation=True, max_length=42)` or                        |\n|                                      |                                   | `tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)`                       |\n|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)` or          |\n|                                      |                                   | `tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)`         |\n|                                      | padding to max model input length | Not possible                                                                                |\n|                                      | padding to specific length        | `tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)` or  |\n|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)` |",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# KV cache strategies\n\nThe key-value (KV) vectors are used to calculate attention scores. For autoregressive models, KV scores are calculated *every* time because the model predicts one token at a time. Each prediction depends on the previous tokens, which means the model performs the same computations each time.",
  "A KV *cache* stores these calculations so they can be reused without recomputing them. Efficient caching is crucial for optimizing model performance because it reduces computation time and improves response rates. Refer to the [Caching](./cache_explanation.md) doc for a more detailed explanation about how a cache works.\n\nTransformers offers several [`Cache`] classes that implement different caching mechanisms. Some of these [`Cache`] classes are optimized to save memory while others are designed to maximize generation speed. Refer to the table below to compare cache types and use it to help you select the best cache for your use case.\n\n| Cache Type             | Memory Efficient  | Supports torch.compile() | Initialization Recommended | Latency | Long Context Generation |\n|------------------------|------------------|--------------------------|----------------------------|---------|-------------------------|\n| Dynamic Cache          | No               | No                       | No                         | Mid     | No                      |\n| Static Cache           | No               | Yes                      | Yes                        | High    | No                      |",
  "| Offloaded Cache         | Yes              | No                       | No                         | Low     | Yes                     |\n| Offloaded Static Cache  | No               | Yes                      | Yes                        | High    | Yes                     |\n| Quantized Cache        | Yes              | No                       | No                         | Low     | Yes                     |\n| Sliding Window Cache   | No               | Yes                      | Yes                        | High    | No                      |\n| Sink Cache             | Yes              | No                       | Yes                        | Mid     | Yes                     |\n\nThis guide introduces you to the different [`Cache`] classes and shows you how to use them for generation.\n\n## Default cache\n\nThe [`DynamicCache`] is the default cache class for most models. It allows the cache size to grow dynamically in order to store an increasing number of keys and values as generation progresses.\n\nDisable the cache by configuring `use_cache=False` in [`~GenerationMixin.generate`].\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM",
  "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\ninputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n\nmodel.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)\n```\n\nCache classes can also be initialized first before calling and passing it to the models [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) parameter. This cache initialization strategy is only recommended for some cache types.\n\nIn most other cases, it's easier to define the cache strategy in the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")",
  "inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n\npast_key_values = DynamicCache()\nout = model.generate(**inputs, do_sample=False, max_new_tokens=20, past_key_values=past_key_values)\n```\n\n## Memory efficient caches\n\nThe KV cache can occupy a significant portion of memory and become a [bottleneck](https://hf.co/blog/llama31#inference-memory-requirements) for long-context generation. Memory efficient caches focus on trading off speed for reduced memory usage. This is especially important for large language models (LLMs) and if your hardware is memory constrained.\n\n### Offloaded cache\n\nThe [`OffloadedCache`] saves GPU memory by moving the KV cache for most model layers to the CPU. Only the current layer cache is maintained on the GPU during a models `forward` iteration over the layers. [`OffloadedCache`] asynchronously prefetches the next layer cache and sends the previous layer cache back to the CPU.\n\nThis cache strategy always generates the same result as [`DynamicCache`] and works as a drop-in replacement or fallback. You may want to use [`OffloadedCache`] if you have a GPU and you're getting out-of-memory (OOM) errors.\n\n> [!WARNING]",
  "> You may notice a small degradation in generation throughput compared to [`DynamicCache`] depending on your model and generation choices (context size, number of generated tokens, number of beams, etc.).\n\nEnable [`OffloadedCache`] by configuring `cache_implementation=\"offloaded\"` in either [`GenerationConfig`] or [`~GenerationMixin.generate`].\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nckpt = \"microsoft/Phi-3-mini-4k-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\ninputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n\nout = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\nprint(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\nFun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.\n```\n\nThe example below shows how you can fallback on [`OffloadedCache`] if you run out of memory.\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef resilient_generate(model, *args, **kwargs):\noom = False\ntry:",
  "return model.generate(*args, **kwargs)\nexcept torch.cuda.OutOfMemoryError as e:\nprint(e)\nprint(\"retrying with cache_implementation='offloaded'\")\noom = True\nif oom:\ntorch.cuda.empty_cache()\nkwargs[\"cache_implementation\"] = \"offloaded\"\nreturn model.generate(*args, **kwargs)\n\nckpt = \"microsoft/Phi-3-mini-4k-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\nprompt = [\"okay \"*1000 + \"Fun fact: The most\"]\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nbeams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }\nout = resilient_generate(model, **inputs, **beams)\nresponses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)\n```\n\n### Quantized cache\n\nThe [`QuantizedCache`] reduces memory requirements by quantizing the KV values to a lower precision. [`QuantizedCache`] currently supports two quantization backends.\n\n- [`HQQQuantizedCache`] supports int2, int4, and int8 datatypes.",
  "- [`QuantoQuantizedCache`] supports int2 and int4 datatypes. This is the default quantization backend.\n\n> [!WARNING]\n> Quantizing the cache can harm latency if the context length is short and there is enough GPU memory available for generation without enabling cache quantization. Try to find a balance between memory efficiency and latency.\n\nEnable [`QuantizedCache`] by configuring `cache_implementation=\"quantized\"` in [`GenerationConfig`], and indicate the quantization backend in [`QuantizedCacheConfig`]. Any additional quantization related parameters should also be passed either as a dict or an instance of [`QuantizedCacheConfig`]. You should use the default values for these additional parameters unless you're running out-of-memory. In that case, consider decreasing the residual length.\n\n<hfoptions id=\"quantized-cache\">\n<hfoption id=\"HQQQuantizedCache\">\n\nFor [`HQQQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `1`.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache, QuantizedCacheConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")",
  "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\ninputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n\nout = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"axis-key\": 1, \"axis-value\": 1, \"backend\": \"hqq\"})\nprint(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\nI like rock music because it's loud and energetic. It's a great way to express myself and rel\n```\n\n</hfoption>\n<hfoption id=\"Quanto\">\n\nFor [`QuantoQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `0`.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache, QuantizedCacheConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\ninputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)",
  "out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"axis-key\": 0, \"axis-value\": 0, \"backend\": \"quanto\"})\nprint(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\nI like rock music because it's loud and energetic. It's a great way to express myself and rel\n```\n\n</hfoption>\n</hfoptions>\n\n### Sink cache\n\n[`SinkCache`] is capable of generating very long sequences (\"infinite length\" according to the paper) by only retaining a few initial tokens from the sequence. These are called the *sink tokens* because they account for a significant portion of the attention scores during generation. Subsequent tokens are discarded on a sliding windowed basis, and only the latest `window_size` tokens are kept. This means most of the previous knowledge is discarded.\n\nThe sink tokens allow a model to maintain stable performance even when it's dealing with very long text sequences.",
  "Enable [`SinkCache`] by initializing it first with the [window_length](https://hf.co/docs/transformers/main/en/internal/generation_utils#transformers.SinkCache.window_length) and [num_sink_tokens](https://hf.co/docs/transformers/main/en/internal/generation_utils#transformers.SinkCache.num_sink_tokens) parameters before passing it to [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) in [`~GenerationMixin.generate`].\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\ninputs = tokenizer(\"This is a long story about unicorns, fairies and magic.\", return_tensors=\"pt\").to(model.device)\n\npast_key_values = SinkCache(window_length=256, num_sink_tokens=4)\nout = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)\ntokenizer.batch_decode(out, skip_special_tokens=True)[0]",
  "\"This is a long story about unicorns, fairies and magic. It is a fantasy world where unicorns and fairies live together in harmony. The story follows a young girl named Lily\"\n```\n\n## Speed optimized caches\n\nThe default [`DynamicCache`] prevents you from taking advantage of just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation.\n\n### Static cache\n\nA [`StaticCache`] pre-allocates a specific maximum cache size for the kv pairs. You can generate up to the maximum cache size without needing to modify it.\n\nEnable [`StaticCache`] by configuring `cache_implementation=\"static\"` in [`~GenerationMixin.generate`].\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")",
  "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n\nout = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\ntokenizer.batch_decode(out, skip_special_tokens=True)[0]\n\"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n```\n\n### Offloaded static cache\n\nThe [`OffloadedStaticCache`] is very similar to the [OffloadedCache](#offloaded-cache) except the cache size is set to a maximum cache size. Otherwise, [`OffloadedStaticCache`] only keeps the current layer cache on the GPU and the rest are moved to the CPU.\n\nEnable [`OffloadedStaticCache`] by configuring `cache_implementation=\"offloaded_static\"` in [`~GenerationMixin.generate`].\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\ninputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)",
  "out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\ntokenizer.batch_decode(out, skip_special_tokens=True)[0]\n\"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n```\nCache offloading requires a CUDA GPU.\n\n### Sliding window cache\n\n[`SlidingWindowCache`] implements a sliding window over the previous kv pairs, and only keeps the last `sliding_window` tokens. This cache type is designed to only work with models that support *sliding window attention*, such as [Mistral](./model_doc/mistral). Older kv states are discarded and replaced by new kv states.\n\nEnable [`SlidingWindowCache`] by configuring `cache_implementation=\"sliding_window\"` in [`~GenerationMixin.generate`].\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16).to(\"cuda:0\")\ninputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)",
  "out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\ntokenizer.batch_decode(out, skip_special_tokens=True)[0]\n```\n\n## Model caches\n\nSome model types, like encoder-decoder models or [Gemma2](./model_doc/gemma2) and [Mamba](./model_doc/mamba), have dedicated cache classes.\n\n### Encoder-decoder cache\n\n[`EncoderDecoderCache`] is designed for encoder-decoder models. It manages both the self-attention and cross-attention caches to ensure storage and retrieval of previous kv pairs. It is possible to individually set a different cache type for the encoder and decoder.\n\nThis cache type doesn't require any setup. It can be used when calling [`~GenerationMixin.generate`] or a models `forward` method.\n\n> [!TIP]\n> The [`EncoderDecoderCache`] currently only supports [Whisper](./model_doc/whisper).\n\n### Model-specific caches\n\nSome models have a unique way of storing past kv pairs or states that is not compatible with any other cache classes.\n\n[Gemma2](./model_doc/gemma2) requires [`HybridCache`], which uses a combination of [`SlidingWindowCache`] for sliding window attention and [`StaticCache`] for global attention under the hood.",
  "[Mamba](./model_doc/mamba) requires [`MambaCache`] because the model doesn't have an attention mechanism or kv states.\n\n## Iterative generation\n\nA cache can also work in iterative generation settings where there is back-and-forth interaction with a model (chatbots). Like regular generation, iterative generation with a cache allows a model to efficiently handle ongoing conversations without recomputing the entire context at each step.\n\nFor iterative generation with a cache, start by initializing an empty cache class and then you can feed in your new prompts. Keep track of dialogue history with a [chat template](./chat_templating).\n\nIf you're using [`SinkCache`], the inputs need to be truncated to the maximum length because [`SinkCache`] can generate text that exceeds its maximum window size. However, the first input shouldn't exceed the maximum cache length.\n\nThe example below demonstrates how to use a cache for iterative generation.\n\n```py\nimport torch\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\nfrom transformers.cache_utils import (\nDynamicCache,\nSinkCache,\nStaticCache,\nSlidingWindowCache,\nQuantoQuantizedCache,\nQuantizedCacheConfig,\n)",
  "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nuser_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n\npast_key_values = DynamicCache()\nmax_cache_length = past_key_values.get_max_length()\n\nmessages = []\nfor prompt in user_prompts:\nmessages.append({\"role\": \"user\", \"content\": prompt})\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\nif isinstance(past_key_values, SinkCache):\ninputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\ninput_length = inputs[\"input_ids\"].shape[1]\noutputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\ncompletion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\nmessages.append({\"role\": \"assistant\", \"content\": completion})\n```\n\n## Prefill a cache\n\nIn some situations, you may want to fill a [`Cache`] with kv pairs for a certain prefix prompt and reuse it to generate different sequences.",
  "The example below initializes a [`StaticCache`], and then caches an initial prompt. Now you can generate several sequences from the prefilled prompt.\n\n```py\nimport copy\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Init StaticCache with big enough max-length (1024 tokens for the below example)\n# You can also init a DynamicCache, if that suits you better\nprompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n\nINITIAL_PROMPT = \"You are a helpful assistant. \"\ninputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n# This is the common prompt cached, we need to run forward without grad to be able to copy\nwith torch.no_grad():\nprompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n\nprompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\nresponses = []",
  "for prompt in prompts:\nnew_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\npast_key_values = copy.deepcopy(prompt_cache)\noutputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\nresponse = tokenizer.batch_decode(outputs)[0]\nresponses.append(response)\n\nprint(responses)\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Accelerate",
  "[Accelerate](https://hf.co/docs/accelerate/index) is a library designed to simplify distributed training on any type of setup with PyTorch by uniting the most common frameworks ([Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/)) for it into a single interface. [`Trainer`] is powered by Accelerate under the hood, enabling loading big models and distributed training.\n\nThis guide will show you two ways to use Accelerate with Transformers, using FSDP as the backend. The first method demonstrates distributed training with [`Trainer`], and the second method demonstrates adapting a PyTorch training loop. For more detailed information about Accelerate, please refer to the [documentation](https://hf.co/docs/accelerate/index).\n\n```bash\npip install accelerate\n```\n\nStart by running [accelerate config](https://hf.co/docs/accelerate/main/en/package_reference/cli#accelerate-config) in the command line to answer a series of prompts about your training system. This creates and saves a configuration file to help Accelerate correctly set up training based on your setup.\n\n```bash",
  "accelerate config\n```\n\nDepending on your setup and the answers you provide, an example configuration file for distributing training with FSDP on one machine with two GPUs may look like the following.\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\nfsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\nfsdp_backward_prefetch_policy: BACKWARD_PRE\nfsdp_forward_prefetch: false\nfsdp_cpu_ram_efficient_loading: true\nfsdp_offload_params: false\nfsdp_sharding_strategy: FULL_SHARD\nfsdp_state_dict_type: SHARDED_STATE_DICT\nfsdp_sync_module_states: true\nfsdp_transformer_layer_cls_to_wrap: BertLayer\nfsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n## Trainer\n\nPass the path to the saved configuration file to [`TrainingArguments`], and from there, pass your [`TrainingArguments`] to [`Trainer`].\n\n```py\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\noutput_dir=\"your-model\",\nlearning_rate=2e-5,",
  "per_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=2,\nfsdp_config=\"path/to/fsdp_config\",\nfsdp_strategy=\"full_shard\",\nweight_decay=0.01,\neval_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nload_best_model_at_end=True,\npush_to_hub=True,\n)\n\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=dataset[\"train\"],\neval_dataset=dataset[\"test\"],\nprocessing_class=tokenizer,\ndata_collator=data_collator,\ncompute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```\n\n## Native PyTorch\n\nAccelerate can also be added to any PyTorch training loop to enable distributed training. The [`~accelerate.Accelerator`] is the main entry point for adapting your PyTorch code to work with Accelerate. It automatically detects your distributed training setup and initializes all the necessary components for training. You don't need to explicitly place your model on a device because [`~accelerate.Accelerator`] knows which device to move your model to.\n\n```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\ndevice = accelerator.device\n```",
  "All PyTorch objects (model, optimizer, scheduler, dataloaders) should be passed to the [`~accelerate.Accelerator.prepare`] method now. This method moves your model to the appropriate device or devices, adapts the optimizer and scheduler to use [`~accelerate.optimizer.AcceleratedOptimizer`] and [`~accelerate.scheduler.AcceleratedScheduler`], and creates a new shardable dataloader.\n\n```py\ntrain_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\ntrain_dataloader, eval_dataloader, model, optimizer\n)\n```\n\nReplace `loss.backward` in your training loop with Accelerates [`~accelerate.Accelerator.backward`] method to scale the gradients and determine the appropriate `backward` method to use depending on your framework (for example, DeepSpeed or Megatron).\n\n```py\nfor epoch in range(num_epochs):\nfor batch in train_dataloader:\noutputs = model(**batch)\nloss = outputs.loss\naccelerator.backward(loss)\noptimizer.step()\nlr_scheduler.step()\noptimizer.zero_grad()\nprogress_bar.update(1)\n```\n\nCombine everything into a function and make it callable as a script.\n\n```py\nfrom accelerate import Accelerator\n\ndef main():\naccelerator = Accelerator()",
  "model, optimizer, training_dataloader, scheduler = accelerator.prepare(\nmodel, optimizer, training_dataloader, scheduler\n)\n\nfor batch in training_dataloader:\noptimizer.zero_grad()\ninputs, targets = batch\noutputs = model(inputs)\nloss = loss_function(outputs, targets)\naccelerator.backward(loss)\noptimizer.step()\nscheduler.step()\n\nif __name__ == \"__main__\":\nmain()\n```\n\nFrom the command line, call [accelerate launch](https://hf.co/docs/accelerate/main/en/package_reference/cli#accelerate-launch) to run your training script. Any additional arguments or parameters can be passed here as well.\n\nTo launch your training script on two GPUs, add the `--num_processes` argument.\n\n```bash\naccelerate launch --num_processes=2 your_script.py\n```\n\nRefer to the [Launching Accelerate scripts](https://hf.co/docs/accelerate/main/en/basic_tutorials/launch) for more details.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Caching\n\nImagine you’re having a conversation with someone, and instead of remembering what they previously said, they have to start from scratch every time you respond. This would be slow and inefficient, right?\n\nYou can extend this analogy to transformer models. Autoregressive model generation can be slow because it makes a prediction one token at a time. Each new prediction is dependent on all the previous context.",
  "To predict the 1000th token, the model requires information from the previous 999 tokens. The information is represented as matrix multiplications across the token representations.\n\nTo predict the 1001th token, you need the same information from the previous 999 tokens in addition to any information from the 1000th token. This is a lot of matrix multiplications a model has to compute over and over for each token!\n\nA key-value (KV) cache eliminates this inefficiency by storing kv pairs derived from the attention layers of previously processed tokens. The stored kv pairs are retrieved from the cache and reused for subsequent tokens, avoiding the need to recompute.\n\n> [!WARNING]\n> Caching should only be used for **inference**. It may cause unexpected errors if it's enabled during training.\n\n## Cache class\n\nWhen you use Transformers' [`Cache`] class, the self-attention module performs several critical steps to integrate past and present information.",
  "1. The attention module concatenates current kv pairs with past kv pairs stored in the cache. This creates attentions weights with the shape `(new_tokens_length, past_kv_length + new_tokens_length)`. The current and past kv pairs are essentially combined to compute the attention scores, ensuring a model is aware of previous context and the current input.\n\n2. When the `forward` method is called iteratively, it's crucial that the attention mask shape matches the combined length of the past and current kv pairs. The attention mask should have the shape `(batch_size, past_kv_length + new_tokens_length)`. This is typically handled internally in [`~GenerationMixin.generate`], but if you want to implement your own generation loop with [`Cache`], keep this in mind! The attention mask should hold the past and current token values.",
  "3. It is also important to be aware of the `cache_position`. This is important if you want to reuse a prefilled [`Cache`] with the `forward` method because you have to pass a valid `cache_position` value. This indicates the input positions in a sequence. `cache_position` is unaffected by padding, and it always adds one more position for each token. For example, if a kv cache contains 10 tokens - regardless of pad tokens - the cache position for the next token should be `torch.tensor([10])`.\n\nThe example below demonstrates how to create a generation loop with [`DynamicCache`]. As discussed, the attention mask is a concatenation of past and current token values and `1` is added to the cache position for the next token.\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\npast_key_values = DynamicCache()\nmessages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]",
  "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n\ngenerated_ids = inputs.input_ids\ncache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\nmax_new_tokens = 10\n\nfor _ in range(max_new_tokens):\noutputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n# Greedily sample one next token\nnext_token_ids = outputs.logits[:, -1:].argmax(-1)\ngenerated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n# Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token\n# and expanding attn mask for the new token, as explained above\nattention_mask = inputs[\"attention_mask\"]\nattention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\ninputs = {\"input_ids\": next_token_ids, \"attention_mask\": attention_mask}\ncache_position = cache_position[-1:] + 1 # add one more position for the next token\n\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n\"[INST] Hello, what's your name. [/INST]  Hello! My name is LLaMA,\"",
  "```\n\n## Legacy cache format\n\nBefore the [`Cache`] class, the cache used to be stored as a tuple of tuples of tensors. This format has is dynamic because it grows as text is generated, similar to [`DynamicCache`].\n\nIf your project depends on this legacy format, you can convert between [`DynamicCache`] and a tuple of tuples as shown below with the [`~DynamicCache.from_legacy_cache`] and [`DynamicCache.to_legacy_cache`] functions. This is helpful if you have custom logic for manipulating a cache in a specific format.\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\ninputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n\n# `return_dict_in_generate=True` is required to return the cache and `return_legacy_cache` forces the returned cache\n# in the the legacy format\ngeneration_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)",
  "cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\nlegacy_format_cache = cache.to_legacy_cache()\n```",
  "<!--⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Community\n\nThis page regroups resources around 🤗 Transformers developed by the community.\n\n## Community resources:\n\n| Resource     |      Description      |      Author      |\n|:----------|:-------------|------:|\n| [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | A set of flashcards based on the [Transformers Docs Glossary](glossary) that has been put into a form which can be easily learned/revised using [Anki](https://apps.ankiweb.net/) an open source, cross platform app specifically designed for long term knowledge retention. See this [Introductory video on how to use the flashcards](https://www.youtube.com/watch?v=Dji_h7PILrw). | [Darigov Research](https://www.darigovresearch.com/) |\n\n## Community notebooks:\n\n| Notebook     |      Description      |      Author      |      |\n|:----------|:-------------|:-------------|------:|",
  "| [Fine-tune a pre-trained Transformer to generate lyrics](https://github.com/AlekseyKorshuk/huggingartists) | How to generate lyrics in the style of your favorite artist by fine-tuning a GPT-2 model |  [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |\n| [Train T5 in Tensorflow 2](https://github.com/snapthat/TF-T5-text-to-text) | How to train T5 for any task using Tensorflow 2. This notebook demonstrates a Question & Answer task implemented in Tensorflow 2 using SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |",
  "| [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)  | How to train T5 on SQUAD with Transformers and Nlp | [Suraj Patil](https://github.com/patil-suraj) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |\n| [Fine-tune T5 for Classification and Multiple Choice](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)  | How to fine-tune T5 for classification and multiple choice tasks using a text-to-text format with PyTorch Lightning |  [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |",
  "| [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)  | How to fine-tune the DialoGPT model on a new dataset for open-dialog conversational chatbots |  [Nathan Cooper](https://github.com/ncoop57) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |\n| [Long Sequence Modeling with Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  | How to train on sequences as long as 500,000 tokens with Reformer |  [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  |",
  "| [Fine-tune BART for Summarization](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | How to fine-tune BART for summarization with fastai using blurr | [Wayde Gilliam](https://ohmeow.com/) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |\n| [Fine-tune a pre-trained Transformer on anyone's tweets](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | How to generate tweets in the style of your favorite Twitter account by fine-tuning a GPT-2 model |  [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |",
  "| [Optimize 🤗 Hugging Face models with Weights & Biases](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | A complete tutorial showcasing W&B integration with Hugging Face | [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |\n| [Pretrain Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)  | How to build a \"long\" version of existing pretrained models |  [Iz Beltagy](https://beltagy.net) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |",
  "| [Fine-tune Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | How to fine-tune longformer model for QA task | [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |\n| [Evaluate Model with 🤗nlp](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | How to evaluate longformer on TriviaQA with `nlp` | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |",
  "| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)  | How to fine-tune T5 for sentiment span extraction using a text-to-text format with PyTorch Lightning |  [Lorenzo Ampil](https://github.com/enzoampil) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |\n| [Fine-tune DistilBert for Multiclass Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | How to fine-tune DistilBert for multiclass classification with PyTorch | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)|",
  "|[Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|How to fine-tune BERT for multi-label classification using PyTorch|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|\n|[Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|How to fine-tune T5 for summarization in PyTorch and track experiments with WandB|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|",
  "|[Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)|How to speed up fine-tuning by a factor of 2 using dynamic padding / bucketing|[Michael Benesty](https://github.com/pommedeterresautee) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)|\n|[Pretrain Reformer for Masked Language Modeling](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)| How to train a Reformer model with bi-directional self-attention layers | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)|",
  "|[Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)| How to increase vocabulary of a pretrained SciBERT model from AllenAI on the CORD dataset and pipeline it. | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)|\n|[Fine Tune BlenderBotSmall for Summarization using the Trainer API](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)| How to fine-tune BlenderBotSmall for summarization on a custom dataset, using the Trainer API. | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)|",
  "|[Fine-tune Electra and interpret with Integrated Gradients](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | How to fine-tune Electra for sentiment analysis and interpret predictions with Captum Integrated Gradients | [Eliza Szczechla](https://elsanns.github.io) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)|\n|[fine-tune a non-English GPT-2 Model with Trainer class](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | How to fine-tune a non-English GPT-2 Model with Trainer class | [Philipp Schmid](https://www.philschmid.de) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)|",
  "|[Fine-tune a DistilBERT Model for Multi Label Classification task](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | How to fine-tune a DistilBERT Model for Multi Label Classification task | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)|\n|[Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair classification task | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)|",
  "|[Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | How to fine-tune a Roberta model for sentiment analysis | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)|\n|[Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev) | How accurate are the answers to questions generated by your seq2seq transformer model? | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)|",
  "|[Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | How to fine-tune DistilBERT for text classification in TensorFlow | [Peter Bayerle](https://github.com/peterbayerle) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)|\n|[Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | How to warm-start a *EncoderDecoderModel* with a *google-bert/bert-base-uncased* checkpoint for summarization on CNN/Dailymail | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)|",
  "|[Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | How to warm-start a shared *EncoderDecoderModel* with a *FacebookAI/roberta-base* checkpoint for summarization on BBC/XSum | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)|\n|[Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | How to fine-tune *TapasForQuestionAnswering* with a *tapas-base* checkpoint on the Sequential Question Answering (SQA) dataset | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)|",
  "|[Evaluate TAPAS on Table Fact Checking (TabFact)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) | How to evaluate a fine-tuned *TapasForSequenceClassification* with a *tapas-base-finetuned-tabfact* checkpoint using a combination of the 🤗 datasets and 🤗 transformers libraries | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)|\n|[Fine-tuning mBART for translation](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) | How to fine-tune mBART using Seq2SeqTrainer for Hindi to English translation | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)|",
  "|[Fine-tune LayoutLM on FUNSD (a form understanding dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) | How to fine-tune *LayoutLMForTokenClassification* on the FUNSD dataset for information extraction from scanned documents | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)|\n|[Fine-Tune DistilGPT2 and Generate Text](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) | How to fine-tune DistilGPT2 and generate text | [Aakash Tripathi](https://github.com/tripathiaakash) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)|",
  "|[Fine-Tune LED on up to 8K tokens](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) | How to fine-tune LED on pubmed for long-range summarization | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)|\n|[Evaluate LED on Arxiv](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) | How to effectively evaluate LED on long-range summarization | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)|",
  "|[Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) | How to fine-tune *LayoutLMForSequenceClassification* on the RVL-CDIP dataset for scanned document classification | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)|\n|[Wav2Vec2 CTC decoding with GPT2 adjustment](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb) | How to decode CTC sequence with language model adjustment | [Eric Lam](https://github.com/voidful) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)|",
  "|[Fine-tune BART for summarization in two languages with Trainer class](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) | How to fine-tune BART for summarization in two languages with Trainer class | [Eliza Szczechla](https://github.com/elsanns) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)|\n|[Evaluate Big Bird on Trivia QA](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | How to evaluate BigBird on long document question answering on Trivia QA | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)|",
  "| [Create video captions using Wav2Vec2](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | How to create YouTube captions from any video by transcribing the audio with Wav2Vec | [Niklas Muennighoff](https://github.com/Muennighoff) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) |\n| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | How to fine-tune the Vision Transformer (ViT) on CIFAR-10 using HuggingFace Transformers, Datasets and PyTorch Lightning | [Niels Rogge](https://github.com/nielsrogge) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) |",
  "| [Fine-tune the Vision Transformer on CIFAR-10 using the 🤗 Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | How to fine-tune the Vision Transformer (ViT) on CIFAR-10 using HuggingFace Transformers, Datasets and the 🤗 Trainer | [Niels Rogge](https://github.com/nielsrogge) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) |\n| [Evaluate LUKE on Open Entity, an entity typing dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | How to evaluate *LukeForEntityClassification* on the Open Entity dataset | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |",
  "| [Evaluate LUKE on TACRED, a relation extraction dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | How to evaluate *LukeForEntityPairClassification* on the TACRED dataset | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |\n| [Evaluate LUKE on CoNLL-2003, an important NER benchmark](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | How to evaluate *LukeForEntitySpanClassification* on the CoNLL-2003 dataset | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |",
  "| [Evaluate BigBird-Pegasus on PubMed dataset](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) | How to evaluate *BigBirdPegasusForConditionalGeneration* on PubMed dataset | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) |\n| [Speech Emotion Classification with Wav2Vec2](https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) | How to leverage a pretrained Wav2Vec2 model for Emotion Classification on the MEGA dataset | [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |",
  "| [Detect objects in an image with DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) | How to use a trained *DetrForObjectDetection* model to detect objects in an image and visualize attention | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) |\n| [Fine-tune DETR on a custom object detection dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) | How to fine-tune *DetrForObjectDetection* on a custom object detection dataset | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) |",
  "| [Finetune T5 for Named Entity Recognition](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb) | How to fine-tune *T5* on a Named Entity Recognition Task | [Ogundepo Odunayo](https://github.com/ToluClassics) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing) |\n| [Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT](https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) | How to use [QLoRA](https://github.com/artidoro/qlora) and [PEFT](https://huggingface.co/docs/peft/en/index) to fine-tune an LLM in a memory-efficient way, while using [MLflow](https://mlflow.org/docs/latest/llms/transformers/index.html) to manage experiment tracking | [Yuki Watanabe](https://github.com/B-Step62) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) |",
  "<!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Troubleshoot\n\nSometimes errors occur, but we are here to help! This guide covers some of the most common issues we've seen and how you can resolve them. However, this guide isn't meant to be a comprehensive collection of every 🤗 Transformers issue. For more help with troubleshooting your issue, try:\n\n<Youtube id=\"S2EEG3JIt2A\"/>",
  "1. Asking for help on the [forums](https://discuss.huggingface.co/). There are specific categories you can post your question to, like [Beginners](https://discuss.huggingface.co/c/beginners/5) or [🤗 Transformers](https://discuss.huggingface.co/c/transformers/9). Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!\n\n<Youtube id=\"_PAli-V4wj0\"/>\n\n2. Create an [Issue](https://github.com/huggingface/transformers/issues/new/choose) on the 🤗 Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out what's wrong and how we can fix it.\n\n3. Check the [Migration](migration) guide if you use an older version of 🤗 Transformers since some important changes have been introduced between versions.\n\nFor more details about troubleshooting and getting help, take a look at [Chapter 8](https://huggingface.co/course/chapter8/1?fw=pt) of the Hugging Face course.\n\n\n## Firewalled environments",
  "Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:\n\n```\nValueError: Connection error, and we cannot find the requested files in the cached path.\nPlease try again or make sure your Internet connection is on.\n```\n\nIn this case, you should try to run 🤗 Transformers on [offline mode](installation#offline-mode) to avoid the connection error.\n\n## CUDA out of memory\n\nTraining large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:\n\n```\nCUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 9.70 GiB already allocated; 179.81 MiB free; 9.85 GiB reserved in total by PyTorch)\n```\n\nHere are some potential solutions you can try to lessen memory use:\n\n- Reduce the [`per_device_train_batch_size`](main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size) value in [`TrainingArguments`].",
  "- Try using [`gradient_accumulation_steps`](main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps) in [`TrainingArguments`] to effectively increase overall batch size.\n\n<Tip>\n\nRefer to the Performance [guide](performance) for more details about memory-saving techniques.\n\n</Tip>\n\n## Unable to load a saved TensorFlow model\n\nTensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because 🤗 Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:\n\n- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n>>> from tensorflow import keras\n\n>>> model.save_weights(\"some_folder/tf_model.h5\")",
  ">>> model = TFPreTrainedModel.from_pretrained(\"some_folder\")\n```\n\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n\n>>> model.save_pretrained(\"path_to/model\")\n>>> model = TFPreTrainedModel.from_pretrained(\"path_to/model\")\n```\n\n## ImportError\n\nAnother common error you may encounter, especially if it is a newly released model, is `ImportError`:\n\n```\nImportError: cannot import name 'ImageGPTImageProcessor' from 'transformers' (unknown location)\n```\n\nFor these error types, check to make sure you have the latest version of 🤗 Transformers installed to access the most recent models:\n\n```bash\npip install transformers --upgrade\n```\n\n## CUDA error: device-side assert triggered\n\nSometimes you may run into a generic CUDA error about an error in the device code.\n\n```\nRuntimeError: CUDA error: device-side assert triggered\n```\n\nYou should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to switch to a CPU:\n\n```py\n>>> import os\n\n>>> os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n```",
  "Another option is to get a better traceback from the GPU. Add the following environment variable to the beginning of your code to get the traceback to point to the source of the error:\n\n```py\n>>> import os\n\n>>> os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n```\n\n## Incorrect output when padding tokens aren't masked\n\nIn some cases, the output `hidden_state` may be incorrect if the `input_ids` include padding tokens. To demonstrate, load a model and tokenizer. You can access a model's `pad_token_id` to see its value. The `pad_token_id` may be `None` for some models, but you can always manually set it.\n\n```py\n>>> from transformers import AutoModelForSequenceClassification\n>>> import torch\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model.config.pad_token_id\n0\n```\n\nThe following example shows the output without masking the padding tokens:\n\n```py\n>>> input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])\n>>> output = model(input_ids)\n>>> print(output.logits)\ntensor([[ 0.0082, -0.2307],\n[ 0.1317, -0.1683]], grad_fn=<AddmmBackward0>)\n```\n\nHere is the actual output of the second sequence:\n\n```py",
  ">>> input_ids = torch.tensor([[7592]])\n>>> output = model(input_ids)\n>>> print(output.logits)\ntensor([[-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)\n```\n\nMost of the time, you should provide an `attention_mask` to your model to ignore the padding tokens to avoid this silent error. Now the output of the second sequence matches its actual output:\n\n<Tip>\n\nBy default, the tokenizer creates an `attention_mask` for you based on your specific tokenizer's defaults.\n\n</Tip>\n\n```py\n>>> attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])\n>>> output = model(input_ids, attention_mask=attention_mask)\n>>> print(output.logits)\ntensor([[ 0.0082, -0.2307],\n[-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)\n```\n\n🤗 Transformers doesn't automatically create an `attention_mask` to mask a padding token if it is provided because:\n\n- Some models don't have a padding token.\n- For some use-cases, users want a model to attend to a padding token.\n\n## ValueError: Unrecognized configuration class XYZ for this kind of AutoModel\n\nGenerally, we recommend using the [`AutoModel`] class to load pretrained instances of models. This class",
  "can automatically infer and load the correct architecture from a given checkpoint based on the configuration. If you see\nthis `ValueError` when loading a model from a checkpoint, this means the Auto class couldn't find a mapping from\nthe configuration in the given checkpoint to the kind of model you are trying to load. Most commonly, this happens when a\ncheckpoint doesn't support a given task.\nFor instance, you'll see this error in the following example because there is no GPT2 for question answering:\n\n```py\n>>> from transformers import AutoProcessor, AutoModelForQuestionAnswering\n\n>>> processor = AutoProcessor.from_pretrained(\"openai-community/gpt2-medium\")\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"openai-community/gpt2-medium\")\nValueError: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ...\n```",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ONNX",
  "[ONNX](http://onnx.ai) is an open standard that defines a common set of operators and a file format to represent deep learning models in different frameworks, including PyTorch and TensorFlow. When a model is exported to ONNX, the operators construct a computational graph (or *intermediate representation*) which represents the flow of data through the model. Standardized operators and data types makes it easy to switch between frameworks.\n\nThe [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to ONNX with configuration objects which are supported for [many architectures]((https://huggingface.co/docs/optimum/exporters/onnx/overview)) and can be easily extended. If a model isn't supported, feel free to make a [contribution](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute) to Optimum.\n\nThe benefits of exporting to ONNX include the following.\n\n- [Graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization) for improving inference.",
  "- Use the [`~optimum.onnxruntime.ORTModel`] API to run a model with [ONNX Runtime](https://onnxruntime.ai/).\n- Use [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines) for ONNX models.\n\nExport a Transformers model to ONNX with the Optimum CLI or the `optimum.onnxruntime` module.\n\n## Optimum CLI\n\nRun the command below to install Optimum and the [exporters](https://huggingface.co/docs/optimum/exporters/overview) module.\n\n```bash\npip install optimum[exporters]\n```\n\n> [!TIP]\n> Refer to the [Export a model to ONNX with optimum.exporters.onnx](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) guide for all available arguments or with the command below.\n> ```bash\n> optimum-cli export onnx --help\n> ```\n\nSet the `--model` argument to export a PyTorch or TensorFlow model from the Hub.\n\n```bash\noptimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/\n```\n\nYou should see logs indicating the progress and showing where the resulting `model.onnx` is saved.\n\n```bash",
  "Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n-[✓] ONNX model output names match reference model (start_logits, end_logits)\n- Validating ONNX Model output \"start_logits\":\n-[✓] (2, 16) matches (2, 16)\n-[✓] all values close (atol: 0.0001)\n- Validating ONNX Model output \"end_logits\":\n-[✓] (2, 16) matches (2, 16)\n-[✓] all values close (atol: 0.0001)\nThe ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx\n```\n\nFor local models, make sure the model weights and tokenizer files are saved in the same directory, for example `local_path`. Pass the directory to the `--model` argument and use `--task` to indicate the [task](https://huggingface.co/docs/optimum/exporters/task_manager) a model can perform. If `--task` isn't provided, the model architecture without a task-specific head is used.\n\n```bash\noptimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/\n```\n\nThe `model.onnx` file can be deployed with any [accelerator](https://onnx.ai/supported-tools.html#deployModel) that supports ONNX. The example below demonstrates loading and running a model with ONNX Runtime.\n\n```python",
  ">>> from transformers import AutoTokenizer\n>>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n>>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n>>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\n\n## optimum.onnxruntime\n\nThe `optimum.onnxruntime` module supports programmatically exporting a Transformers model. Instantiate a [`~optimum.onnxruntime.ORTModel`] for a task and set `export=True`. Use [`~OptimizedModel.save_pretrained`] to save the ONNX model.\n\n```python\n>>> from optimum.onnxruntime import ORTModelForSequenceClassification\n>>> from transformers import AutoTokenizer\n\n>>> model_checkpoint = \"distilbert/distilbert-base-uncased-distilled-squad\"\n>>> save_directory = \"onnx/\"\n\n>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n>>> ort_model.save_pretrained(save_directory)\n>>> tokenizer.save_pretrained(save_directory)\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fine-tuning\n\n[[open-in-colab]]\n\nFine-tuning adapts a pretrained model to a specific task with a smaller specialized dataset. This approach requires far less data and compute compared to training a model from scratch, which makes it a more accessible option for many users.\n\nTransformers provides the [`Trainer`] API, which offers a comprehensive set of training features, for fine-tuning any of the models on the [Hub](https://hf.co/models).",
  "> [!TIP]\n> Learn how to fine-tune models for other tasks in our Task Recipes section in Resources!\n\nThis guide will show you how to fine-tune a model with [`Trainer`] to classify Yelp reviews.\n\nLog in to your Hugging Face account with your user token to ensure you can access gated models and share your models on the Hub.\n\n```py\nfrom huggingface_hub import login\n\nlogin()\n```\n\nStart by loading the [Yelp Reviews](https://hf.co/datasets/yelp_review_full) dataset and [preprocess](./fast_tokenizers#preprocess) (tokenize, pad, and truncate) it for training. Use [`~datasets.Dataset.map`] to preprocess the entire dataset in one step.\n\n```py\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\ndef tokenize(examples):\nreturn tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ndataset = dataset.map(tokenize, batched=True)\n```\n\n> [!TIP]",
  "> Fine-tune on a smaller subset of the full dataset to reduce the time it takes. The results won't be as good compared to fine-tuning on the full dataset, but it is useful to make sure everything works as expected first before committing to training on the full dataset.\n> ```py\n> small_train = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n> small_eval = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n> ```\n\n## Trainer\n\n<Youtube id=\"nvBXf7s7vTI\"/>\n\n[Trainer](./trainer) is an optimized training loop for Transformers models, making it easy to start training right away without manually writing your own training code. Pick and choose from a wide range of training features in [`TrainingArguments`] such as gradient accumulation, mixed precision, and options for reporting and logging training metrics.\n\nLoad a model and provide the number of expected labels (you can find this information on the Yelp Review [dataset card](https://huggingface.co/datasets/yelp_review_full#data-fields)).\n\n```py\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)",
  "\"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\"\n\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n```\n\n> [!TIP]\n> The message above is a reminder that the models pretrained head is discarded and replaced with a randomly initialized classification head. The randomly initialized head needs to be fine-tuned on your specific task to output meanginful predictions.\n\nWith the model loaded, set up your training hyperparameters in [`TrainingArguments`]. Hyperparameters are variables that control the training process - such as the learning rate, batch size, number of epochs - which in turn impacts model performance. Selecting the correct hyperparameters is important and you should experiment with them to find the best configuration for your task.",
  "For this guide, you can use the default hyperparameters which provide a good baseline to begin with. The only settings to configure in this guide are where to save the checkpoint, how to evaluate model performance during training, and pushing the model to the Hub.\n\n[`Trainer`] requires a function to compute and report your metric. For a classification task, you'll use [`evaluate.load`] to load the [accuracy](https://hf.co/spaces/evaluate-metric/accuracy) function from the [Evaluate](https://hf.co/docs/evaluate/index) library. Gather the predictions and labels in [`~evaluate.EvaluationModule.compute`] to calculate the accuracy.\n\n```py\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\nlogits, labels = eval_pred\n# convert the logits to their predicted class\npredictions = np.argmax(logits, axis=-1)\nreturn metric.compute(predictions=predictions, references=labels)\n```\n\nSet up [`TrainingArguments`] with where to save the model and when to compute accuracy during training. The example below sets it to `\"epoch\"`, which reports the accuracy at the end of each epoch. Add `push_to_hub=True` to upload the model to the Hub after training.",
  "```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\noutput_dir=\"yelp_review_classifier\",\neval_strategy=\"epoch\",\npush_to_hub=True,\n)\n```\n\nCreate a [`Trainer`] instance and pass it the model, training arguments, training and test datasets, and evaluation function. Call [`~Trainer.train`] to start training.\n\n```py\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=dataset[\"train\"],\neval_dataset=dataset[\"test\"],\ncompute_metrics=compute_metrics,\n)\ntrainer.train()\n```\n\nFinally, use [`~Trainer.push_to_hub`] to upload your model and tokenizer to the Hub.\n\n```py\ntrainer.push_to_hub()\n```\n\n## TensorFlow\n\n[`Trainer`] is incompatible with Transformers TensorFlow models. Instead, fine-tune these models with [Keras](https://keras.io/) since they're implemented as a standard [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n\n```py\nfrom transformers import TFAutoModelForSequenceClassification\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\ndataset = load_dataset(\"yelp_review_full\")",
  "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\ndef tokenize(examples):\nreturn tokenizer(examples[\"text\"])\n\ndataset = dataset.map(tokenize)\n```\n\nThere are two methods to convert a dataset to [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n\n- [`~TFPreTrainedModel.prepare_tf_dataset`] is the recommended way to create a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) because you can inspect the model to figure out which columns to use as inputs and which columns to discard. This allows you to create a simpler, more performant dataset.\n- [`~datasets.Dataset.to_tf_dataset`] is a more low-level method from the [Datasets](https://hf.co/docs/datasets/index) library that gives you more control over how a dataset is created by specifying the columns and label columns to use.\n\nAdd the tokenizer to [`~TFPreTrainedModel.prepare_tf_dataset`] to pad each batch, and you can optionally shuffle the dataset. For more complicated preprocessing, pass the preprocessing function to the `collate_fn` parameter instead.\n\n```py\ntf_dataset = model.prepare_tf_dataset(",
  "dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer\n)\n```\n\nFinally, [compile](https://keras.io/api/models/model_training_apis/#compile-method) and [fit](https://keras.io/api/models/model_training_apis/#fit-method) the model to start training.\n\n> [!TIP]\n> It isn't necessary to pass a loss argument to [compile](https://keras.io/api/models/model_training_apis/#compile-method) because Transformers automatically chooses a loss that is appropriate for the task and architecture. However, you can always specify a loss argument if you want.\n\n```py\nfrom tensorflow.keras.optimizers import Adam\n\nmodel.compile(optimizer=Adam(3e-5))\nmodel.fit(tf_dataset)\n```\n\n## Resources\n\nRefer to the Transformers [examples](https://github.com/huggingface/transformers/tree/main/examples) for more detailed training scripts on various tasks. You can also check out the [notebooks](./notebooks) for interactive examples.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Chat basics",
  "Chat models are conversational models you can send and receive messages from. There are many chat models available to choose from, but in general, larger models tend to be better though that's not always the case. The model size is often included in the name, like \"8B\" or \"70B\", and it describes the number of parameters. Mixture-of-expert (MoE) models have names like \"8x7B\" or \"141B-A35B\" which means it's a 56B and 141B parameter model. You can try quantizing larger models to reduce memory requirements, otherwise you'll need ~2 bytes of memory per parameter.\n\nCheck model leaderboards like [OpenLLM](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard) and [LMSys Chatbot Arena](https://chat.lmsys.org/?leaderboard) to further help you identify the best chat models for your use case. Models that are specialized in certain domains (medical, legal text, non-English languages, etc.) may sometimes outperform larger general purpose models.\n\n> [!TIP]\n> Chat with a number of open-source models for free on [HuggingChat](https://hf.co/chat/)!",
  "This guide shows you how to quickly start chatting with Transformers from the command line, how build and format a conversation, and how to chat using the [`TextGenerationPipeline`].\n\n## transformers-cli\n\nChat with a model directly from the command line as shown below. It launches an interactive session with a model. Enter `clear` to reset the conversation, `exit` to terminate the session, and `help` to display all the command options.\n\n```bash\ntransformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers-chat-cli.png\"/>\n</div>\n\nFor a full list of options, run the command below.\n\n```bash\ntransformers-cli chat -h\n```\n\nThe chat is implemented on top of the [AutoClass](./model_doc/auto), using tooling from [text generation](./llm_tutorial) and [chat](./chat_templating).\n\n## TextGenerationPipeline\n\n[`TextGenerationPipeline`] is a high-level text generation class with a \"chat mode\". Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).",
  "To start, build a chat history with the following two roles.\n\n- `system` describes how the model should behave and respond when you're chatting with it. This role isn't supported by all chat models.\n- `user` is where you enter your first message to the model.\n\n```py\nchat = [\n{\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n{\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n]\n```\n\nCreate the [`TextGenerationPipeline`] and pass `chat` to it. For large models, setting [device_map=\"auto\"](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Changing the data type to [torch.bfloat16](./models#model-data-type) also helps save memory.\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\n```txt\n(sigh) Oh boy, you're asking me for advice? You're gonna need a map, pal! Alright,",
  "alright, I'll give you the lowdown. But don't say I didn't warn you, I'm a robot, not a tour guide!\n\nSo, you wanna know what's fun to do in the Big Apple? Well, let me tell you, there's a million\nthings to do, but I'll give you the highlights. First off, you gotta see the sights: the Statue of\nLiberty, Central Park, Times Square... you know, the usual tourist traps. But if you're lookin' for\nsomething a little more... unusual, I'd recommend checkin' out the Museum of Modern Art. It's got\nsome wild stuff, like that Warhol guy's soup cans and all that jazz.\n\nAnd if you're feelin' adventurous, take a walk across the Brooklyn Bridge. Just watch out for\nthose pesky pigeons, they're like little feathered thieves! (laughs) Get it? Thieves? Ah, never mind.\n\nNow, if you're lookin' for some serious fun, hit up the comedy clubs in Greenwich Village. You might\neven catch a glimpse of some up-and-coming comedians... or a bunch of wannabes tryin' to make it big. (winks)\n\nAnd finally, if you're feelin' like a real New Yorker, grab a slice of pizza from one of the many amazing\npizzerias around the city. Just don't try to order a \"robot-sized\" slice, trust me, it won't end well. (laughs)",
  "So, there you have it, pal! That's my expert advice on what to do in New York. Now, if you'll\nexcuse me, I've got some oil changes to attend to. (winks)\n```\n\nUse the `append` method on `chat` to respond to the models message.\n\n```py\nchat = response[0][\"generated_text\"]\nchat.append(\n{\"role\": \"user\", \"content\": \"Wait, what's so wild about soup cans?\"}\n)\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\n```txt\n(laughs) Oh, you're killin' me, pal! You don't get it, do you? Warhol's soup cans are like, art, man!\nIt's like, he took something totally mundane, like a can of soup, and turned it into a masterpiece. It's\nlike, \"Hey, look at me, I'm a can of soup, but I'm also a work of art!\"\n(sarcastically) Oh, yeah, real original, Andy.\n\nBut, you know, back in the '60s, it was like, a big deal. People were all about challenging the\nstatus quo, and Warhol was like, the king of that. He took the ordinary and made it extraordinary.\nAnd, let me tell you, it was like, a real game-changer. I mean, who would've thought that a can of soup could be art? (laughs)",
  "But, hey, you're not alone, pal. I mean, I'm a robot, and even I don't get it. (winks)\nBut, hey, that's what makes art, art, right? (laughs)\n```\n\n## Performance\n\nTransformers load models in full precision by default, and for a 8B model, this requires ~32GB of memory! Reduce memory usage by loading a model in half-precision or bfloat16 (only uses ~2 bytes per parameter). You can even quantize the model to a lower precision like 8-bit or 4-bit with [bitsandbytes](https://hf.co/docs/bitsandbytes/index).\n\n> [!TIP]\n> Refer to the [Quantization](./quantization/overview) docs for more information about the different quantization backends available.\n\nCreate a [`BitsAndBytesConfig`] with your desired quantization settings and pass it to the pipelines `model_kwargs` parameter. The example below quantizes a model to 8-bits.\n\n```py\nfrom transformers import pipeline, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\npipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", model_kwargs={\"quantization_config\": quantization_config})\n```",
  "In general, larger models are slower in addition to requiring more memory because text generation is bottlenecked by **memory bandwidth** instead of compute power. Each active parameter must be read from memory for every generated token. For a 16GB model, 16GB must be read from memory for every generated token.\n\nThe number of generated tokens/sec is proportional to the total memory bandwidth of the system divided by the model size. Depending on your hardware, total memory bandwidth can vary. Refer to the table below for approximate generation speeds for different hardware types.\n\n| Hardware | Memory bandwidth |\n|---|---|\n| consumer CPU | 20-100GB/sec |\n| specialized CPU (Intel Xeon, AMD Threadripper/Epyc, Apple silicon) | 200-900GB/sec |\n| data center GPU (NVIDIA A100/H100) | 2-3TB/sec |\n\nThe easiest solution for improving generation speed is to either quantize a model or use hardware with higher memory bandwidth.",
  "You can also try techniques like [speculative decoding](./generation_strategies#speculative-decoding), where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token per `forward` pass. This significantly alleviates the bandwidth bottleneck and improves generation speed.\n\n> [!TIP]\n> Parameters may not be active for every generated token in MoE models such as [Mixtral](./model_doc/mixtral), [Qwen2MoE](./model_doc/qwen2_moe.md), and [DBRX](./model_doc/dbrx). As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because parameters become activated with each new speculated token.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image processors\n\nImage processors converts images into pixel values, tensors that represent image colors and size. The pixel values are inputs to a vision or video model. To ensure a pretrained model receives the correct input, an image processor can perform the following operations to make sure an image is exactly like the images a model was pretrained on.\n\n- [`~BaseImageProcessor.center_crop`] to resize an image",
  "- [`~BaseImageProcessor.normalize`] or [`~BaseImageProcessor.rescale`] pixel values\n\nUse [`~ImageProcessingMixin.from_pretrained`] to load an image processors configuration (image size, whether to normalize and rescale, etc.) from a vision model on the Hugging Face [Hub](https://hf.co) or local directory. The configuration for each pretrained model is saved in a [preprocessor_config.json](https://huggingface.co/google/vit-base-patch16-224/blob/main/preprocessor_config.json) file.\n\n```py\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n```\n\nPass an image to the image processor to transform it into pixel values, and set `return_tensors=\"pt\"` to return PyTorch tensors. Feel free to print out the inputs to see what the image looks like as a tensor.\n\n```py\nfrom PIL import Image\nimport requests\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/image_processor_example.png\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\ninputs = image_processor(image, return_tensors=\"pt\")\n```",
  "This guide covers the image processor class and how to preprocess images for vision models.\n\n## Image processor classes\n\nImage processors inherit from the [`BaseImageProcessor`] class which provides the [`~BaseImageProcessor.center_crop`], [`~BaseImageProcessor.normalize`], and [`~BaseImageProcessor.rescale`] functions. There are two types of image processors.\n\n- [`BaseImageProcessor`] is a Python implementation.\n- [`BaseImageProcessorFast`] is a faster [torchvision-backed](https://pytorch.org/vision/stable/index.html) version. For a batch of [torch.Tensor](https://pytorch.org/docs/stable/tensors.html) inputs, this can be up to 33x faster. [`BaseImageProcessorFast`] is not available for all vision models at the moment. Refer to a models API documentation to check if it is supported.\n\nEach image processor subclasses the [`ImageProcessingMixin`] class which provides the [`~ImageProcessingMixin.from_pretrained`] and [`~ImageProcessingMixin.save_pretrained`] methods for loading and saving image processors.\n\nThere are two ways you can load an image processor, with [`AutoImageProcessor`] or a model-specific image processor.\n\n<hfoptions id=\"image-processor-classes\">",
  "<hfoption id=\"AutoImageProcessor\">\n\nThe [AutoClass](./model_doc/auto) API provides a convenient method to load an image processor without directly specifying the model the image processor is associated with.\n\nUse [`~AutoImageProcessor.from_pretrained`] to load an image processor, and set `use_fast=True` to load a fast image processor if it's supported.\n\n```py\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n```\n\n</hfoption>\n<hfoption id=\"model-specific image processor\">\n\nEach image processor is associated with a specific pretrained vision model, and the image processors configuration contains the models expected size and whether to normalize and resize.\n\nThe image processor can be loaded directly from the model-specific class. Check a models API documentation to see whether it supports a fast image processor.\n\n```py\nfrom transformers import ViTImageProcessor\n\nimage_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n```\n\nTo load a fast image processor, use the fast implementation class.\n\n```py\nfrom transformers import ViTImageProcessorFast",
  "image_processor = ViTImageProcessorFast.from_pretrained(\"google/vit-base-patch16-224\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Fast image processors\n\n[`BaseImageProcessorFast`] is based on [torchvision](https://pytorch.org/vision/stable/index.html) and is significantly faster, especially when processing on a GPU. This class can be used as a drop-in replacement for [`BaseImageProcessor`] if it's available for a model because it has the same design. Make sure [torchvision](https://pytorch.org/get-started/locally/#mac-installation) is installed, and set the `use_fast` parameter to `True`.\n\n```py\nfrom transformers import AutoImageProcessor\n\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", use_fast=True)\n```\n\nControl which device processing is performed on with the `device` parameter. Processing is performed on the same device as the input by default if the inputs are tensors, otherwise they are processed on the CPU. The example below places the fast processor on a GPU.\n\n```py\nfrom torchvision.io import read_image\nfrom transformers import DetrImageProcessorFast\n\nimages = read_image(\"image.jpg\")",
  "processor = DetrImageProcessorFast.from_pretrained(\"facebook/detr-resnet-50\")\nimages_processed = processor(images, return_tensors=\"pt\", device=\"cuda\")\n```\n\n<details>\n<summary>Benchmarks</summary>\n\nThe benchmarks are obtained from an [AWS EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) instance with a NVIDIA A10G Tensor Core GPU.\n\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_padded.png\" />\n</div>\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_batched_compiled.png\" />\n</div>\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_single.png\" />\n</div>\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_batched.png\" />\n</div>\n</details>\n\n## Preprocess",
  "Transformers' vision models expects the input as PyTorch tensors of pixel values. An image processor handles the conversion of images to pixel values, which is represented by the batch size, number of channels, height, and width. To achieve this, an image is resized (center cropped) and the pixel values are normalized and rescaled to the models expected values.\n\nImage preprocessing is not the same as *image augmentation*. Image augmentation makes changes (brightness, colors, rotatation, etc.) to an image for the purpose of either creating new training examples or prevent overfitting. Image preprocessing makes changes to an image for the purpose of matching a pretrained model's expected input format.\n\nTypically, images are augmented (to increase performance) and then preprocessed before being passed to a model. You can use any library ([Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb), [Kornia](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)) for augmentation and an image processor for preprocessing.",
  "This guide uses the torchvision [transforms](https://pytorch.org/vision/stable/transforms.html) module for augmentation.\n\nStart by loading a small sample of the [food101](https://hf.co/datasets/food101) dataset.\n\n```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"food101\", split=\"train[:100]\")\n```\n\nFrom the [transforms](https://pytorch.org/vision/stable/transforms.html) module, use the [Compose](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html) API to chain together [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) and [ColorJitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html). These transforms randomly crop and resize an image, and randomly adjusts an images colors.\n\nThe image size to randomly crop to can be retrieved from the image processor. For some models, an exact height and width are expected while for others, only the `shortest_edge` is required.\n\n```py\nfrom torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n\nsize = (\nimage_processor.size[\"shortest_edge\"]\nif \"shortest_edge\" in image_processor.size",
  "else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n```\n\nApply the transforms to the images and convert them to the RGB format. Then pass the augmented images to the image processor to return the pixel values.\n\nThe `do_resize` parameter is set to `False` because the images have already been resized in the augmentation step by [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html). If you don't augment the images, then the image processor automatically resizes and normalizes the images with the `image_mean` and `image_std` values. These values are found in the preprocessor configuration file.\n\n```py\ndef transforms(examples):\nimages = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\nexamples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\nreturn examples\n```\n\nApply the combined augmentation and preprocessing function to the entire dataset on the fly with [`~datasets.Dataset.set_transform`].\n\n```py\ndataset.set_transform(transforms)\n```",
  "Convert the pixel values back into an image to see how the image has been augmented and preprocessed.\n\n```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimg = dataset[0][\"pixel_values\"]\nplt.imshow(img.permute(1, 2, 0))\n```\n\n<div class=\"flex gap-4\">\n<div>\n<img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png\" />\n<figcaption class=\"mt-2 text-center text-sm text-gray-500\">before</figcaption>\n</div>\n<div>\n<img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png\" />\n<figcaption class=\"mt-2 text-center text-sm text-gray-500\">after</figcaption>\n</div>\n</div>\n\nFor other vision tasks like object detection or segmentation, the image processor includes post-processing methods to convert a models raw output into meaningful predictions like bounding boxes or segmentation maps.\n\n### Padding",
  "Some models, like [DETR](./model_doc/detr), applies [scale augmentation](https://paperswithcode.com/method/image-scale-augmentation) during training which can cause images in a batch to have different sizes. Images with different sizes can't be batched together.\n\nTo fix this, pad the images with the special padding token `0`. Use the [pad](https://github.com/huggingface/transformers/blob/9578c2597e2d88b6f0b304b5a05864fd613ddcc1/src/transformers/models/detr/image_processing_detr.py#L1151) method to pad the images, and define a custom collate function to batch them together.\n\n```py\ndef collate_fn(batch):\npixel_values = [item[\"pixel_values\"] for item in batch]\nencoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\nlabels = [item[\"labels\"] for item in batch]\nbatch = {}\nbatch[\"pixel_values\"] = encoding[\"pixel_values\"]\nbatch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\nbatch[\"labels\"] = labels\nreturn batch\n```",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# How 🤗 Transformers solve tasks",
  "In [What 🤗 Transformers can do](task_summary), you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what's happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, a decoder, or an encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. We'll also explain how a modern CNN works.\n\nTo explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.\n\n- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\n- [DETR](model_doc/detr) for object detection",
  "- [Mask2Former](model_doc/mask2former) for image segmentation\n- [GLPN](model_doc/glpn) for depth estimation\n- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\n- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\n\n<Tip>\n\nBefore you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information!\n\n</Tip>\n\n## Speech and audio\n\n[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n</div>",
  "This model has four main components:\n\n1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\n\n2. Waveforms are continuous by nature, so they can't be divided into separate units like a sequence of text can be split into words. That's why the feature vectors are passed to a *quantization module*, which aims to learn discrete speech units. The speech unit is chosen from a collection of codewords, known as a *codebook* (you can think of this as the vocabulary). From the codebook, the vector or speech unit, that best represents the continuous audio input is chosen and forwarded through the model.\n\n3. About half of the feature vectors are randomly masked, and the masked feature vector is fed to a *context network*, which is a Transformer encoder that also adds relative positional embeddings.",
  "4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification\n\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.\n\nReady to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!",
  "### Automatic speech recognition\n\nTo use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription.\n\nReady to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference!\n\n## Computer vision\n\nThere are two ways to approach computer vision tasks:\n\n1. Split an image into a sequence of patches and process them in parallel with a Transformer.\n2. Use a modern CNN, like [ConvNeXT](model_doc/convnext), which relies on convolutional layers but adopts modern network designs.\n\n<Tip>",
  "A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt) or [LeViT](model_doc/levit)). We won't discuss those because they just combine the two approaches we examine here.\n\n</Tip>\n\nViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.\n\n### Image classification\n\nViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\n\n#### Transformer\n\n[ViT](model_doc/vit) replaces convolutions entirely with a pure Transformer architecture. If you're familiar with the original Transformer, then you're already most of the way toward understanding ViT.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"/>\n</div>\n\nThe main change ViT introduced was in how images are fed to a Transformer:",
  "1. An image is split into square non-overlapping patches, each of which gets turned into a vector or *patch embedding*. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is \"tokenized\" into a sequence of patches.\n\n2. A *learnable embedding* - a special `[CLS]` token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the `[CLS]` token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image.\n\n3. The last thing to add to the patch and learnable embeddings are the *position embeddings* because the model doesn't know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.",
  "4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (MLP). ViT's pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.\n\nReady to try your hand at image classification? Check out our complete [image classification guide](tasks/image_classification) to learn how to finetune ViT and use it for inference!\n\n#### CNN\n\n<Tip>\n\nThis section briefly explains convolutions, but it'd be helpful to have a prior understanding of how they change an image's shape and size. If you're unfamiliar with convolutions, check out the [Convolution Neural Networks chapter](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) from the fastai book!\n\n</Tip>",
  "[ConvNeXT](model_doc/convnext) is a CNN architecture that adopts new and modern network designs to improve performance. However, convolutions are still at the core of the model. From a high-level perspective, a [convolution](glossary#convolution) is an operation where a smaller matrix (*kernel*) is multiplied by a small window of the image pixels. It computes some features from it, such as a particular texture or curvature of a line. Then it slides over to the next window of pixels; the distance the convolution travels is known as the *stride*.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n</div>\n\n<small>A basic convolution without padding or stride, taken from <a href=\"https://arxiv.org/abs/1603.07285\">A guide to convolution arithmetic for deep learning.</a></small>",
  "You can feed this output to another convolutional layer, and with each successive layer, the network learns more complex and abstract things like hotdogs or rockets. Between convolutional layers, it is common to add a pooling layer to reduce dimensionality and make the model more robust to variations of a feature's position.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png\"/>\n</div>\n\nConvNeXT modernizes a CNN in five ways:\n\n1. Change the number of blocks in each stage and \"patchify\" an image with a larger stride and corresponding kernel size. The non-overlapping sliding window makes this patchifying strategy similar to how ViT splits an image into patches.\n\n2. A *bottleneck* layer shrinks the number of channels and then restores it because it is faster to do a 1x1 convolution, and you can increase the depth. An inverted bottleneck does the opposite by expanding the number of channels and shrinking them, which is more memory efficient.",
  "3. Replace the typical 3x3 convolutional layer in the bottleneck layer with *depthwise convolution*, which applies a convolution to each input channel separately and then stacks them back together at the end. This widens the network width for improved performance.\n\n4. ViT has a global receptive field which means it can see more of an image at once thanks to its attention mechanism. ConvNeXT attempts to replicate this effect by increasing the kernel size to 7x7.\n\n5. ConvNeXT also makes several layer design changes that imitate Transformer models. There are fewer activation and normalization layers,  the activation function is switched to GELU instead of ReLU, and it uses LayerNorm instead of BatchNorm.\n\nThe output from the convolution blocks is passed to a classification head which converts the outputs into logits and calculates the cross-entropy loss to find the most likely label.\n\n### Object detection\n\n[DETR](model_doc/detr), *DEtection TRansformer*, is an end-to-end object detection model that combines a CNN with a Transformer encoder-decoder.\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png\"/>\n</div>\n\n1. A pretrained CNN *backbone* takes an image, represented by its pixel values, and creates a low-resolution feature map of it. A 1x1 convolution is applied to the feature map to reduce dimensionality and it creates a new feature map with a high-level image representation. Since the Transformer is a sequential model, the feature map is flattened into a sequence of feature vectors that are combined with positional embeddings.\n\n2. The feature vectors are passed to the encoder, which learns the image representations using its attention layers. Next, the encoder hidden states are combined with *object queries* in the decoder. Object queries are learned embeddings that focus on the different regions of an image, and they're updated as they progress through each attention layer. The decoder hidden states are passed to a feedforward network that predicts the bounding box coordinates and class label for each object query, or `no object` if there isn't one.",
  "DETR decodes each object query in parallel to output *N* final predictions, where *N* is the number of queries. Unlike a typical autoregressive model that predicts one element at a time, object detection is a set prediction task (`bounding box`, `class label`) that makes *N* predictions in a single pass.\n\n3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions with a fixed set of ground truth labels. If there are fewer ground truth labels in the set of *N* labels, then they're padded with a `no object` class. This loss function encourages DETR to find a one-to-one assignment between the predictions and ground truth labels. If either the bounding boxes or class labels aren't correct, a loss is incurred. Likewise, if DETR predicts an object that doesn't exist, it is penalized. This encourages DETR to find other objects in an image instead of focusing on one really prominent object.",
  "An object detection head is added on top of DETR to find the class label and the coordinates of the bounding box. There are two components to the object detection head: a linear layer to transform the decoder hidden states into logits over the class labels, and a MLP to predict the bounding box.\n\nReady to try your hand at object detection? Check out our complete [object detection guide](tasks/object_detection) to learn how to finetune DETR and use it for inference!\n\n### Image segmentation\n\n[Mask2Former](model_doc/mask2former) is a universal architecture for solving all types of image segmentation tasks. Traditional segmentation models are typically tailored towards a particular subtask of image segmentation, like instance, semantic or panoptic segmentation. Mask2Former frames each of those tasks as a *mask classification* problem. Mask classification groups pixels into *N* segments, and predicts *N* masks and their corresponding class label for a given image. We'll explain how Mask2Former works in this section, and then you can try finetuning SegFormer at the end.\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png\"/>\n</div>\n\nThere are three main components to Mask2Former:\n\n1. A [Swin](model_doc/swin) backbone accepts an image and creates a low-resolution image feature map from 3 consecutive 3x3 convolutions.\n\n2. The feature map is passed to a *pixel decoder* which gradually upsamples the low-resolution features into high-resolution per-pixel embeddings. The pixel decoder actually generates multi-scale features (contains both low- and high-resolution features) with resolutions 1/32, 1/16, and 1/8th of the original image.\n\n3. Each of these feature maps of differing scales is fed successively to one Transformer decoder layer at a time in order to capture small objects from the high-resolution features. The key to Mask2Former is the *masked attention* mechanism in the decoder. Unlike cross-attention which can attend to the entire image, masked attention only focuses on a certain area of the image. This is faster and leads to better performance because the local features of an image are enough for the model to learn from.",
  "4. Like [DETR](tasks_explained#object-detection), Mask2Former also uses learned object queries and combines them with the image features from the pixel decoder to make a set prediction (`class label`, `mask prediction`). The decoder hidden states are passed into a linear layer and transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and class label to find the most likely one.\n\nThe mask predictions are generated by combining the pixel-embeddings with the final decoder hidden states. The sigmoid cross-entropy and dice loss is calculated between the logits and the ground truth mask to find the most likely mask.\n\nReady to try your hand at image segmentation? Check out our complete [image segmentation guide](tasks/semantic_segmentation) to learn how to finetune SegFormer and use it for inference!\n\n### Depth estimation\n\n[GLPN](model_doc/glpn), *Global-Local Path Network*, is a Transformer for depth estimation that combines a [SegFormer](model_doc/segformer) encoder with a lightweight decoder.\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"/>\n</div>\n\n1. Like ViT, an image is split into a sequence of patches, except these image patches are smaller. This is better for dense prediction tasks like segmentation or depth estimation. The image patches are transformed into patch embeddings (see the [image classification](#image-classification) section for more details about how patch embeddings are created), which are fed to the encoder.\n\n2. The encoder accepts the patch embeddings, and passes them through several encoder blocks. Each block consists of attention and Mix-FFN layers. The purpose of the latter is to provide positional information. At the end of each encoder block is a *patch merging* layer for creating hierarchical representations. The features of each group of neighboring patches are concatenated, and a linear layer is applied to the concatenated features to reduce the number of patches to a resolution of 1/4. This becomes the input to the next encoder block, where this whole process is repeated until you have image features with resolutions of 1/8, 1/16, and 1/32.",
  "3. A lightweight decoder takes the last feature map (1/32 scale) from the encoder and upsamples it to 1/16 scale. From here, the feature is passed into a *Selective Feature Fusion (SFF)* module, which selects and combines local and global features from an attention map for each feature and then upsamples it to 1/8th. This process is repeated until the decoded features are the same size as the original image. The output is passed through two convolution layers and then a sigmoid activation is applied to predict the depth of each pixel.\n\n## Natural language processing\n\nThe Transformer was initially designed for machine translation, and since then, it has practically become the default architecture for solving all NLP tasks. Some tasks lend themselves to the Transformer's encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer's encoder-decoder structure.\n\n### Text classification\n\n[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.",
  "1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.\n\n2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and \"predict\" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.",
  "The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (`IsNext` and `NotNext`).\n\n3. The input embeddings are passed through multiple encoder layers to output some final hidden states.\n\nTo use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.\n\nReady to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference!\n\n### Token classification",
  "To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n\nReady to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n\n### Question answering\n\nTo use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.\n\nReady to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!",
  "<Tip>\n\n💡 Notice how easy it is to use BERT for different tasks once it's been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!\n\n</Tip>\n\n### Text generation\n\n[GPT-2](model_doc/gpt2) is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png\"/>\n</div>",
  "1. GPT-2 uses [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can't attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT's [`mask`] token because, in masked self-attention, an attention mask is used to set the score to `0` for future tokens.\n\n2. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.",
  "GPT-2's pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.\n\nReady to try your hand at text generation? Check out our complete [causal language modeling guide](tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!\n\n<Tip>\n\nFor more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n\n</Tip>\n\n### Summarization\n\nEncoder-decoder models like [BART](model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\n</div>",
  "1. BART's encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn't add a final feedforward network at the end to predict a word.",
  "2. The encoder's output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder's output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.\n\nReady to try your hand at summarization? Check out our complete [summarization guide](tasks/summarization) to learn how to finetune T5 and use it for inference!\n\n<Tip>\n\nFor more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n\n</Tip>\n\n### Translation\n\nTranslation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](model_doc/bart) or [T5](model_doc/t5) to do it. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.",
  "BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder's embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step.\n\nBART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages.\n\nReady to try your hand at translation? Check out our complete [translation guide](tasks/translation) to learn how to finetune T5 and use it for inference!\n\n<Tip>\n\nFor more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n\n</Tip>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Generation strategies\n\nA decoding strategy informs how a model should select the next generated token. There are many types of decoding strategies, and choosing the appropriate one has a significant impact on the quality of the generated text.\n\nThis guide will help you understand the different decoding strategies available in Transformers and how and when to use them.\n\n## Greedy search",
  "Greedy search is the default decoding strategy. It selects the next most likely token at each step. Unless specified in [`GenerationConfig`], this strategy generates a maximum of 20 tokens.\n\nGreedy search works well for tasks with relatively short outputs. However, it breaks down when generating longer sequences because it begins to repeat itself.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ninputs = tokenizer(\"I look forward to\", return_tensors=\"pt\").to(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n# explicitly set to default length because Llama2 generation length is 4096\noutputs = model.generate(**inputs, max_new_tokens=20)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n'Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing'\n```\n\n## Contrastive search",
  "[Contrastive search](https://huggingface.co/papers/2202.06417) is a decoding strategy that aims to reduce repetition even while generating longer sequences. This strategy compares how similar a generated token is against previous tokens, and if they're more similar, a penalty is applied.\n\nEnable contrastive search with the `penalty_alpha` and `top_k` parameters. The `penalty_alpha` manages the penalty applied and `top_k` is the number of most likely tokens to return.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n# explicitly set to 100 because Llama2 generation length is 4096\noutputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)",
  "'Hugging Face is an open-source company that provides a platform for building and deploying AI models.\\nHugging Face is an open-source company that provides a platform for building and deploying AI models. The platform allows developers to build and deploy AI models, as well as collaborate with other developers.\\nHugging Face was founded in 2019 by Thibault Wittemberg and Clément Delangue. The company is based in Paris, France.\\nHugging Face has'\n```\n\n## Beam search\n\nBeam search keeps track of several generated sequences (beams) at each time step. After a certain number of steps, it selects the sequence with the highest *overall* probability. Unlike greedy search, this strategy can \"look ahead\" and pick a sequence with a higher probability overall even if the initial tokens have a lower probability.\n\n> [!TIP]\n> Check out the [beam search visualizer](https://huggingface.co/spaces/m-ric/beam_search_visualizer) to see how beam search works.\n\nEnable beam search with the `num_beams` parameter (should be greater than 1 otherwise it's equivalent to greedy search).\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer",
  "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n# explicitly set to 100 because Llama2 generation length is 4096\noutputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n\"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']\"\n```\n\n## Diverse beam search\n\n[Diverse beam search](https://hf.co/papers/1610.02424) is a variant of beam search that produces more diverse output candidates to choose from. This strategy measures the dissimilarity of sequences and a penalty is applied if sequences are too similar. To avoid high computation costs, the number of beams is divided into groups.",
  "Enable diverse beam search with the `num_beams`, `num_beam_groups` and `diversity_penalty` parameters (the `num_beams` parameter should be divisible by `num_beam_groups`).\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n# explicitly set to 100 because Llama2 generation length is 4096\noutputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n'Hugging Face is an open-source company 🤗\\nWe are an open-source company. Our mission is to democratize AI and make it accessible to everyone. We believe that AI should be used for the benefit of humanity, not for the benefit of a'\n```\n\n## Multinomial sampling",
  "Search methods selects the most likely tokens. Sampling, or multinomial sampling, randomly selects a token based on the probability distribution over the entire models vocabulary. This means every token with a non-zero probability has a chance to be selected. Sampling strategies reduce repetition and can generate more creative and diverse outputs.\n\nEnable multinomial sampling with `do_sample=True` and `num_beams=1`.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n# explicitly set to 100 because Llama2 generation length is 4096\noutputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)",
  "'Hugging Face is an open-source company 🤗\\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.'\n```\n\n## Beam search multinomial sampling\n\nThis decoding strategy is a combination of beam search and multinomial sampling. It generates multiple beams and uses a sampling strategy for each beam.\n\nEnable beam search multinomial sampling by setting `num_beams` to a value greater than 1 and `do_sample=True`.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n# explicitly set to 100 because Llama2 generation length is 4096\noutputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=4)",
  "'Hugging Face is an open-source company 100% dedicated to making AI more accessible. We believe that AI should be available to everyone, and we’re working hard to make that a reality.\\nWe’re a team of passionate engineers, designers,'\n```\n\n## Speculative decoding\n\n[Speculative](https://hf.co/papers/2211.17192) or assistive decoding isn't a search or sampling strategy. Instead, speculative decoding adds a second smaller model to generate candidate tokens. The main model verifies the candidate tokens in a single `forward` pass, which speeds up the decoding process overall. This method is especially useful for LLMs where it can be more costly and slower to generate tokens. Refer to the [speculative decoding](./llm_optims#speculative-decoding) guide to learn more.\n\nCurrently, only greedy search and multinomial sampling are supported with speculative decoding. Batched inputs aren't supported either.\n\nEnable speculative decoding with the `assistant_model` parameter. You'll notice the fastest speed up with an assistant model that is much smaller than the main model. Add `do_sample=True` to enable token validation with resampling.\n\n<hfoptions id=\"spec-decoding\">",
  "<hfoption id=\"greedy search\">\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nassistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, assistant_model=assistant_model)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine'\n```\n\nSpeculative decoding is also supported in [`Pipeline`] with the `assistant_model` parameter.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n\"text-generation\",\nmodel=\"meta-llama/Llama-3.1-8B\",\nassistant_model=\"meta-llama/Llama-3.2-1B\",\ntorch_dtype=torch.bfloat16\n)\npipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\npipe_output[0][\"generated_text\"]\n```\n\n</hfoption>\n<hfoption id=\"multinomial sampling\">",
  "Add the `temperature` parameter to control sampling randomness. For speculative decoding, a lower temperature may improve latency.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nassistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n'Hugging Face is an open-source company that is dedicated to creating a better world through technology.'\n```\n\n</hfoption>\n</hfoptions>\n\n### Prompt lookup decoding\n\n[Prompt lookup decoding](./llm_optims#prompt-lookup-decoding) is a variant of speculative decoding that uses overlapping n-grams as the candidate tokens. It works well for input-grounded tasks such as summarization. Refer to the [prompt lookup decoding](./llm_optims#prompt-lookup-decoding) guide to learn more.",
  "Enable prompt lookup decoding with the `prompt_lookup_num_tokens` parameter.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\nassistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=torch.float16).to(\"cuda\")\ninputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=20, prompt_lookup_num_tokens=5)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools'\n```\n\n### Self-speculative decoding",
  "Early exiting uses the earlier hidden states from the language modeling head as inputs, effectively skipping layers to yield a lower quality output. The lower quality output is used as the assistant output and self-speculation is applied to fix the output using the remaining layers. The final generated result from this self-speculative method is the same (or has the same distribution) as the original models generation.\n\nThe assistant model is also part of the target model, so the caches and weights can be shared, resulting in lower memory requirements.\n\nFor a model trained with early exit, pass `assistant_early_exit` to [`~GenerationMixin.generate`].\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprompt = \"Alice and Bob\"\ncheckpoint = \"facebook/layerskip-llama3.2-1B\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\noutputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n```\n\n### Universal assisted decoding",
  "Universal assisted decoding (UAD) enables the main and assistant models to use different tokenizers. The main models input tokens are re-encoded into assistant model tokens. Candidate tokens are generated in the assistant encoding which are re-encoded into the main model candidate tokens. The candidate tokens are verified as explained in [speculative decoding](#speculative-decoding).\n\nRe-encoding involves decoding token ids into text and encoding the text with a different tokenizer. To prevent tokenization discrepancies during re-encoding, UAD finds the longest common sub-sequence between the source and target encodings to ensure the new tokens include the correct prompt suffix.\n\nAdd the `tokenizer` and `assistant_tokenizer` parameters to [`~GenerationMixin.generate`] to enable UAD.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprompt = \"Alice and Bob\"\n\nassistant_tokenizer = AutoTokenizer.from_pretrained(\"double7/vicuna-68m\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\")",
  "assistant_model = AutoModelForCausalLM.from_pretrained(\"double7/vicuna-68m\")\noutputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n```\n\n## DoLa\n\n[Decoding by Contrasting Layers (DoLa)](https://hf.co/papers/2309.03883) is a contrastive decoding strategy for improving factuality and reducing hallucination. This strategy works by contrasting the logit differences between the final and early layers. As a result, factual knowledge localized to particular layers are amplified. DoLa is not recommended for smaller models like GPT-2.\n\nEnable DoLa with the following parameters.\n\n- `dola_layers` are the candidate layers to be contrasted with the final layer. It can be a string (`low` or `high`) to contrast the lower or higher parts of a layer. `high` is recommended for short-answer tasks like TruthfulQA. `low` is recommended for long-answer reasoning tasks like GSM8K, StrategyQA, FACTOR, and VicunaQA.",
  "When a model has tied word embeddings, layer 0 is skipped and it begins from layer 2.\n\nIt can also be a list of integers that represent the layer indices between 0 and the total number of layers. Layer 0 is the word embedding, 1 is the first transformer layer, and so on. Refer to the table below for the range of layer indices depending on the number of model layers.\n\n| layers | low | high |\n|---|---|---|\n| > 40 | (0, 20, 2) | (N - 20, N, 2) |\n| <= 40 | range(0, N // 2, 2) | range(N // 2, N, 2) |\n\n- `repetition_penalty` reduces repetition and it is recommended to set it to 1.2.\n\n<hfoptions id=\"dola\">\n<hfoption id=\"contrast higher layers\">\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\ninputs = tokenizer(\"What is the highest peak in the world??\", return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=50, dola_layers=\"high\", do_sample=False)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)",
  "\" Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak, lying almost 9.5 kilometers above the sea level and the tallest mountain from 19,036.91 ft. The mountain was\"\n```\n\n</hfoption>\n<hfoption id=\"contrast specific layers\">\n\nContrast layers 18 and 20 with the final layer.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(\"cuda\")\ninputs = tokenizer(\"What is the highest peak in the world?\", return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=50, dola_layers=[18,20], do_sample=False, repetition_penalty=1.2)\ntokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n\" Mount EverestMount Everest, called Himalaya in Nepali, is the world's highest peak above sea level and it rises to an incredible height of 29,028 feet above the ocean. Its summit is over a mile taller than Mt\"\n```\n\n</hfoption>\n</hfoptions>\n\n## Resources",
  "Read the [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) blog post for an explanation of how common decoding strategies work.",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Glossary\n\nThis glossary defines general machine learning and 🤗 Transformers terms to help you better understand the\ndocumentation.\n\n## A\n\n### attention mask\n\nThe attention mask is an optional argument used when batching sequences together.\n\n<Youtube id=\"M6adb1j2jPI\"/>\n\nThis argument indicates to the model which tokens should be attended to, and which should not.\n\nFor example, consider these two sequences:\n\n```python",
  ">>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n>>> sequence_a = \"This is a short sequence.\"\n>>> sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n\n>>> encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n>>> encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]\n```\n\nThe encoded versions have different lengths:\n\n```python\n>>> len(encoded_sequence_a), len(encoded_sequence_b)\n(8, 19)\n```\n\nTherefore, we can't put them together in the same tensor as-is. The first sequence needs to be padded up to the length\nof the second one, or the second one needs to be truncated down to the length of the first one.\n\nIn the first case, the list of IDs will be extended by the padding indices. We can pass a list to the tokenizer and ask\nit to pad like this:\n\n```python\n>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n```\n\nWe can see that 0s have been added on the right of the first sentence to make it the same length as the second one:\n\n```python\n>>> padded_sequences[\"input_ids\"]",
  "[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n```\n\nThis can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the\nposition of the padded indices so that the model does not attend to them. For the [`BertTokenizer`], `1` indicates a\nvalue that should be attended to, while `0` indicates a padded value. This attention mask is in the dictionary returned\nby the tokenizer under the key \"attention_mask\":\n\n```python\n>>> padded_sequences[\"attention_mask\"]\n[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n```\n\n### autoencoding models\n\nSee [encoder models](#encoder-models) and [masked language modeling](#masked-language-modeling-mlm)\n\n### autoregressive models\n\nSee [causal language modeling](#causal-language-modeling) and [decoder models](#decoder-models)\n\n## B\n\n### backbone",
  "The backbone is the network (embeddings and layers) that outputs the raw hidden states or features. It is usually connected to a [head](#head) which accepts the features as its input to make a prediction. For example, [`ViTModel`] is a backbone without a specific head on top. Other models can also use [`VitModel`] as a backbone such as [DPT](model_doc/dpt).\n\n## C\n\n### causal language modeling\n\nA pretraining task where the model reads the texts in order and has to predict the next word. It's usually done by\nreading the whole sentence but using a mask inside the model to hide the future tokens at a certain timestep.\n\n### channel\n\nColor images are made up of some combination of values in three channels: red, green, and blue (RGB) and grayscale images only have one channel. In 🤗 Transformers, the channel can be the first or last dimension of an image's tensor: [`n_channels`, `height`, `width`] or [`height`, `width`, `n_channels`].\n\n### connectionist temporal classification (CTC)",
  "An algorithm which allows a model to learn without knowing exactly how the input and output are aligned; CTC calculates the distribution of all possible outputs for a given input and chooses the most likely output from it. CTC is commonly used in speech recognition tasks because speech doesn't always cleanly align with the transcript for a variety of reasons such as a speaker's different speech rates.\n\n### convolution\n\nA type of layer in a neural network where the input matrix is multiplied element-wise by a smaller matrix (kernel or filter) and the values are summed up in a new matrix. This is known as a convolutional operation which is repeated over the entire input matrix. Each operation is applied to a different segment of the input matrix. Convolutional neural networks (CNNs) are commonly used in computer vision.\n\n## D\n\n### DataParallel (DP)\n\nParallelism technique for training on multiple GPUs where the same setup is replicated multiple times, with each instance\nreceiving a distinct data slice. The processing is done in parallel and all setups are synchronized at the end of each training step.",
  "Learn more about how DataParallel works [here](perf_train_gpu_many#dataparallel-vs-distributeddataparallel).\n\n### decoder input IDs\n\nThis input is specific to encoder-decoder models, and contains the input IDs that will be fed to the decoder. These\ninputs should be used for sequence to sequence tasks, such as translation or summarization, and are usually built in a\nway specific to each model.\n\nMost encoder-decoder models (BART, T5) create their `decoder_input_ids` on their own from the `labels`. In such models,\npassing the `labels` is the preferred way to handle training.\n\nPlease check each model's docs to see how they handle these input IDs for sequence to sequence training.\n\n### decoder models\n\nAlso referred to as autoregressive models, decoder models involve a pretraining task (called causal language modeling) where the model reads the texts in order and has to predict the next word. It's usually done by\nreading the whole sentence with a mask to hide future tokens at a certain timestep.\n\n<Youtube id=\"d_ixlCubqQw\"/>\n\n### deep learning (DL)\n\nMachine learning algorithms which use neural networks with several layers.\n\n## E\n\n### encoder models",
  "Also known as autoencoding models, encoder models take an input (such as text or images) and transform them into a condensed numerical representation called an embedding. Oftentimes, encoder models are pretrained using techniques like [masked language modeling](#masked-language-modeling-mlm), which masks parts of the input sequence and forces the model to create more meaningful representations.\n\n<Youtube id=\"H39Z_720T5s\"/>\n\n## F\n\n### feature extraction\n\nThe process of selecting and transforming raw data into a set of features that are more informative and useful for machine learning algorithms. Some examples of feature extraction include transforming raw text into word embeddings and extracting important features such as edges or shapes from image/video data.\n\n### feed forward chunking\n\nIn each residual attention block in transformers the self-attention layer is usually followed by 2 feed forward layers.\nThe intermediate embedding size of the feed forward layers is often bigger than the hidden size of the model (e.g., for\n`google-bert/bert-base-uncased`).\n\nFor an input of size `[batch_size, sequence_length]`, the memory required to store the intermediate feed forward",
  "embeddings `[batch_size, sequence_length, config.intermediate_size]` can account for a large fraction of the memory\nuse. The authors of [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) noticed that since the\ncomputation is independent of the `sequence_length` dimension, it is mathematically equivalent to compute the output\nembeddings of both feed forward layers `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`\nindividually and concat them afterward to `[batch_size, sequence_length, config.hidden_size]` with `n = sequence_length`, which trades increased computation time against reduced memory use, but yields a mathematically\n**equivalent** result.\n\nFor models employing the function [`apply_chunking_to_forward`], the `chunk_size` defines the number of output\nembeddings that are computed in parallel and thus defines the trade-off between memory and time complexity. If\n`chunk_size` is set to 0, no feed forward chunking is done.\n\n### finetuned models",
  "Finetuning is a form of transfer learning which involves taking a pretrained model, freezing its weights, and replacing the output layer with a newly added [model head](#head). The model head is trained on your target dataset.\n\nSee the [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training) tutorial for more details, and learn how to fine-tune models with 🤗 Transformers.\n\n## H\n\n### head\n\nThe model head refers to the last layer of a neural network that accepts the raw hidden states and projects them onto a different dimension. There is a different model head for each task. For example:\n\n* [`GPT2ForSequenceClassification`] is a sequence classification head - a linear layer - on top of the base [`GPT2Model`].\n* [`ViTForImageClassification`] is an image classification head - a linear layer on top of the final hidden state of the `CLS` token - on top of the base [`ViTModel`].\n* [`Wav2Vec2ForCTC`] is a language modeling head with [CTC](#connectionist-temporal-classification-ctc) on top of the base [`Wav2Vec2Model`].\n\n## I\n\n### image patch",
  "Vision-based Transformers models split an image into smaller patches which are linearly embedded, and then passed as a sequence to the model. You can find the `patch_size` - or resolution - of the model in its configuration.\n\n### inference\n\nInference is the process of evaluating a model on new data after training is complete. See the [Pipeline for inference](https://huggingface.co/docs/transformers/pipeline_tutorial) tutorial to learn how to perform inference with 🤗 Transformers.\n\n### input IDs\n\nThe input ids are often the only required parameters to be passed to the model as input. They are token indices,\nnumerical representations of tokens building the sequences that will be used as input by the model.\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nEach tokenizer works differently but the underlying mechanism remains the same. Here's an example using the BERT\ntokenizer, which is a [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) tokenizer:\n\n```python\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n>>> sequence = \"A Titan RTX has 24GB of VRAM\"\n```",
  "The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary.\n\n```python\n>>> tokenized_sequence = tokenizer.tokenize(sequence)\n```\n\nThe tokens are either words or subwords. Here for instance, \"VRAM\" wasn't in the model vocabulary, so it's been split\nin \"V\", \"RA\" and \"M\". To indicate those tokens are not separate words but parts of the same word, a double-hash prefix\nis added for \"RA\" and \"M\":\n\n```python\n>>> print(tokenized_sequence)\n['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n```\n\nThese tokens can then be converted into IDs which are understandable by the model. This can be done by directly feeding the sentence to the tokenizer, which leverages the Rust implementation of [🤗 Tokenizers](https://github.com/huggingface/tokenizers) for peak performance.\n\n```python\n>>> inputs = tokenizer(sequence)\n```\n\nThe tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The\ntoken indices are under the key `input_ids`:\n\n```python\n>>> encoded_sequence = inputs[\"input_ids\"]\n>>> print(encoded_sequence)",
  "[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n```\n\nNote that the tokenizer automatically adds \"special tokens\" (if the associated model relies on them) which are special\nIDs the model sometimes uses.\n\nIf we decode the previous sequence of ids,\n\n```python\n>>> decoded_sequence = tokenizer.decode(encoded_sequence)\n```\n\nwe will see\n\n```python\n>>> print(decoded_sequence)\n[CLS] A Titan RTX has 24GB of VRAM [SEP]\n```\n\nbecause this is the way a [`BertModel`] is going to expect its inputs.\n\n## L\n\n### labels\n\nThe labels are an optional argument which can be passed in order for the model to compute the loss itself. These labels\nshould be the expected prediction of the model: it will use the standard loss in order to compute the loss between its\npredictions and the expected value (the label).\n\nThese labels are different according to the model head, for example:\n\n- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tensor of dimension\n`(batch_size)` with each value of the batch corresponding to the expected label of the entire sequence.",
  "- For token classification models, ([`BertForTokenClassification`]), the model expects a tensor of dimension\n`(batch_size, seq_length)` with each value corresponding to the expected label of each individual token.\n- For masked language modeling, ([`BertForMaskedLM`]), the model expects a tensor of dimension `(batch_size,\nseq_length)` with each value corresponding to the expected label of each individual token: the labels being the token\nID for the masked token, and values to be ignored for the rest (usually -100).\n- For sequence to sequence tasks, ([`BartForConditionalGeneration`], [`MBartForConditionalGeneration`]), the model\nexpects a tensor of dimension `(batch_size, tgt_seq_length)` with each value corresponding to the target sequences\nassociated with each input sequence. During training, both BART and T5 will make the appropriate\n`decoder_input_ids` and decoder attention masks internally. They usually do not need to be supplied. This does not\napply to models leveraging the Encoder-Decoder framework.\n- For image classification models, ([`ViTForImageClassification`]), the model expects a tensor of dimension",
  "`(batch_size)` with each value of the batch corresponding to the expected label of each individual image.\n- For semantic segmentation models, ([`SegformerForSemanticSegmentation`]), the model expects a tensor of dimension\n`(batch_size, height, width)` with each value of the batch corresponding to the expected label of each individual pixel.\n- For object detection models, ([`DetrForObjectDetection`]), the model expects a list of dictionaries with a\n`class_labels` and `boxes` key where each value of the batch corresponds to the expected label and number of bounding boxes of each individual image.\n- For automatic speech recognition models, ([`Wav2Vec2ForCTC`]), the model expects a tensor of dimension `(batch_size,\ntarget_length)` with each value corresponding to the expected label of each individual token.\n\n<Tip>\n\nEach model's labels may be different, so be sure to always check the documentation of each model for more information\nabout their specific labels!\n\n</Tip>\n\nThe base models ([`BertModel`]) do not accept labels, as these are the base transformer models, simply outputting\nfeatures.\n\n### large language models (LLM)",
  "A generic term that refers to transformer language models (GPT-3, BLOOM, OPT) that were trained on a large quantity of data. These models also tend to have a large number of learnable parameters (e.g. 175 billion for GPT-3).\n\n## M\n\n### masked language modeling (MLM)\n\nA pretraining task where the model sees a corrupted version of the texts, usually done by\nmasking some tokens randomly, and has to predict the original text.\n\n### multimodal\n\nA task that combines texts with another kind of inputs (for instance images).\n\n## N\n\n### Natural language generation (NLG)\n\nAll tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/), translation).\n\n### Natural language processing (NLP)\n\nA generic way to say \"deal with texts\".\n\n### Natural language understanding (NLU)\n\nAll tasks related to understanding what is in a text (for instance classifying the\nwhole text, individual words).\n\n## P\n\n### pipeline",
  "A pipeline in 🤗 Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.\n\nFor more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).\n\n### PipelineParallel (PP)\n\nParallelism technique in which the model is split up vertically (layer-level) across multiple GPUs, so that only one or\nseveral layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline\nand working on a small chunk of the batch. Learn more about how PipelineParallel works [here](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism).\n\n### pixel values\n\nA tensor of the numerical representations of an image that is passed to a model. The pixel values have a shape of [`batch_size`, `num_channels`, `height`, `width`], and are generated from an image processor.\n\n### pooling",
  "An operation that reduces a matrix into a smaller matrix, either by taking the maximum or average of the pooled dimension(s). Pooling layers are commonly found between convolutional layers to downsample the feature representation.\n\n### position IDs\n\nContrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of\neach token. Therefore, the position IDs (`position_ids`) are used by the model to identify each token's position in the\nlist of tokens.\n\nThey are an optional parameter. If no `position_ids` are passed to the model, the IDs are automatically created as\nabsolute positional embeddings.\n\nAbsolute positional embeddings are selected in the range `[0, config.max_position_embeddings - 1]`. Some models use\nother types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.\n\n### preprocessing",
  "The task of preparing raw data into a format that can be easily consumed by machine learning models. For example, text is typically preprocessed by tokenization. To gain a better idea of what preprocessing looks like for other input types, check out the [Preprocess](https://huggingface.co/docs/transformers/preprocessing) tutorial.\n\n### pretrained model\n\nA model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve a\nself-supervised objective, which can be reading the text and trying to predict the next word (see [causal language\nmodeling](#causal-language-modeling)) or masking some words and trying to predict them (see [masked language\nmodeling](#masked-language-modeling-mlm)).",
  "Speech and vision models have their own pretraining objectives. For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the \"true\" speech representation from a set of \"false\" speech representations. On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective).\n\n## R\n\n### recurrent neural network (RNN)\n\nA type of model that uses a loop over a layer to process texts.\n\n### representation learning\n\nA subfield of machine learning which focuses on learning meaningful representations of raw data. Some examples of representation learning techniques include word embeddings, autoencoders, and Generative Adversarial Networks (GANs).\n\n## S\n\n### sampling rate\n\nA measurement in hertz of the number of samples (the audio signal) taken per second. The sampling rate is a result of discretizing a continuous signal such as speech.\n\n### self-attention\n\nEach element of the input finds out which other elements of the input they should attend to.\n\n### self-supervised learning",
  "A category of machine learning techniques in which a model creates its own learning objective from unlabeled data. It differs from [unsupervised learning](#unsupervised-learning) and [supervised learning](#supervised-learning) in that the learning process is supervised, but not explicitly from the user.\n\nOne example of self-supervised learning is [masked language modeling](#masked-language-modeling-mlm), where a model is passed sentences with a proportion of its tokens removed and learns to predict the missing tokens.\n\n### semi-supervised learning\n\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike [supervised learning](#supervised-learning) and [unsupervised learning](#unsupervised-learning).\n\nAn example of a semi-supervised learning approach is \"self-training\", in which a model is trained on labeled data, and then used to make predictions on the unlabeled data. The portion of the unlabeled data that the model predicts with the most confidence gets added to the labeled dataset and used to retrain the model.\n\n### sequence-to-sequence (seq2seq)",
  "Models that generate a new sequence from an input, like translation models, or summarization models (such as\n[Bart](model_doc/bart) or [T5](model_doc/t5)).\n\n### Sharded DDP\n\nAnother name for the foundational [ZeRO](#zero-redundancy-optimizer-zero) concept as used by various other implementations of ZeRO.\n\n### stride\n\nIn [convolution](#convolution) or [pooling](#pooling), the stride refers to the distance the kernel is moved over a matrix. A stride of 1 means the kernel is moved one pixel over at a time, and a stride of 2 means the kernel is moved two pixels over at a time.\n\n### supervised learning\n\nA form of model training that directly uses labeled data to correct and instruct model performance. Data is fed into the model being trained, and its predictions are compared to the known labels. The model updates its weights based on how incorrect its predictions were, and the process is repeated to optimize model performance.\n\n## T\n\n### Tensor Parallelism (TP)\n\nParallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of",
  "having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets\nprocessed separately and in parallel on different GPUs and the results are synced at the end of the processing step.\nThis is what is sometimes called horizontal parallelism, as the splitting happens on horizontal level.\nLearn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism).\n\n### token\n\nA part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a\npunctuation symbol.\n\n### token Type IDs\n\nSome models' purpose is to do classification on pairs of sentences or question answering.\n\n<Youtube id=\"0u3ioSwev3s\"/>\n\nThese require two different sequences to be joined in a single \"input_ids\" entry, which usually is performed with the\nhelp of special tokens, such as the classifier (`[CLS]`) and separator (`[SEP]`) tokens. For example, the BERT model\nbuilds its two sequence input as such:\n\n```python\n>>> # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\n```\n\nWe can use our tokenizer to automatically generate such a sentence by passing the two sequences to `tokenizer` as two",
  "arguments (and not a list, like before) like this:\n\n```python\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n>>> sequence_a = \"HuggingFace is based in NYC\"\n>>> sequence_b = \"Where is HuggingFace based?\"\n\n>>> encoded_dict = tokenizer(sequence_a, sequence_b)\n>>> decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n```\n\nwhich will return:\n\n```python\n>>> print(decoded)\n[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n```\n\nThis is enough for some models to understand where one sequence ends and where another begins. However, other models,\nsuch as BERT, also deploy token type IDs (also called segment IDs). They are represented as a binary mask identifying\nthe two types of sequence in the model.\n\nThe tokenizer returns this mask as the \"token_type_ids\" entry:\n\n```python\n>>> encoded_dict[\"token_type_ids\"]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```\n\nThe first sequence, the \"context\" used for the question, has all its tokens represented by a `0`, whereas the second\nsequence, corresponding to the \"question\", has all its tokens represented by a `1`.",
  "Some models, like [`XLNetModel`] use an additional token represented by a `2`.\n\n### transfer learning\n\nA technique that involves taking a pretrained model and adapting it to a dataset specific to your task. Instead of training a model from scratch, you can leverage knowledge obtained from an existing model as a starting point. This speeds up the learning process and reduces the amount of training data needed.\n\n### transformer\n\nSelf-attention based deep learning model architecture.\n\n## U\n\n### unsupervised learning\n\nA form of model training in which data provided to the model is not labeled. Unsupervised learning techniques leverage statistical information of the data distribution to find patterns useful for the task at hand.\n\n## Z\n\n### Zero Redundancy Optimizer (ZeRO)\n\nParallelism technique which performs sharding of the tensors somewhat similar to [TensorParallel](#tensor-parallelism-tp),\nexcept the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn't need\nto be modified. This method also supports various offloading techniques to compensate for limited GPU memory.",
  "Learn more about ZeRO [here](perf_train_gpu_many#zero-data-parallelism).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLA\n\n[[open-in-colab]]\n\n[Accelerated Linear Algebra (XLA)](https://openxla.org/xla) is a linear algebra compiler that optimizes model runtime across different hardware and frameworks.\n\nThis guide will look specifically at how to accelerate *TensorFlow* models with XLA.\n\n## TensorFlow",
  "XLA can potentially accelerate a TensorFlow model without making any source code changes. It is already packaged with the TensorFlow library, and it is triggered with `jit_compile` in any graph creating function such as [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n\nIf you're using Keras methods like [fit](https://keras.io/api/models/model_training_apis/#fit-method) and [predict](https://keras.io/api/models/model_training_apis/#predict-method), enable XLA by passing `jit_compile=True` to [compile](https://keras.io/api/models/model_training_apis/#compile-method).\n\n```py\nmodel.compile(jit_compile=True)\n```\n\nXLA can be used to accelerate any arbitrary [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n\nModels with a TensorFlow implementation like [GPT2](./model_doc/gpt2), [T5](./model_doc/t5), [OPT](./model_doc/opt), and [Whisper](./model_doc/whisper) are XLA compatible. The speed up depends on a model, but in general, TensorFlow models in Transformers get a ~100x speed up.\n\n### Functions",
  "A typical forward pass in a TensorFlow model is shown below. To run a forward pass with XLA, wrap the model with [tf.function](https://www.tensorflow.org/api_docs/python/tf/function) and set `jit_compile=True`.\n\n```diff\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential(\n[tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n)\n# Generate random inputs for the model.\nbatch_size = 16\ninput_vector_dim = 10\nrandom_inputs = tf.random.normal((batch_size, input_vector_dim))\n\n# Run a forward pass.\n- _ = model(random_inputs)\n+ xla_fn = tf.function(model, jit_compile=True)\n+ _ = xla_fn(random_inputs)\n```\n\nThe default `call` function of the model is used to compile the XLA graph. But if there's any other model function you want to compile with XLA, wrap them with [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n\n```py\nmy_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n```\n\n### Text generation",
  "You could also compile other model functions with XLA. For example, enable XLA for text generation by wrapping [`~TFGenerationMixin.generate`] with [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n# Will error if the minimal version of Transformers is not installed.\nfrom transformers.utils import check_min_version\n\ncheck_min_version(\"4.21.0\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ninput_string = [\"TensorFlow is\"]\n\nxla_generate = tf.function(model.generate, jit_compile=True)\n\ntokenized_input = tokenizer(input_string, return_tensors=\"tf\")\ngenerated_tokens = xla_generate(**tokenized_input, num_beams=2)\n\ndecoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\nprint(f\"Generated -- {decoded_text}\")\n\"Generated -- TensorFlow is an open-source, open-source, distributed-source application framework for the\"\n```\n\n## Tracing",
  "When executing an XLA-enabled function for the first time, it tries to infer the computation graph in a process known as *tracing*. This is a time-consuming step, but any consecutive calls to the function will be much faster because it won't have to trace the computation graph again.\n\nTo ensure a function is only traced once, the inputs must have the same shape as when the graph was built. This usually isn't an issue for fixed input shapes like images, but it can be an issue for inputs with variable shapes like text.\n\nOne way to handle this is to pad your text so it always has the same shape. Configure padding options such as [pad_to_multiple_of](https://hf.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad.pad_to_multiple_of) in the tokenizer.\n\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ninput_string = [\"TensorFlow is\"]\n\nxla_generate = tf.function(model.generate, jit_compile=True)",
  "# Call tokenizer with padding options.\ntokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n\ngenerated_tokens = xla_generate(**tokenized_input, num_beams=2)\ndecoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\nprint(f\"Generated -- {decoded_text}\")\n```\n\nIn addition to the input shape, any changes to the generation options at any point also triggers tracing.\n\n## Resources\n\nLearn more about XLA with the following resources.\n\n- A [notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb) demonstrating XLA-compatible encoder-decoder and decoder-only text generation models.\n- The [Faster Text Generation with TensorFlow and XLA](https://hf.co/blog/tf-xla-generate) blog post compares benchmarks for XLA-compatible models and provides a friendly introduction to XLA in TensorFlow.\n- The [How Hugging Face improved Text Generation performance with XLA](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) blog post discusses the design philosophy behind adding XLA to TensorFlow models in Transformers.",
  "- The [Introduction to graphs and tf.function](https://www.tensorflow.org/guide/intro_to_graphs) guide.\n- The [Better performance with tf.function](https://www.tensorflow.org/guide/function) guide.\n- The [XLA](https://openxla.org/xla) documentation.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Optimizing inference\n\nInference with large language models (LLMs) can be challenging because they have to store and handle billions of parameters. To load a 70B parameter [Llama 2](https://hf.co/meta-llama/Llama-2-70b-hf) model, it requires 256GB of memory for full precision weights and 128GB of memory for half-precision weights. The most powerful GPUs today - the A100 and H100 - only have 80GB of memory.",
  "On top of the memory requirements, inference is slow because LLMs are called repeatedly to generate the next token. The input sequence increases as generation progresses, which takes longer and longer to process.\n\nThis guide will show you how to optimize LLM inference to accelerate generation and reduce memory usage.\n\n> [!TIP]\n> Try out [Text Generation Inference (TGI)](https://hf.co/docs/text-generation-inference), a Hugging Face library dedicated to deploying and serving highly optimized LLMs for inference.\n\n## Static kv-cache and torch.compile\n\nLLMs compute key-value (kv) values for each input token, and it performs the same kv computation each time because the generated output becomes part of the input. However, performing the same kv computation every time is not very efficient.\n\nA *kv-cache* stores the past keys and values instead of recomputing them each time. As a result, the kv-cache is dynamic and it grows with each generation step which prevents you from taking advantage of [torch.compile](./perf_torch_compile), a powerful optimization method that fuses PyTorch code into optimized kernels.",
  "The *static kv-cache* solves this issue by pre-allocating the kv-cache size to a maximum value, so you can combine it with [torch.compile](./perf_torch_compile) for up to a 4x speed up. Your speed up may vary depending on the model size (larger models have a smaller speed up) and hardware.\n\n> [!WARNING]\n> Follow this [issue](https://github.com/huggingface/transformers/issues/28981) to track which models (Llama, Gemma, Mistral, etc.) support a static kv-cache and torch.compile.\n\nDepending on your task, there are several ways you can use the static kv-cache.\n\n1. For basic use cases, set [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) to `\"static\"` (recommended).\n2. For multi-turn generation or a custom generation loop, initialize and handle [`StaticCache`] directly.\n3. For more unique hardware or use cases, it may be better to compile the entire [`~GenerationMixin.generate`] function into a single graph.\n\n> [!TIP]",
  "> Regardless of how you use the static kv-cache and torch.compile, left-pad your inputs with [pad_to_multiple_of](https://hf.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) to a limited set of values to avoid shape-related recompilations.\n\n<hfoptions id=\"static-kv\">\n<hfoption id=\"1. cache_implementation\">\n\n1. Set the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) to `\"static\"` in a models [`GenerationConfig`].\n2. Call [torch.compile](./perf_torch_compile) to compile the forward pass with the static kv-cache.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n\nmodel.generation_config.cache_implementation = \"static\"\n\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\ninput_text = \"The theory of special relativity states \"",
  "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['The theory of special relativity states 1. The speed of light is constant in all inertial reference']\n```\n\nUnder the hood, [`~GenerationMixin.generate`] attempts to reuse the same cache object to avoid recompilation at each call, which is critical to get the most out of [torch.compile](./perf_torch_compile). Be aware of the following to avoid triggering recompilation or if generation is slower than expected.\n\n1. If the batch size changes or the maximum output length increases between calls, the cache is reinitialized and recompiled.\n2. The first several calls of the compiled function are slower because it is being compiled.\n\n</hfoption>\n<hfoption id=\"2. StaticCache\">\n\nDirectly initialize a [`StaticCache`] object and pass it to the `past_key_values` parameter in [`~GenerationMixin.generate`]. The [`StaticCache`] keeps the cache contents, so you can pass it to a new [`~GenerationMixin.generate`] call to continue generation, similar to a dynamic cache.\n\n```py",
  "from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\nimport torch\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\ninput_text = \"The theory of special relativity states \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\nprompt_length = input_ids.input_ids.shape[1]\nmodel.generation_config.max_new_tokens = 16\n\npast_key_values = StaticCache(\nconfig=model.config,\nbatch_size=1,\n# If you plan to reuse the cache, make sure the cache length is large enough for all cases\nmax_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\ndevice=model.device,\ndtype=model.dtype\n)\noutputs = model.generate(**input_ids, past_key_values=past_key_values)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2']",
  "# pass in the generated text and the same cache object to continue generation from where it left off. Optionally, in a\n# multi-turn conversation, append the new user input to the generated text.\nnew_input_ids = outputs\noutputs = model.generate(new_input_ids, past_key_values=past_key_values)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2. The speed of light is constant in all inertial reference frames. 3.']\n```\n\n> [!TIP]\n> To reuse [`StaticCache`] on a new prompt, use [`~StaticCache.reset`] to reset the cache contents between calls.\n\nAnother option for using [`StaticCache`] is to pass it to a models forward pass using the same `past_key_values` argument. This allows you to write your own custom decoding function to decode the next token given the current token, position, and cache position of previously generated tokens.\n\n```py\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\nfrom transformers.testing_utils import CaptureLogger\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n\nprompts = [",
  "\"Simply put, the theory of relativity states that \",\n\"My favorite all time favorite condiment is ketchup.\",\n]\n\nNUM_TOKENS_TO_GENERATE = 40\ntorch_device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n\ntokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token=\"</s>\", padding_side=\"right\")\nmodel = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"sequential\")\ninputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n\ndef decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_values):\nlogits = model(\ncur_token,\nposition_ids=input_pos,\ncache_position=cache_position,\npast_key_values=past_key_values,\nreturn_dict=False,\nuse_cache=True\n)[0]\nnew_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\nreturn new_token\n```\n\nTo enable static kv-cache and [torch.compile](./perf_torch_compile) with [`StaticCache`], follow the steps below.\n\n1. Initialize [`StaticCache`] before using the model for inference to configure parameters like the maximum batch size and sequence length.",
  "2. Call [torch.compile](./perf_torch_compile) on the model to compile the forward pass with the static kv-cache.\n3. se SDPBackend.MATH in the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more.\n\n```py\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\nbatch_size, seq_length = inputs[\"input_ids\"].shape\nwith torch.no_grad():\npast_key_values = StaticCache(\nconfig=model.config, batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype\n)\ncache_position = torch.arange(seq_length, device=torch_device)\ngenerated_ids = torch.zeros(\nbatch_size, seq_length + NUM_TOKENS_TO_GENERATE + 1, dtype=torch.int, device=torch_device\n)\ngenerated_ids[:, cache_position] = inputs[\"input_ids\"].to(torch_device).to(torch.int)\n\nlogits = model(\n**inputs, cache_position=cache_position, past_key_values=past_key_values,return_dict=False, use_cache=True\n)[0]\nnext_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\ngenerated_ids[:, seq_length] = next_token[:, 0]",
  "decode_one_tokens = torch.compile(decode_one_tokens, mode=\"reduce-overhead\", fullgraph=True)\ncache_position = torch.tensor([seq_length + 1], device=torch_device)\nfor _ in range(1, NUM_TOKENS_TO_GENERATE):\nwith sdpa_kernel(SDPBackend.MATH):\nnext_token = decode_one_tokens(model, next_token.clone(), None, cache_position, past_key_values)\ngenerated_ids[:, cache_position] = next_token.int()\ncache_position += 1\n\ntext = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\ntext\n['Simply put, the theory of relativity states that 1) the speed of light is constant, 2) the speed of light is the same for all observers, and 3) the laws of physics are the same for all observers.',\n'My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my p']\n```\n\n</hfoption>\n<hfoption id=\"3. compile entire generate function\">",
  "Compiling the entire [`~GenerationMixin.generate`] function also compiles the input preparation logit processor operations, and more, in addition to the forward pass. With this approach, you don't need to initialize [`StaticCache`] or set the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n\nmodel.generate = torch.compile(model.generate, mode=\"reduce-overhead\", fullgraph=True)\ninput_text = \"The theory of special relativity states \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['The theory of special relativity states 1. The speed of light is constant in all inertial reference']\n```",
  "This usage pattern is more appropriate for unique hardware or use cases, but there are several drawbacks to consider.\n\n1. Compilation is much slower.\n2. Parameters must be configured through [`GenerationConfig`].\n3. Many warnings and exceptions are suppressed. We recommend testing the uncompiled model first.\n4. Many features are unavailable at the moment. For example, generation does not stop if an `EOS` token is selected.\n\n</hfoption>\n</hfoptions>\n\n## Decoding strategies\n\nDecoding can also be optimized to accelerate generation. You can use a lightweight assistant model to generate candidate tokens faster than the LLM itself or you can use a variant of this decoding strategy that works especially well for input-grounded tasks.\n\n### Speculative decoding\n\n> [!TIP]\n> For a more in-depth explanation, take a look at the [Assisted Generation: a new direction toward low-latency text generation](https://hf.co/blog/assisted-generation) blog post!",
  "For each input token, the model weights are loaded each time during the forward pass, which is slow and cumbersome when a model has billions of parameters. Speculative decoding alleviates this slowdown by using a second smaller and faster assistant model to generate candidate tokens that are verified by the larger model in a single forward pass. If the verified tokens are correct, the LLM essentially gets them for \"free\" without having to generate them itself. There is no degradation in accuracy because the verification forward pass ensures the same outputs are generated as if the LLM had generated them on its own.\n\nTo get the largest speed up, the assistant model should be a lot smaller than the LLM so that it can generate tokens quickly. The assistant and LLM model must also share the same tokenizer to avoid re-encoding and decoding tokens.\n\n> [!WARNING]\n> Speculative decoding is only supported for the greedy search and sampling decoding strategies, and it doesn't support batched inputs.\n\nEnable speculative decoding by loading an assistant model and passing it to [`~GenerationMixin.generate`].\n\n<hfoptions id=\"spec-decoding\">\n<hfoption id=\"greedy search\">\n\n```py",
  "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\ninputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\nassistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\noutputs = model.generate(**inputs, assistant_model=assistant_model)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n[\"Einstein's theory of relativity states that the speed of light is constant.    \"]\n```\n\n</hfoption>\n<hfoption id=\"sampling\">\n\nFor speculative sampling decoding, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n\n```py",
  "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\ninputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\nassistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\noutputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n[\"Einstein's theory of relativity states that motion in the universe is not a straight line.\\n\"]\n```\n\n</hfoption>\n</hfoptions>\n\n### Prompt lookup decoding",
  "Prompt lookup decoding is a variant of speculative decoding that is also compatible with greedy search and sampling. Prompt lookup works especially well for input-grounded tasks - such as summarization - where there is often overlapping words between the prompt and output. These overlapping n-grams are used as the LLM candidate tokens.\n\nTo enable prompt lookup decoding, specify the number of tokens that should be overlapping in the [prompt_lookup_num_tokens](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.prompt_lookup_num_tokens) parameter. Then pass this parameter to [`~GenerationMixin.generate`].\n\n<hfoptions id=\"pld\">\n<hfoption id=\"greedy decoding\">\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\ninputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)",
  "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\nassistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=3)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['The second law of thermodynamics states that entropy increases with temperature.      ']\n```\n\n</hfoption>\n<hfoption id=\"sampling\">\n\nFor prompt lookup decoding with sampling, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\ninputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)",
  "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=3, do_sample=True, temperature=0.7)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n[\"The second law of thermodynamics states that energy cannot be created nor destroyed. It's not a\"]\n```\n\n</hfoption>\n</hfoptions>\n\n## Attention\n\nA known issue with transformer models is that the self-attention mechanism grows quadratically in compute and memory with the number of input tokens. This limitation is only magnified in LLMs which handles much longer sequences. To address this, try FlashAttention2 or PyTorch's scaled dot product attention (SDPA), which are more memory efficient attention implementations.\n\n### FlashAttention-2",
  "FlashAttention and [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to the GPU memory to speed up inference. FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead.\n\nTo use FlashAttention-2, set [attn_implementation](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.attn_implementation) to `\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2b\",\nquantization_config=quant_config,\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\n)\n```\n\n### PyTorch scaled dot product attention",
  "Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch's C++ implementation. SDPA chooses the most performant attention algorithm if you're using a CUDA backend. For other backends, SDPA defaults to the PyTorch C++ implementation.\n\n> [!TIP]\n> SDPA automaticallysupports FlashAttention-2 as long as you have the latest PyTorch version installed.\n\nUse the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to explicitly enable or disable any of the four attention algorithms. For example, use `SDPBackend.FLASH_ATTENTION` to enable FlashAttention.\n\n```py\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2b\",\ntorch_dtype=torch.bfloat16,\n)\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\noutputs = model.generate(**inputs)\n```\n\n## Quantization",
  "Quantization reduces the size of model weights by storing them in a lower precision. This translates to lower memory usage and makes loading LLMs for inference more accessible if you're constrained by GPU memory.\n\nIf you aren't limited by your GPU, you don't necessarily need to quantize your model because it can increase latency slightly (except for AWQ and fused AWQ modules) due to the extra step required to quantize and dequantize the weights.\n\n> [!TIP]\n> There are many quantization libraries (see the [Quantization](./quantization) guide for more details) available, such as Quanto, AQLM, VPTQ, AWQ, and AutoGPTQ. Feel free to try them out and see which one works best for your use case. We also recommend reading the [Overview of natively supported quantization schemes in 🤗 Transformers](https://hf.co/blog/overview-quantization-transformers) blog post which compares AutoGPTQ and bitsandbytes.\n\nUse the Model Memory Calculator below to estimate and compare how much memory is required to load a model. For example, try estimating the memory required to load [Mistral-7B-v0.1](https://hf.co/mistralai/Mistral-7B-v0.1).\n\n<iframe\nsrc=\"https://hf-accelerate-model-memory-usage.hf.space\"",
  "frameborder=\"0\"\nwidth=\"850\"\nheight=\"450\"\n></iframe>\n\nTo load a model in half-precision, set the [torch_dtype](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.torch_dtype) parameter in [`~transformers.AutoModelForCausalLM.from_pretrained`] to `torch.bfloat16`. This requires 13.74GB of memory.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\",\n)\n```\n\nTo load a quantized model (8-bit or 4-bit), try [bitsandbytes](https://hf.co/docs/bitsandbytes) and set the [load_in_4bit](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.BitsAndBytesConfig.load_in_4bit) or [load_in_8bit](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.BitsAndBytesConfig.load_in_8bit) parameters to `True`. Loading the model in 8-bits only requires 6.87 GB of memory.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)",
  "model = AutoModelForCausalLM.from_pretrained(\n\"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\n)\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Text generation\n\n[[open-in-colab]]\n\nText generation is the most popular application for large language models (LLMs). A LLM is trained to generate the next word (token) given some initial text (prompt) along with its own generated outputs up to a predefined length or when it reaches an end-of-sequence (`EOS`) token.",
  "In Transformers, the [`~GenerationMixin.generate`] API handles text generation, and it is available for all models with generative capabilities.\n\nThis guide will show you the basics of text generation with [`~GenerationMixin.generate`] and some common pitfalls to avoid.\n\n## Default generate\n\nBefore you begin, it's helpful to install [bitsandbytes](https://hf.co/docs/bitsandbytes/index) to quantize really large models to reduce their memory usage.\n\n```bash\n!pip install -U transformers bitsandbytes\n```\nBitsandbytes supports multiple backends in addition to CUDA-based GPUs. Refer to the multi-backend installation [guide](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend) to learn more.\n\nLoad a LLM with [`~PreTrainedModel.from_pretrained`] and add the following two parameters to reduce the memory requirements.\n\n- `device_map=\"auto\"` enables Accelerates' [Big Model Inference](./models#big-model-inference) feature for automatically initiating the model skeleton and loading and dispatching the model weights across all available devices, starting with the fastest device (GPU).",
  "- `quantization_config` is a configuration object that defines the quantization settings. This examples uses bitsandbytes as the quantization backend (see the [Quantization](./quantization/overview) section for more available backends) and it loads the model in [4-bits](./quantization/bitsandbytes).\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=quantization_config)\n```\n\nTokenize your input, and set the [`~PreTrainedTokenizer.padding_side`] parameter to `\"left\"` because a LLM is not trained to continue generation from padding tokens. The tokenizer returns the input ids and attention mask.\n\n> [!TIP]\n> Process more than one prompt at a time by passing a list of strings to the tokenizer. Batch the inputs to improve throughput at a small cost to latency and memory.\n\n```py\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\nmodel_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n```",
  "Pass the inputs to [`~GenerationMixin.generate`] to generate tokens, and [`~PreTrainedTokenizer.batch_decode`] the generated tokens back to text.\n\n```py\ngenerated_ids = model.generate(**model_inputs)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\"A list of colors: red, blue, green, yellow, orange, purple, pink,\"\n```\n\n## Generation configuration\n\nAll generation settings are contained in [`GenerationConfig`]. In the example above, the generation settings are derived from the `generation_config.json` file of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1). A default decoding strategy is used when no configuration is saved with a model.\n\nInspect the configuration through the `generation_config` attribute. It only shows values that are different from the default configuration, in this case, the `bos_token_id` and `eos_token_id`.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")\nmodel.generation_config\nGenerationConfig {\n\"bos_token_id\": 1,\n\"eos_token_id\": 2\n}\n```",
  "You can customize [`~GenerationMixin.generate`] by overriding the parameters and values in [`GenerationConfig`]. Some of the most commonly adjusted parameters are [max_new_tokens](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens), [num_beams](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.num_beams), [do_sample](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.do_sample), and [num_return_sequences](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences).\n\n```py\n# enable beam search sampling strategy\nmodel.generate(**inputs, num_beams=4, do_sample=True)\n```",
  "[`~GenerationMixin.generate`] can also be extended with external libraries or custom code. The `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manipulating the next token probability distribution. `stopping_criteria` supports custom [`StoppingCriteria`] to stop text generation. Check out the [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo) for more examples of external [`~GenerationMixin.generate`]-compatible extensions.\n\nRefer to the [Generation strategies](./generation_strategies) guide to learn more about search, sampling, and decoding strategies.\n\n### Saving\n\nCreate an instance of [`GenerationConfig`] and specify the decoding parameters you want.\n\n```py\nfrom transformers import AutoModelForCausalLM, GenerationConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\"my_account/my_model\")\ngeneration_config = GenerationConfig(\nmax_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n)\n```\n\nUse [`~GenerationConfig.save_pretrained`] to save a specific generation configuration and set the `push_to_hub` parameter to `True` to upload it to the Hub.\n\n```py",
  "generation_config.save_pretrained(\"my_account/my_model\", push_to_hub=True)\n```\n\nLeave the `config_file_name` parameter empty. This parameter should be used when storing multiple generation configurations in a single directory. It gives you a way to specify which generation configuration to load. You can create different configurations for different generative tasks (creative text generation with sampling, summarization with beam search) for use with a single model.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n\ntranslation_generation_config = GenerationConfig(\nnum_beams=4,\nearly_stopping=True,\ndecoder_start_token_id=0,\neos_token_id=model.config.eos_token_id,\npad_token=model.config.pad_token_id,\n)\n\ntranslation_generation_config.save_pretrained(\"/tmp\", config_file_name=\"translation_generation_config.json\", push_to_hub=True)\n\ngeneration_config = GenerationConfig.from_pretrained(\"/tmp\", config_file_name=\"translation_generation_config.json\")",
  "inputs = tokenizer(\"translate English to French: Configuration files are easy to use!\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n```\n\n## Pitfalls\n\nThe section below covers some common issues you may encounter during text generation and how to solve them.\n\n### Output length\n\n[`~GenerationMixin.generate`] returns up to 20 tokens by default unless otherwise specified in a models [`GenerationConfig`]. It is highly recommended to manually set the number of generated tokens with the [`max_new_tokens`] parameter to control the output length. [Decoder-only](https://hf.co/learn/nlp-course/chapter1/6?fw=pt) models returns the initial prompt along with the generated tokens.\n\n```py\nmodel_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n```\n\n<hfoptions id=\"output-length\">\n<hfoption id=\"default length\">\n\n```py\ngenerated_ids = model.generate(**model_inputs)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'A sequence of numbers: 1, 2, 3, 4, 5'\n```\n\n</hfoption>\n<hfoption id=\"max_new_tokens\">\n\n```py",
  "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'\n```\n\n</hfoption>\n</hfoptions>\n\n### Decoding strategy\n\nThe default decoding strategy in [`~GenerationMixin.generate`] is *greedy search*, which selects the next most likely token, unless otherwise specified in a models [`GenerationConfig`]. While this decoding strategy works well for input-grounded tasks (transcription, translation), it is not optimal for more creative use cases (story writing, chat applications).\n\nFor example, enable a [multinomial sampling](./generation_strategies#multinomial-sampling) strategy to generate more diverse outputs. Refer to the [Generation strategy](./generation_strategies) guide for more decoding strategies.\n\n```py\nmodel_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n```\n\n<hfoptions id=\"decoding\">\n<hfoption id=\"greedy search\">\n\n```py\ngenerated_ids = model.generate(**model_inputs)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n</hfoption>\n<hfoption id=\"multinomial sampling\">\n\n```py",
  "generated_ids = model.generate(**model_inputs, do_sample=True)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n</hfoption>\n</hfoptions>\n\n### Padding side\n\nInputs need to be padded if they don't have the same length. But LLMs aren't trained to continue generation from padding tokens, which means the [`~PreTrainedTokenizer.padding_side`] parameter needs to be set to the left of the input.\n\n<hfoptions id=\"padding\">\n<hfoption id=\"right pad\">\n\n```py\nmodel_inputs = tokenizer(\n[\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n).to(\"cuda\")\ngenerated_ids = model.generate(**model_inputs)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'1, 2, 33333333333'\n```\n\n</hfoption>\n<hfoption id=\"left pad\">\n\n```py\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel_inputs = tokenizer(\n[\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n).to(\"cuda\")\ngenerated_ids = model.generate(**model_inputs)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n'1, 2, 3, 4, 5, 6,'\n```\n\n</hfoption>\n</hfoptions>\n\n### Prompt format",
  "Some models and tasks expect a certain input prompt format, and if the format is incorrect, the model returns a suboptimal output. You can learn more about prompting in the [prompt engineering](./tasks/prompting) guide.\n\nFor example, a chat model expects the input as a [chat template](./chat_templating). Your prompt should include a `role` and `content` to indicate who is participating in the conversation. If you try to pass your prompt as a single string, the model doesn't always return the expected output.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n)\n```\n\n<hfoptions id=\"format\">\n<hfoption id=\"no format\">\n\n```py\nprompt = \"\"\"How many cats does it take to change a light bulb? Reply as a pirate.\"\"\"\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\ninput_length = model_inputs.input_ids.shape[1]\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=50)\nprint(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])",
  "\"Aye, matey! 'Tis a simple task for a cat with a keen eye and nimble paws. First, the cat will climb up the ladder, carefully avoiding the rickety rungs. Then, with\"\n```\n\n</hfoption>\n<hfoption id=\"chat template\">\n\n```py\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n},\n{\"role\": \"user\", \"content\": \"How many cats does it take to change a light bulb?\"},\n]\nmodel_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\ninput_length = model_inputs.shape[1]\ngenerated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\nprint(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n\"Arr, matey! According to me beliefs, 'twas always one cat to hold the ladder and another to climb up it an’ change the light bulb, but if yer looking to save some catnip, maybe yer can\n```\n\n</hfoption>\n</hfoptions>\n\n## Resources\n\nTake a look below for some more specific and specialized text generation libraries.",
  "- [Optimum](https://github.com/huggingface/optimum): an extension of Transformers focused on optimizing training and inference on specific hardware devices\n- [Outlines](https://github.com/dottxt-ai/outlines): a library for constrained text generation (generate JSON files for example).\n- [SynCode](https://github.com/uiuc-focal-lab/syncode): a library for context-free grammar guided generation (JSON, SQL, Python).\n- [Text Generation Inference](https://github.com/huggingface/text-generation-inference): a production-ready server for LLMs.\n- [Text generation web UI](https://github.com/oobabooga/text-generation-webui): a Gradio web UI for text generation.\n- [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo): additional logits processors for controlling text generation.",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TorchScript\n\n[TorchScript](https://pytorch.org/docs/stable/jit.html) serializes PyTorch models into programs that can be executed in non-Python processes. This is especially advantageous in production environments where Python may the most performant choice.\n\nTransformers can export a model to TorchScript by:\n\n1. creating dummy inputs to create a *trace* of the model to serialize to TorchScript",
  "2. enabling the `torchscript` parameter in either [`~PretrainedConfig.torchscript`] for a randomly initialized model or [`~PreTrainedModel.from_pretrained`] for a pretrained model\n\n## Dummy inputs\n\nThe dummy inputs are used in the forward pass, and as the input values are propagated through each layer, PyTorch tracks the different operations executed on each tensor. The recorded operations are used to create the model trace. Once it is recorded, it is serialized into a TorchScript program.\n\n```py\nfrom transformers import BertModel, BertTokenizer, BertConfig\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\n\nmasked_index = 8\ntokenized_text[masked_index] = \"[MASK]\"\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\nsegments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n\n# creating a dummy input\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])\ndummy_input = [tokens_tensor, segments_tensors]\n```",
  "The trace is created based on the provided inputs dimensions and it can only handle inputs with the same shape as the provided input during tracing. An input with a different size raises the error message shown below.\n\n```bash\n`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`.\n```\n\nTry to create a trace with a dummy input size at least as large as the largest expected input during inference. Padding can help fill missing values for larger inputs. It may be slower though since a larger input size requires more calculations. Be mindful of the total number of operations performed on each input and track the model performance when exporting models with variable sequence lengths.\n\n## Tied weights\n\nWeights between the `Embedding` and `Decoding` layers are tied in Transformers and TorchScript can't export models with tied weights. Instantiating a model with `torchscript=True`, separates the `Embedding` and `Decoding` layers and they aren't trained any further because it would throw the two layers out of sync which can lead to unexpected results.",
  "Models *without* a language model head don't have tied weights and can be safely exported without the `torchscript` parameter.\n\n<hfoptions id=\"torchscript\">\n<hfoption id=\"randomly initialized model\">\n\n```py\nconfig = BertConfig(\nvocab_size_or_config_json_file=32000,\nhidden_size=768,\nnum_hidden_layers=12,\nnum_attention_heads=12,\nintermediate_size=3072,\ntorchscript=True,\n)\n\nmodel = BertModel(config)\nmodel.eval()\n```\n\n</hfoption>\n<hfoption id=\"pretrained model\">\n\n```py\nmodel = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\nmodel.eval()\n```\n\n</hfoption>\n</hfoptions>\n\n## Export to TorchScript\n\nCreate the Torchscript program with [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html), and save with [torch.jit.save](https://pytorch.org/docs/stable/generated/torch.jit.save.html).\n\n```py\ntraced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\ntorch.jit.save(traced_model, \"traced_bert.pt\")\n```\n\nUse [torch.jit.load](https://pytorch.org/docs/stable/generated/torch.jit.load.html) to load the traced model.\n\n```py\nloaded_model = torch.jit.load(\"traced_bert.pt\")\nloaded_model.eval()",
  "all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n```\n\nTo use the traced model for inference, use the `__call__` dunder method.\n\n```py\ntraced_model(tokens_tensor, segments_tensors)\n```\n\n## Deploy to AWS\n\nTorchScript programs serialized from Transformers can be deployed on [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) instances. The instance is powered by AWS Inferentia chips, a custom hardware accelerator designed for deep learning inference workloads. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) supports tracing Transformers models for deployment on Inf1 instances.\n\n> [!TIP]\n> AWS Neuron requires a [Neuron SDK environment](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/inference-torch-neuron.html#inference-torch-neuron) which is preconfigured on [AWS DLAMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).",
  "Instead of [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html), use [torch.neuron.trace](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/api-compilation-python-api.html) to trace a model and optimize it for Inf1 instances.\n\n```py\nimport torch.neuron\n\ntorch.neuron.trace(model, [tokens_tensor, segments_tensors])\n```\n\nRefer to the [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) documentation for more information.\n\n### Model architectures\n\nBERT-based models - like [DistilBERT](./model_doc/distilbert) or [RoBERTa](./model_doc/roberta) - run best on Inf1 instances for non-generative tasks such as extractive question answering, and sequence or token classification.\n\nText generation can be adapted to run on an Inf1 instance as shown in the [Transformers MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html) tutorial.",
  "Refer to the [Inference Samples/Tutorials (Inf1)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/models/inference-inf1-samples.html#model-samples-inference-inf1) guide for more information about which models can be converted out of the box to run on Inf1 instances.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Customizing model components\n\nAnother way to customize a model is to modify their components, rather than writing a new model entirely, allowing you to tailor a model to your specific use case. For example, you can add new layers or optimize the attention mechanism of an architecture. Customizations are applied directly to a Transformers model so that you can continue to use features such as [`Trainer`], [`PreTrainedModel`], and the [PEFT](https://huggingface.co/docs/peft/en/index) library.",
  "This guide will show you how to customize a models attention mechanism in order to apply [Low-Rank Adaptation (LoRA)](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora) to it.\n\n> [!TIP]\n> The [clear_import_cache](https://github.com/huggingface/transformers/blob/9985d06add07a4cc691dc54a7e34f54205c04d40/src/transformers/utils/import_utils.py#L2286) utility is very useful when you're iteratively modifying and developing model code. It removes all cached Transformers modules and allows Python to reload the modified code without constantly restarting your environment.\n>\n> ```py\n> from transformers import AutoModel\n> from transformers.utils.import_utils import clear_import_cache\n>\n> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n> # modifications to model code\n> # clear cache to reload modified code\n> clear_import_cache()\n> # re-import to use updated code\n> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n> ```\n\n## Attention class",
  "[Segment Anything](./model_doc/sam) is an image segmentation model, and it combines the query-key-value (`qkv`) projection in its attention mechanisms. To reduce the number of trainable parameters and computational overhead, you can apply LoRA to the `qkv` projection. This requires splitting the `qkv` projection so that you can separately target the `q` and `v` with LoRA.\n\n1. Create a custom attention class, `SamVisionAttentionSplit`, by subclassing the original `SamVisionAttention` class. In the `__init__`, delete the combined `qkv` and create a separate linear layer for `q`, `k` and `v`.\n\n```py\nimport torch\nimport torch.nn as nn\nfrom transformers.models.sam.modeling_sam import SamVisionAttention\n\nclass SamVisionAttentionSplit(SamVisionAttention, nn.Module):\ndef __init__(self, config, window_size):\nsuper().__init__(config, window_size)\n# remove combined qkv\ndel self.qkv\n# separate q, k, v projections\nself.q = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\nself.k = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\nself.v = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)",
  "self._register_load_state_dict_pre_hook(self.split_q_k_v_load_hook)\n```\n\n2. The `_split_qkv_load_hook` function splits the pretrained `qkv` weights into separate `q`, `k`, and `v` weights when loading the model to ensure compatibility with any pretrained model.\n\n```py\ndef split_q_k_v_load_hook(self, state_dict, prefix, *args):\nkeys_to_delete = []\nfor key in list(state_dict.keys()):\nif \"qkv.\" in key:\n# split q, k, v from the combined projection\nq, k, v = state_dict[key].chunk(3, dim=0)\n# replace with individual q, k, v projections\nstate_dict[key.replace(\"qkv.\", \"q.\")] = q\nstate_dict[key.replace(\"qkv.\", \"k.\")] = k\nstate_dict[key.replace(\"qkv.\", \"v.\")] = v\n# mark the old qkv key for deletion\nkeys_to_delete.append(key)\n\n# remove old qkv keys\nfor key in keys_to_delete:\ndel state_dict[key]\n```\n\n3. In the `forward` pass, `q`, `k`, and `v` are computed separately while the rest of the attention mechanism remains the same.\n\n```py\ndef forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\nbatch_size, height, width, _ = hidden_states.shape\nqkv_shapes = (batch_size *  self.num_attention_heads,  height * width, -1)",
  "query = self.q(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)\nkey = self.k(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)\nvalue = self.v(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)\n\nattn_weights = (query * self.scale) @ key.transpose(-2, -1)\n\nif self.use_rel_pos:\nattn_weights = self.add_decomposed_rel_pos(\nattn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n)\n\nattn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\nattn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\nattn_output = (attn_probs @ value).reshape(batch_size, self.num_attention_heads, height, width, -1)\nattn_output = attn_output.permute(0, 2, 3, 1, 4).reshape(batch_size, height, width, -1)\nattn_output = self.proj(attn_output)\n\nif output_attentions:\noutputs = (attn_output, attn_weights)\nelse:\noutputs = (attn_output, None)\nreturn outputs\n```",
  "Assign the custom `SamVisionAttentionSplit` class to the original models `SamVisionAttention` module to replace it. All instances of `SamVisionAttention` in the model is replaced with the split attention version.\n\nLoad the model with [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import SamModel\nfrom transformers.models.sam import modeling_sam\n\n# replace the attention class in the modeling_sam module\nmodeling_sam.SamVisionAttention = SamVisionAttentionSplit\n\n# load the pretrained SAM model\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n```\n\n## LoRA\n\nWith separate `q`, `k`, and `v` projections, apply LoRA to `q` and `v`.\n\nCreate a [LoraConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) and specify the rank `r`, `lora_alpha`, `lora_dropout`, `task_type`, and most importantly, the modules to target.\n\n```py\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\nr=16,\nlora_alpha=32,\n# apply LoRA to q and v\ntarget_modules=[\"q\", \"v\"],\nlora_dropout=0.1,\ntask_type=\"mask-generation\"\n)\n```",
  "Pass the model and [LoraConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) to [get_peft_model](https://huggingface.co/docs/peft/package_reference/peft_model#peft.get_peft_model) to apply LoRA to the model.\n\n```py\nmodel = get_peft_model(model, config)\n```\n\nCall [print_trainable_parameters](https://huggingface.co/docs/peft/package_reference/peft_model#peft.PeftMixedModel.print_trainable_parameters) to view the number of parameters you're training as a result versus the total number of parameters.\n\n```py\nmodel.print_trainable_parameters()\n\"trainable params: 608,256 || all params: 94,343,728 || trainable%: 0.6447\"\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Processors\n\nMultimodal models require a preprocessor capable of handling inputs that combine more than one modality. Depending on the input modality, a processor needs to convert text into an array of tensors, images into pixel values, and audio into an array with tensors with the correct sampling rate.",
  "For example, [PaliGemma](./model_doc/paligemma) is a vision-language model that uses the [SigLIP](./model_doc/siglip) image processor and the [Llama](./model_doc/llama) tokenizer. A [`ProcessorMixin`] class wraps both of these preprocessor types, providing a single and unified processor class for a multimodal model.\n\nCall [`~ProcessorMixin.from_pretrained`] to load a processor. Pass the input type to the processor to generate the expected model inputs, input ids and pixel values.\n\n```py\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\n\nprocessor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n\nprompt = \"answer en Where is the cow standing?\"\nurl = \"https://huggingface.co/gv-hf/PaliGemma-test-224px-hf/resolve/main/cow_beach_1.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninputs\n```\n\nThis guide describes the processor class and how to preprocess multimodal inputs.\n\n## Processor classes",
  "All processors inherit from the [`ProcessorMixin`] class which provides methods like [`~ProcessorMixin.from_pretrained`], [`~ProcessorMixin.save_pretrained`], and [`~ProcessorMixin.push_to_hub`] for loading, saving, and sharing processors to the Hub.\n\nThere are two ways to load a processor, with an [`AutoProcessor`] and with a model-specific processor class.\n\n<hfoptions id=\"processor-class\">\n<hfoption id=\"AutoProcessor\">\n\nThe [AutoClass](./model_doc/auto) API provides a simple interface to load processors without directly specifying the specific model class it belongs to.\n\nUse [`~AutoProcessor.from_pretrained`] to load a processor.\n\n```py\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n```\n\n</hfoption>\n<hfoption id=\"model-specific processor\">\n\nProcessors are also associated with a specific pretrained multimodal model class. You can load a processor directly from the model class with [`~ProcessorMixin.from_pretrained`].\n\n```py\nfrom transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n```",
  "You could also separately load the two preprocessor types, [`WhisperTokenizerFast`] and [`WhisperFeatureExtractor`].\n\n```py\nfrom transformers import WhisperTokenizerFast, WhisperFeatureExtractor, WhisperProcessor\n\ntokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-tiny\")\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\nprocessor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n```\n\n</hfoption>\n</hfoptions>\n\n## Preprocess\n\nProcessors preprocess multimodal inputs into the expected Transformers format. There are a couple combinations of input modalities that a processor can handle such as text and audio or text and image.\n\nAutomatic speech recognition (ASR) tasks require a processor that can handle text and audio inputs. Load a dataset and take a look at the `audio` and `text` columns (you can remove the other columns which aren't needed).\n\n```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"lj_speech\", split=\"train\")\ndataset = dataset.map(remove_columns=[\"file\", \"id\", \"normalized_text\"])\ndataset[0][\"audio\"]\n{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,",
  "7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',\n'sampling_rate': 22050}\n\ndataset[0][\"text\"]\n'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'\n```\n\nRemember to resample the sampling rate to match the pretrained models required sampling rate.\n\n```py\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n```\n\nLoad a processor and pass the audio `array` and `text` columns to it.\n\n```py\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"openai/whisper-tiny\")\n\ndef prepare_dataset(example):\naudio = example[\"audio\"]\nexample.update(processor(audio=audio[\"array\"], text=example[\"text\"], sampling_rate=16000))\nreturn example\n```\n\nApply the `prepare_dataset` function to preprocess the dataset. The processor returns `input_features` for the `audio` column and `labels` for the text column.\n\n```py\nprepare_dataset(dataset[0])\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TPU\n\nTPU (Tensor Processing Unit) is a type of hardware designed to accelerate tensor computations for training and inference. TPUs are generally accessed through Google cloud services, but smaller TPUs are also available for free from [Google Colab](https://colab.research.google.com/notebooks/tpu.ipynb) or [Kaggle](https://www.kaggle.com/docs/tpu).",
  "This guide focuses on training a Keras model for sequence classification on a TPU from Google Colab. Make sure the TPU runtime is enabled by going to **Runtime > Change runtime type** and selecting a TPU.\n\nRun the command below to install the latest version of Transformers and [Datasets](https://huggingface.co/docs/datasets).\n\n```py\n!pip install --U transformers datasets\n```\n\nCreate an instance of [tf.distribute.cluster_resolver.TPUClusterResolver](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver), and then connect to the remote cluster and initialize the TPUs.\n\n```py\nimport tensorflow as tf\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\n```\n\nThere are various distribution strategies for running your model on multiple TPUs. The [tpu.distribute.TPUStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) offers synchronized distributed training.\n\n```py\nstrategy = tf.distribute.TPUStrategy(resolver)\n```",
  "Load and tokenize a dataset - this example uses [CoLA](https://huggingface.co/datasets/nyu-mll/glue/viewer/cola) from the GLUE benchmark - and pad all samples to the maximum length so it is easier to load as an array and to avoid [XLA compilation issues](#xla).\n\n```py\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport numpy as np\n\ndataset = load_dataset(\"glue\", \"cola\")[\"train\"]\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n\ntrain_data = tokenizer(\ndataset[\"sentence\"],\npadding=\"max_length\",\ntruncation=True,\nmax_length=128,\nreturn_tensors=\"np\",\n)\ntrain_data = dict(train_data)\ntrain_labels = np.array(dataset[\"label\"])\n```\n\nThe model **must** be created inside [Strategy.scope](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#scope) in order to replicate the model layers on each TPU device.\n\n```py\nfrom transformers import TFAutoModelForSequenceClassification\n\nwith strategy.scope():\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nmodel.compile(optimizer=\"adam\")\n```",
  "TPUs only accept [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) inputs unlike the Keras [fit](https://keras.io/api/models/model_training_apis/#fit-method) method which accepts a broader range of inputs.\n\n```py\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\n\ntf_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\ntf_dataset = tf_dataset.shuffle(len(tf_dataset))\ntf_dataset = tf_dataset.batch(BATCH_SIZE, drop_remainder=True)\n```\n\nFinally, call [fit](https://keras.io/api/models/model_training_apis/#fit-method) to start training.\n\n```py\nmodel.fit(tf_dataset)\n```\n\n## Large datasets\n\nThe dataset created above pads every sample to the maximum length and loads the whole dataset into memory. This may not be possible if you're working with larger datasets. When training on large datasets, you may want to create a [tf.TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) or stream the data.\n\n### tf.TFRecord",
  "[tf.TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) is the standard [tf.data](https://www.tensorflow.org/guide/data) format for storing training data. For very large training jobs, it's worth preprocessing your data and storing it in the `tf.TFRecord` format and building a `tf.data` pipeline on top. Refer to the table below to help you decide whether `tf.TFRecord` is helpful for you.\n\n| pros | cons |\n|---|---|\n| works on all TPU instances | costs associated with cloud storage |\n| supports huge datasets and massive throughput | some data types (images) can take a lot of space to store |\n| suitable for training on entire TPU pods |  |\n| preprocessing is done in advance, maximizing training speed |  |\n\nPreprocess and tokenize the dataset before writing it to a `tf.TFRecord` to avoid writing every time the data is loaded.\n\nAn exception is made for *train-time augmentations*, because augmentations applied after writing to a `tf.TFRecord` results in the same augmentation for each epoch. Instead, apply augmentations in the `tf.data` pipeline that loads the data.\n\n> [!TIP]",
  "> In practice, you probably won't be able to load the entire dataset in memory. Load a chunk of the dataset at a time and convert it to `TFRecord`, and repeat until the entire dataset is in the `TFRecord` format. Then you can use a list of all the files to create a `TFRecordDataset`. The example below demonstrates a single file for simplicity.\n\n```py\ntokenized_data = tokenizer(\ndataset[\"sentence\"],\npadding=\"max_length\",\ntruncation=True,\nmax_length=128,\nreturn_tensors=\"np\",\n)\nlabels = dataset[\"label\"]\n\nwith tf.io.TFRecordWriter(\"dataset.tfrecords\") as file_writer:\nfor i in range(len(labels)):\nfeatures = {\n\"input_ids\": tf.train.Feature(\nint64_list=tf.train.Int64List(value=tokenized_data[\"input_ids\"][i])\n),\n\"attention_mask\": tf.train.Feature(\nint64_list=tf.train.Int64List(value=tokenized_data[\"attention_mask\"][i])\n),\n\"labels\": tf.train.Feature(\nint64_list=tf.train.Int64List(value=[labels[i]])\n),\n}\nfeatures = tf.train.Features(feature=features)\nexample = tf.train.Example(features=features)\nrecord_bytes = example.SerializeToString()\nfile_writer.write(record_bytes)\n```",
  "Build a [TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) using the saved filename to load it.\n\n```py\ndef decode_fn(sample):\nfeatures = {\n\"input_ids\": tf.io.FixedLenFeature((128,), dtype=tf.int64),\n\"attention_mask\": tf.io.FixedLenFeature((128,), dtype=tf.int64),\n\"labels\": tf.io.FixedLenFeature((1,), dtype=tf.int64),\n}\nreturn tf.io.parse_example(sample, features)\n\n# TFRecordDataset can handle gs:// paths\ntf_dataset = tf.data.TFRecordDataset([\"gs://matt-tf-tpu-tutorial-datasets/cola/dataset.tfrecords\"])\ntf_dataset = tf_dataset.map(decode_fn)\ntf_dataset = tf_dataset.shuffle(len(dataset)).batch(BATCH_SIZE, drop_remainder=True)\ntf_dataset = tf_dataset.apply(\ntf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n)\n```\n\nThe dataset can now be passed to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n\n```py\nmodel.fit(tf_dataset)\n```\n\n### Stream from raw data",
  "Data can be stored in its native format and preprocessed in a [tf.data](https://www.tensorflow.org/guide/data) pipeline as the data is loaded. This approach isn't supported for many models with complex tokenization schemes, but some models like BERT are supported because their tokenization can be compiled. Refer to the table below to help you decide whether this approach is helpful for you.\n\n| pros | cons |\n|---|---|\n| suitable for highly compressed big data in native format (images, audio) | requires writing a full preprocessing pipeline |\n| convenient if raw data is available in a public cloud bucket | complex preprocessing on-the-fly can hurt throughput |\n| works on all TPU instances if data is stored in Google Cloud | must place data in cloud storage if not already there |\n|  | not as suitable for text data because writing a tokenization pipeline is hard (use `TFRecord` for text) |\n\nThe example below demonstrates streaming data for an image model.\n\nLoad an image dataset and get a list of the underlying image file paths and labels.\n\n```py\nfrom datasets import load_dataset\n\nimage_dataset = load_dataset(\"beans\", split=\"train\")\nfilenames = image_dataset[\"image_file_path\"]",
  "labels = image_dataset[\"labels\"]\n```\n\nConvert the local filenames in the dataset into `gs://` paths in Google Cloud Storage.\n\n```py\n# strip everything but the category directory and filenames\nbase_filenames = ['/'.join(filename.split('/')[-2:]) for filename in filenames]\n# prepend the Google Cloud base path to everything instead\ngs_paths = [\"gs://matt-tf-tpu-tutorial-datasets/beans/\"+filename for filename in base_filenames]\n\n# create tf_dataset\ntf_dataset = tf.data.Dataset.from_tensor_slices(\n{\"filename\": gs_paths, \"labels\": labels}\n)\ntf_dataset = tf_dataset.shuffle(len(tf_dataset))\n```\n\nTransformers preprocessing classes like [`AutoImageProcessor`] are framework-agnostic and can't be compiled into a pipeline by `tf.data`. To get around this, get the normalization values (`mean` and `std`) from the [`AutoImageProcessor`] and use them in the `tf.data` pipeline.\n\n```py\nfrom transformers import AutoImageProcessor\n\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nimage_size = (processor.size[\"height\"], processor.size[\"width\"])\nimage_mean = processor.image_mean\nimage_std = processor.image_std\n```",
  "Use these normalization values to create a function to load and preprocess the images.\n\n```py\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\n\ndef decode_fn(sample):\nimage_data = tf.io.read_file(sample[\"filename\"])\nimage = tf.io.decode_jpeg(image_data, channels=3)\nimage = tf.image.resize(image, image_size)\narray = tf.cast(image, tf.float32)\narray /= 255.0\narray = (array - image_mean) / image_std\narray = tf.transpose(array, perm=[2, 0, 1])\nreturn {\"pixel_values\": array, \"labels\": sample[\"labels\"]}\n\ntf_dataset = tf_dataset.map(decode_fn)\ntf_dataset = tf_dataset.batch(BATCH_SIZE, drop_remainder=True)\nprint(tf_dataset.element_spec)\n```\n\nThe dataset can now be passed to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n\n```py\nfrom transformers import TFAutoModelForImageClassification\n\nwith strategy.scope():\nmodel = TFAutoModelForImageClassification.from_pretrained(image_model_checkpoint)\nmodel.compile(optimizer=\"adam\")\n\nmodel.fit(tf_dataset)\n```\n\n### Stream with prepare_tf_dataset",
  "[`~TFPreTrainedModel.prepare_tf_dataset`] creates a `tf.data` pipeline that loads samples from [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). The pipeline uses [tf.numpy_function]() or [`~datasets.Dataset.from_generator`], which can't be compiled by TensorFlow, to access the underlying `tf.data.Dataset`. It also won't work on a Colab TPU or TPU Nodes because the pipeline streams data from a local disk. Refer to the table below to help you decide whether this approach is helpful for you.\n\n| pros | cons |\n|---|---|\n| simple code | only works on TPU VM |\n| same approach on TPU/GPU | data must be available as a Hugging Face Dataset |\n| dataset doesn't have to fit in memory | data must fit on local storage |\n| supports variable padding | data loading may be a bottleneck on a big TPU pod slice |\n\n[`~TFPreTrainedModel.prepare_tf_dataset`] only works on [TPU VM](#tpu-types). Add the tokenizer output as columns in the dataset since the dataset is stored on disk, which means it can handle data larger than the available memory. Use [`~TFPreTrainedModel.prepare_tf_dataset`] to stream data from the dataset by wrapping it with a `tf.data` pipeline.\n\n```py",
  "def tokenize_function(examples):\nreturn tokenizer(\nexamples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128\n)\n# add the tokenizer output to the dataset as new columns\ndataset = dataset.map(tokenize_function)\n\n# prepare_tf_dataset() chooses columns that match the models input names\ntf_dataset = model.prepare_tf_dataset(\ndataset, batch_size=BATCH_SIZE, shuffle=True, tokenizer=tokenizer\n)\n```\n\nThe dataset can now be passed to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\nwith strategy.scope():\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nmodel.compile(optimizer=\"adam\")\n\nmodel.fit(tf_dataset)\n```\n\n## TPU types\n\nThere are two types of TPUs, a TPU Node and a TPU VM.\n\nA TPU Node indirectly accesses a remote TPU. It requires a separate VM to initialize your network and data pipeline, and then forwards it to the remote node. Google Colab TPUs are an example of a TPU Node. You can't use local data because the TPU is remotely located, and data must be stored in Google Cloud Storage where the data pipeline can access it.",
  "TPU VM are connected directly to the machine the TPU is located on, and they are generally easier to work with, especially when it comes to your data pipeline.\n\n> [!TIP]\n> We recommend avoiding TPU Nodes if possible because it is more difficult to debug than TPU VMs. TPU Nodes may also be unsupported in the future and become a legacy access method.\n\nA single TPU (v2-8, v3-8, v4-8) runs 8 replicas. TPUs can exist in **pods** which run hundreds or even thousands of replicas simultaneously. When you only use a portion of a pod, it is referred to as a **pod slice**. On Google Colab, you'll typically get a single v2-8 TPU.\n\n## XLA\n\n[XLA](https://openxla.org/xla) is a linear algebra compiler for high-performance execution and it is used by default to improve performance on TPUs.\n\nBefore executing your code on a TPU, it's a good idea to try it first on a CPU or GPU because it is easier to debug. You can train for a few steps to make sure the model and data pipeline work as expected. Set `jit_compile=True` in the [compile](https://keras.io/api/models/model_training_apis/#compile-method) method to enable XLA compilation (but remember to remove this line of code before running on a TPU).",
  "The section below outlines three rules for making your code XLA-compatible. Transformers enforce the first two rules for models and loss functions by default, but don't forget about them if you're writing your own models and loss functions.\n\n### Data dependent conditionals\n\nAny `if` statements cannot depend on values inside a [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor). The code below can't be compiled by XLA.\n\n```py\nif tf.reduce_sum(tensor) > 10:\ntensor = tensor / 2.0\n```\n\nTo compile with XLA, use [tf.cond](https://www.tensorflow.org/api_docs/python/tf/cond) or remove the conditional and use indicator variables instead as shown below.\n\n```py\nsum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\ntensor = tensor / (1.0 + sum_over_10)\n```\n\n### Data dependent shapes",
  "The shape of a [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) cannot depend on their values. For example, [tf.unique](https://www.tensorflow.org/api_docs/python/tf/unique) can't be compiled because it returns a tensor containing an instance of each unique value in the input. The shape of this output depends on how repetitive the input [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) is.\n\nThis is an issue during **label masking**, where labels are set to a negative value to indicate they should be ignored when computing the loss. The code below can't be compiled by XLA because the shape of `masked_outputs` and `masked_labels` depend on how many positions are masked.\n\n```py\nlabel_mask = labels >= 0\nmasked_outputs = outputs[label_mask]\nmasked_labels = labels[label_mask]\nloss = compute_loss(masked_outputs, masked_labels)\nmean_loss = torch.mean(loss)\n```\n\nTo compile with XLA, avoid the data-dependent shapes by computing the loss for every position and zeroing out the masked positions in both the numerator and denominator when calculating the mean. Convert `tf.bool` to `tf.float32` as an indicator variable to make your code XLA-compatible.\n\n```py",
  "label_mask = tf.cast(labels >= 0, tf.float32)\nloss = compute_loss(outputs, labels)\nloss = loss * label_mask\nmean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)\n```\n\n### Recompile different input shapes\n\nXLA recompiles your model if input shapes are variable which create huge performance problems. It is especially common in text models because input texts have variable lengths after tokenization.\n\n> [!WARNING]\n> Execessive padding can also severely slow down training because requires more compute and memory to process.\n\nTo avoid different shapes, use padding to pad all your inputs to the same length and use an `attention_mask`. Try padding batches of samples to a multiple of 32 or 64 tokens. Use the parameters `padding=\"max_length\"`, `padding=\"longest\"`, or `pad_to_multiple_of` to help with padding. This often increases the number of tokens by a small amount, but it significantly reduces the number of unique input shapes because every input shape is a multiple of 32 or 64. Fewer unique input shapes requires fewer recompilation.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Quickstart\n\n[[open-in-colab]]\n\nTransformers is designed to be fast and easy to use so that everyone can start learning or building with transformer models.\n\nThe number of user-facing abstractions is limited to only three classes for instantiating a model, and two APIs for inference or training. This quickstart introduces you to Transformers' key features and shows you how to:\n\n- load a pretrained model\n- run inference with [`Pipeline`]",
  "- fine-tune a model with [`Trainer`]\n\n## Set up\n\nTo start, we recommend creating a Hugging Face [account](https://hf.co/join). An account lets you host and access version controlled models, datasets, and [Spaces](https://hf.co/spaces) on the Hugging Face [Hub](https://hf.co/docs/hub/index), a collaborative platform for discovery and building.\n\nCreate a [User Access Token](https://hf.co/docs/hub/security-tokens#user-access-tokens) and log in to your account.\n\n```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\nInstall a machine learning framework.\n\n<hfoptions id=\"installation\">\n<hfoption id=\"PyTorch\">\n\n```bash\n!pip install torch\n```\n\n</hfoption>\n<hfoption id=\"TensorFlow\">\n\n```bash\n!pip install tensorflow\n```\n\n</hfoption>\n</hfoptions>\n\nThen install an up-to-date version of Transformers and some additional libraries from the Hugging Face ecosystem for accessing datasets and vision models, evaluating training, and optimizing training for large models.\n\n```bash\n!pip install -U transformers datasets evaluate accelerate timm\n```\n\n## Pretrained models\n\nEach pretrained model inherits from three base classes.\n\n| **Class** | **Description** |\n|---|---|",
  "| [`PretrainedConfig`] | A file that specifies a models attributes such as the number of attention heads or vocabulary size. |\n| [`PreTrainedModel`] | A model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states. For a specific task, use the appropriate model head to convert the raw hidden states into a meaningful result (for example, [`LlamaModel`] versus [`LlamaForCausalLM`]). |\n| Preprocessor | A class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model. For example, [`PreTrainedTokenizer`] converts text into tensors and [`ImageProcessingMixin`] converts pixels into tensors. |\n\nWe recommend using the [AutoClass](./model_doc/auto) API to load models and preprocessors because it automatically infers the appropriate architecture for each task and machine learning framework based on the name or path to the pretrained weights and configuration file.\n\nUse [`~PreTrainedModel.from_pretrained`] to load the weights and configuration file from the Hub into the model and preprocessor class.\n\n<hfoptions id=\"base-classes\">\n<hfoption id=\"PyTorch\">",
  "When you load a model, configure the following parameters to ensure the model is optimally loaded.\n\n- `device_map=\"auto\"` automatically allocates the model weights to your fastest device first, which is typically the GPU.\n- `torch_dtype=\"auto\"` directly initializes the model weights in the data type they're stored in, which can help avoid loading the weights twice (PyTorch loads weights in `torch.float32` by default).\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=\"auto\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n```\n\nTokenize the text and return PyTorch tensors with the tokenizer. Move the model to a GPU if it's available to accelerate inference.\n\n```py\nmodel_inputs = tokenizer([\"The secret to baking a good cake is \"], return_tensors=\"pt\").to(\"cuda\")\n```\n\nThe model is now ready for inference or training.\n\nFor inference, pass the tokenized inputs to [`~GenerationMixin.generate`] to generate text. Decode the token ids back into text with [`~PreTrainedTokenizerBase.batch_decode`].\n\n```py",
  "generated_ids = model.generate(**model_inputs, max_length=30)\ntokenizer.batch_decode(generated_ids)[0]\n'<s> The secret to baking a good cake is 100% in the preparation. There are so many recipes out there,'\n```\n\n</hfoption>\n<hfoption id=\"TensorFlow\">\n\n```py\nfrom transformers import TFAutoModelForCausalLM, AutoTokenizer\n\nmodel = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\")\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n```\n\nTokenize the text and return TensorFlow tensors with the tokenizer.\n\n```py\nmodel_inputs = tokenizer([\"The secret to baking a good cake is \"], return_tensors=\"tf\")\n```\n\nThe model is now ready for inference or training.\n\nFor inference, pass the tokenized inputs to [`~GenerationMixin.generate`] to generate text. Decode the token ids back into text with [`~PreTrainedTokenizerBase.batch_decode`].\n\n```py\ngenerated_ids = model.generate(**model_inputs, max_length=30)\ntokenizer.batch_decode(generated_ids)[0]\n'The secret to baking a good cake is \\xa0to use the right ingredients. \\xa0The secret to baking a good cake is to use the right'\n```\n\n</hfoption>\n</hfoptions>\n\n> [!TIP]",
  "> Skip ahead to the [Trainer](#trainer-api) section to learn how to fine-tune a model.\n\n## Pipeline\n\nThe [`Pipeline`] class is the most convenient way to inference with a pretrained model. It supports many tasks such as text generation, image segmentation, automatic speech recognition, document question answering, and more.\n\n> [!TIP]\n> Refer to the [Pipeline](./main_classes/pipelines) API reference for a complete list of available tasks.\n\nCreate a [`Pipeline`] object and select a task. By default, [`Pipeline`] downloads and caches a default pretrained model for a given task. Pass the model name to the `model` parameter to choose a specific model.\n\n<hfoptions id=\"pipeline-tasks\">\n<hfoption id=\"text generation\">\n\nSet `device=\"cuda\"` to accelerate inference with a GPU.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=\"cuda\")\n```\n\nPrompt [`Pipeline`] with some initial text to generate more text.\n\n```py\npipeline(\"The secret to baking a good cake is \", max_length=50)",
  "[{'generated_text': 'The secret to baking a good cake is 100% in the batter. The secret to a great cake is the icing.\\nThis is why we’ve created the best buttercream frosting reci'}]\n```\n\n</hfoption>\n<hfoption id=\"image segmentation\">\n\nSet `device=\"cuda\"` to accelerate inference with a GPU.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50-panoptic\", device=\"cuda\")\n```\n\nPass an image - a URL or local path to the image - to [`Pipeline`].\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"/>\n</div>\n\n```py\nsegments = pipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\nsegments[0][\"label\"]\n'bird'\nsegments[1][\"label\"]\n'bird'\n```\n\n</hfoption>\n<hfoption id=\"automatic speech recognition\">\n\nSet `device=\"cuda\"` to accelerate inference with a GPU.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device=\"cuda\")\n```\n\nPass an audio file to [`Pipeline`].\n\n```py\npipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")",
  "{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'}\n```\n\n</hfoption>\n</hfoptions>\n\n## Trainer\n\n[`Trainer`] is a complete training and evaluation loop for PyTorch models. It abstracts away a lot of the boilerplate usually involved in manually writing a training loop, so you can start training faster and focus on training design choices. You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.\n\nUse the [`TrainingArguments`] class to customize the training process. It provides many options for training, evaluation, and more. Experiment with training hyperparameters and features like batch size, learning rate, mixed precision, torch.compile, and more to meet your training needs. You could also use the default training parameters to quickly produce a baseline.\n\nLoad a model, tokenizer, and dataset for training.\n\n```py\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom datasets import load_dataset",
  "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\ndataset = load_dataset(\"rotten_tomatoes\")\n```\n\nCreate a function to tokenize the text and convert it into PyTorch tensors. Apply this function to the whole dataset with the [`~datasets.Dataset.map`] method.\n\n```py\ndef tokenize_dataset(dataset):\nreturn tokenizer(dataset[\"text\"])\ndataset = dataset.map(tokenize_dataset, batched=True)\n```\n\nLoad a data collator to create batches of data and pass the tokenizer to it.\n\n```py\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n```\n\nNext, set up [`TrainingArguments`] with the training features and hyperparameters.\n\n```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\noutput_dir=\"distilbert-rotten-tomatoes\",\nlearning_rate=2e-5,\nper_device_train_batch_size=8,\nper_device_eval_batch_size=8,\nnum_train_epochs=2,\npush_to_hub=True,\n)\n```\n\nFinally, pass all these separate components to [`Trainer`] and call [`~Trainer.train`] to start.\n\n```py\nfrom transformers import Trainer",
  "trainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=dataset[\"train\"],\neval_dataset=dataset[\"test\"],\ntokenizer=tokenizer,\ndata_collator=data_collator,\n)\n\ntrainer.train()\n```\n\nShare your model and tokenizer to the Hub with [`~Trainer.push_to_hub`].\n\n```py\ntrainer.push_to_hub()\n```\n\nCongratulations, you just trained your first model with Transformers!\n\n### TensorFlow\n\n> [!WARNING]\n> Not all pretrained models are available in TensorFlow. Refer to a models API doc to check whether a TensorFlow implementation is supported.\n\n[`Trainer`] doesn't work with TensorFlow models, but you can still train a Transformers model implemented in TensorFlow with [Keras](https://keras.io/). Transformers TensorFlow models are a standard [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model), which is compatible with Keras' [compile](https://keras.io/api/models/model_training_apis/#compile-method) and [fit](https://keras.io/api/models/model_training_apis/#fit-method) methods.\n\nLoad a model, tokenizer, and dataset for training.\n\n```py\nfrom transformers import TFAutoModelForSequenceClassification, AutoTokenizer",
  "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\nCreate a function to tokenize the text and convert it into TensorFlow tensors. Apply this function to the whole dataset with the [`~datasets.Dataset.map`] method.\n\n```py\ndef tokenize_dataset(dataset):\nreturn tokenizer(dataset[\"text\"])\ndataset = dataset.map(tokenize_dataset)\n```\n\nTransformers provides the [`~TFPreTrainedModel.prepare_tf_dataset`] method to collate and batch a dataset.\n\n```py\ntf_dataset = model.prepare_tf_dataset(\ndataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer\n)\n```\n\nFinally, call [compile](https://keras.io/api/models/model_training_apis/#compile-method) to configure the model for training and [fit](https://keras.io/api/models/model_training_apis/#fit-method) to start.\n\n```py\nfrom tensorflow.keras.optimizers import Adam\n\nmodel.compile(optimizer=\"adam\")\nmodel.fit(tf_dataset)\n```\n\n## Next steps\n\nNow that you have a better understanding of Transformers and what it offers, it's time to keep exploring and learning what interests you the most.",
  "- **Base classes**: Learn more about the configuration, model and processor classes. This will help you understand how to create and customize models, preprocess different types of inputs (audio, images, multimodal), and how to share your model.\n- **Inference**: Explore the [`Pipeline`] further, inference and chatting with LLMs, agents, and how to optimize inference with your machine learning framework and hardware.\n- **Training**: Study the [`Trainer`] in more detail, as well as distributed training and optimizing training on specific hardware.\n- **Quantization**: Reduce memory and storage requirements with quantization and speed up inference by representing weights with fewer bits.\n- **Resources**: Looking for end-to-end recipes for how to train and inference with a model for a specific task? Check out the task recipes!",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Trainer\n\n[`Trainer`] is a complete training and evaluation loop for Transformers' PyTorch models. Plug a model, preprocessor, dataset, and training arguments into [`Trainer`] and let it handle the rest to start training faster.\n\n[`Trainer`] is also powered by [Accelerate](https://hf.co/docs/accelerate/index), a library for handling large models for distributed training.",
  "This guide will show you how [`Trainer`] works and how to customize it for your use case with a callback.\n\n```bash\n!pip install accelerate --upgrade\n```\n\n[`Trainer`] contains all the necessary components of a training loop.\n\n1. calculate the loss from a training step\n2. calculate the gradients with the [`~accelerate.Accelerator.backward`] method\n3. update the weights based on the gradients\n4. repeat until the predetermined number of epochs is reached\n\nManually coding this training loop everytime can be inconvenient or a barrier if you're just getting started with machine learning. [`Trainer`] abstracts this process, allowing you to focus on the model, dataset, and training design choices.\n\nConfigure your training with hyperparameters and options from [`TrainingArguments`] which supports many features such as distributed training, torch.compile, mixed precision training, and saving the model to the Hub.\n\n> [!TIP]",
  "> The number of available parameters available in [`TrainingArguments`] may be intimidating at first. If there is a specific hyperparameter or feature you want to use, try searching for it directly. Otherwise, feel free to start with the default values and gradually customize them as you become more familiar with the training process.\n\nThe example below demonstrates an example of [`TrainingArguments`] that evaluates and saves the model at the end of each epoch. It also loads the best model found during training and pushes it to the Hub.\n\n```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\noutput_dir=\"your-model\",\nlearning_rate=2e-5,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=2,\nweight_decay=0.01,\neval_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nload_best_model_at_end=True,\npush_to_hub=True,\n)\n```\n\nPass your model, dataset, preprocessor, and [`TrainingArguments`] to [`Trainer`], and call [`~Trainer.train`] to start training.\n\n> [!TIP]\n> Refer to the [Fine-tuning](./training) guide for a more complete overview of the training process.\n\n```py\nfrom transformers import Trainer\n\ntrainer = Trainer(\nmodel=model,",
  "args=training_args,\ntrain_dataset=dataset[\"train\"],\neval_dataset=dataset[\"test\"],\nprocessing_class=tokenizer,\ndata_collator=data_collator,\ncompute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```\n\n## Checkpoints\n\n[`Trainer`] saves checkpoints (the optimizer state is not saved by default) to the directory in `output_dir` in [`TrainingArguments`] to a subfolder named `checkpoint-000`. The number at the end is the training step at which the checkpoint was saved.\n\nSaving checkpoints are useful for resuming training or recovering your training progress if you encounter an error. Set the `resume_from_checkpoint` parameter in [`~Trainer.train`] to resume training from the last checkpoint or a specific checkpoint.\n\n<hfoptions id=\"ckpt\">\n<hfoption id=\"latest checkpoint\">\n\n```py\ntrainer.train(resume_from_checkpoint=True)\n```\n\n</hfoption>\n<hfoption id=\"specific checkpoint\">\n\n```py\ntrainer.train(resume_from_checkpoint=\"your-model/checkpoint-1000\")\n```\n\n</hfoption>\n</hfoptions>",
  "Checkpoints can be saved to the Hub by setting `push_to_hub=True` in [`TrainingArguments`]. The default method (`\"every_save\"`) saves a checkpoint to the Hub every time a model is saved, which is typically the final model at the end of training. Some other options for deciding how to save checkpoints to the Hub include the following.\n\n- `hub_strategy=\"end\"` only pushes a checkpoint when [`~Trainer.save_model`] is called\n- `hub_strategy=\"checkpoint\"` pushes the latest checkpoint to a subfolder named *last-checkpoint* from which training can be resumed\n- `hub_strategy=\"all_checkpoints\"` pushes all checkpoints to the Hub with one checkpoint per subfolder in your model repository\n\n[`Trainer`] attempts to maintain the same Python, NumPy, and PyTorch RNG states when you resume training from a checkpoint. But PyTorch has various non-deterministic settings which can't guarantee the RNG states are identical. To enable full determinism, refer to the [Controlling sources of randomness](https://pytorch.org/docs/stable/notes/randomness#controlling-sources-of-randomness) guide to learn what settings to adjust to make training fully deterministic (some settings may result in slower training).",
  "## Logging\n\n[`Trainer`] is set to `logging.INFO` by default to report errors, warnings, and other basic information. Use [`~TrainingArguments.log_level`] to change the logging level and log verbosity.\n\nThe example below sets the main code and modules to use the same log level.\n\n```py\nlogger = logging.getLogger(__name__)\n\nlogging.basicConfig(\nformat=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\ndatefmt=\"%m/%d/%Y %H:%M:%S\",\nhandlers=[logging.StreamHandler(sys.stdout)],\n)\n\nlog_level = training_args.get_process_log_level()\nlogger.setLevel(log_level)\ndatasets.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.set_verbosity(log_level)\n\ntrainer = Trainer(...)\n```\n\nIn a distributed environment, [`Trainer`] replicas are set to `logging.WARNING` to only report errors and warnings. Use [`~TrainingArguments.log_level_replica`] to change the logging level and log verbosity. To configure the log level for each node, use [`~TrainingArguments.log_on_each_node`] to determine whether to use a specific log level on each node or only the main node.\n\nUse different combinations of `log_level` and `log_level_replica` to configure what gets logged on each node.",
  "<hfoptions id=\"nodes\">\n<hfoption id=\"single node\">\n\n```bash\nmy_app.py ... --log_level warning --log_level_replica error\n```\n\n</hfoption>\n<hfoption id=\"multi-node\">\n\nAdd `log_on_each_node 0` for distributed environments.\n\n```bash\nmy_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0\n\n# set to only report errors\nmy_app.py ... --log_level error --log_level_replica error --log_on_each_node 0\n```\n\n</hfoption>\n</hfoptions>\n\n> [!TIP]\n> The log level is separately set for each node in the [`~Trainer.__init__`] method. Consider setting this sooner if you're using other Transformers functionalities before creating the [`Trainer`] instance.\n\n## Customize\n\nTailor [`Trainer`] to your use case by subclassing or overriding its methods to support the functionality you want to add or use, without rewriting the entire training loop from scratch. The table below lists some of the methods that can be customized.\n\n| method | description |\n|---|---|\n| [`~Trainer.get_train_dataloader`] | create a training DataLoader |\n| [`~Trainer.get_eval_dataloader`] | create an evaluation DataLoader |\n| [`~Trainer.get_test_dataloader`] | create a test DataLoader |",
  "| [`~Trainer.log`] | log information about the training process |\n| [`~Trainer.create_optimizer_and_scheduler`] | create an optimizer and learning rate scheduler (can also be separately customized with [`~Trainer.create_optimizer`] and [`~Trainer.create_scheduler`] if they weren't passed in `__init__`) |\n| [`~Trainer.compute_loss`] | compute the loss of a batch of training inputs |\n| [`~Trainer.training_step`] | perform the training step |\n| [`~Trainer.prediction_step`] | perform the prediction and test step |\n| [`~Trainer.evaluate`] | evaluate the model and return the evaluation metric |\n| [`~Trainer.predict`] | make a prediction (with metrics if labels are available) on the test set |\n\nFor example, to use weighted loss, rewrite [`~Trainer.compute_loss`] inside [`Trainer`].\n\n```py\nfrom torch import nn\nfrom transformers import Trainer\n\nclass CustomTrainer(Trainer):\ndef compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\nlabels = inputs.pop(\"labels\")\n# forward pass\noutputs = model(**inputs)\nlogits = outputs.get(\"logits\")\n# compute custom loss for 3 labels with different weights",
  "loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\nloss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\nreturn (loss, outputs) if return_outputs else loss\n```\n\n### Callbacks\n\n[Callbacks](./main_classes/callback) are another way to customize [`Trainer`], but they don't change anything *inside the training loop*. Instead, a callback inspects the training loop state and executes some action (early stopping, logging, etc.) depending on the state. For example, you can't implement a custom loss function with a callback because that requires overriding [`~Trainer.compute_loss`].\n\nTo use a callback, create a class that inherits from [`TrainerCallback`] and implements the functionality you want. Then pass the callback to the `callback` parameter in [`Trainer`]. The example below implements an early stopping callback that stops training after 10 steps.\n\n```py\nfrom transformers import TrainerCallback, Trainer\n\nclass EarlyStoppingCallback(TrainerCallback):\ndef __init__(self, num_steps=10):\nself.num_steps = num_steps\n\ndef on_step_end(self, args, state, control, **kwargs):\nif state.global_step >= self.num_steps:",
  "return {\"should_training_stop\": True}\nelse:\nreturn {}\n\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=dataset[\"train\"],\neval_dataset=dataset[\"test\"],\nprocessing_class=tokenizer,\ndata_collator=data_collator,\ncompute_metrics=compute_metrics,\ncallbacks=[EarlyStoppingCallback()],\n)\n```\n\n## Accelerate\n\n[Accelerate](https://hf.co/docs/accelerate/index) is a library that simplifies training in distributed environments and across different hardware. Its integration with [`Trainer`] means [`Trainer`] supports distributed training frameworks like [Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/).\n\n> [!TIP]\n> Learn more about FSDP sharding strategies, CPU offloading, and more with [`Trainer`] in the [Fully Sharded Data Parallel](./fsdp) guide.",
  "To use Accelerate with [`Trainer`], run the [accelerate_config](https://hf.co/docs/accelerate/package_reference/cli#accelerate-config) command to configure your training environment. This command creates a `config_file.yaml` file that stores the configuration settings of your training environment and it's used whenever you launch your training script. Some example distributed training configurations are shown below.\n\n<hfoptions id=\"distributed-training\">\n<hfoption id=\"DistributedDataParallel\">\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndistributed_type: MULTI_GPU\ndowncast_bf16: 'no'\ngpu_ids: all\nmachine_rank: 0 #change rank as per the node\nmain_process_ip: 192.168.20.1\nmain_process_port: 9898\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 2\nnum_processes: 8\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n</hfoption>\n<hfoption id=\"FullyShardedDataParallel\">\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\nfsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\nfsdp_backward_prefetch_policy: BACKWARD_PRE\nfsdp_forward_prefetch: true",
  "fsdp_offload_params: false\nfsdp_sharding_strategy: 1\nfsdp_state_dict_type: FULL_STATE_DICT\nfsdp_sync_module_states: true\nfsdp_transformer_layer_cls_to_wrap: BertLayer\nfsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n</hfoption>\n<hfoption id=\"DeepSpeed\">\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\ndeepspeed_config_file: /home/user/configs/ds_zero3_config.json\nzero3_init_flag: true\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n</hfoption>\n<hfoption id=\"DeepSpeed with Accelerate plugin\">\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\ngradient_accumulation_steps: 1\ngradient_clipping: 0.7\noffload_optimizer_device: cpu\noffload_param_device: cpu\nzero3_init_flag: true\nzero_stage: 2\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main",
  "mixed_precision: bf16\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n</hfoption>\n<hfoption id=\"Tensor parallelism with PyTorch 2\">\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ntp_config:\ntp_size: 4\ndistributed_type: TP\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n</hfoptions>\n\nRun [accelerate_launch](https://hf.co/docs/accelerate/package_reference/cli#accelerate-launch) to start training with the configurations set in `config_file.yaml`. This file is saved to the Accelerate cache folder and automatically loaded when you run `accelerate_launch`.\n\nThe example below launches the [run_glue.py](../../../examples/pytorch/text-classification/run_glue) script with the FSDP configuration shown earlier. Parameters from the `config_file.yaml` file can also be directly set in the command line.\n\n```bash\naccelerate launch \\\n./examples/pytorch/text-classification/run_glue.py \\",
  "--model_name_or_path google-bert/bert-base-cased \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/ \\\n--overwrite_output_dir\n```\n\n> [!TIP]\n> Refer to the [Launching your Accelerate scripts](https://hf.co/docs/accelerate/basic_tutorials/launch) tutorial to learn more about `accelerate_launch` and custom configurations.\n\n## Optimizations\n\n[`Trainer`] supports various optimizations to improve *training* performance - reduce memory and increase training speed - and *model* performance.\n\n### torch.compile\n\n[torch.compile](./perf_torch_compile) can significantly speed up training and reduce computational overhead. Configure your torch.compile settings in [`TrainingArguments`]. Set `torch.compile` to `True`, and select a backend and compile mode.\n\n```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\ntorch.compile=True,\ntorch.compile_backend=\"inductor\",\ntorch_compile_mode=\"default\",\n...,\n)\n```\n\n### GaLore",
  "[Gradient Low-Rank Projection (GaLore)](https://hf.co/papers/2403.03507) significantly reduces memory usage when training large language models (LLMs). One of GaLores key benefits is *full-parameter* learning, unlike low-rank adaptation methods like [LoRA](https://hf.co/papers/2106.09685), which produces better model performance.\n\nInstall the [GaLore](https://github.com/jiaweizzhao/GaLore) library, [TRL](https://hf.co/docs/trl/index), and [Datasets](https://hf.co/docs/datasets/index).\n\n```bash\npip install galore-torch trl datasets\n```\n\nPick a GaLore optimizer (`\"galore_adamw\"`, `\"galore_adafactor\"`, `\"galore_adamw_8bit`\") and pass it to the `optim` parameter in [`TrainingArguments`]. Use the `optim_target_modules` parameter to specify which modules to adapt (can be a list of strings, regex, or a full path).\n\nExtra parameters supported by GaLore, `rank`, `update_proj_gap`, and `scale`, should be passed to the `optim_args` parameter in [`TrainingArguments`].\n\nThe example below enables GaLore with [`~trl.SFTTrainer`] that targets the `attn` and `mlp` layers with regex.\n\n> [!TIP]\n> It can take some time before training starts (~3 minutes for a 2B model on a NVIDIA A100).",
  "<hfoptions id=\"galore\">\n<hfoption id=\"GaLore optimizer\">\n\n```py\nimport torch\nimport datasets\nimport trl\nfrom transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n\ntrain_dataset = datasets.load_dataset('imdb', split='train')\nargs = TrainingArguments(\noutput_dir=\"./test-galore\",\nmax_steps=100,\nper_device_train_batch_size=2,\noptim=\"galore_adamw\",\noptim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\noptim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n)\nconfig = AutoConfig.from_pretrained(\"google/gemma-2b\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_config(\"google/gemma-2b\").to(0)\ntrainer = trl.SFTTrainer(\nmodel=model,\nargs=args,\ntrain_dataset=train_dataset,\ndataset_text_field='text',\nmax_seq_length=512,\n)\ntrainer.train()\n```\n\n</hfoption>\n<hfoption id=\"GaLore optimizer with layerwise optimization\">",
  "Append `layerwise` to the optimizer name to enable layerwise optimization. For example, `\"galore_adamw\"` becomes `\"galore_adamw_layerwise\"`. This feature is still experimental and does not support Distributed Data Parallel (DDP). The code below can only be run on a [single GPU](https://github.com/jiaweizzhao/GaLore?tab=readme-ov-file#train-7b-model-with-a-single-gpu-with-24gb-memory). Other features like gradient clipping and DeepSpeed may not be available out of the box. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter any problems!\n\n```py\nimport torch\nimport datasets\nimport trl\nfrom transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n\ntrain_dataset = datasets.load_dataset('imdb', split='train')\nargs = TrainingArguments(\noutput_dir=\"./test-galore\",\nmax_steps=100,\nper_device_train_batch_size=2,\noptim=\"galore_adamw_layerwise\",\noptim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\noptim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n)\nconfig = AutoConfig.from_pretrained(\"google/gemma-2b\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")",
  "model = AutoModelForCausalLM.from_config(\"google/gemma-2b\").to(0)\ntrainer = trl.SFTTrainer(\nmodel=model,\nargs=args,\ntrain_dataset=train_dataset,\ndataset_text_field='text',\nmax_seq_length=512,\n)\ntrainer.train()\n```\n\n</hfoption>\n</hfoptions>\n\nOnly linear layers that are considered GaLore layers can be trained with low-rank decomposition. The rest of the model layers are optimized in the usual way.\n\n### Liger\n\n[Liger Kernel](https://github.com/linkedin/Liger-Kernel) is a collection of layers such as RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy, and more that have been fused into a single Triton kernel for training LLMs. These kernels are also compatible with FlashAttention, FSDP, and DeepSpeed. As a result, Liger Kernel can increase multi-GPU training throughput and reduce memory usage. This is useful for multi-head training and supporting larger vocabulary sizes, larger batch sizes, and longer context lengths.\n\n```bash\npip install liger-kernel\n```\n\nEnable Liger Kernel for training by setting `use_liger_kernel=True` in [`TrainingArguments`]. This patches the corresponding layers in the model with Ligers kernels.\n\n> [!TIP]",
  "> Liger Kernel supports Llama, Gemma, Mistral, and Mixtral models. Refer to the [patching](https://github.com/linkedin/Liger-Kernel#patching) list for the latest list of supported models.\n\n```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\noutput_dir=\"your-model\",\nlearning_rate=2e-5,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=2,\nweight_decay=0.01,\neval_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nload_best_model_at_end=True,\npush_to_hub=True,\nuse_liger_kernel=True\n)\n```\n\n### NEFTune\n\n[NEFTune](https://hf.co/papers/2310.05914) adds noise to the embedding vectors during training to improve model performance. Enable it in [`Trainer`] with the `neftune_noise_alpha` parameter in [`TrainingArguments`] to control how much noise is added.\n\n```py\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(..., neftune_noise_alpha=0.1)\ntrainer = Trainer(..., args=training_args)\n```\n\nThe original embedding layer is restored after training to avoid any unexpected behavior.",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Training scripts",
  "Transformers provides many example training scripts for deep learning frameworks (PyTorch, TensorFlow, Flax) and tasks in [transformers/examples](https://github.com/huggingface/transformers/tree/main/examples). There are additional scripts in [transformers/research projects](https://github.com/huggingface/transformers-research-projects/) and [transformers/legacy](https://github.com/huggingface/transformers/tree/main/examples/legacy), but these aren't actively maintained and requires a specific version of Transformers.\n\nExample scripts are only examples and you may need to adapt the script to your use-case. To help you with this, most scripts are very transparent in how data is preprocessed, allowing you to edit it as necessary.\n\nFor any feature you'd like to implement in an example script, please discuss it on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues) before submitting a pull request. While we welcome contributions, it is unlikely a pull request that adds more functionality is added at the cost of readability.",
  "This guide will show you how to run an example summarization training script in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) and [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization).\n\n## Setup\n\nInstall Transformers from source in a new virtual environment to run the latest version of the example script.\n\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install .\n```\n\nRun the command below to checkout a script from a specific or older version of Transformers.\n\n```bash\ngit checkout tags/v3.5.1\n```\n\nAfter you've setup the correct version, navigate to the example folder of your choice and install the example specific requirements.\n\n```bash\npip install -r requirements.txt\n```\n\n## Run a script\n\nStart with a smaller dataset by including the `max_train_samples`, `max_eval_samples`, and `max_predict_samples` parameters to truncate the dataset to a maximum number of samples. This helps ensure training works as expected before committing to the entire dataset which can take hours to complete.\n\n> [!WARNING]",
  "> Not all example scripts support the `max_predict_samples` parameter. Run the command below to check whether a script supports it or not.\n> ```bash\n> examples/pytorch/summarization/run_summarization.py -h\n> ```\n\nThe example below fine-tunes [T5-small](https://huggingface.co/google-t5/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset. T5 requires an additional `source_prefix` parameter to prompt it to summarize.\n\n<hfoptions id=\"script\">\n<hfoption id=\"PyTorch\">\n\nThe example script downloads and preprocesses a dataset, and then fine-tunes it with [`Trainer`] with a supported model architecture.\n\nResuming training from a checkpoint is very useful if training is interrupted because you don't have to start over again. There are two ways to resume training from a checkpoint.\n\n* `--output dir previous_output_dir` resumes training from the latest checkpoint stored in `output_dir`. Remove the `--overwrite_output_dir` parameter if you're using this method.\n* `--resume_from_checkpoint path_to_specific_checkpoint` resumes training from a specific checkpoint folder.",
  "Share your model on the [Hub](https://huggingface.co/) with the `--push_to_hub` parameter. It creates a repository and uploads the model to the folder name specified in `--output_dir`. You could also use the `--push_to_hub_model_id` parameter to specify the repository name.\n\n```bash\npython examples/pytorch/summarization/run_summarization.py \\\n--model_name_or_path google-t5/t5-small \\\n# remove the `max_train_samples`, `max_eval_samples` and `max_predict_samples` if everything works\n--max_train_samples 50 \\\n--max_eval_samples 50 \\\n--max_predict_samples 50 \\\n--do_train \\\n--do_eval \\\n--dataset_name cnn_dailymail \\\n--dataset_config \"3.0.0\" \\\n--source_prefix \"summarize: \" \\\n--output_dir /tmp/tst-summarization \\\n--per_device_train_batch_size=4 \\\n--per_device_eval_batch_size=4 \\\n--push_to_hub \\\n--push_to_hub_model_id finetuned-t5-cnn_dailymail \\\n# remove if using `output_dir previous_output_dir`\n# --overwrite_output_dir \\\n--output_dir previous_output_dir \\\n# --resume_from_checkpoint path_to_specific_checkpoint \\\n--predict_with_generate \\\n```",
  "For mixed precision and distributed training, include the following parameters and launch training with [torchrun](https://pytorch.org/docs/stable/elastic/run.html).\n\n* Add the `fp16` or `bf16` parameters to enable mixed precision training. XPU devices only supports `bf16`.\n* Add the `nproc_per_node` parameter to set number of GPUs to train with.\n\n```bash\ntorchrun \\\n--nproc_per_node 8 pytorch/summarization/run_summarization.py \\\n--fp16 \\\n...\n...\n```\n\nPyTorch supports TPUs, hardware designed to accelerate performance, through the [PyTorch/XLA](https://github.com/pytorch/xla/blob/master/README.md) package. Launch the `xla_spawn.py` script and use `num _cores` to set the number of TPU cores to train with.\n\n```bash\npython xla_spawn.py --num_cores 8 pytorch/summarization/run_summarization.py \\\n--model_name_or_path google-t5/t5-small \\\n...\n...\n```\n\n</hfoption>\n<hfoption id=\"TensorFlow\">\n\n```bash\npython examples/tensorflow/summarization/run_summarization.py  \\\n--model_name_or_path google-t5/t5-small \\\n# remove the `max_train_samples`, `max_eval_samples` and `max_predict_samples` if everything works\n--max_train_samples 50 \\\n--max_eval_samples 50 \\\n--max_predict_samples 50 \\",
  "--dataset_name cnn_dailymail \\\n--dataset_config \"3.0.0\" \\\n--output_dir /tmp/tst-summarization  \\\n--per_device_train_batch_size 8 \\\n--per_device_eval_batch_size 16 \\\n--num_train_epochs 3 \\\n--do_train \\\n--do_eval \\\n```\n\nTensorFlow uses the [MirroredStrategy](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) for distributed training and doesn't require adding any additional parameters. The script uses multiple GPUs by default if they are available.\n\nFor TPU training, TensorFlow scripts use the [TPUStrategy](https://www.tensorflow.org/guide/distributed_training#tpustrategy). Pass the TPU resource name to the `--tpu` parameter.\n\n```bash\npython run_summarization.py  \\\n--tpu name_of_tpu_resource \\\n...\n...\n```\n\n</hfoption>\n</hfoptions>\n\n## Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate) is designed to simplify distributed training while offering complete visibility into the PyTorch training loop. If you're planning on training with a script with Accelerate, use the `_no_trainer.py` version of the script.\n\nInstall Accelerate from source to ensure you have the latest version.\n\n```bash\npip install git+https://github.com/huggingface/accelerate\n```",
  "Run the [accelerate config](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to answer a few questions about your training setup. This creates and saves a config file about your system.\n\n```bash\naccelerate config\n```\n\nYou can use [accelerate test](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-test) to ensure your system is properly configured.\n\n```bash\naccelerate test\n```\n\nRun [accelerate launch](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch) to start training.\n\n```bash\naccelerate launch run_summarization_no_trainer.py \\\n--model_name_or_path google-t5/t5-small \\\n--dataset_name cnn_dailymail \\\n--dataset_config \"3.0.0\" \\\n--source_prefix \"summarize: \" \\\n--output_dir ~/tmp/tst-summarization \\\n```\n\n## Custom dataset\n\nThe summarization scripts supports custom datasets as long as they are a CSV or JSONL file. When using your own dataset, you need to specify the following additional parameters.\n\n* `train_file` and `validation_file` specify the path to your training and validation files.\n* `text_column` is the input text to summarize.\n* `summary_column` is the target text to output.",
  "An example command for summarizing a custom dataset is shown below.\n\n```bash\npython examples/pytorch/summarization/run_summarization.py \\\n--model_name_or_path google-t5/t5-small \\\n--do_train \\\n--do_eval \\\n--train_file path_to_csv_or_jsonlines_file \\\n--validation_file path_to_csv_or_jsonlines_file \\\n--text_column text_column_name \\\n--summary_column summary_column_name \\\n--source_prefix \"summarize: \" \\\n--output_dir /tmp/tst-summarization \\\n--overwrite_output_dir \\\n--per_device_train_batch_size=4 \\\n--per_device_eval_batch_size=4 \\\n--predict_with_generate \\\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Customizing models\n\nTransformers models are designed to be customizable. A models code is fully contained in the [model](https://github.com/huggingface/transformers/tree/main/src/transformers/models) subfolder of the Transformers repository. Each folder contains a `modeling.py` and a `configuration.py` file. Copy these files to start customizing a model.\n\n> [!TIP]",
  "> It may be easier to start from scratch if you're creating an entirely new model. But for models that are very similar to an existing one in Transformers, it is faster to reuse or subclass the same configuration and model class.\n\nThis guide will show you how to customize a ResNet model, enable [AutoClass](./models#autoclass) support, and share it on the Hub.\n\n## Configuration\n\nA configuration, given by the base [`PretrainedConfig`] class, contains all the necessary information to build a model. This is where you'll configure the attributes of the custom ResNet model. Different attributes gives different ResNet model types.\n\nThe main rules for customizing a configuration are:\n\n1. A custom configuration must subclass [`PretrainedConfig`]. This ensures a custom model has all the functionality of a Transformers' model such as [`~PretrainedConfig.from_pretrained`], [`~PretrainedConfig.save_pretrained`], and [`~PretrainedConfig.push_to_hub`].",
  "2. The [`PretrainedConfig`] `__init__` must accept any `kwargs` and they must be passed to the superclass `__init__`. [`PretrainedConfig`] has more fields than the ones set in your custom configuration, so when you load a configuration with [`~PretrainedConfig.from_pretrained`], those fields need to be accepted by your configuration and passed to the superclass.\n\n> [!TIP]\n> It is useful to check the validity of some of the parameters. In the example below, a check is implemented to ensure `block_type` and `stem_type` belong to one of the predefined values.\n>\n> Add `model_type` to the configuration class to enable [AutoClass](./models#autoclass) support.\n\n```py\nfrom transformers import PretrainedConfig\nfrom typing import List\n\nclass ResnetConfig(PretrainedConfig):\nmodel_type = \"resnet\"\n\ndef __init__(\nself,\nblock_type=\"bottleneck\",\nlayers: List[int] = [3, 4, 6, 3],\nnum_classes: int = 1000,\ninput_channels: int = 3,\ncardinality: int = 1,\nbase_width: int = 64,\nstem_width: int = 64,\nstem_type: str = \"\",\navg_down: bool = False,\n**kwargs,\n):\nif block_type not in [\"basic\", \"bottleneck\"]:\nraise ValueError(f\"`block_type` must be 'basic' or bottleneck', got {block_type}.\")",
  "if stem_type not in [\"\", \"deep\", \"deep-tiered\"]:\nraise ValueError(f\"`stem_type` must be '', 'deep' or 'deep-tiered', got {stem_type}.\")\n\nself.block_type = block_type\nself.layers = layers\nself.num_classes = num_classes\nself.input_channels = input_channels\nself.cardinality = cardinality\nself.base_width = base_width\nself.stem_width = stem_width\nself.stem_type = stem_type\nself.avg_down = avg_down\nsuper().__init__(**kwargs)\n```\n\nSave the configuration to a JSON file in your custom model folder, `custom-resnet`, with [`~PretrainedConfig.save_pretrained`].\n\n```py\nresnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\nresnet50d_config.save_pretrained(\"custom-resnet\")\n```\n\n## Model\n\nWith the custom ResNet configuration, you can now create and customize the model. The model subclasses the base [`PreTrainedModel`] class. Like [`PretrainedConfig`], inheriting from [`PreTrainedModel`] and initializing the superclass with the configuration extends Transformers' functionalities such as saving and loading to the custom model.",
  "Transformers' models follow the convention of accepting a `config` object in the `__init__` method. This passes the entire `config` to the model sublayers, instead of breaking the `config` object into multiple arguments that are individually passed to the sublayers.\n\nWriting models this way produces simpler code with a clear source of truth for any hyperparameters. It also makes it easier to reuse code from other Transformers' models.\n\nYou'll create two ResNet models, a barebones ResNet model that outputs the hidden states and a ResNet model with an image classification head.\n\n<hfoptions id=\"resnet\">\n<hfoption id=\"ResnetModel\">\n\nDefine a mapping between the block types and classes. Everything else is created by passing the configuration class to the ResNet model class.\n\n> [!TIP]\n> Add `config_class` to the model class to enable [AutoClass](#autoclass-support) support.\n\n```py\nfrom transformers import PreTrainedModel\nfrom timm.models.resnet import BasicBlock, Bottleneck, ResNet\nfrom .configuration_resnet import ResnetConfig\n\nBLOCK_MAPPING = {\"basic\": BasicBlock, \"bottleneck\": Bottleneck}\n\nclass ResnetModel(PreTrainedModel):\nconfig_class = ResnetConfig",
  "def __init__(self, config):\nsuper().__init__(config)\nblock_layer = BLOCK_MAPPING[config.block_type]\nself.model = ResNet(\nblock_layer,\nconfig.layers,\nnum_classes=config.num_classes,\nin_chans=config.input_channels,\ncardinality=config.cardinality,\nbase_width=config.base_width,\nstem_width=config.stem_width,\nstem_type=config.stem_type,\navg_down=config.avg_down,\n)\n\ndef forward(self, tensor):\nreturn self.model.forward_features(tensor)\n```\n\n</hfoption>\n<hfoption id=\"ResnetModelForImageClassification\">\n\nThe `forward` method needs to be rewrittten to calculate the loss for each logit if labels are available. Otherwise, the ResNet model class is the same.\n\n> [!TIP]\n> Add `config_class` to the model class to enable [AutoClass](#autoclass-support) support.\n\n```py\nimport torch\n\nclass ResnetModelForImageClassification(PreTrainedModel):\nconfig_class = ResnetConfig\n\ndef __init__(self, config):\nsuper().__init__(config)\nblock_layer = BLOCK_MAPPING[config.block_type]\nself.model = ResNet(\nblock_layer,\nconfig.layers,\nnum_classes=config.num_classes,\nin_chans=config.input_channels,\ncardinality=config.cardinality,\nbase_width=config.base_width,\nstem_width=config.stem_width,",
  "stem_type=config.stem_type,\navg_down=config.avg_down,\n)\n\ndef forward(self, tensor, labels=None):\nlogits = self.model(tensor)\nif labels is not None:\nloss = torch.nn.functional.cross_entropy(logits, labels)\nreturn {\"loss\": loss, \"logits\": logits}\nreturn {\"logits\": logits}\n```\n\n</hfoption>\n</hfoptions>\n\nA model can return any output format. Returning a dictionary (like `ResnetModelForImageClassification`) with losses when labels are available makes the custom model compatible with [`Trainer`]. For other output formats, you'll need your own training loop or a different library for training.\n\nInstantiate the custom model class with the configuration.\n\n```py\nresnet50d = ResnetModelForImageClassification(resnet50d_config)\n```\n\nAt this point, you can load pretrained weights into the model or train it from scratch. In this guide, you'll load pretrained weights.\n\nLoad the pretrained weights from the [timm](https://hf.co/docs/timm/index) library, and then transfer those weights to the custom model with [load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict).\n\n```py\nimport timm",
  "pretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\nresnet50d.model.load_state_dict(pretrained_model.state_dict())\n```\n\n## AutoClass\n\nThe [AutoClass](./models#model-classes) API is a shortcut for automatically loading the correct architecture for a given model. It is convenient to enable this for users loading your custom model.\n\nMake sure you have the `model_type` attribute (must be different from existing model types) in the configuration class and `config_class` attribute in the model class. Use the [`~AutoConfig.register`] method to add the custom configuration and model to the [AutoClass](./models#model-classes) API.\n\n> [!TIP]\n> The first argument to [`AutoConfig.register`] must match the `model_type` attribute in the custom configuration class, and the first argument to [`AutoModel.register`] must match the `config_class` of the custom model class.\n\n```py\nfrom transformers import AutoConfig, AutoModel, AutoModelForImageClassification\n\nAutoConfig.register(\"resnet\", ResnetConfig)\nAutoModel.register(ResnetConfig, ResnetModel)\nAutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)\n```",
  "Your custom model code is now compatible with the [AutoClass](./models#autoclass) API. Users can load the model with the [AutoModel](./model_doc/auto#automodel) or [`AutoModelForImageClassification`] classes.\n\n## Upload\n\nUpload a custom model to the [Hub](https://hf.co/models) to allow other users to easily load and use it.\n\nEnsure the model directory is structured correctly as shown below. The directory should contain:\n\n- `modeling.py`: Contains the code for `ResnetModel` and `ResnetModelForImageClassification`. This file can rely on relative imports to other files as long as they're in the same directory.\n\n> [!WARNING]\n> When copying a Transformers' model file, replace all relative imports at the top of the `modeling.py` file to import from Transformers instead.\n\n- `configuration.py`: Contains the code for `ResnetConfig`.\n- `__init__.py`: Can be empty, this file allows Python `resnet_model` to be used as a module.\n\n```bash\n.\n└── resnet_model\n├── __init__.py\n├── configuration_resnet.py\n└── modeling_resnet.py\n```\n\nTo share the model, import the ResNet model and configuration.\n\n```py\nfrom resnet_model.configuration_resnet import ResnetConfig",
  "from resnet_model.modeling_resnet import ResnetModel, ResnetModelForImageClassification\n```\n\nCopy the code from the model and configuration files. To make sure the AutoClass objects are saved with [`~PreTrainedModel.save_pretrained`], call the [`~PretrainedConfig.register_for_auto_class`] method. This modifies the configuration JSON file to include the AutoClass objects and mapping.\n\nFor a model, pick the appropriate `AutoModelFor` class based on the task.\n\n```py\nResnetConfig.register_for_auto_class()\nResnetModel.register_for_auto_class(\"AutoModel\")\nResnetModelForImageClassification.register_for_auto_class(\"AutoModelForImageClassification\")\n```\n\nTo map more than one task to the model, edit `auto_map` in the configuration JSON file directly.\n\n```json\n\"auto_map\": {\n\"AutoConfig\": \"<your-repo-name>--<config-name>\",\n\"AutoModel\": \"<your-repo-name>--<config-name>\",\n\"AutoModelFor<Task>\": \"<your-repo-name>--<config-name>\",\n},\n```\n\nCreate the configuration and model and load pretrained weights into it.\n\n```py\nresnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\nresnet50d = ResnetModelForImageClassification(resnet50d_config)",
  "pretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\nresnet50d.model.load_state_dict(pretrained_model.state_dict())\n```\n\nThe model is ready to be pushed to the Hub now. Log in to your Hugging Face account from the command line or notebook.\n\n<hfoptions id=\"push\">\n<hfoption id=\"huggingface-CLI\">\n\n```bash\nhuggingface-cli login\n```\n\n</hfoption>\n<hfoption id=\"notebook\">\n\n```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n</hfoption>\n</hfoptions>\n\nCall [`~PreTrainedModel.push_to_hub`] on the model to upload the model to the Hub.\n\n```py\nresnet50d.push_to_hub(\"custom-resnet50d\")\n```\n\nThe pretrained weights, configuration, `modeling.py` and `configuration.py` files should all be uploaded to the Hub now in a [repository](https://hf.co/sgugger/custom-resnet50d) under your namespace.\n\nBecause a custom model doesn't use the same modeling code as a Transformers' model, you need to add `trust_remode_code=True` in [`~PreTrainedModel.from_pretrained`] to load it. Refer to the load [custom models](./models#custom-models) section for more information.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CPU\n\nCPUs are a viable and cost-effective inference option. With a few optimization methods, it is possible to achieve good performance with large models on CPUs. These methods include fusing kernels to reduce overhead and compiling your code to a faster intermediate format that can be deployed in production environments.\n\nThis guide will show you a few ways to optimize inference on a CPU.\n\n## Optimum",
  "[Optimum](https://hf.co/docs/optimum/en/index) is a Hugging Face library focused on optimizing model performance across various hardware. It supports [ONNX Runtime](https://onnxruntime.ai/docs/) (ORT), a model accelerator, for a wide range of hardware and frameworks including CPUs.\n\nOptimum provides the [`~optimum.onnxruntime.ORTModel`] class for loading ONNX models. For example, load the [optimum/roberta-base-squad2](https://hf.co/optimum/roberta-base-squad2) checkpoint for question answering inference. This checkpoint contains a [model.onnx](https://hf.co/optimum/roberta-base-squad2/blob/main/model.onnx) file.\n\n```py\nfrom transformers import AutoTokenizer, pipeline\nfrom optimum.onnxruntime import ORTModelForQuestionAnswering\n\nonnx_qa = pipeline(\"question-answering\", model=\"optimum/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n\nquestion = \"What's my name?\"\ncontext = \"My name is Philipp and I live in Nuremberg.\"\npred = onnx_qa(question, context)\n```\n\n> [!TIP]",
  "> Optimum includes an [Intel](https://hf.co/docs/optimum/intel/index) extension that provides additional optimizations such as quantization, pruning, and knowledge distillation for Intel CPUs. This extension also includes tools to convert models to [OpenVINO](https://hf.co/docs/optimum/intel/inference), a toolkit for optimizing and deploying models, for even faster inference.\n\n### BetterTransformer\n\n[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) is a *fastpath* execution of specialized Transformers functions directly on the hardware level such as a CPU. There are two main components of the fastpath execution.\n\n- fusing multiple operations into a single kernel for faster and more efficient execution\n- skipping unnecessary computation of padding tokens with nested tensors\n\n> [!WARNING]\n> BetterTransformer isn't supported for all models. Check this [list](https://hf.co/docs/optimum/bettertransformer/overview#supported-models) to see whether a model supports BetterTransformer.\n\nBetterTransformer is available through Optimum with [`~PreTrainedModel.to_bettertransformer`].\n\n```py\nfrom transformers import AutoModelForCausalLM",
  "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\")\nmodel = model.to_bettertransformer()\n```\n\n## TorchScript\n\n[TorchScript](https://pytorch.org/docs/stable/jit.html) is an intermediate PyTorch model format that can be run in non-Python environments, like C++, where performance is critical. Train a PyTorch model and convert it to a TorchScript function or module with [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html). This function optimizes the model with just-in-time (JIT) compilation, and compared to the default eager mode, JIT-compiled models offer better inference performance.\n\n> [!TIP]\n> Refer to the [Introduction to PyTorch TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html) tutorial for a gentle introduction to TorchScript.\n\nOn a CPU, enable `torch.jit.trace` with the `--jit_mode_eval` flag in [`Trainer`].\n\n```bash\npython examples/pytorch/question-answering/run_qa.py \\\n--model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n--dataset_name squad \\\n--do_eval \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/ \\\n--no_cuda \\\n--jit_mode_eval\n```\n\n## IPEX",
  "[Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/getting_started.html) (IPEX) offers additional optimizations for PyTorch on Intel CPUs. IPEX further optimizes TorchScript with [graph optimization](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html) which fuses operations like Multi-head attention, Concat Linear, Linear + Add, Linear + Gelu, Add + LayerNorm, and more, into single kernels for faster execution.\n\nMake sure IPEX is installed, and set the `--use_opex` and `--jit_mode_eval` flags in [`Trainer`] to enable IPEX graph optimization and TorchScript.\n\n```bash\n!pip install intel_extension_for_pytorch\n```\n\n```bash\npython examples/pytorch/question-answering/run_qa.py \\\n--model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n--dataset_name squad \\\n--do_eval \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/ \\\n--no_cuda \\\n--use_ipex \\\n--jit_mode_eval\n```",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Testing\n\n\nLet's take a look at how 🤗 Transformers models are tested and how you can write new tests and improve the existing ones.\n\nThere are 2 test suites in the repository:\n\n1. `tests` -- tests for the general API\n2. `examples` -- tests primarily for various applications that aren't part of the API\n\n## How transformers are tested",
  "1. Once a PR is submitted it gets tested with 9 CircleCi jobs. Every new commit to that PR gets retested. These jobs\nare defined in this [config file](https://github.com/huggingface/transformers/tree/main/.circleci/config.yml), so that if needed you can reproduce the same\nenvironment on your machine.\n\nThese CI jobs don't run `@slow` tests.\n\n2. There are 3 jobs run by [github actions](https://github.com/huggingface/transformers/actions):\n\n- [torch hub integration](https://github.com/huggingface/transformers/tree/main/.github/workflows/github-torch-hub.yml): checks whether torch hub\nintegration works.\n\n- [self-hosted (push)](https://github.com/huggingface/transformers/tree/main/.github/workflows/self-push.yml): runs fast tests on GPU only on commits on\n`main`. It only runs if a commit on `main` has updated the code in one of the following folders: `src`,\n`tests`, `.github` (to prevent running on added model cards, notebooks, etc.)\n\n- [self-hosted runner](https://github.com/huggingface/transformers/tree/main/.github/workflows/self-scheduled.yml): runs normal and slow tests on GPU in\n`tests` and `examples`:\n\n```bash\nRUN_SLOW=1 pytest tests/\nRUN_SLOW=1 pytest examples/\n```",
  "The results can be observed [here](https://github.com/huggingface/transformers/actions).\n\n\n\n## Running tests\n\n\n\n### Choosing which tests to run\n\nThis document goes into many details of how tests can be run. If after reading everything, you need even more details\nyou will find them [here](https://docs.pytest.org/en/latest/usage.html).\n\nHere are some most useful ways of running tests.\n\nRun all:\n\n```console\npytest\n```\n\nor:\n\n```bash\nmake test\n```\n\nNote that the latter is defined as:\n\n```bash\npython -m pytest -n auto --dist=loadfile -s -v ./tests/\n```\n\nwhich tells pytest to:\n\n- run as many test processes as they are CPU cores (which could be too many if you don't have a ton of RAM!)\n- ensure that all tests from the same file will be run by the same test process\n- do not capture output\n- run in verbose mode\n\n\n\n### Getting the list of all tests\n\nAll tests of the test suite:\n\n```bash\npytest --collect-only -q\n```\n\nAll tests of a given test file:\n\n```bash\npytest tests/test_optimization.py --collect-only -q\n```\n\n### Run a specific test module\n\nTo run an individual test module:\n\n```bash\npytest tests/utils/test_logging.py\n```\n\n### Run specific tests",
  "Since unittest is used inside most of the tests, to run specific subtests you need to know the name of the unittest\nclass containing those tests. For example, it could be:\n\n```bash\npytest tests/test_optimization.py::OptimizationTest::test_adam_w\n```\n\nHere:\n\n- `tests/test_optimization.py` - the file with tests\n- `OptimizationTest` - the name of the class\n- `test_adam_w` - the name of the specific test function\n\nIf the file contains multiple classes, you can choose to run only tests of a given class. For example:\n\n```bash\npytest tests/test_optimization.py::OptimizationTest\n```\n\nwill run all the tests inside that class.\n\nAs mentioned earlier you can see what tests are contained inside the `OptimizationTest` class by running:\n\n```bash\npytest tests/test_optimization.py::OptimizationTest --collect-only -q\n```\n\nYou can run tests by keyword expressions.\n\nTo run only tests whose name contains `adam`:\n\n```bash\npytest -k adam tests/test_optimization.py\n```\n\nLogical `and` and `or` can be used to indicate whether all keywords should match or either. `not` can be used to\nnegate.\n\nTo run all tests except those whose name contains `adam`:\n\n```bash\npytest -k \"not adam\" tests/test_optimization.py",
  "```\n\nAnd you can combine the two patterns in one:\n\n```bash\npytest -k \"ada and not adam\" tests/test_optimization.py\n```\n\nFor example to run both `test_adafactor` and `test_adam_w` you can use:\n\n```bash\npytest -k \"test_adafactor or test_adam_w\" tests/test_optimization.py\n```\n\nNote that we use `or` here, since we want either of the keywords to match to include both.\n\nIf you want to include only tests that include both patterns, `and` is to be used:\n\n```bash\npytest -k \"test and ada\" tests/test_optimization.py\n```\n\n### Run `accelerate` tests\n\nSometimes you need to run `accelerate` tests on your models. For that you can just add `-m accelerate_tests` to your command, if let's say you want to run these tests on `OPT` run:\n\n```bash\nRUN_SLOW=1 pytest -m accelerate_tests tests/models/opt/test_modeling_opt.py\n```\n\n\n### Run documentation tests\n\nIn order to test whether the documentation examples are correct, you should check that the `doctests` are passing.\nAs an example, let's use [`WhisperModel.forward`'s docstring](https://github.com/huggingface/transformers/blob/1124d95dbb1a3512d3e80791d73d0f541d1d7e9f/src/transformers/models/whisper/modeling_whisper.py#L1591-L1609)\n\n```python\nr\"\"\"",
  "Returns:\n\nExample:\n```python\n>>> import torch\n>>> from transformers import WhisperModel, WhisperFeatureExtractor\n>>> from datasets import load_dataset\n\n>>> model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n>>> feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n>>> input_features = inputs.input_features\n>>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n>>> list(last_hidden_state.shape)\n[1, 2, 512]\n```\"\"\"\n\n```\n\nJust run the following line to automatically test every docstring example in the desired file:\n```bash\npytest --doctest-modules <path_to_file_or_dir>\n```\nIf the file has a markdown extension, you should add the `--doctest-glob=\"*.md\"` argument.\n\n### Run only modified tests",
  "You can run the tests related to the unstaged files or the current branch (according to Git) by using [pytest-picked](https://github.com/anapaulagomes/pytest-picked). This is a great way of quickly testing your changes didn't break\nanything, since it won't run the tests related to files you didn't touch.\n\n```bash\npip install pytest-picked\n```\n\n```bash\npytest --picked\n```\n\nAll tests will be run from files and folders which are modified, but not yet committed.\n\n### Automatically rerun failed tests on source modification\n\n[pytest-xdist](https://github.com/pytest-dev/pytest-xdist) provides a very useful feature of detecting all failed\ntests, and then waiting for you to modify files and continuously re-rerun those failing tests until they pass while you\nfix them. So that you don't need to re start pytest after you made the fix. This is repeated until all tests pass after\nwhich again a full run is performed.\n\n```bash\npip install pytest-xdist\n```\n\nTo enter the mode: `pytest -f` or `pytest --looponfail`\n\nFile changes are detected by looking at `looponfailroots` root directories and all of their contents (recursively).",
  "If the default for this value does not work for you, you can change it in your project by setting a configuration\noption in `setup.cfg`:\n\n```ini\n[tool:pytest]\nlooponfailroots = transformers tests\n```\n\nor `pytest.ini`/``tox.ini`` files:\n\n```ini\n[pytest]\nlooponfailroots = transformers tests\n```\n\nThis would lead to only looking for file changes in the respective directories, specified relatively to the ini-file’s\ndirectory.\n\n[pytest-watch](https://github.com/joeyespo/pytest-watch) is an alternative implementation of this functionality.\n\n\n### Skip a test module\n\nIf you want to run all test modules, except a few you can exclude them by giving an explicit list of tests to run. For\nexample, to run all except `test_modeling_*.py` tests:\n\n```bash\npytest *ls -1 tests/*py | grep -v test_modeling*\n```\n\n### Clearing state\n\nCI builds and when isolation is important (against speed), cache should be cleared:\n\n```bash\npytest --cache-clear tests\n```\n\n### Running tests in parallel\n\nAs mentioned earlier `make test` runs tests in parallel via `pytest-xdist` plugin (`-n X` argument, e.g. `-n 2`\nto run 2 parallel jobs).",
  "`pytest-xdist`'s `--dist=` option allows one to control how the tests are grouped. `--dist=loadfile` puts the\ntests located in one file onto the same process.\n\nSince the order of executed tests is different and unpredictable, if running the test suite with `pytest-xdist`\nproduces failures (meaning we have some undetected coupled tests), use [pytest-replay](https://github.com/ESSS/pytest-replay) to replay the tests in the same order, which should help with then somehow\nreducing that failing sequence to a minimum.\n\n### Test order and repetition\n\nIt's good to repeat the tests several times, in sequence, randomly, or in sets, to detect any potential\ninter-dependency and state-related bugs (tear down). And the straightforward multiple repetition is just good to detect\nsome problems that get uncovered by randomness of DL.\n\n\n#### Repeat tests\n\n- [pytest-flakefinder](https://github.com/dropbox/pytest-flakefinder):\n\n```bash\npip install pytest-flakefinder\n```\n\nAnd then run every test multiple times (50 by default):\n\n```bash\npytest --flake-finder --flake-runs=5 tests/test_failing_test.py\n```\n\n<Tip>\n\nThis plugin doesn't work with `-n` flag from `pytest-xdist`.\n\n</Tip>\n\n<Tip>",
  "There is another plugin `pytest-repeat`, but it doesn't work with `unittest`.\n\n</Tip>\n\n#### Run tests in a random order\n\n```bash\npip install pytest-random-order\n```\n\nImportant: the presence of `pytest-random-order` will automatically randomize tests, no configuration change or\ncommand line options is required.\n\nAs explained earlier this allows detection of coupled tests - where one test's state affects the state of another. When\n`pytest-random-order` is installed it will print the random seed it used for that session, e.g:\n\n```bash\npytest tests\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\n```\n\nSo that if the given particular sequence fails, you can reproduce it by adding that exact seed, e.g.:\n\n```bash\npytest --random-order-seed=573663\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\n```\n\nIt will only reproduce the exact order if you use the exact same list of tests (or no list at all). Once you start to\nmanually narrowing down the list you can no longer rely on the seed, but have to list them manually in the exact order\nthey failed and tell pytest to not randomize them instead using `--random-order-bucket=none`, e.g.:\n\n```bash",
  "pytest --random-order-bucket=none tests/test_a.py tests/test_c.py tests/test_b.py\n```\n\nTo disable the shuffling for all tests:\n\n```bash\npytest --random-order-bucket=none\n```\n\nBy default `--random-order-bucket=module` is implied, which will shuffle the files on the module levels. It can also\nshuffle on `class`, `package`, `global` and `none` levels. For the complete details please see its\n[documentation](https://github.com/jbasko/pytest-random-order).\n\nAnother randomization alternative is: [`pytest-randomly`](https://github.com/pytest-dev/pytest-randomly). This\nmodule has a very similar functionality/interface, but it doesn't have the bucket modes available in\n`pytest-random-order`. It has the same problem of imposing itself once installed.\n\n### Look and feel variations\n\n#### pytest-sugar\n\n[pytest-sugar](https://github.com/Frozenball/pytest-sugar) is a plugin that improves the look-n-feel, adds a\nprogressbar, and show tests that fail and the assert instantly. It gets activated automatically upon installation.\n\n```bash\npip install pytest-sugar\n```\n\nTo run tests without it, run:\n\n```bash\npytest -p no:sugar\n```\n\nor uninstall it.\n\n\n\n#### Report each sub-test name and its progress",
  "For a single or a group of tests via `pytest` (after `pip install pytest-pspec`):\n\n```bash\npytest --pspec tests/test_optimization.py\n```\n\n#### Instantly shows failed tests\n\n[pytest-instafail](https://github.com/pytest-dev/pytest-instafail) shows failures and errors instantly instead of\nwaiting until the end of test session.\n\n```bash\npip install pytest-instafail\n```\n\n```bash\npytest --instafail\n```\n\n### To GPU or not to GPU\n\nOn a GPU-enabled setup, to test in CPU-only mode add `CUDA_VISIBLE_DEVICES=\"\"` for CUDA GPUs:\n\n```bash\nCUDA_VISIBLE_DEVICES=\"\" pytest tests/utils/test_logging.py\n```\n\nor if you have multiple gpus, you can specify which one is to be used by `pytest`. For example, to use only the\nsecond gpu if you have gpus `0` and `1`, you can run:\n\n```bash\nCUDA_VISIBLE_DEVICES=\"1\" pytest tests/utils/test_logging.py\n```\n\nFor Intel GPUs, use `ZE_AFFINITY_MASK` instead of `CUDA_VISIBLE_DEVICES` in the above example.\n\nThis is handy when you want to run different tasks on different GPUs.\n\nSome tests must be run on CPU-only, others on either CPU or GPU or TPU, yet others on multiple-GPUs. The following skip\ndecorators are used to set the requirements of tests CPU/GPU/XPU/TPU-wise:",
  "- `require_torch` - this test will run only under torch\n- `require_torch_gpu` - as `require_torch` plus requires at least 1 GPU\n- `require_torch_multi_gpu` - as `require_torch` plus requires at least 2 GPUs\n- `require_torch_non_multi_gpu` - as `require_torch` plus requires 0 or 1 GPUs\n- `require_torch_up_to_2_gpus` - as `require_torch` plus requires 0 or 1 or 2 GPUs\n- `require_torch_xla` - as `require_torch` plus requires at least 1 TPU\n\nLet's depict the GPU requirements in the following table:\n\n\n| n gpus | decorator                      |\n|--------|--------------------------------|\n| `>= 0` | `@require_torch`               |\n| `>= 1` | `@require_torch_gpu`           |\n| `>= 2` | `@require_torch_multi_gpu`     |\n| `< 2`  | `@require_torch_non_multi_gpu` |\n| `< 3`  | `@require_torch_up_to_2_gpus`  |\n\n\nFor example, here is a test that must be run only when there are 2 or more GPUs available and pytorch is installed:\n\n```python no-style\n@require_torch_multi_gpu\ndef test_example_with_multi_gpu():\n```\n\nIf a test requires `tensorflow` use the `require_tf` decorator. For example:\n\n```python no-style\n@require_tf\ndef test_tf_thing_with_tensorflow():\n```",
  "These decorators can be stacked. For example, if a test is slow and requires at least one GPU under pytorch, here is\nhow to set it up:\n\n```python no-style\n@require_torch_gpu\n@slow\ndef test_example_slow_on_gpu():\n```\n\nSome decorators like `@parametrized` rewrite test names, therefore `@require_*` skip decorators have to be listed\nlast for them to work correctly. Here is an example of the correct usage:\n\n```python no-style\n@parameterized.expand(...)\n@require_torch_multi_gpu\ndef test_integration_foo():\n```\n\nThis order problem doesn't exist with `@pytest.mark.parametrize`, you can put it first or last and it will still\nwork. But it only works with non-unittests.\n\nInside tests:\n\n- How many GPUs are available:\n\n```python\nfrom transformers.testing_utils import get_gpu_count\n\nn_gpu = get_gpu_count()  # works with torch and tf\n```\n\n### Testing with a specific PyTorch backend or device\n\nTo run the test suite on a specific torch device add `TRANSFORMERS_TEST_DEVICE=\"$device\"` where `$device` is the target backend. For example, to test on CPU only:\n\n```bash\nTRANSFORMERS_TEST_DEVICE=\"cpu\" pytest tests/utils/test_logging.py\n```",
  "This variable is useful for testing custom or less common PyTorch backends such as `mps`, `xpu` or `npu`. It can also be used to achieve the same effect as `CUDA_VISIBLE_DEVICES` by targeting specific GPUs or testing in CPU-only mode.\n\nCertain devices will require an additional import after importing `torch` for the first time. This can be specified using the environment variable `TRANSFORMERS_TEST_BACKEND`:\n\n```bash\nTRANSFORMERS_TEST_BACKEND=\"torch_npu\" pytest tests/utils/test_logging.py\n```\nAlternative backends may also require the replacement of device-specific functions. For example `torch.cuda.manual_seed` may need to be replaced with a device-specific seed setter like `torch.npu.manual_seed` or `torch.xpu.manual_seed` to correctly set a random seed on the device. To specify a new backend with backend-specific device functions when running the test suite, create a Python device specification file `spec.py` in the format:\n\n```python\nimport torch\nimport torch_npu # for xpu, replace it with `import intel_extension_for_pytorch`\n# !! Further additional imports can be added here !!\n\n# Specify the device name (eg. 'cuda', 'cpu', 'npu', 'xpu', 'mps')\nDEVICE_NAME = 'npu'",
  "# Specify device-specific backends to dispatch to.\n# If not specified, will fallback to 'default' in 'testing_utils.py`\nMANUAL_SEED_FN = torch.npu.manual_seed\nEMPTY_CACHE_FN = torch.npu.empty_cache\nDEVICE_COUNT_FN = torch.npu.device_count\n```\nThis format also allows for specification of any additional imports required. To use this file to replace equivalent methods in the test suite, set the environment variable `TRANSFORMERS_TEST_DEVICE_SPEC` to the path of the spec file, e.g. `TRANSFORMERS_TEST_DEVICE_SPEC=spec.py`.\n\nCurrently, only `MANUAL_SEED_FN`, `EMPTY_CACHE_FN` and `DEVICE_COUNT_FN` are supported for device-specific dispatch.\n\n### Distributed training\n\n`pytest` can't deal with distributed training directly. If this is attempted - the sub-processes don't do the right\nthing and end up thinking they are `pytest` and start running the test suite in loops. It works, however, if one\nspawns a normal process that then spawns off multiple workers and manages the IO pipes.\n\nHere are some tests that use it:\n\n- [test_trainer_distributed.py](https://github.com/huggingface/transformers/tree/main/tests/trainer/test_trainer_distributed.py)",
  "- [test_deepspeed.py](https://github.com/huggingface/transformers/tree/main/tests/deepspeed/test_deepspeed.py)\n\nTo jump right into the execution point, search for the `execute_subprocess_async` call in those tests.\n\nYou will need at least 2 GPUs to see these tests in action:\n\n```bash\nCUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/test_trainer_distributed.py\n```\n\n### Output capture\n\nDuring test execution any output sent to `stdout` and `stderr` is captured. If a test or a setup method fails, its\naccording captured output will usually be shown along with the failure traceback.\n\nTo disable output capturing and to get the `stdout` and `stderr` normally, use `-s` or `--capture=no`:\n\n```bash\npytest -s tests/utils/test_logging.py\n```\n\nTo send test results to JUnit format output:\n\n```bash\npytest tests --junitxml=result.xml\n```\n\n### Color control\n\nTo have no color (e.g., yellow on white background is not readable):\n\n```bash\npytest --color=no tests/utils/test_logging.py\n```\n\n### Sending test report to online pastebin service\n\nCreating a URL for each test failure:\n\n```bash\npytest --pastebin=failed tests/utils/test_logging.py\n```",
  "This will submit test run information to a remote Paste service and provide a URL for each failure. You may select\ntests as usual or add for example -x if you only want to send one particular failure.\n\nCreating a URL for a whole test session log:\n\n```bash\npytest --pastebin=all tests/utils/test_logging.py\n```\n\n## Writing tests\n\n🤗 transformers tests are based on `unittest`, but run by `pytest`, so most of the time features from both systems\ncan be used.\n\nYou can read [here](https://docs.pytest.org/en/stable/unittest.html) which features are supported, but the important\nthing to remember is that most `pytest` fixtures don't work. Neither parametrization, but we use the module\n`parameterized` that works in a similar way.\n\n\n### Parametrization\n\nOften, there is a need to run the same test multiple times, but with different arguments. It could be done from within\nthe test, but then there is no way of running that test for just one set of arguments.\n\n```python\n# test_this1.py\nimport unittest\nfrom parameterized import parameterized\n\n\nclass TestMathUnitTest(unittest.TestCase):\n@parameterized.expand(\n[\n(\"negative\", -1.5, -2.0),\n(\"integer\", 1, 1.0),\n(\"large fraction\", 1.6, 1),\n]\n)",
  "def test_floor(self, name, input, expected):\nassert_equal(math.floor(input), expected)\n```\n\nNow, by default this test will be run 3 times, each time with the last 3 arguments of `test_floor` being assigned the\ncorresponding arguments in the parameter list.\n\nand you could run just the `negative` and `integer` sets of params with:\n\n```bash\npytest -k \"negative and integer\" tests/test_mytest.py\n```\n\nor all but `negative` sub-tests, with:\n\n```bash\npytest -k \"not negative\" tests/test_mytest.py\n```\n\nBesides using the `-k` filter that was just mentioned, you can find out the exact name of each sub-test and run any\nor all of them using their exact names.\n\n```bash\npytest test_this1.py --collect-only -q\n```\n\nand it will list:\n\n```bash\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\n```\n\nSo now you can run just 2 specific sub-tests:\n\n```bash\npytest test_this1.py::TestMathUnitTest::test_floor_0_negative  test_this1.py::TestMathUnitTest::test_floor_1_integer\n```",
  "The module [parameterized](https://pypi.org/project/parameterized/) which is already in the developer dependencies\nof `transformers` works for both: `unittests` and `pytest` tests.\n\nIf, however, the test is not a `unittest`, you may use `pytest.mark.parametrize` (or you may see it being used in\nsome existing tests, mostly under `examples`).\n\nHere is the same example, this time using `pytest`'s `parametrize` marker:\n\n```python\n# test_this2.py\nimport pytest\n\n\n@pytest.mark.parametrize(\n\"name, input, expected\",\n[\n(\"negative\", -1.5, -2.0),\n(\"integer\", 1, 1.0),\n(\"large fraction\", 1.6, 1),\n],\n)\ndef test_floor(name, input, expected):\nassert_equal(math.floor(input), expected)\n```\n\nSame as with `parameterized`, with `pytest.mark.parametrize` you can have a fine control over which sub-tests are\nrun, if the `-k` filter doesn't do the job. Except, this parametrization function creates a slightly different set of\nnames for the sub-tests. Here is what they look like:\n\n```bash\npytest test_this2.py --collect-only -q\n```\n\nand it will list:\n\n```bash\ntest_this2.py::test_floor[integer-1-1.0]\ntest_this2.py::test_floor[negative--1.5--2.0]\ntest_this2.py::test_floor[large fraction-1.6-1]\n```",
  "So now you can run just the specific test:\n\n```bash\npytest test_this2.py::test_floor[negative--1.5--2.0] test_this2.py::test_floor[integer-1-1.0]\n```\n\nas in the previous example.\n\n\n\n### Files and directories\n\nIn tests often we need to know where things are relative to the current test file, and it's not trivial since the test\ncould be invoked from more than one directory or could reside in sub-directories with different depths. A helper class\n`transformers.test_utils.TestCasePlus` solves this problem by sorting out all the basic paths and provides easy\naccessors to them:\n\n- `pathlib` objects (all fully resolved):\n\n- `test_file_path` - the current test file path, i.e. `__file__`\n- `test_file_dir` - the directory containing the current test file\n- `tests_dir` - the directory of the `tests` test suite\n- `examples_dir` - the directory of the `examples` test suite\n- `repo_root_dir` - the directory of the repository\n- `src_dir` - the directory of `src` (i.e. where the `transformers` sub-dir resides)\n\n- stringified paths---same as above but these return paths as strings, rather than `pathlib` objects:\n\n- `test_file_path_str`\n- `test_file_dir_str`\n- `tests_dir_str`\n- `examples_dir_str`",
  "- `repo_root_dir_str`\n- `src_dir_str`\n\nTo start using those all you need is to make sure that the test resides in a subclass of\n`transformers.test_utils.TestCasePlus`. For example:\n\n```python\nfrom transformers.testing_utils import TestCasePlus\n\n\nclass PathExampleTest(TestCasePlus):\ndef test_something_involving_local_locations(self):\ndata_dir = self.tests_dir / \"fixtures/tests_samples/wmt_en_ro\"\n```\n\nIf you don't need to manipulate paths via `pathlib` or you just need a path as a string, you can always invoked\n`str()` on the `pathlib` object or use the accessors ending with `_str`. For example:\n\n```python\nfrom transformers.testing_utils import TestCasePlus\n\n\nclass PathExampleTest(TestCasePlus):\ndef test_something_involving_stringified_locations(self):\nexamples_dir = self.examples_dir_str\n```\n\n### Temporary files and directories\n\nUsing unique temporary files and directories are essential for parallel test running, so that the tests won't overwrite\neach other's data. Also we want to get the temporary files and directories removed at the end of each test that created\nthem. Therefore, using packages like `tempfile`, which address these needs is essential.",
  "However, when debugging tests, you need to be able to see what goes into the temporary file or directory and you want\nto know it's exact path and not having it randomized on every test re-run.\n\nA helper class `transformers.test_utils.TestCasePlus` is best used for such purposes. It's a sub-class of\n`unittest.TestCase`, so we can easily inherit from it in the test modules.\n\nHere is an example of its usage:\n\n```python\nfrom transformers.testing_utils import TestCasePlus\n\n\nclass ExamplesTests(TestCasePlus):\ndef test_whatever(self):\ntmp_dir = self.get_auto_remove_tmp_dir()\n```\n\nThis code creates a unique temporary directory, and sets `tmp_dir` to its location.\n\n- Create a unique temporary dir:\n\n```python\ndef test_whatever(self):\ntmp_dir = self.get_auto_remove_tmp_dir()\n```\n\n`tmp_dir` will contain the path to the created temporary dir. It will be automatically removed at the end of the\ntest.\n\n- Create a temporary dir of my choice, ensure it's empty before the test starts and don't empty it after the test.\n\n```python\ndef test_whatever(self):\ntmp_dir = self.get_auto_remove_tmp_dir(\"./xxx\")\n```",
  "This is useful for debug when you want to monitor a specific directory and want to make sure the previous tests didn't\nleave any data in there.\n\n- You can override the default behavior by directly overriding the `before` and `after` args, leading to one of the\nfollowing behaviors:\n\n- `before=True`: the temporary dir will always be cleared at the beginning of the test.\n- `before=False`: if the temporary dir already existed, any existing files will remain there.\n- `after=True`: the temporary dir will always be deleted at the end of the test.\n- `after=False`: the temporary dir will always be left intact at the end of the test.\n\n<Tip>\n\nIn order to run the equivalent of `rm -r` safely, only subdirs of the project repository checkout are allowed if\nan explicit `tmp_dir` is used, so that by mistake no `/tmp` or similar important part of the filesystem will\nget nuked. i.e. please always pass paths that start with `./`.\n\n</Tip>\n\n<Tip>\n\nEach test can register multiple temporary directories and they all will get auto-removed, unless requested\notherwise.\n\n</Tip>\n\n### Temporary sys.path override",
  "If you need to temporary override `sys.path` to import from another test for example, you can use the\n`ExtendSysPath` context manager. Example:\n\n\n```python\nimport os\nfrom transformers.testing_utils import ExtendSysPath\n\nbindir = os.path.abspath(os.path.dirname(__file__))\nwith ExtendSysPath(f\"{bindir}/..\"):\nfrom test_trainer import TrainerIntegrationCommon  # noqa\n```\n\n### Skipping tests\n\nThis is useful when a bug is found and a new test is written, yet the bug is not fixed yet. In order to be able to\ncommit it to the main repository we need make sure it's skipped during `make test`.\n\nMethods:\n\n-  A **skip** means that you expect your test to pass only if some conditions are met, otherwise pytest should skip\nrunning the test altogether. Common examples are skipping windows-only tests on non-windows platforms, or skipping\ntests that depend on an external resource which is not available at the moment (for example a database).\n\n-  A **xfail** means that you expect a test to fail for some reason. A common example is a test for a feature not yet\nimplemented, or a bug not yet fixed. When a test passes despite being expected to fail (marked with",
  "pytest.mark.xfail), it’s an xpass and will be reported in the test summary.\n\nOne of the important differences between the two is that `skip` doesn't run the test, and `xfail` does. So if the\ncode that's buggy causes some bad state that will affect other tests, do not use `xfail`.\n\n#### Implementation\n\n- Here is how to skip whole test unconditionally:\n\n```python no-style\n@unittest.skip(reason=\"this bug needs to be fixed\")\ndef test_feature_x():\n```\n\nor via pytest:\n\n```python no-style\n@pytest.mark.skip(reason=\"this bug needs to be fixed\")\n```\n\nor the `xfail` way:\n\n```python no-style\n@pytest.mark.xfail\ndef test_feature_x():\n```\n\n\nHere's how to skip a test based on internal checks within the test:\n\n```python\ndef test_feature_x():\nif not has_something():\npytest.skip(\"unsupported configuration\")\n```\n\nor the whole module:\n\n```python\nimport pytest\n\nif not pytest.config.getoption(\"--custom-flag\"):\npytest.skip(\"--custom-flag is missing, skipping tests\", allow_module_level=True)\n```\n\nor the `xfail` way:\n\n```python\ndef test_feature_x():\npytest.xfail(\"expected to fail until bug XYZ is fixed\")\n```\n\n- Here is how to skip all tests in a module if some import is missing:\n\n```python",
  "docutils = pytest.importorskip(\"docutils\", minversion=\"0.3\")\n```\n\n-  Skip a test based on a condition:\n\n```python no-style\n@pytest.mark.skipif(sys.version_info < (3,6), reason=\"requires python3.6 or higher\")\ndef test_feature_x():\n```\n\nor:\n\n```python no-style\n@unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\ndef test_feature_x():\n```\n\nor skip the whole module:\n\n```python no-style\n@pytest.mark.skipif(sys.platform == 'win32', reason=\"does not run on windows\")\nclass TestClass():\ndef test_feature_x(self):\n```\n\nMore details, example and ways are [here](https://docs.pytest.org/en/latest/skipping.html).\n\n### Slow tests\n\nThe library of tests is ever-growing, and some of the tests take minutes to run, therefore we can't afford waiting for\nan hour for the test suite to complete on CI. Therefore, with some exceptions for essential tests, slow tests should be\nmarked as in the example below:\n\n```python no-style\nfrom transformers.testing_utils import slow\n@slow\ndef test_integration_foo():\n```\n\nOnce a test is marked as `@slow`, to run such tests set `RUN_SLOW=1` env var, e.g.:\n\n```bash\nRUN_SLOW=1 pytest tests\n```",
  "Some decorators like `@parameterized` rewrite test names, therefore `@slow` and the rest of the skip decorators\n`@require_*` have to be listed last for them to work correctly. Here is an example of the correct usage:\n\n```python no-style\n@parameterized.expand(...)\n@slow\ndef test_integration_foo():\n```\n\nAs explained at the beginning of this document, slow tests get to run on a scheduled basis, rather than in PRs CI\nchecks. So it's possible that some problems will be missed during a PR submission and get merged. Such problems will\nget caught during the next scheduled CI job. But it also means that it's important to run the slow tests on your\nmachine before submitting the PR.\n\nHere is a rough decision making mechanism for choosing which tests should be marked as slow:\n\nIf the test is focused on one of the library's internal components (e.g., modeling files, tokenization files,\npipelines), then we should run that test in the non-slow test suite. If it's focused on an other aspect of the library,\nsuch as the documentation or the examples, then we should run these tests in the slow test suite. And then, to refine\nthis approach we should have exceptions:",
  "- All tests that need to download a heavy set of weights or a dataset that is larger than ~50MB (e.g., model or\ntokenizer integration tests, pipeline integration tests) should be set to slow. If you're adding a new model, you\nshould create and upload to the hub a tiny version of it (with random weights) for integration tests. This is\ndiscussed in the following paragraphs.\n- All tests that need to do a training not specifically optimized to be fast should be set to slow.\n- We can introduce exceptions if some of these should-be-non-slow tests are excruciatingly slow, and set them to\n`@slow`. Auto-modeling tests, which save and load large files to disk, are a good example of tests that are marked\nas `@slow`.\n- If a test completes under 1 second on CI (including downloads if any) then it should be a normal test regardless.\n\nCollectively, all the non-slow tests need to cover entirely the different internals, while remaining fast. For example,\na significant coverage can be achieved by testing with specially created tiny models with random weights. Such models\nhave the very minimal number of layers (e.g., 2), vocab size (e.g., 1000), etc. Then the `@slow` tests can use large",
  "slow models to do qualitative testing. To see the use of these simply look for *tiny* models with:\n\n```bash\ngrep tiny tests examples\n```\n\nHere is an example of a [script](https://github.com/huggingface/transformers/tree/main/scripts/fsmt/fsmt-make-tiny-model.py) that created the tiny model\n[stas/tiny-wmt19-en-de](https://huggingface.co/stas/tiny-wmt19-en-de). You can easily adjust it to your specific\nmodel's architecture.\n\nIt's easy to measure the run-time incorrectly if for example there is an overheard of downloading a huge model, but if\nyou test it locally the downloaded files would be cached and thus the download time not measured. Hence check the\nexecution speed report in CI logs instead (the output of `pytest --durations=0 tests`).\n\nThat report is also useful to find slow outliers that aren't marked as such, or which need to be re-written to be fast.\nIf you notice that the test suite starts getting slow on CI, the top listing of this report will show the slowest\ntests.\n\n\n### Testing the stdout/stderr output\n\nIn order to test functions that write to `stdout` and/or `stderr`, the test can access those streams using the",
  "`pytest`'s [capsys system](https://docs.pytest.org/en/latest/capture.html). Here is how this is accomplished:\n\n```python\nimport sys\n\n\ndef print_to_stdout(s):\nprint(s)\n\n\ndef print_to_stderr(s):\nsys.stderr.write(s)\n\n\ndef test_result_and_stdout(capsys):\nmsg = \"Hello\"\nprint_to_stdout(msg)\nprint_to_stderr(msg)\nout, err = capsys.readouterr()  # consume the captured output streams\n# optional: if you want to replay the consumed streams:\nsys.stdout.write(out)\nsys.stderr.write(err)\n# test:\nassert msg in out\nassert msg in err\n```\n\nAnd, of course, most of the time, `stderr` will come as a part of an exception, so try/except has to be used in such\na case:\n\n```python\ndef raise_exception(msg):\nraise ValueError(msg)\n\n\ndef test_something_exception():\nmsg = \"Not a good value\"\nerror = \"\"\ntry:\nraise_exception(msg)\nexcept Exception as e:\nerror = str(e)\nassert msg in error, f\"{msg} is in the exception:\\n{error}\"\n```\n\nAnother approach to capturing stdout is via `contextlib.redirect_stdout`:\n\n```python\nfrom io import StringIO\nfrom contextlib import redirect_stdout\n\n\ndef print_to_stdout(s):\nprint(s)\n\n\ndef test_result_and_stdout():\nmsg = \"Hello\"\nbuffer = StringIO()\nwith redirect_stdout(buffer):",
  "print_to_stdout(msg)\nout = buffer.getvalue()\n# optional: if you want to replay the consumed streams:\nsys.stdout.write(out)\n# test:\nassert msg in out\n```\n\nAn important potential issue with capturing stdout is that it may contain `\\r` characters that in normal `print`\nreset everything that has been printed so far. There is no problem with `pytest`, but with `pytest -s` these\ncharacters get included in the buffer, so to be able to have the test run with and without `-s`, you have to make an\nextra cleanup to the captured output, using `re.sub(r'~.*\\r', '', buf, 0, re.M)`.\n\nBut, then we have a helper context manager wrapper to automatically take care of it all, regardless of whether it has\nsome `\\r`'s in it or not, so it's a simple:\n\n```python\nfrom transformers.testing_utils import CaptureStdout\n\nwith CaptureStdout() as cs:\nfunction_that_writes_to_stdout()\nprint(cs.out)\n```\n\nHere is a full test example:\n\n```python\nfrom transformers.testing_utils import CaptureStdout\n\nmsg = \"Secret message\\r\"\nfinal = \"Hello World\"\nwith CaptureStdout() as cs:\nprint(msg + final)\nassert cs.out == final + \"\\n\", f\"captured: {cs.out}, expecting {final}\"\n```",
  "If you'd like to capture `stderr` use the `CaptureStderr` class instead:\n\n```python\nfrom transformers.testing_utils import CaptureStderr\n\nwith CaptureStderr() as cs:\nfunction_that_writes_to_stderr()\nprint(cs.err)\n```\n\nIf you need to capture both streams at once, use the parent `CaptureStd` class:\n\n```python\nfrom transformers.testing_utils import CaptureStd\n\nwith CaptureStd() as cs:\nfunction_that_writes_to_stdout_and_stderr()\nprint(cs.err, cs.out)\n```\n\nAlso, to aid debugging test issues, by default these context managers automatically replay the captured streams on exit\nfrom the context.\n\n\n### Capturing logger stream\n\nIf you need to validate the output of a logger, you can use `CaptureLogger`:\n\n```python\nfrom transformers import logging\nfrom transformers.testing_utils import CaptureLogger\n\nmsg = \"Testing 1, 2, 3\"\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")\nwith CaptureLogger(logger) as cl:\nlogger.info(msg)\nassert cl.out, msg + \"\\n\"\n```\n\n### Testing with environment variables\n\nIf you want to test the impact of environment variables for a specific test you can use a helper decorator\n`transformers.testing_utils.mockenv`",
  "```python\nfrom transformers.testing_utils import mockenv\n\n\nclass HfArgumentParserTest(unittest.TestCase):\n@mockenv(TRANSFORMERS_VERBOSITY=\"error\")\ndef test_env_override(self):\nenv_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\n```\n\nAt times an external program needs to be called, which requires setting `PYTHONPATH` in `os.environ` to include\nmultiple local paths. A helper class `transformers.test_utils.TestCasePlus` comes to help:\n\n```python\nfrom transformers.testing_utils import TestCasePlus\n\n\nclass EnvExampleTest(TestCasePlus):\ndef test_external_prog(self):\nenv = self.get_env()\n# now call the external program, passing `env` to it\n```\n\nDepending on whether the test file was under the `tests` test suite or `examples` it'll correctly set up\n`env[PYTHONPATH]` to include one of these two directories, and also the `src` directory to ensure the testing is\ndone against the current repo, and finally with whatever `env[PYTHONPATH]` was already set to before the test was\ncalled if anything.\n\nThis helper method creates a copy of the `os.environ` object, so the original remains intact.\n\n\n### Getting reproducible results",
  "In some situations you may want to remove randomness for your tests. To get identical reproducible results set, you\nwill need to fix the seed:\n\n```python\nseed = 42\n\n# python RNG\nimport random\n\nrandom.seed(seed)\n\n# pytorch RNGs\nimport torch\n\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available():\ntorch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nimport numpy as np\n\nnp.random.seed(seed)\n\n# tf RNG\nimport tensorflow as tf\n\ntf.random.set_seed(seed)\n```\n\n### Debugging tests\n\nTo start a debugger at the point of the warning, do this:\n\n```bash\npytest tests/utils/test_logging.py -W error::UserWarning --pdb\n```\n\n## Working with github actions workflows\n\nTo trigger a self-push workflow CI job, you must:\n\n1. Create a new branch on `transformers` origin (not a fork!).\n2. The branch name has to start with either `ci_` or `ci-` (`main` triggers it too, but we can't do PRs on\n`main`). It also gets triggered only for specific paths - you can find the up-to-date definition in case it\nchanged since this document has been written [here](https://github.com/huggingface/transformers/blob/main/.github/workflows/self-push.yml) under *push:*",
  "3. Create a PR from this branch.\n4. Then you can see the job appear [here](https://github.com/huggingface/transformers/actions/workflows/self-push.yml). It may not run right away if there\nis a backlog.\n\n\n\n\n## Testing Experimental CI Features\n\nTesting CI features can be potentially problematic as it can interfere with the normal CI functioning. Therefore if a\nnew CI feature is to be added, it should be done as following.\n\n1. Create a new dedicated job that tests what needs to be tested\n2. The new job must always succeed so that it gives us a green ✓ (details below).\n3. Let it run for some days to see that a variety of different PR types get to run on it (user fork branches,\nnon-forked branches, branches originating from github.com UI direct file edit, various forced pushes, etc. - there\nare so many) while monitoring the experimental job's logs (not the overall job green as it's purposefully always\ngreen)\n4. When it's clear that everything is solid, then merge the new changes into existing jobs.\n\nThat way experiments on CI functionality itself won't interfere with the normal workflow.\n\nNow how can we make the job always succeed while the new CI feature is being developed?",
  "Some CIs, like TravisCI support ignore-step-failure and will report the overall job as successful, but CircleCI and\nGithub Actions as of this writing don't support that.\n\nSo the following workaround can be used:\n\n1. `set +euo pipefail` at the beginning of the run command to suppress most potential failures in the bash script.\n2. the last command must be a success: `echo \"done\"` or just `true` will do\n\nHere is an example:\n\n```yaml\n- run:\nname: run CI experiment\ncommand: |\nset +euo pipefail\necho \"setting run-all-despite-any-errors-mode\"\nthis_command_will_fail\necho \"but bash continues to run\"\n# emulate another failure\nfalse\n# but the last command must be a success\necho \"during experiment do not remove: reporting success to CI, even if there were failures\"\n```\n\nFor simple commands you could also do:\n\n```bash\ncmd_that_may_fail || true\n```\n\nOf course, once satisfied with the results, integrate the experimental step or job with the rest of the normal jobs,\nwhile removing `set +euo pipefail` or any other things you may have added to ensure that the experimental job doesn't\ninterfere with the normal CI functioning.",
  "This whole process would have been much easier if we only could set something like `allow-failure` for the\nexperimental step, and let it fail without impacting the overall status of PRs. But as mentioned earlier CircleCI and\nGithub Actions don't support it at the moment.\n\nYou can vote for this feature and see where it is at these CI-specific threads:\n\n- [Github Actions:](https://github.com/actions/toolkit/issues/399)\n- [CircleCI:](https://ideas.circleci.com/ideas/CCI-I-344)\n\n## DeepSpeed integration\n\nFor a PR that involves the DeepSpeed integration, keep in mind our CircleCI PR CI setup doesn't have GPUs. Tests requiring GPUs are run on a different CI nightly. This means if you get a passing CI report in your PR, it doesn’t mean the DeepSpeed tests pass.\n\nTo run DeepSpeed tests:\n\n```bash\nRUN_SLOW=1 pytest tests/deepspeed/test_deepspeed.py\n```\n\nAny changes to the modeling or PyTorch examples code requires running the model zoo tests as well.\n\n```bash\nRUN_SLOW=1 pytest tests/deepspeed\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Feature extractors\n\nFeature extractors preprocess audio data into the correct format for a given model. It takes the raw audio signal and converts it into a tensor that can be fed to a model. The tensor shape depends on the model, but the feature extractor will correctly preprocess the audio data for you given the model you're using. Feature extractors also include methods for padding, truncation, and resampling.",
  "Call [`~AutoFeatureExtractor.from_pretrained`] to load a feature extractor and its preprocessor configuration from the Hugging Face [Hub](https://hf.co/models) or local directory. The feature extractor and preprocessor configuration is saved in a [preprocessor_config.json](https://hf.co/openai/whisper-tiny/blob/main/preprocessor_config.json) file.\n\nPass the audio signal, typically stored in `array`, to the feature extractor and set the `sampling_rate` parameter to the pretrained audio models sampling rate. It is important the sampling rate of the audio data matches the sampling rate of the data a pretrained audio model was trained on.\n\n```py\nfrom transformers import AutoFeatureExtractor\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\nprocessed_sample = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=16000)\nprocessed_sample\n{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n-2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}\n```\n\nThe feature extractor returns an input, `input_values`, that is ready for the model to consume.",
  "This guide walks you through the feature extractor classes and how to preprocess audio data.\n\n## Feature extractor classes\n\nTransformers feature extractors inherit from the base [`SequenceFeatureExtractor`] class which subclasses [`FeatureExtractionMixin`].\n\n- [`SequenceFeatureExtractor`] provides a method to [`~SequenceFeatureExtractor.pad`] sequences to a certain length to avoid uneven sequence lengths.\n- [`FeatureExtractionMixin`] provides [`~FeatureExtractionMixin.from_pretrained`] and [`~FeatureExtractionMixin.save_pretrained`] to load and save a feature extractor.\n\nThere are two ways you can load a feature extractor, [`AutoFeatureExtractor`] and a model-specific feature extractor class.\n\n<hfoptions id=\"feature-extractor-classes\">\n<hfoption id=\"AutoFeatureExtractor\">\n\nThe [AutoClass](./model_doc/auto) API automatically loads the correct feature extractor for a given model.\n\nUse [`~AutoFeatureExtractor.from_pretrained`] to load a feature extractor.\n\n```py\nfrom transformers import AutoFeatureExtractor\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n```\n\n</hfoption>\n<hfoption id=\"model-specific feature extractor\">",
  "Every pretrained audio model has a specific associated feature extractor for correctly processing audio data. When you load a feature extractor, it retrieves the feature extractors configuration (feature size, chunk length, etc.) from [preprocessor_config.json](https://hf.co/openai/whisper-tiny/blob/main/preprocessor_config.json).\n\nA feature extractor can be loaded directly from its model-specific class.\n\n```py\nfrom transformers import WhisperFeatureExtractor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Preprocess\n\nA feature extractor expects the input as a PyTorch tensor of a certain shape. The exact input shape can vary depending on the specific audio model you're using.\n\nFor example, [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper) expects `input_features` to be a tensor of shape `(batch_size, feature_size, sequence_length)` but [Wav2Vec2](https://hf.co/docs/transformers/model_doc/wav2vec2) expects `input_values` to be a tensor of shape `(batch_size, sequence_length)`.\n\nThe feature extractor generates the correct input shape for whichever audio model you're using.",
  "A feature extractor also sets the sampling rate (the number of audio signal values taken per second) of the audio files. The sampling rate of your audio data must match the sampling rate of the dataset a pretrained model was trained on. This value is typically given in the model card.\n\nLoad a dataset and feature extractor with [`~FeatureExtractionMixin.from_pretrained`].\n\n```py\nfrom datasets import load_dataset, Audio\nfrom transformers import AutoFeatureExtractor\n\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n```\n\nCheck out the first example from the dataset and access the `audio` column which contains `array`, the raw audio signal.\n\n```py\ndataset[0][\"audio\"][\"array\"]\narray([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n0.        ,  0.        ])\n```\n\nThe feature extractor preprocesses `array` into the expected input format for a given audio model. Use the `sampling_rate` parameter to set the appropriate sampling rate.\n\n```py\nprocessed_dataset = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=16000)\nprocessed_dataset",
  "{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n-2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}\n```\n\n### Padding\n\nAudio sequence lengths that are different is an issue because Transformers expects all sequences to have the same lengths so they can be batched. Uneven sequence lengths can't be batched.\n\n```py\ndataset[0][\"audio\"][\"array\"].shape\n(86699,)\n\ndataset[1][\"audio\"][\"array\"].shape\n(53248,)\n```\n\nPadding adds a special *padding token* to ensure all sequences have the same length. The feature extractor adds a `0` - interpreted as silence - to `array` to pad it. Set `padding=True` to pad sequences to the longest sequence length in the batch.\n\n```py\ndef preprocess_function(examples):\naudio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\ninputs = feature_extractor(\naudio_arrays,\nsampling_rate=16000,\npadding=True,\n)\nreturn inputs\n\nprocessed_dataset = preprocess_function(dataset[:5])\nprocessed_dataset[\"input_values\"][0].shape\n(86699,)\n\nprocessed_dataset[\"input_values\"][1].shape\n(86699,)\n```\n\n### Truncation\n\nModels can only process sequences up to a certain length before crashing.",
  "Truncation is a strategy for removing excess tokens from a sequence to ensure it doesn't exceed the maximum length. Set `truncation=True` to truncate a sequence to the length in the `max_length` parameter.\n\n```py\ndef preprocess_function(examples):\naudio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\ninputs = feature_extractor(\naudio_arrays,\nsampling_rate=16000,\nmax_length=50000,\ntruncation=True,\n)\nreturn inputs\n\nprocessed_dataset = preprocess_function(dataset[:5])\nprocessed_dataset[\"input_values\"][0].shape\n(50000,)\n\nprocessed_dataset[\"input_values\"][1].shape\n(50000,)\n```\n\n### Resampling\n\nThe [Datasets](https://hf.co/docs/datasets/index) library can also resample audio data to match an audio models expected sampling rate. This method resamples the audio data on the fly when they're loaded which can be faster than resampling the entire dataset in-place.\n\nThe audio dataset you've been working on has a sampling rate of 8kHz and the pretrained model expects 16kHz.\n\n```py\ndataset[0][\"audio\"]\n{'path': '/root/.cache/huggingface/datasets/downloads/extracted/f507fdca7f475d961f5bb7093bcc9d544f16f8cab8608e772a2ed4fbeb4d6f50/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',",
  "'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n0.        ,  0.        ]),\n'sampling_rate': 8000}\n```\n\nCall [`~datasets.Dataset.cast_column`] on the `audio` column to upsample the sampling rate to 16kHz.\n\n```py\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n```\n\nWhen you load the dataset sample, it is now resampled to 16kHz.\n\n```py\ndataset[0][\"audio\"]\n{'path': '/root/.cache/huggingface/datasets/downloads/extracted/f507fdca7f475d961f5bb7093bcc9d544f16f8cab8608e772a2ed4fbeb4d6f50/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n'array': array([ 1.70562416e-05,  2.18727451e-04,  2.28099874e-04, ...,\n3.43842403e-05, -5.96364771e-06, -1.76846661e-05]),\n'sampling_rate': 16000}\n```",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# What 🤗 Transformers can do",
  "🤗 Transformers is a library of pretrained state-of-the-art models for natural language processing (NLP), computer vision, and audio and speech processing tasks. Not only does the library contain Transformer models, but it also has non-Transformer models like modern convolutional networks for computer vision tasks. If you look at some of the most popular consumer products today, like smartphones, apps, and televisions, odds are that some kind of deep learning technology is behind it. Want to remove a background object from a picture taken by your smartphone? This is an example of a panoptic segmentation task (don't worry if you don't know what this means yet, we'll describe it in the following sections!).\n\nThis page provides an overview of the different speech and audio, computer vision, and NLP tasks that can be solved with the 🤗 Transformers library in just three lines of code!\n\n## Audio",
  "Audio and speech processing tasks are a little different from the other modalities mainly because audio as an input is a continuous signal. Unlike text, a raw audio waveform can't be neatly split into discrete chunks the way a sentence can be divided into words. To get around this, the raw audio signal is typically sampled at regular intervals. If you take more samples within an interval, the sampling rate is higher, and the audio more closely resembles the original audio source.\n\nPrevious approaches preprocessed the audio to extract useful features from it. It is now more common to start audio and speech processing tasks by directly feeding the raw audio waveform to a feature encoder to extract an audio representation. This simplifies the preprocessing step and allows the model to learn the most essential features.\n\n### Audio classification\n\nAudio classification is a task that labels audio data from a predefined set of classes. It is a broad category with many specific applications, some of which include:\n\n* acoustic scene classification: label audio with a scene label (\"office\", \"beach\", \"stadium\")",
  "* acoustic event detection: label audio with a sound event label (\"car horn\", \"whale calling\", \"glass breaking\")\n* tagging: label audio containing multiple sounds (birdsongs, speaker identification in a meeting)\n* music classification: label music with a genre label (\"metal\", \"hip-hop\", \"country\")\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n>>> preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.4532, 'label': 'hap'},\n{'score': 0.3622, 'label': 'sad'},\n{'score': 0.0943, 'label': 'neu'},\n{'score': 0.0903, 'label': 'ang'}]\n```\n\n### Automatic speech recognition\n\nAutomatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in \"smart\" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather.",
  "But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.\n\n```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n## Computer vision\n\nOne of the first and earliest successful computer vision tasks was recognizing images of zip code numbers using a [convolutional neural network (CNN)](glossary#convolution). An image is composed of pixels, and each pixel has a numerical value. This makes it easy to represent an image as a matrix of pixel values. Each particular combination of pixel values describes the colors of an image.\n\nTwo general ways computer vision tasks can be solved are:",
  "1. Use convolutions to learn the hierarchical features of an image from low-level features to high-level abstract things.\n2. Split an image into patches and use a Transformer to gradually learn how each image patch is related to each other to form an image. Unlike the bottom-up approach favored by a CNN, this is kind of like starting out with a blurry image and then gradually bringing it into focus.\n\n### Image classification\n\nImage classification labels an entire image from a predefined set of classes. Like most classification tasks, there are many practical use cases for image classification, some of which include:\n\n* healthcare: label medical images to detect disease or monitor patient health\n* environment: label satellite images to monitor deforestation, inform wildland management or detect wildfires\n* agriculture: label images of crops to monitor plant health or satellite images for land use monitoring\n* ecology: label images of animal or plant species to monitor wildlife populations or track endangered species\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"image-classification\")\n>>> preds = classifier(",
  "...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> print(*preds, sep=\"\\n\")\n{'score': 0.4335, 'label': 'lynx, catamount'}\n{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n{'score': 0.0239, 'label': 'Egyptian cat'}\n{'score': 0.0229, 'label': 'tiger cat'}\n```\n\n### Object detection\n\nUnlike image classification, object detection identifies multiple objects within an image and the objects' positions in an image (defined by the bounding box). Some example applications of object detection include:\n\n* self-driving vehicles: detect everyday traffic objects such as other vehicles, pedestrians, and traffic lights\n* remote sensing: disaster monitoring, urban planning, and weather forecasting\n* defect detection: detect cracks or structural damage in buildings, and manufacturing defects\n\n```py\n>>> from transformers import pipeline\n\n>>> detector = pipeline(task=\"object-detection\")\n>>> preds = detector(",
  "...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n>>> preds\n[{'score': 0.9865,\n'label': 'cat',\n'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n```\n\n### Image segmentation\n\nImage segmentation is a pixel-level task that assigns every pixel in an image to a class. It differs from object detection, which uses bounding boxes to label and predict objects in an image because segmentation is more granular. Segmentation can detect objects at a pixel-level. There are several types of image segmentation:\n\n* instance segmentation: in addition to labeling the class of an object, it also labels each distinct instance of an object (\"dog-1\", \"dog-2\")\n* panoptic segmentation: a combination of semantic and instance segmentation; it labels each pixel with a semantic class **and** each distinct instance of an object",
  "Segmentation tasks are helpful in self-driving vehicles to create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging, where the task's finer granularity can help identify abnormal cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real world through your camera.\n\n```py\n>>> from transformers import pipeline\n\n>>> segmenter = pipeline(task=\"image-segmentation\")\n>>> preds = segmenter(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> print(*preds, sep=\"\\n\")\n{'score': 0.9879, 'label': 'LABEL_184'}\n{'score': 0.9973, 'label': 'snow'}\n{'score': 0.9972, 'label': 'cat'}\n```\n\n### Depth estimation",
  "Depth estimation predicts the distance of each pixel in an image from the camera. This computer vision task is especially important for scene understanding and reconstruction. For example, in self-driving cars, vehicles need to understand how far objects like pedestrians, traffic signs, and other vehicles are to avoid obstacles and collisions. Depth information is also helpful for constructing 3D representations from 2D images and can be used to create high-quality 3D representations of biological structures or buildings.\n\nThere are two approaches to depth estimation:\n\n* stereo: depths are estimated by comparing two images of the same image from slightly different angles\n* monocular: depths are estimated from a single image\n\n```py\n>>> from transformers import pipeline\n\n>>> depth_estimator = pipeline(task=\"depth-estimation\")\n>>> preds = depth_estimator(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n```\n\n## Natural language processing",
  "NLP tasks are among the most common types of tasks because text is such a natural way for us to communicate. To get text into a format recognized by a model, it needs to be tokenized. This means dividing a sequence of text into separate words or subwords (tokens) and then converting these tokens into numbers. As a result, you can represent a sequence of text as a sequence of numbers, and once you have a sequence of numbers, it can be input into a model to solve all sorts of NLP tasks!\n\n### Text classification\n\nLike classification tasks in any modality, text classification labels a sequence of text (it can be sentence-level, a paragraph, or a document) from a predefined set of classes. There are many practical applications for text classification, some of which include:\n\n* sentiment analysis: label text according to some polarity like `positive` or `negative` which can inform and support decision-making in fields like politics, finance, and marketing\n* content classification: label text according to some topic to help organize and filter information in news and social media feeds (`weather`, `sports`, `finance`, etc.)\n\n```py\n>>> from transformers import pipeline",
  ">>> classifier = pipeline(task=\"sentiment-analysis\")\n>>> preds = classifier(\"Hugging Face is the best thing since sliced bread!\")\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.9991, 'label': 'POSITIVE'}]\n```\n\n### Token classification\n\nIn any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes.\n\nTwo common types of token classification are:\n\n* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"ner\")",
  ">>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\n>>> preds = [\n...     {\n...         \"entity\": pred[\"entity\"],\n...         \"score\": round(pred[\"score\"], 4),\n...         \"index\": pred[\"index\"],\n...         \"word\": pred[\"word\"],\n...         \"start\": pred[\"start\"],\n...         \"end\": pred[\"end\"],\n...     }\n...     for pred in preds\n... ]\n>>> print(*preds, sep=\"\\n\")\n{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n```\n\n### Question answering",
  "Question answering is another token-level task that returns an answer to a question, sometimes with context (open-domain) and other times without context (closed-domain). This task happens whenever we ask a virtual assistant something like whether a restaurant is open. It can also provide customer or technical support and help search engines retrieve the relevant information you're asking for.\n\nThere are two common types of question answering:\n\n* extractive: given a question and some context, the answer is a span of text from the context the model must extract\n* abstractive: given a question and some context, the answer is generated from the context; this approach is handled by the [`Text2TextGenerationPipeline`] instead of the [`QuestionAnsweringPipeline`] shown below\n\n\n```py\n>>> from transformers import pipeline\n\n>>> question_answerer = pipeline(task=\"question-answering\")\n>>> preds = question_answerer(\n...     question=\"What is the name of the repository?\",\n...     context=\"The name of the repository is huggingface/transformers\",\n... )\n>>> print(\n...     f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n... )",
  "score: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n```\n\n### Summarization\n\nSummarization creates a shorter version of a text from a longer one while trying to preserve most of the meaning of the original document. Summarization is a sequence-to-sequence task; it outputs a shorter text sequence than the input. There are a lot of long-form documents that can be summarized to help readers quickly understand the main points. Legislative bills, legal and financial documents, patents, and scientific papers are a few examples of documents that could be summarized to save readers time and serve as a reading aid.\n\nLike question answering, there are two types of summarization:\n\n* extractive: identify and extract the most important sentences from the original text\n* abstractive: generate the target summary (which may include new words not in the input document) from the original text; the [`SummarizationPipeline`] uses the abstractive approach\n\n```py\n>>> from transformers import pipeline\n\n>>> summarizer = pipeline(task=\"summarization\")\n>>> summarizer(",
  "...     \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\"\n... )\n[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n```\n\n### Translation",
  "Translation converts a sequence of text in one language to another. It is important in helping people from different backgrounds communicate with each other, help translate content to reach wider audiences, and even be a learning tool to help people learn a new language. Along with summarization, translation is a sequence-to-sequence task, meaning the model receives an input sequence and returns a target output sequence.\n\nIn the early days, translation models were mostly monolingual, but recently, there has been increasing interest in multilingual models that can translate between many pairs of languages.\n\n```py\n>>> from transformers import pipeline\n\n>>> text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n>>> translator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")\n>>> translator(text)\n[{'translation_text': \"Hugging Face est une tribune communautaire de l'apprentissage des machines.\"}]\n```\n\n### Language modeling",
  "Language modeling is a task that predicts a word in a sequence of text. It has become a very popular NLP task because a pretrained language model can be finetuned for many other downstream tasks. Lately, there has been a lot of interest in large language models (LLMs) which demonstrate zero- or few-shot learning. This means the model can solve tasks it wasn't explicitly trained to do! Language models can be used to generate fluent and convincing text, though you need to be careful since the text may not always be accurate.\n\nThere are two types of language modeling:\n\n* causal: the model's objective is to predict the next token in a sequence, and future tokens are masked\n\n```py\n>>> from transformers import pipeline\n\n>>> prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n>>> generator = pipeline(task=\"text-generation\")\n>>> generator(prompt)  # doctest: +SKIP\n```\n\n* masked: the model's objective is to predict a masked token in a sequence with full access to the tokens in the sequence\n\n```py\n>>> text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n>>> fill_mask = pipeline(task=\"fill-mask\")",
  ">>> preds = fill_mask(text, top_k=1)\n>>> preds = [\n...     {\n...         \"score\": round(pred[\"score\"], 4),\n...         \"token\": pred[\"token\"],\n...         \"token_str\": pred[\"token_str\"],\n...         \"sequence\": pred[\"sequence\"],\n...     }\n...     for pred in preds\n... ]\n>>> preds\n[{'score': 0.224, 'token': 3944, 'token_str': ' tool', 'sequence': 'Hugging Face is a community-based open-source tool for machine learning.'}]\n```\n\n## Multimodal\n\nMultimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image.\n\nAlthough multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.\n\n### Document question answering",
  "Document question answering is a task that answers natural language questions from a document. Unlike a token-level question answering task which takes text as input, document question answering takes an image of a document as input along with a question about the document and returns an answer. Document question answering can be used to parse structured documents and extract key information from it. In the example below, the total amount and change due can be extracted from a receipt.\n\n```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> doc_question_answerer = pipeline(\"document-question-answering\", model=\"magorshunov/layoutlm-invoices\")\n>>> preds = doc_question_answerer(\n...     question=\"What is the total amount?\",\n...     image=image,\n... )\n>>> preds\n[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]\n```",
  "Hopefully, this page has given you some more background information about all the types of tasks in each modality and the practical importance of each one. In the next [section](tasks_explained), you'll learn **how** 🤗 Transformers work to solve these tasks.",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Hyperparameter search\n\nHyperparameter search discovers an optimal set of hyperparameters that produces the best model performance. [`Trainer`] supports several hyperparameter search backends - [Optuna](https://optuna.readthedocs.io/en/stable/index.html), [SigOpt](https://docs.sigopt.com/), [Weights & Biases](https://docs.wandb.ai/), [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) - through  [`~Trainer.hyperparameter_search`] to optimize an objective or even multiple objectives.",
  "This guide will go over how to set up a hyperparameter search for each of the backends.\n\n```bash\npip install optuna/sigopt/wandb/ray[tune]\n```\n\nTo use [`~Trainer.hyperparameter_search`], you need to create a `model_init` function. This function includes basic model information (arguments and configuration) because it needs to be reinitialized for each search trial in the run.\n\n> [!WARNING]\n> The `model_init` function is incompatible with the [optimizers](./main_classes/trainer#transformers.Trainer.optimizers) parameter. Subclass [`Trainer`] and override the [`~Trainer.create_optimizer_and_scheduler`] method to create a custom optimizer and scheduler.\n\nAn example `model_init` function is shown below.\n\n```py\ndef model_init(trial):\nreturn AutoModelForSequenceClassification.from_pretrained(\nmodel_args.model_name_or_path,\nfrom_tf=bool(\".ckpt\" in model_args.model_name_or_path),\nconfig=config,\ncache_dir=model_args.cache_dir,\nrevision=model_args.model_revision,\ntoken=True if model_args.use_auth_token else None,\n)\n```\n\nPass `model_init` to [`Trainer`] along with everything else you need for training. Then you can call [`~Trainer.hyperparameter_search`] to start the search.",
  "[`~Trainer.hyperparameter_search`] accepts a [direction](./main_classes/trainer#transformers.Trainer.hyperparameter_search.direction) parameter to specify whether to minimize, maximize, or minimize and maximize multiple objectives. You'll also need to set the [backend](./main_classes/trainer#transformers.Trainer.hyperparameter_search.backend) you're using, an [object](./main_classes/trainer#transformers.Trainer.hyperparameter_search.hp_space) containing the hyperparameters to optimize for, the [number of trials](./main_classes/trainer#transformers.Trainer.hyperparameter_search.n_trials) to run, and a [compute_objective](./main_classes/trainer#transformers.Trainer.hyperparameter_search.compute_objective) to return the objective values.\n\n> [!TIP]\n> If [compute_objective](./main_classes/trainer#transformers.Trainer.hyperparameter_search.compute_objective) isn't defined, the default [compute_objective](./main_classes/trainer#transformers.Trainer.hyperparameter_search.compute_objective) is called which is the sum of an evaluation metric like F1.\n\n```py\nfrom transformers import Trainer\n\ntrainer = Trainer(\nmodel=None,\nargs=training_args,\ntrain_dataset=small_train_dataset,",
  "eval_dataset=small_eval_dataset,\ncompute_metrics=compute_metrics,\nprocessing_class=tokenizer,\nmodel_init=model_init,\ndata_collator=data_collator,\n)\ntrainer.hyperparameter_search(...)\n```\n\nThe following examples demonstrate how to perform a hyperparameter search for the learning rate and training batch size using the different backends.\n\n<hfoptions id=\"backends\">\n<hfoption id=\"Optuna\">\n\n[Optuna](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py) optimizes categories, integers, and floats.\n\n```py\ndef optuna_hp_space(trial):\nreturn {\n\"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n\"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n}\n\nbest_trials = trainer.hyperparameter_search(\ndirection=[\"minimize\", \"maximize\"],\nbackend=\"optuna\",\nhp_space=optuna_hp_space,\nn_trials=20,\ncompute_objective=compute_objective,\n)\n```\n\n</hfoption>\n<hfoption id=\"Ray Tune\">",
  "[Ray Tune](https://docs.ray.io/en/latest/tune/api/search_space.html) optimizes floats, integers, and categorical parameters. It also offers multiple sampling distributions for each parameter such as uniform and log-uniform.\n\n```py\ndef ray_hp_space(trial):\nreturn {\n\"learning_rate\": tune.loguniform(1e-6, 1e-4),\n\"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n}\n\nbest_trials = trainer.hyperparameter_search(\ndirection=[\"minimize\", \"maximize\"],\nbackend=\"ray\",\nhp_space=ray_hp_space,\nn_trials=20,\ncompute_objective=compute_objective,\n)\n```\n\n</hfoption>\n<hfoption id=\"SigOpt\">\n\n[SigOpt](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter) optimizes double, integer, and categorical parameters.\n\n```py\ndef sigopt_hp_space(trial):\nreturn [\n{\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n{\n\"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n\"name\": \"per_device_train_batch_size\",\n\"type\": \"categorical\",\n},\n]\n\nbest_trials = trainer.hyperparameter_search(\ndirection=[\"minimize\", \"maximize\"],\nbackend=\"sigopt\",\nhp_space=sigopt_hp_space,\nn_trials=20,\ncompute_objective=compute_objective,\n)\n```\n\n</hfoption>",
  "<hfoption id=\"Weights & Biases\">\n\n[Weights & Biases](https://docs.wandb.ai/guides/sweeps/sweep-config-keys) also optimizes integers, floats, and categorical parameters. It also includes support for different search strategies and distribution options.\n\n```py\ndef wandb_hp_space(trial):\nreturn {\n\"method\": \"random\",\n\"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n\"parameters\": {\n\"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4},\n\"per_device_train_batch_size\": {\"values\": [16, 32, 64, 128]},\n},\n}\n\nbest_trials = trainer.hyperparameter_search(\ndirection=[\"minimize\", \"maximize\"],\nbackend=\"wandb\",\nhp_space=wandb_hp_space,\nn_trials=20,\ncompute_objective=compute_objective,\n)\n```\n\n</hfoption>\n</hfoptions>\n\n## Distributed Data Parallel\n\n[`Trainer`] only supports hyperparameter search for distributed data parallel (DDP) on the Optuna and SigOpt backends. Only the rank-zero process is used to generate the search trial, and the resulting parameters are passed along to the other ranks.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Tools and RAG\n\nThe [`~PreTrainedTokenizerBase.apply_chat_template`] method supports virtually any additional argument types - strings, lists, dicts - besides the chat message. This makes it possible to use chat templates for many use cases.\n\nThis guide will demonstrate how to use chat templates with tools and retrieval-augmented generation (RAG).\n\n## Tools",
  "Tools are functions a large language model (LLM) can call to perform specific tasks. It is a powerful way to extend the capabilities of conversational agents with real-time information, computational tools, or access to large databases.\n\nFollow the rules below when creating a tool.\n\n1. The function should have a descriptive name.\n2. The function arguments must have a type hint in the function header (don't include in the `Args` block).\n3. The function must have a [Google-style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings) docstring.\n4. The function can have a return type and `Returns` block, but these are optional because most tool use models ignore them.\n\nAn example tool to get temperature and wind speed is shown below.\n\n```py\ndef get_current_temperature(location: str, unit: str) -> float:\n\"\"\"\nGet the current temperature at a location.\n\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nunit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\nReturns:\nThe current temperature at the specified location in the specified units, as a float.\n\"\"\"",
  "return 22.  # A real function should probably actually get the temperature!\n\ndef get_current_wind_speed(location: str) -> float:\n\"\"\"\nGet the current wind speed in km/h at a given location.\n\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nReturns:\nThe current wind speed at the given location in km/h, as a float.\n\"\"\"\nreturn 6.  # A real function should probably actually get the wind speed!\n\ntools = [get_current_temperature, get_current_wind_speed]\n```\n\nLoad a model and tokenizer that supports tool-use like [NousResearch/Hermes-2-Pro-Llama-3-8B](https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B), but you can also consider a larger model like [Command-R](./model_doc/cohere) and [Mixtral-8x22B](./model_doc/mixtral) if your hardware can support it.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\ntokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\nmodel = AutoModelForCausalLM.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n```",
  "Create a chat message.\n\n```py\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"},\n{\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n```\n\nPass `messages` and a list of tools to [`~PreTrainedTokenizerBase.apply_chat_template`]. Then you can pass the inputs to the model for generation.\n\n```py\ninputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\ninputs = {k: v for k, v in inputs.items()}\noutputs = model.generate(**inputs, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):]))\n```\n\n```txt\n<tool_call>\n{\"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}, \"name\": \"get_current_temperature\"}\n</tool_call><|im_end|>\n```\n\nThe chat model called the `get_current_temperature` tool with the correct parameters from the docstring. It inferred France as the location based on Paris, and that it should use Celsius for the units of temperature.",
  "Now append the `get_current_temperature` function and these arguments to the chat message as `tool_call`. The `tool_call` dictionary should be provided to the `assistant` role instead of the `system` or `user`.\n\n> [!WARNING]\n> The OpenAI API uses a JSON string as its `tool_call` format. This may cause errors or strange model behavior if used in Transformers, which expects a dict.\n\n<hfoptions id=\"tool-call\">\n<hfoption id=\"Llama\">\n\n```py\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nAllow the assistant to read the function outputs and chat with the user.\n\n```py\ninputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\ninputs = {k: v for k, v in inputs.items()}\nout = model.generate(**inputs, max_new_tokens=128)\nprint(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n```\n\n```txt\nThe temperature in Paris, France right now is approximately 12°C (53.6°F).<|im_end|>\n```\n\n</hfoption>\n<hfoption id=\"Mistral/Mixtral\">",
  "For [Mistral](./model_doc/mistral) and [Mixtral](./model_doc/mixtral) models, you need an additional `tool_call_id`. The `tool_call_id` is 9 randomly generated alphanumeric characters assigned to the `id` key in the `tool_call` dictionary.\n\n```py\ntool_call_id = \"9Ae3bDc2F\"\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n```\n\n```py\ninputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\ninputs = {k: v for k, v in inputs.items()}\nout = model.generate(**inputs, max_new_tokens=128)\nprint(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n```\n\n</hfoption>\n</hfoptions>\n\n## Schema",
  "[`~PreTrainedTokenizerBase.apply_chat_template`] converts functions into a [JSON schema](https://json-schema.org/learn/getting-started-step-by-step) which is passed to the chat template. A LLM never sees the code inside the function. In other words, a LLM doesn't care how the function works technically, it only cares about function **definition** and **arguments**.\n\nThe JSON schema is automatically generated behind the scenes as long as your function follows the [rules](#tools) listed earlier above. But you can use [get_json_schema](https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/src/transformers/utils/chat_template_utils.py#L205) to manually convert a schema for more visibility or debugging.\n\n```py\nfrom transformers.utils import get_json_schema\n\ndef multiply(a: float, b: float):\n\"\"\"\nA function that multiplies two numbers\n\nArgs:\na: The first number to multiply\nb: The second number to multiply\n\"\"\"\nreturn a * b\n\nschema = get_json_schema(multiply)\nprint(schema)\n```\n\n```json\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"multiply\",\n\"description\": \"A function that multiplies two numbers\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {",
  "\"a\": {\n\"type\": \"number\",\n\"description\": \"The first number to multiply\"\n},\n\"b\": {\n\"type\": \"number\",\n\"description\": \"The second number to multiply\"\n}\n},\n\"required\": [\"a\", \"b\"]\n}\n}\n}\n```\n\nYou can edit the schema or write one entirely from scratch. This gives you a lot of flexibility to define precise schemas for more complex functions.\n\n> [!WARNING]\n> Try keeping your function signatures simple and the arguments to a minimum. These are easier for a model to understand and use than complex functions for example with nested arguments.\n\nThe example below demonstrates writing a schema manually and then passing it to [`~PreTrainedTokenizerBase.apply_chat_template`].\n\n```py\n# A simple function that takes no arguments\ncurrent_time = {\n\"type\": \"function\",\n\"function\": {\n\"name\": \"current_time\",\n\"description\": \"Get the current local time as a string.\",\n\"parameters\": {\n'type': 'object',\n'properties': {}\n}\n}\n}\n\n# A more complete function that takes two numerical arguments\nmultiply = {\n'type': 'function',\n'function': {\n'name': 'multiply',\n'description': 'A function that multiplies two numbers',\n'parameters': {\n'type': 'object',\n'properties': {\n'a': {\n'type': 'number',",
  "'description': 'The first number to multiply'\n},\n'b': {\n'type': 'number', 'description': 'The second number to multiply'\n}\n},\n'required': ['a', 'b']\n}\n}\n}\n\nmodel_input = tokenizer.apply_chat_template(\nmessages,\ntools = [current_time, multiply]\n)\n```\n\n## RAG\n\nRetrieval-augmented generation (RAG) models enhance a models existing knowledge by allowing it to search documents for additional information before returning a query. For RAG models, add a `documents` parameter to [`~PreTrainedTokenizerBase.apply_chat_template`]. This `documents` parameter should be a list of documents, and each document should be a single dict with `title` and `content` keys.\n\n> [!TIP]\n> The `documents` parameter for RAG isn't widely supported and many models have chat templates that ignore `documents`. Verify if a model supports `documents` by reading its model card or executing `print(tokenizer.chat_template)` to see if the `documents` key is present. [Command-R](https://hf.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://hf.co/CohereForAI/c4ai-command-r-plus-08-2024) both support `documents` in their RAG chat templates.\n\nCreate a list of documents to pass to the model.\n\n```py\ndocuments = [",
  "{\n\"title\": \"The Moon: Our Age-Old Foe\",\n\"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n},\n{\n\"title\": \"The Sun: Our Age-Old Friend\",\n\"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n}\n]\n```\n\nSet `chat_template=\"rag\"` in [`~PreTrainedTokenizerBase.apply_chat_template`] and generate a response.\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01-4bit\")\nmodel = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01-4bit\", device_map=\"auto\")\ndevice = model.device # Get the device the model is loaded on\n\n# Define conversation input\nconversation = [\n{\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(\nconversation=conversation,\ndocuments=documents,\nchat_template=\"rag\",\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\").to(device)\n\n# Generate a response\ngenerated_tokens = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)",
  "# Decode and print the generated text along with generation prompt\ngenerated_text = tokenizer.decode(generated_tokens[0])\nprint(generated_text)\n```",
  "# Modular Transformers\n\nModular Transformers lowers the bar for contributing models and significantly reduces the code required to add a model by allowing imports and inheritance.\n\nOne of Transformers' core design feature is the [single model, single file](https://huggingface.co/blog/transformers-design-philosophy) policy. Model components - such as attention layers - are repeated across many files and any independent implementations tend to diverge as fixes and changes are applied to specific parts of the code.\n\nThe [`# Copied from`](./pr_checks#check-copies) statements prevents the code from diverging, and it is enforced by our continuous integration tests and local commands. The downside is that this approach is tedious and adds significantly more lines of code, most of which is boilerplate.\n\n## Motivation\n\nModular Transformers addresses these issues by adding a *modular* file to a model folder. The modular file can import code from other models and inherit code from other classes unlike traditional modeling and processing files.\n\n> [!TIP]",
  "> Modular Transformers isn't meant to replace the modeling code, and if your model isn't based on an existing model, you'll need to add a `modeling.py` file manually. Likewise, if a configuration, tokenization or processing file can't easily inherit from a similar file, you can add that file directly.\n\nA modular file contains model, processor, and configuration class code that would otherwise be in separate files under the single model, single file policy.\n\nModel users still import and use the single-file interface they've grown familiar with. In doing so, we hope to enable simpler contributions while sticking to our philosophy.\n\n## Create a modeling.py file\n\nA linter \"unravels\" the modular file into a `modeling.py` file to preserve the single model, single file directory structure (modeling, processor, etc.). Inheritance is flattened to only a **single** level.\n\nRun the command below to automatically generate a `modeling.py` file from a modular file.\n\n```bash\npython utils/modular_model_converter.py --files_to_parse src/transformers/models/<your_model>/modular_<your_model>.py\n```\n\nFor example:",
  "- If a configuration class inherits from another class, but adds and deletes an argument, the generated file directly references it if an argument is added or completely removes it if an argument is deleted.\n- If a class inherits from another, like `GemmaModel(LlamaModel)`, the dependencies are automatically inferred. All submodules are also automatically inferred from the superclass.\n- If a new function is defined in the modular file and used inside classes, the linter automatically infers these as well.\n\nYou should be able to write everything (tokenizer, image processor, model, config, etc.) in a modular and their corresponding single-files are generated.\n\nRun the command below to ensure the generated content matches `modular_<your_model>.py`.\n\n```bash\npython utils/check_modular_conversion.py --files src/transformers/models/<your_model>/modular_<your_model>.py\n```\n\nThe example below demonstrates how a model can be added with significantly fewer lines of code with Modular Transformers.\n\n### BERT and RoBERTa\n\nBERT and RoBERTa, two very similar models, differ solely in how the embedding layer is implemented.",
  "Instead of redefining the model entirely, consider the `modular_roberta.py` file shown below for the modeling and configuration classes (the tokenizer isn't shown in this example).\n\n```py\nfrom torch import nn\nfrom ..bert.configuration_bert import BertConfig\nfrom ..bert.modeling_bert import (\nBertModel,\nBertEmbeddings,\nBertForMaskedLM\n)\n\n# RoBERTa and BERT config is identical\nclass RobertaConfig(BertConfig):\nmodel_type = 'roberta'\n\n# Redefine the embeddings to highlight the padding id difference, and redefine the position embeddings\nclass RobertaEmbeddings(BertEmbeddings):\ndef __init__(self, config):\nsuper().__init__(config())\n\nself.padding_idx = config.pad_token_id\nself.position_embeddings = nn.Embedding(\nconfig.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n)\n\n# RoBERTa and BERT model is identical except for the embedding layer, which is defined above, so no need for additional changes here\nclass RobertaModel(BertModel):\ndef __init__(self, config):\nsuper().__init__(config)\nself.embeddings = RobertaEmbeddings(config)\n\n\n# The model heads now only need to redefine the model inside to `RobertaModel`\nclass RobertaForMaskedLM(BertForMaskedLM):",
  "def __init__(self, config):\nsuper().__init__(config)\nself.model = RobertaModel(config)\n```\n\nIf you don't use the defined dependency, you'll receive the following error.\n\n```\nValueError: You defined `RobertaEmbeddings` in the modular_roberta.py, it should be used when you define `BertModel`, as it is one of it's direct dependencies. Make sure you use it in the `__init__` function.\n```\n\n## Implementing a modular file\n\nThe easiest way to start is by browsing Transformers for a model similar to yours in order to inherit from it. Some good starting points are [Mistral](./model_doc/mistral), [Qwen2](./model_doc/qwen2), [Cohere](./model_doc/cohere) and [Cohere](./model_doc/cohere2), and [Llama](./model_doc/llama). Refer to the table below for components your model might be using and where you can inherit from.\n\n| Component | Model |\n|---|---|\n| Mixture of expert | SwitchTransformers or Mixtral |\n| Interleaved (and/or partial) rotary embedding | GLM, Phi |\n| State space models | Jamba, Bamba, Zamba, Mamba2 |\n| Recurrent hidden states | Gemma2 |\n| Sliding window attention/full attention patterns per layer | Gemma2, Cohere2 |\n| QKV clipping | Olmo |\n| QK normalization | Olmo2, Cohere |",
  "| Fused QKV (not recommended) | Phi3 |\n\nThis section will walk you through how to implement [Olmo2](./model_doc/olmo2) from [Olmo](./model_doc/olmo) with modular Transformers (you can refer to the original [modeling.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo2/modular_olmo2.py) file).\n\n### Config\n\nThe modular `Olmo2Config` is shown below.\n\n```py\nfrom ..olmo.configuration_olmo import OlmoConfig\n\nclass Olmo2Config(OlmoConfig):\nr\"\"\"\nThis is the configuration class to store the configuration of a [Olmo2Model](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Model).\n\"\"\"\n\ndef __init__(\nself,\nvocab_size=50304,\nhidden_size=4096,\nintermediate_size=11008,\nnum_hidden_layers=32,\nnum_attention_heads=32,\nnum_key_value_heads=None,\nhidden_act=\"silu\",\nmax_position_embeddings=2048,\ninitializer_range=0.02,\nuse_cache=True,\npad_token_id=1,\nbos_token_id=None,\neos_token_id=50279,\ntie_word_embeddings=False,\nrope_theta=10000.0,\nrope_scaling=None,\nattention_bias=False,\nattention_dropout=0.0,\nrms_norm_eps=1e-5,\n**kwargs,\n):\nsuper().__init__(\nvocab_size=vocab_size,\nhidden_size=hidden_size,\nintermediate_size=intermediate_size,",
  "num_hidden_layers=num_hidden_layers,\nnum_attention_heads=num_attention_heads,\nnum_key_value_heads=num_key_value_heads,\nhidden_act=hidden_act,\nmax_position_embeddings=max_position_embeddings,\ninitializer_range=initializer_range,\nuse_cache=use_cache,\npad_token_id=pad_token_id,\nbos_token_id=bos_token_id,\neos_token_id=eos_token_id,\ntie_word_embeddings=tie_word_embeddings,\nrope_theta=rope_theta,\nrope_scaling=rope_scaling,\nattention_bias=attention_bias,\nattention_dropout=attention_dropout,\n**kwargs,\n)\n\nself.rms_norm_eps = rms_norm_eps\ndel self.clip_qkv\n```\n\nThere are three points where the `Olmo2Config` is different from the original `OlmoConfig`.\n\n1. The default value of most arguments have changed.\n2. There is a new argument, `rms_norm_eps`.\n3. The `clip_qkv` argument isn't used anymore.\n\nFor the new default values and argument, overwrite the `__init__` function with the new default values and add `rms_norm_eps`. Assign `rms_norm_eps` to `self` in the body of `__init__`. For the `clip_qkv` argument, use `del self.clip_qkv` to remove the assignment of this attribute in the unraveled code (post-linter conversion).",
  "Notice how the `super().__init__(...)` is used. Typically, it calls the parent `__init__`.\n\nBut in modular Transformers, if there is a call like `super().my_function(...)`, the linter takes the body of `my_function` in the parent and unravels it where the call to `super().my_function(...)` occurred. The `del self.clip_qkv` statement removes the reference to `self.clip_qkv` in the unraveled body.\n\n`del self.` and `super().my_function(..)` work together, and it should always be placed after `super().my_function(...)`. You can add whatever you want *before* calling `super()`, and it is placed before the parents body.\n\n### Norm\n\n```py\nfrom ..llama.modeling_llama import LlamaRMSNorm\n\nclass Olmo2RMSNorm(LlamaRMSNorm):\npass\n```\n\nNothing needs to be modified in `LlamaRMSNorm`. The linter unravels the exact content of `LlamaRMSNorm` into `Olmo2RMSNorm`. References to Llama in the docstrings, type hints, and comments are also changed to Olmo2.\n\n### Attention\n\nThe modular `Olmo2Attention` is shown below.\n\n```py\nfrom ..llama.modeling_llama import eager_attention_forward\nfrom ..olmo.modeling_olmo import OlmoAttention, apply_rotary_pos_emb",
  "# Olmo2 attention is identical to OLMo attention except:\n# - Norm is applied to attention queries and keys.\n# - No qkv clipping.\nclass Olmo2Attention(OlmoAttention):\ndef __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\nsuper().__init__(config, layer_idx=layer_idx)\nself.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\nself.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n\ndef forward(\nself,\nhidden_states: torch.Tensor,\nposition_embeddings: Tuple[torch.Tensor, torch.Tensor],\nattention_mask: Optional[torch.Tensor],\npast_key_value: Optional[Cache] = None,\ncache_position: Optional[torch.LongTensor] = None,\n**kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\ninput_shape = hidden_states.shape[:-1]\nhidden_shape = (*input_shape, -1, self.head_dim)\n\nquery_states = self.q_norm(self.q_proj(hidden_states))\nkey_states = self.k_norm(self.k_proj(hidden_states))\nvalue_states = self.v_proj(hidden_states)\n\nquery_states = query_states.view(hidden_shape).transpose(1, 2)\nkey_states = key_states.view(hidden_shape).transpose(1, 2)",
  "value_states = value_states.view(hidden_shape).transpose(1, 2)\n\ncos, sin = position_embeddings\nquery_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\nif past_key_value is not None:\n# sin and cos are specific to RoPE models; cache_position needed for the static cache\ncache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\nkey_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\nattention_interface: Callable = eager_attention_forward\nif self.config._attn_implementation != \"eager\":\nif self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\nlogger.warning_once(\n\"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n)\nelse:\nattention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n\nattn_output, attn_weights = attention_interface(\nself,\nquery_states,\nkey_states,\nvalue_states,\nattention_mask,\ndropout=0.0 if not self.training else self.attention_dropout,",
  "scaling=self.scaling,\n**kwargs,\n)\n\nattn_output = attn_output.reshape(*input_shape, -1).contiguous()\nattn_output = self.o_proj(attn_output)\nreturn attn_output, attn_weights\n```\n\nThe `super().__init__(...)` copies the parent definition and adds 2 new layers from `Olmo2RMSNorm`. The forward pass needs to be overwritten to use these 2 new layers. A pass with the norm layers is added before projecting with `q_proj` and `k_proj`. To make it easier, the `eager_attention_forward` function is directly imported from Llama and the `apply_rotary_pos_emb` is imported from Olmo.\n\nThe linter automatically adds these imported functions in the final `modeling_olmo2.py` file by copying their definitions from the source files. The `rotate_half` and `repeat_kv` functions are also added because they are used inside `apply_rotary_pos_emb` and `eager_attention_forward`.\n\nThe `Attention` class had to be redefined because there weren't any existing models with an `Attention` layer that included a `RMSNorm` layer.\n\n### DecoderLayer\n\nThe modular `DecoderLayer` is shown below.\n\n```py\nfrom ..olmo.modeling_olmo import OlmoDecoderLayer\n\n# The OLMo2 layers are identical to those of the OLMo model except:",
  "# - RMSNorm is used instead of standard layer norm.\n# - Norm is applied after attention/feedforward rather than before.\nclass Olmo2DecoderLayer(OlmoDecoderLayer):\ndef __init__(self, config: Olmo2Config, layer_idx: int):\nsuper().__init__(config, layer_idx=layer_idx)\nself.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\nself.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\nself.self_attn = Olmo2Attention(config=config, layer_idx=layer_idx)\ndel self.input_layernorm\n\ndef forward(\nself,\nhidden_states: torch.Tensor,\nattention_mask: Optional[torch.Tensor] = None,\nposition_ids: Optional[torch.LongTensor] = None,\npast_key_value: Optional[Cache] = None,\noutput_attentions: Optional[bool] = False,\nuse_cache: Optional[bool] = False,\ncache_position: Optional[torch.LongTensor] = None,\nposition_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n**kwargs,\n) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\nresidual = hidden_states\n\n# Self Attention\nhidden_states, self_attn_weights = self.self_attn(\nhidden_states=hidden_states,",
  "attention_mask=attention_mask,\nposition_ids=position_ids,\npast_key_value=past_key_value,\noutput_attentions=output_attentions,\nuse_cache=use_cache,\ncache_position=cache_position,\nposition_embeddings=position_embeddings,\n**kwargs,\n)\nhidden_states = self.post_attention_layernorm(hidden_states)\nhidden_states = residual + hidden_states\n\n# Fully Connected\nresidual = hidden_states\nhidden_states = self.mlp(hidden_states)\nhidden_states = self.post_feedforward_layernorm(hidden_states)\nhidden_states = residual + hidden_states\n\noutputs = (hidden_states,)\nif output_attentions:\noutputs += (self_attn_weights,)\n\nreturn outputs\n```\n\nThe norm type is switched in `__init__` by overwriting `self.post_attention_layernorm` after the call to `super().__init__(...)`. Delete the `self.input_layernorm` attributed and replace it with `self.post_feedforward_layernorm` because it is applied after in Olmo2. The forward method is overwritten to reflect this change.\n\nIf you only switched `self.post_feedforward_layernorm` and `self.input_layernorm` from `LayerNorm` to `RMSNorm` without also changing the name and logic of `self.input_layernorm`, then you wouldn't have to rewrite the forward method.\n\n### Model",
  "The modular `Olmo2Model` class is shown below.\n\n```py\nfrom ..olmo.modeling_olmo import OlmoModel\n\n# The OLMo2 model is identical to the OLMo model, except RMSNorm is used instead of\n# standard layer norm for the output norm.\nclass Olmo2Model(OlmoModel):\ndef __init__(self, config: Olmo2Config):\nsuper().__init__(config)\nself.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\nself.layers = nn.ModuleList(\n[Olmo2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n)\n```\n\nYou only need to change the *type* of the `self.norm` attribute to use `RMSNorm` instead of `LayerNorm`. This change doesn't affect the logic in the forward method (layer name and usage is identical to the parent class), so you don't need to overwrite it. The linter automatically unravels it.\n\n### Model head\n\nThe modular causal modeling head is shown below.\n\n```py\nfrom ..olmo.modeling_olmo import OlmoForCausalLM\n\nclass Olmo2ForCausalLM(OlmoForCausalLM):\npass\n```\n\nThe logic is identical to `OlmoForCausalLM` which means you don't need to make any changes here.\n\n### Other classes",
  "The [modeling_olmo2.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo2/modeling_olmo2.py) generated by the linter also contains some classes (`Olmo2MLP`, `Olmo2RotaryEmbedding`, `Olmo2PreTrainedModel`) that weren't explicitly defined in `modular_olmo2.py`.\n\nClasses that are a dependency of an inherited class but aren't explicitly defined are automatically added as a part of dependency tracing. This is similar to how some functions were added to the `Attention` class without directly importing them.\n\nFor example, `OlmoDecoderLayer` has an attribute defined as `self.mlp = OlmoMLP(config)`. This class was never explicitly redefined in `Olmo2MLP`, so the linter automatically created a `Olmo2MLP` class similar to `OlmoMLP`. It is identical to the code below if it was explicitly written in `modular_olmo2.py`.\n\n```py\nfrom ..olmo.modeling_olmo import OlmoMLP\n\nclass Olmo2MLP(OlmoMLP):\npass\n```\n\nHowever, it was necessary to rewrite `Olmo2RMSNorm` because the layer norm needed to be redefined in the `Attention` and `DecoderLayer` classes. Similarly, this is why you didn't need to create the `Olmo2PreTrainedModel` and `Olmo2RotaryEmbedding` classes.",
  "Classes that aren't rewritten are copied from the file where the inherited module first uses them. This means if you wanted `Olmo2MLP` to inherit from `MistralMLP` instead, you would need to be more explicit as shown below.\n\n```py\n# switch to mistral definition\nfrom ..mistral.modeling_mistral import MistralMLP\n\nclass Olmo2MLP(MistralMLP):\npass\n```\n\n## Removing attributes\n\nYou can `del` to remove attributes defined in the parent after using `super().__init__()`. However, this doesn't work if the attribute is also used somewhere else as shown below. It only suppresses the assignment. The `self.attribute = config.attribute` line is removed, but the `if` statement remains and references the attribute.\n\n```py\nclass DummyModel(nn.Module):\n\ndef __init__(self, config: DummyConfig):\nsuper().__init__()\nself.attribute = config.attribute\nif self.attribute:\n# do more stuff with `self.attribute` here\n...\n\nclass MyNewDummyModel(DummyModel):\n\ndef __init__(self, config: MyNewDummyConfig):\nsuper().__init__(config)\ndel self.attribute\n```\n\n## Explicit super() calls",
  "If you still want to inherit from `DummyModel` but don't want to remove the `self.attribute`, be explicit about which class' `super()` you're calling. The example below shows how to call the `super()` of `nn.Module` (unraveled code shown on the right)\n\n```py\nclass MyNewDummyModel(DummyModel, nn.Module):        |     class MyNewDummyModel(nn.Module):\n|\ndef __init__(self, config: MyNewDummyConfig):      |       def __init__(self, config: MyNewDummyConfig):\nnn.Module.__init__(config)                       |         super().__init__()\nself.foo = config.foo                            |         self.foo = config.foo\n...                                              |         ...\n```\n\n## Deleting unused methods\n\nRemove an attribute by overwriting it with a `raise AttributeError(\"\")` statement to mimic the behavior you want when you remove a parent function in Python. The example below removes the methods in the unraveled code.\n\n```py\nclass GemmaTokenizer(LlamaTokenizer):\n...\n\ndef get_spm_processor(self):\nraise AttributeError(\"Not needed for Gemma\")\n\ndef unk_token_length(self):\nraise AttributeError(\"Not needed for Gemma\")\n```\n\n## Defining new functions",
  "By default, if you inherit from a class and override a method with one or more decorators in the parent method, the decorators are also added to the unraveled code *only if you don't add any yourself*. Otherwise, the redefined decorator is used.\n\nFor example, if you had a parent class shown below and you overwrite it, the parent decorator is kept.\n\n```py\nclass DummyModel(nn.Module):\n...\n\n@decorator(...)\ndef forward(...)\n# do stuff here\n```\n\nModular code is shown on the left, and the unraveled code is shown on the right.\n\n```py\nclass NewModel(DummyModel):       |   class NewModel(nn.Module):\n...                             |     ...\n|\ndef forward(...):               |     @decorator(...)\n...                           |     def forward(...):\n|       ...\n```\n\nBut if you add a new decorator, your new decorator is used instead.\n\n```py\nclass NewModel(DummyModel):       |   class NewModel(nn.Module):\n...                             |     ...\n|\n@my_new_decorator(...)          |     @my_new_decorator(...)\ndef forward(...):               |     def forward(...):\n...                           |       ...\n```\n\n## super_kwargs",
  "In scenarios where a forward method is really long and you want to switch decorators, you don't need to redefine everything and copy/paste the function. You can use `super().forward(...)` to unravel the parent body. When there are a lot of arguments in the function signature, use the special `**super_kwargs` syntax in the overwritten signature.\n\nThis syntax indicates to the linter to unravel all the parent signature arguments here. An example signature in a [`AutoModelForCausalLM`] model is shown below, with lots of arguments.\n\n```py\nclass LlamaForCausalLM(nn.Module):\n...\n\n@add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\ndef forward(\nself,\ninput_ids: torch.LongTensor = None,\nattention_mask: Optional[torch.Tensor] = None,\nposition_ids: Optional[torch.LongTensor] = None,\npast_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\ninputs_embeds: Optional[torch.FloatTensor] = None,\nlabels: Optional[torch.LongTensor] = None,\nuse_cache: Optional[bool] = None,\noutput_attentions: Optional[bool] = None,\noutput_hidden_states: Optional[bool] = None,",
  "return_dict: Optional[bool] = None,\ncache_position: Optional[torch.LongTensor] = None,\nnum_logits_to_keep: int = 0,\n**kwargs: Unpack[KwargsForCausalLM],\n) -> Union[Tuple, CausalLMOutputWithPast]:\n...\n```\n\nInstead of rewriting and copying/pasting all of those arguments, use the `super().forward(**super_kwargs)` statement (modular code shown on the left, unraveled code on the right).\n\n```py\nclass NewModelForCausalLM(LlamaForCausalLM):    |    class LlamaForCausalLM(nn.Module):\n...                                           |      ...\n|\n@my_new_decorator                             |     @my_new_decorator\ndef forward(self, **super_kwargs):            |     def forward(\nsuper().forward(**super_kwargs)             |         self,\n|         input_ids: torch.LongTensor = None,\n|         attention_mask: Optional[torch.Tensor] = None,\n|         position_ids: Optional[torch.LongTensor] = None,\n|         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = |None,\n|         inputs_embeds: Optional[torch.FloatTensor] = None,\n|         labels: Optional[torch.LongTensor] = None,\n|         use_cache: Optional[bool] = None,",
  "|         output_attentions: Optional[bool] = None,\n|         output_hidden_states: Optional[bool] = None,\n|         return_dict: Optional[bool] = None,\n|         cache_position: Optional[torch.LongTensor] = None,\n|         num_logits_to_keep: int = 0,\n|         **kwargs: Unpack[KwargsForCausalLM],\n|     ) -> Union[Tuple, CausalLMOutputWithPast]:\n|       ...\n```\n\nThis makes it very easy to switch decorators and makes it explicit that the only change you want to apply is the decorator.\n\n`**super_kwargs` should not be used to avoid being explicit when redefining methods though. If you overwrite a method, you should explicitly write the signature as you normally would. The `**super_kwargs` syntax is a shortcut for switching decorators and a few other niche cases.\n\n## Docstring variables",
  "If an object defined in both the modular and modeling file from which it inherits, the modular definition has precedence unless for assignments containing the pattern `DOCSTRING`. These variables are typically used in `MODEL_START_DOCSTRING` and `MODEL_INPUT_DOCSTRING` in the modeling files. They are big blocks of docstrings and the linter rewrites the names everywhere. For this reason, assignments containing the `DOCSTRING` variable can use the definition found in the source file without copying the whole docstring, by simply setting the variable to `None` in the modular file.\n\nThis is very useful if you need the variable reference somewhere but you don't want to clutter the modular file with docstrings which are always the same. The example code below allows you to automatically use the same docstrings from [Mistral](./model_doc/mistral) in [Starcoder2](./model_doc/starcoder2).\n\n```py\nSTARCODER2_INPUTS_DOCSTRING = None  # will be automatically redefined\n\nclass Starcoder2Model(MistralModel):\n...\n\n@add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\ndef forward(...)\n...\n```",
  "Setting the variable to anything other than `None` will override the docstring, so that you can customize the docstrings if needed.\n\n## Special naming\n\nThe linter automatically renames everything when inheriting from a class. For consistency, you should always use the same class name prefix when inheriting from different classes from the same file.\n\nThe example below is not recommended. It breaks standards in the library, `MyModelIncredibleMLP` instead of `LlamaMLP`, and because the linter doesn't know how to rename potential higher-order dependencies (`MyModelIncredible` or just `MyModel`).\n\n```py\nclass MyModelIncredibleMLP(LlamaMLP):\n...\n\nclass MyModelDecoderLayer(LlamaDecoderLayer):\n...\n```\n\nHowever, if there aren't any [implicit dependencies](#other-classes), then you can locally rename a single class. Make sure you still explicitly redefine every other mention of the class with the new name pattern though. For example, all mentions of `LlamaMLP` should be renamed to `MyModelIncredibleMLP` otherwise the linter may add a new and unwanted `MyModelMLP` class.",
  "The linter raises a warning if an ambiguous case is detected. It explains what is happening and which prefix is used by default for getting the dependencies. These warning and renaming pattern complications usually only come up when defining multimodal models. For example, adding `Text` to class names in a multimodal model to make it clear which modality it refers to.\n\n```py\nWe detected multiple prefix names when inheriting from transformers.models.llama.modeling_llama: ('Emu3Text', 'Emu3'). We will only use the most used 'Emu3' prefix when grabbing args and dependencies. Make sure to subclass the intermediate classes with the prefix you want (if different from 'Emu3') or use a single prefix in all the modular (best).\n```\n\nIf there are automatic dependencies with a prefix, but you want another one, explicitly rename the classes locally with a `pass` class as shown in the following.\n\n```py\nclass Emu3TextMLP(LlamaMLP):\npass\n```\n\n## Config docstrings",
  "When inheriting a `Config` class or adding and deleting attributes, you may want to only redefine the new attributes in the docstring. However, the linter doesn't support this yet. You need to directly add the while docstring directly in the modular file under the class definition.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Parallelism methods\n\nMulti-GPU setups are effective for accelerating training and fitting large models in memory that otherwise wouldn't fit on a single GPU. It relies on parallelizing the workload across GPUs. There are several types of parallelism such as data parallelism, tensor parallelism, pipeline parallelism, and model parallelism. Each type of parallelism splits the workload differently, whether it's the data or the model.",
  "This guide will discuss the various parallelism methods, combining them, and choosing an appropriate strategy for your setup. For more details about distributed training, refer to the [Accelerate](https://hf.co/docs/accelerate/index) documentation.\n\nFor a comprehensive guide on scaling large language models, check out the [Ultrascale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook), which provides detailed strategies and best practices for training at scale.\n\n## Scalability strategy\n\nUse the [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to calculate how much memory a model requires. Then refer to the table below to select a strategy based on your setup.\n\n| setup | scenario | strategy |\n|---|---|---|\n| single node/multi-GPU | fits on single GPU | DistributedDataParallel or ZeRO |\n|  | doesn't fit on single GPU | PipelineParallel, ZeRO or TensorParallel |\n|  | largest model layer doesn't fit | TensorParallel or ZeRO |\n| multi-node/multi-GPU | fast inter-node connectivity (NVLink or NVSwitch) | ZeRO or 3D parallelism (PipelineParallel, TensorParallel, DataParallel) |",
  "|  | slow inter-node connectivity | ZeRO or 3D parallelism (PipelineParallel, TensorParallel, DataParallel) |\n\n## Data parallelism\n\nData parallelism evenly distributes data across multiple GPUs. Each GPU holds a copy of the model and concurrently processes their portion of the data. At the end, the results from each GPU are synchronized and combined.\n\nData parallelism significantly reduces training time by processing data in parallel, and it is scalable to the number of GPUs available. However, synchronizing results from each GPU can add overhead.\n\nThere are two types of data parallelism, DataParallel (DP) and DistributedDataParallel (DDP).\n\n### DataParallel\n\n[DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html) supports distributed training on a *single machine* with multiple GPUs.\n\n1. The default GPU, `GPU 0`, reads a batch of data and sends a mini batch of it to the other GPUs.\n2. An up-to-date model is replicated from `GPU 0` to the other GPUs.\n3. A `forward` pass is performed on each GPU and their outputs are sent to `GPU 0` to compute the loss.\n4. The loss is distributed from `GPU 0` to the other GPUs for the `backward` pass.",
  "5. The gradients from each GPU are sent back to `GPU 0` and averaged.\n\n### DistributedDataParallel\n\n[DistributedDataParallel](https://pytorch.org/docs/main/notes/ddp.html) supports distributed training across *multiple machines* with multiple GPUs.\n\n1. The main process replicates the model from the default GPU, `GPU 0`, to each GPU.\n2. Each GPU directly processes a mini batch of data.\n3. The local gradients are averaged across all GPUs during the `backward` pass.\n\nDDP is recommended because it reduces communication overhead between GPUs, efficiently utilizes each GPU, and scales to more than one machine.\n\n### ZeRO data parallelism\n\n[Zero Redundancy Optimizer](https://www.deepspeed.ai/tutorials/zero/) is a more memory efficient type of data parallelism. It significantly improves memory efficiency by partitioning parameters, gradients, and optimizer states across data parallel processes to reduce memory usage. There are three ZeRO stages:\n\n- Stage 1 partitions the optimizer states\n- Stage 2 partitions the optimizer and gradient states\n- Stage 3 partitions the optimizer, gradient, and parameters\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png\"/>\n</div>\n\n## Model parallelism\n\nModel parallelism distributes a model across multiple GPUs. There are several ways to split a model, but the typical method distributes the model layers across GPUs. On the `forward` pass, the first GPU processes a batch of data and passes it to the next group of layers on the next GPU. For the `backward` pass, the data is sent backward from the final layer to the first layer.\n\nModel parallelism is a useful strategy for training models that are too large to fit into the memory of a single GPU. However, GPU utilization is unbalanced because only one GPU is active at a time. Passing results between GPUs also adds communication overhead and it can be a bottleneck.\n\n## Pipeline parallelism",
  "Pipeline parallelism is conceptually very similar to model parallelism, but it's more efficient because it reduces the amount of idle GPU time. Instead of waiting for each GPU to finish processing a batch of data, pipeline parallelism creates *micro-batches* of data. As soon as one micro-batch is finished, it is passed to the next GPU. This way, each GPU can concurrently process part of the data without waiting for the other GPU to completely finish processing a mini batch of data.\n\nPipeline parallelism shares the same advantages as model parallelism, but it optimizes GPU utilization and reduces idle time. But pipeline parallelism can be more complex because models may need to be rewritten as a sequence of [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) modules and it also isn't possible to completely reduce idle time because the last `forward` pass must also wait for the `backward` pass to finish.\n\n## Tensor parallelism",
  "Tensor parallelism distributes large tensor computations across multiple GPUs. The tensors are sliced horizontally or vertically and each slice is processed by a separate GPU. Each GPU performs its calculations on its tensor slice and the results are synchronized at the end to reconstruct the final result.\n\nTensor parallelism is effective for training large models that don't fit into the memory of a single GPU. It is also faster and more efficient because each GPU can process its tensor slice in parallel, and it can be combined with other parallelism methods. Like other parallelism methods though, tensor parallelism adds communication overhead between GPUs.\n\n## Hybrid parallelism\n\nParallelism methods can be combined to achieve even greater memory savings and more efficiently train models with billions of parameters.\n\n### Data parallelism and pipeline parallelism\n\nData and pipeline parallelism distributes the data across GPUs and divides each mini batch of data into micro-batches to achieve pipeline parallelism.",
  "Each data parallel rank treats the process as if there were only one GPU instead of two, but GPUs 0 and 1 can offload micro-batches of data to GPUs 2 and 3 and reduce idle time.\n\nThis approach optimizes parallel data processing by reducing idle GPU utilization.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero-dp-pp.png\"/>\n</div>\n\n### ZeRO data parallelism, pipeline parallelism, and model parallelism (3D parallelism)\n\nData, pipeline and model parallelism combine to form [3D parallelism](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/) to optimize memory and compute efficiency.\n\nMemory effiiciency is achieved by splitting the model across GPUs and also dividing it into stages to create a pipeline. This allows GPUs to work in parallel on micro-batches of data, reducing the memory usage of the model, optimizer, and activations.",
  "Compute efficiency is enabled by ZeRO data parallelism where each GPU only stores a slice of the model, optimizer, and activations. This allows higher communication bandwidth between data parallel nodes because communication can occur independently or in parallel with the other pipeline stages.\n\nThis approach is scalable to extremely large models with trillions of parameters.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-deepspeed-3d.png\"/>\n</div>",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Serving\n\nTransformer models can be served for inference with specialized libraries such as Text Generation Inference (TGI) and vLLM. These libraries are specifically designed to optimize performance with LLMs and include many unique optimization features that may not be included in Transformers.\n\n## TGI",
  "[TGI](https://huggingface.co/docs/text-generation-inference/index) can serve models that aren't [natively implemented](https://huggingface.co/docs/text-generation-inference/supported_models) by falling back on the Transformers implementation of the model. Some of TGIs high-performance features aren't available in the Transformers implementation, but other features like continuous batching and streaming are still supported.\n\n> [!TIP]\n> Refer to the [Non-core model serving](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models) guide for more details.\n\nServe a Transformers implementation the same way you'd serve a TGI model.\n\n```docker\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id gpt2\n```\n\nAdd `--trust-remote_code` to the command to serve a custom Transformers model.\n\n```docker\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id <CUSTOM_MODEL_ID> --trust-remote-code\n```\n\n## vLLM",
  "[vLLM](https://docs.vllm.ai/en/latest/index.html) can also serve a Transformers implementation of a model if it isn't [natively implemented](https://docs.vllm.ai/en/latest/models/supported_models.html#list-of-text-only-language-models) in vLLM.\n\nMany features like quantization, LoRA adapters, and distributed inference and serving are supported for the Transformers implementation.\n\n> [!TIP]\n> Refer to the [Transformers fallback](https://docs.vllm.ai/en/latest/models/supported_models.html#transformers-fallback) section for more details.\n\nBy default, vLLM serves the native implementation and if it doesn't exist, it falls back on the Transformers implementation. But you can also set `--model-impl transformers` to explicitly use the Transformers model implementation.\n\n```shell\nvllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n--task generate \\\n--model-impl transformers \\\n```\n\nAdd the `trust-remote-code` parameter to enable loading a remote code model.\n\n```shell\nvllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n--task generate \\\n--model-impl transformers \\\n--trust-remote-code \\\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Templates\n\nThe [chat pipeline](./conversations) guide introduced [`TextGenerationPipeline`] and the concept of a chat prompt or chat template for conversing with a model. Underlying this high-level pipeline is the [`apply_chat_template`] method. A chat template is a part of the tokenizer and it specifies how to convert conversations into a single tokenizable string in the expected model format.",
  "In the example below, Mistral-7B-Instruct and Zephyr-7B are finetuned from the same base model but they’re trained with different chat formats. Without chat templates, you have to manually write formatting code for each model and even minor errors can hurt performance. Chat templates offer a universal way to format chat inputs to any model.\n\n<hfoptions id=\"template\">\n<hfoption id=\"Mistral\">\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nchat = [\n{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n{\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n{\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\n\ntokenizer.apply_chat_template(chat, tokenize=False)\n```\n```md\n<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\n```\n\n</hfoption>\n<hfoption id=\"Zephyr\">\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nchat = [\n{\"role\": \"user\", \"content\": \"Hello, how are you?\"},",
  "{\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n{\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\n\ntokenizer.apply_chat_template(chat, tokenize=False)\n```\n```md\n<|user|>\\nHello, how are you?</s>\\n<|assistant|>\\nI'm doing great. How can I help you today?</s>\\n<|user|>\\nI'd like to show off how chat templating works!</s>\\n\n```\n\n</hfoption>\n</hfoptions>\n\nThis guide explores [`apply_chat_template`] and chat templates in more detail.\n\n## apply_chat_template\n\nChats should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker (usually between you and the system), and the `content` key contains your message. For the system, the `content` is a high-level description of how the model should behave and respond when you’re chatting with it.\n\nPass your messages to [`apply_chat_template`] to tokenize and format them. You can set [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` to indicate the start of a message.\n\n```py\nimport torch",
  "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n```\n```md\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\n```\n\nNow pass the tokenized chat to [`~GenerationMixin.generate`] to generate a response.\n\n```py\noutputs = model.generate(tokenized_chat, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0]))\n```\n```md\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>",
  "Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```\n\n### add_generation_prompt\nThe [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) parameter adds tokens that indicate the start of a response. This ensures the chat model generates a system response instead of continuing a users message.\n\nNot all models require generation prompts, and some models, like [Llama](./model_doc/llama), don’t have any special tokens before the system response. In this case, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\n\n```py\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\ntokenized_chat\n```",
  "```md\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n```\n\n### continue_final_message\n\nThe [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message.\n\nThis is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy for instruction following when you know how to start its replies.\n\n```py\nchat = [\n{\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n{\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n]\n\nformatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\nmodel.generate(**formatted_chat)\n```\n\n> [!WARNING]",
  "> You shouldn’t use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.",
  "[`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the “assistant” role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) to the pipeline.\n\n## Multiple templates\n\nA model may have several different templates for different use cases. For example, a model may have a template for regular chat, tool use, and RAG.\n\nWhen there are multiple templates, the chat template is a dictionary. Each key corresponds to the name of a template. [`apply_chat_template`] handles multiple templates based on their name. It looks for a template named `default` in most cases and if it can’t find one, it raises an error.",
  "For a tool calling template, if a user passes a `tools` parameter and a `tool_use` template exists, the tool calling template is used instead of `default`.\n\nTo access templates with other names, pass the template name to the `chat_template` parameter in [`apply_chat_template`]. For example, if you’re using a RAG template then set `chat_template=\"rag\"`.\n\nIt can be confusing to manage multiple templates though, so we recommend using a single template for all use cases. Use Jinja statements like `if tools is defined` and `{% macro %}` definitions to wrap multiple code paths in a single template.\n\n## Template selection\n\nIt is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if you’re training the model further, performance is best if the chat tokens are kept constant.",
  "But if you’re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexbile enough to handle many use cases. It even includes support for [generation prompts](#add_generation_prompt), but it doesn’t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\n\n```py\n{%- for message in messages %}\n{{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n{%- endfor %}\n```\n\nSet the template with the following logic to support [generation prompts](#add_generation_prompt). The template wraps each message with `<|im_start|>` and `<|im_end|>` tokens and writes the role as a string. This allows you to easily customize the roles you want to train with.\n\n```py",
  "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```\n\nThe `user`, `system` and `assistant` roles are standard roles in chat templates. We recommend using these roles when it makes sense, especially if you’re using your model with the [`TextGenerationPipeline`].\n\n```py\n<|im_start|>system\nYou are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great!<|im_end|>\n```\n\n## Model training\n\nTraining a model with a chat template is a good way to ensure a chat template matches the tokens a model is trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren’t helpful during training.\n\nAn example of preprocessing a dataset with a chat template is shown below.\n\n```py",
  "from transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\nchat1 = [\n{\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n{\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n{\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n{\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\n\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\nprint(dataset['formatted_chat'][0])\n```\n```md\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n```\n\nAfter this step, you can continue following the [training recipe](./tasks/language_modeling) for causal language models using the `formatted_chat` column.",
  "Some tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` as well to avoid duplicating them.\n\n```py\napply_chat_template(messages, tokenize=False, add_special_tokens=False)\n```\n\nThis isn’t an issue if `apply_chat_template(tokenize=True)`.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Adding a new model to Transformers\n\n> [!TIP]\n> Try adding new models with a more [modular](./modular_transformers) approach first. This makes it significantly easier to contribute a model to Transformers!\n\nMany of the models in Transformers are contributed by developers and researchers. As an open-source first project, we're invested in empowering the community to actively and independently add more models.\n\nWhen you add a model to Transformers, you'll learn:\n\n- more about open-source best practices",
  "- about a models architecture\n- about Transformers' design principles\n- how to efficiently test large models\n- how to use Python utilities like [Black](https://black.readthedocs.io/en/stable/) and [Ruff](https://docs.astral.sh/ruff/) to create clean and readable code\n\nIt is a challenging but rewarding process.\n\nThis guide will walk you through adding an example BrandNewLlama PyTorch model to Transformers. Before you begin, it is a good idea to familiarize yourself with the library.\n\n## Transformers overview\n\nTransformers is an opinionated library with its own unique philosophy and design choices. These choices help us sustainably scale and maintain Transformers.\n\n> [!TIP]\n> Learn more about our design principles on the [Philosophy](./philosophy) doc.\n\nSome of these design choices are:\n\n- composition > over-abstraction\n- duplicate code isn't always bad if it greatly improves readability and accessibility\n- model files are self-contained and all the necessary model code is found in the `modeling_mymodel.py` file\n\nThese design choices are important *for everyone* interacting with the model. It is easier to read, understand, and modify.",
  "This section describes how the model and configuration classes interact and the Transformers code style.\n\n### Model and configuration\n\nAll Transformers' models inherit from a base [`PreTrainedModel`] and [`PretrainedConfig`] class. The configuration is the models blueprint.\n\nThere is never more than two levels of abstraction for any model to keep the code readable. The example model here, BrandNewLlama, inherits from `BrandNewLlamaPreTrainedModel` and [`PreTrainedModel`]. It is important that a new model only depends on [`PreTrainedModel`] so that it can use the [`~PreTrainedModel.from_pretrained`] and [`~PreTrainedModel.save_pretrained`] methods.\n\nOther important functions like the forward method are defined in the `modeling.py` file.\n\nSpecific model heads (for example, sequence classification or language modeling) should call the base model in the forward pass rather than inheriting from it to keep abstraction low.\n\nNew models require a configuration, for example `BrandNewLlamaConfig`, that is stored as an attribute of [`PreTrainedModel`].\n\n```py\nmodel = BrandNewLlamaModel.from_pretrained(\"username/brand_new_llama\")\nmodel.config\n```",
  "[`PretrainedConfig`] provides the [`~PretrainedConfig.from_pretrained`] and [`~PretrainedConfig.save_pretrained`] methods.\n\nWhen you use [`PreTrainedModel.save_pretrained`], it automatically calls [`PretrainedConfig.save_pretrained`] so that both the model and configuration are saved together.\n\nA model is saved to a `model.safetensors` file and a configuration is saved to a `config.json` file.\n\n### Code style\n\nTransformers prefers a clean and readable code over a more abstracted code style. Some of the code style choices include:\n\n- The code should be accessible to non-English users. Pick descriptive variable names and avoid abbreviations. For example, \"activation\" is preferred over \"act\". One letter variables names are highly discouraged unless it's an index in a for loop.\n\n- Explicit code is preferred - even if it's longer - over shorter code.\n\n- Avoid subclassing [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Subclass [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) instead so the code can be quickly debugged with print statements or breakpoints.",
  "- Function signatures should be type-annotated. Otherwise, use good variable names so they're more understandable.\n\n## New model addition issue\n\nOpen a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml) issue to add a specific model.\n\n> [!TIP]\n> Filter by the [New model](https://github.com/huggingface/transformers/labels/New%20model) label on GitHub to view and add any existing model requests.\n\nNow is a good time to get familiar with BrandNewLlama. It is helpful to read a models research paper to understand its technical design and implementation. You don't necessarily have to worry too much about the theoretical details. Instead, focus on the practical ones. Use the questions below to guide your reading.\n\n- What type of model is BrandNewLlama? Is it a encoder, decoder, or encoder-decoder model?\n- What tasks can BrandNewLlama be used for?\n- What makes BrandNewLlama different from other models?\n- What models in Transformers are most similar to BrandNewLlama?\n- What tokenizer does BrandNewLlama use?\n\nIn addition to learning more about your model, use the tips below to help you add a model faster.",
  "> [!TIP]\n> Each contributor has a unique style and workflow for adding models to Transformers. For an example, take a look at how [Gemma](https://github.com/huggingface/transformers/pull/29167) was added.\n\n- Don't reinvent the wheel! Take your time to explore existing models and tokenizers to see what you can copy and reuse. [Grep](https://www.gnu.org/software/grep/) and [ripgrep](https://github.com/BurntSushi/ripgrep) are great tools for this.\n- This is more of an engineering than a science challenge. Focus on the more practical (setting up an efficient debugging environment for example) instead of the theorertical aspects of the model.\n- Don't be shy to ask for help! We are here to support you. 🤗\n\n## Dev environment\n\nClick on the **Fork** button on the [Transformers](https://github.com/huggingface/transformers) repository to create your own copy to work on. Clone the repository to your local disk and add the base repository as the remote.\n\n```bash\ngit clone https://github.com/[your Github handle]/transformers.git\ncd transformers\ngit remote add upstream https://github.com/huggingface/transformers.git\n```",
  "Create a virtual environment and perform an [editable install](./installation#editable-install) of the library with the \"dev\" or development dependencies.\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -e \".[dev]\"\n```\n\nDue to the number of optional dependencies as Transformers grows, this command may fail. In this case, install the \"quality\" dependencies. Also make sure you have a deep learning framework installed.\n\n```bash\npip install -e \".[quality]\"\n```\n\nReturn to the parent directory and clone and install the original BrandNewLlama repository.\n\n```bash\ngit clone https://github.com/org_that_created_brand_new_llama_org/brand_new_llama.git\ncd brand_new_bert\npip install -e .\n```\n\nReturn to your clone of Transformers to begin porting BrandNewLlama.\n\n```bash\ncd transformers\n```\n\nThere are two possible debugging environments for running the original model, a notebook ([Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) or [Jupyter](https://jupyter.org/)) or a local Python script.\n\n> [!WARNING]",
  "> We don't recommend setting up a GPU environment to run the original model because it can be expensive. Instead, work in a CPU environment first to verify the model works in Transformers. Once it does, then you can verify it on a GPU.\n\nNotebooks are great for executing code cell-by-cell which can help split logical components from one another. It can also accelerate debugging cycles because intermediate results can be stored. You can also share notebooks when working with other contributors.\n\nThe downside is that if you aren't used to them, it may take some time to get used to.\n\n> [!TIP]\n> If the model architecture is identical to an existing model, skip ahead to add a [conversion script](#conversion-script), because you can reuse the architecture of the existing model.\n\nRun the command below to start and complete the questionnaire with some basic information about the new model. This command jumpstarts the process by automatically generating some model code that you'll need to adapt.\n\n```bash\ntransformers-cli add-new-model-like\n```\n\n## Create a pull request",
  "Before you start adapting the code, create a pull request to track your progress and get feedback from the Transformers team. Title your pull request **[WIP] Add BrandNewLlama** so it's clear that this is a work in progress.\n\nCreate a branch with a descriptive name from your main branch.\n\n```bash\ngit checkout -b add_brand_new_bert\n```\n\nCommit the code, and then fetch and rebase on the main branch.\n\n```bash\ngit add .\ngit commit\ngit fetch upstream\ngit rebase upstream/main\n```\n\nPush any changes to your branch and click on **Compare & pull request** to open a pull request on GitHub. Open the pull request as a *draft* to indicate it's a work in progress.\n\n```bash\ngit push -u origin a-descriptive-name-for-my-changes\n```\n\nInclude relevant Hugging Face team members by adding their GitHub handles in the pull request for questions, feedback, comments, and reviews. Direct team members to specific parts of the code you want by clicking on the **Files changed** tab, and then clicking on **+** to the left of the line number to add a comment. When a question or problem is solved, click on **Resolve** to indicate the issue is resolved. This keeps the conversation organized and clean.",
  "Remember to periodically commit and push your work, and update your work with the current main branch.\n\n```bash\ngit fetch upstream\ngit merge upstream/main\n```\n\n## Original checkpoint\n\nTake some time to work on the original model implementation first to understand how it works.\n\nThis can be difficult if the original model repository is lacking documentation or if the codebase is complex. But you should use this as your motivation to implement the model in Transformers. Your contribution makes it more accessible and user-friendly to everyone!\n\nOrient yourself with the original repository by doing the following.\n\n- Locate the pretrained weights.\n- Figure out how to the load pretrained weights into the model.\n- Figure out how to run the tokenizer independently of the model.\n- Trace one forward pass to understand which classes and functions are required. These are probably the only classes and functions you'll have to implement.\n- Locate all the important components (model class, model subclasses, self-attention layer, etc.) of the model.",
  "- Figure out how to debug the model in the original repository. Add print statements, use interactive debuggers like [ipdb](https://github.com/gotcha/ipdb), or a efficient integrated development environment (IDE) like [PyCharm](https://www.jetbrains.com/pycharm/).\n\nThe last point is especially important because you'll need a thorough understanding of what's happening inside the original model before you can reimplement it in Transformers. Feel free to open issues and pull requests in the original repository if you encounter any issues.\n\nA good first step is to load a *small* pretrained checkpoint and try to reproduce a single forward pass with an example integer vector of inputs. For example, in pseudocode, this could look like the following.\n\n```py\nmodel = BrandNewLlamaModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\ninput_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids\noriginal_output = model.generate(input_ids)\n```\n\n### Debugging\n\nIf you run into issues, you'll need to choose one of the following debugging strategies depending on the original models codebase.\n\n<hfoptions id=\"debug-strategy\">\n<hfoption id=\"sub-components\">",
  "This strategy relies on breaking the original model into smaller sub-components, such as when the code can be easily run in eager mode. While more difficult, there are some advantages to this approach.\n\n1. It is easier later to compare the original model to your implementation. You can automatically verify that each individual component matches its corresponding component in the Transformers' implementation. This is better than relying on a visual comparison based on print statements.\n2. It is easier to port individual components instead of the entire model.\n3. It is easier for understanding how a model works by breaking it up into smaller parts.\n4. It is easier to prevent regressions at a later stage when you change your code thanks to component-by-component tests.\n\n> [!TIP]\n> Refer to the ELECTRA [integration checks](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed) for a good example of how to decompose a model into smaller components.\n\n</hfoption>\n<hfoption id=\"model and tokenizer\">",
  "This strategy is viable when the original codebase is too complex, only allows intermediate components to be run in compiled mode, or if it's too time-consuming (maybe even impossible) to separate the model into smaller sub-components.\n\nFor example, the MeshTensorFlow implementation of [T5](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow) is too complex and doesn't offer a simple way to decompose the model into its sub-components. In this situation, you'll have to rely on verifying print statements.\n\n</hfoption>\n</hfoptions>\n\nWhichever strategy you choose, it is recommended to debug the initial layers first and the final layers last. Retrieve the output, either with print statements or sub-component functions, of the following layers in this order.\n\n1. input ids passed to the model\n2. word embeddings\n3. input of the first Transformer layer\n4. output of the first Transformer layer\n5. output of the following n-1 Transformer layers\n6. output of the whole model\n\nThe input ids should just be an array of integers like `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`.\n\nLayer outputs often consist of multi-dimensional float arrays.\n\n```py\n[[",
  "[-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\n[-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],\n[-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],\n...,\n[-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],\n[-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],\n[-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],\n```\n\nEvery Transformers model output should have a precision or error tolerance of *1e-3*. This accounts for any output differences that arise from using a different library framework. Compare the intermediate outputs of the original model with the Transformers implementation to ensure they're nearly identical. Having an *efficient* debugging environment is crucial for this step.\n\nHere are some tips for an efficient debugging environment.",
  "- To debug intermediate results, it depends on the machine learning framework the original model repository is using. For PyTorch, you should write a script to decompose the original model into smaller sub-components to retrieve the intermediate values. For TensorFlow, you may need to use [tf.print](https://www.tensorflow.org/api_docs/python/tf/print). For Flax, make sure the model is *not jitted* during the forward pass (refer to this GitHub [Issue](https://github.com/google/jax/issues/196) for more details).\n\n- It is faster to debug with a smaller pretrained checkpoint versus a larger checkpoint where the forward pass takes more than 10 seconds. If only large checkpoints are available, create a dummy model with randomly initialized weights and save those weights to compare against the Transformers implementation.\n\n- Find the easiest way to call the model's forward pass. Ideally, this function (may be called `predict`, `evaluate`, `forward`, or `__call__`) should only call the forward pass *once*. It is more difficult to debug a function that calls the forward pass multiple times.",
  "- Separate tokenization from the forward pass. Locate where a string input is changed to input ids in the forward pass and start here. You may need to create a small script or modify the original code to directly input the input ids instead of an input string.\n\n- Ensure the model is *not* in training mode. This can produce random outputs due to multiple dropout layers in a model. The forward pass in your debugging environment should be *deterministic* so that the dropout layers aren't used.\n\nOnce you're able to run the original checkpoint, you're ready to start adapting the model code for Transformers.\n\n## Adapt the model code\n\nThe `transformers-cli add-new-model-like` command should have generated a model and configuration file.\n\n- `src/transformers/models/brand_new_llama/modeling_brand_new_llama.py`\n- `src/transformers/models/brand_new_llama/configuration_brand_new_llama.py`",
  "The automatically generated code in the `modeling.py` file has the same architecture as Llama if you answered it's a decoder-only model or it will have the same architecture as BART if you answered it's an encoder-decoder model. The generated code is just a starting point. Based on your research on the new model, you'll need to implement those specific changes by adapting the generated code. This may involve changes to the self-attention layer, the order of the normalization layer, and so on.\n\n### Model initialization\n\nAt this point, your code doesn't have to be clean or even fully correct, It is more efficient to quickly create a first draft and then iteratively improve on it. The most important thing is that your model can be instantiated from Transformers. The command below creates a model from the configuration with random weights, verifying that the `__init__` method works.\n\n```py\nfrom transformers import BrandNewLlama, BrandNewLlamaConfig\nmodel = BrandNewLlama(BrandNewLlamaConfig())\n```\n\nRandom initialization occurs in the `_init_weights` method of `BrandNewLlamaPreTrainedModel`. All leaf modules are initialized depending on the configuration's variables.\n\n```py",
  "def _init_weights(self, module):\n\"\"\"Initialize the weights\"\"\"\nif isinstance(module, nn.Linear):\nmodule.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\nif module.bias is not None:\nmodule.bias.data.zero_()\nelif isinstance(module, nn.Embedding):\nmodule.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\nif module.padding_idx is not None:\nmodule.weight.data[module.padding_idx].zero_()\nelif isinstance(module, nn.LayerNorm):\nmodule.bias.data.zero_()\nmodule.weight.data.fill_(1.0)\n```\n\nThe initialization scheme can look different if you need to adapt it to your model. For example, [`Wav2Vec2ForPreTraining`] initializes [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) in its last two linear layers.\n\nThe `_is_hf_initialized` flag makes sure the submodule is only initialized once. Setting `module.project_q` and `module.project_hid` to `True` ensures the custom initialization is not overridden later. The `_init_weights` function won't be applied to these modules.\n\n```py\ndef _init_weights(self, module):\n\"\"\"Initialize the weights\"\"\"\nif isinstance(module, Wav2Vec2ForPreTraining):\nmodule.project_hid.reset_parameters()",
  "module.project_q.reset_parameters()\nmodule.project_hid._is_hf_initialized = True\nmodule.project_q._is_hf_initialized = True\nelif isinstance(module, nn.Linear):\nmodule.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\nif module.bias is not None:\nmodule.bias.data.zero_()\n```\n\n### Convert checkpoints to Transformers\n\nThe original checkpoint must be converted to a Transformers compatible checkpoint.\n\n> [!TIP]\n> Try looking for an existing conversion script to copy, adapt, and reuse for your model!\n>\n> - If you're porting a model from TensorFlow to PyTorch, a good starting point may be the BERT [conversion script](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91).\n> - If you're porting a model from PyTorch to PyTorch, a good starting point may be the BART [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py).",
  "Make sure **all** required weights are initialized and print out all the checkpoint weights that weren't used for initialization to make sure the model has been converted correctly.\n\nYou may encounter wrong shape statements or name assignments during the conversion. This is most likely because of incorrect parameters in `BrandNewLlamaConfig`, the wrong architecture, a bug in the `init` method of your implementation, or you need to transpose one of the checkpoint weights.\n\nKeep iterating on the [Adapt the model code](#adapt-the-model-code) section until all the checkpoint weights are correctly loaded. Once you can load a checkpoint in your model, save it to a folder. This should contain a `model.safetensors` file and a `config.json` file.\n\n```py\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n```\n\nTo help with conversion, the next section briefly describes how PyTorch models stores and defines layer weights and names.\n\n#### PyTorch layer weights and names\n\nIt is helpful to create a basic PyTorch model to understand how layer names are defined and weights are initialized.\n\n```py\nfrom torch import nn\n\nclass SimpleModel(nn.Module):\ndef __init__(self):",
  "super().__init__()\nself.dense = nn.Linear(10, 10)\nself.intermediate = nn.Linear(10, 10)\nself.layer_norm = nn.LayerNorm(10)\n```\n\nPyTorch layer names are defined by the class attribute name of the layer (`dense`, `intermediate`, `layer_norm`). Create a instance of `SimpleModel` to fill all the layers with random weights.\n\n```py\nmodel = SimpleModel()\nprint(model)\nSimpleModel(\n(dense): Linear(in_features=10, out_features=10, bias=True)\n(intermediate): Linear(in_features=10, out_features=10, bias=True)\n(layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n)\n```\n\nThe weight values of a specific layer are randomly initialized.\n\n```py\nprint(model.dense.weight.data)\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n-0.2077,  0.2157],\n[ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,\n0.2166, -0.0212],\n[-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,\n-0.1023, -0.0447],\n[-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,\n-0.1876, -0.2467],\n[ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,\n0.2577,  0.0402],",
  "[ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,\n0.2132,  0.1680],\n[ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,\n0.2707, -0.2509],\n[-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\n0.1829, -0.1568],\n[-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,\n0.0333, -0.0536],\n[-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,\n0.2220,  0.2358]]).\n```\n\nIn the conversion script, the random weights should be replaced with the exact weights from the corresponding layer in the original checkpoint.\n\n```py\n# retrieve matching layer weights with recursive algorithm\nlayer_name = \"dense\"\npretrained_weight = array_of_dense_layer\n\nmodel_pointer = getattr(model, \"dense\")\nmodel_pointer.weight.data = torch.from_numpy(pretrained_weight)\n```\n\nVerify the randomly initialized weights and their corresponding pretrained checkpoint weights have the identical **shape** and **name**. Add assert statements for the shape and print out the checkpoint weight names.\n\n```py\nassert (\nmodel_pointer.weight.shape == pretrained_weight.shape",
  "), f\"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched\"\n\nlogger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")\n```\n\nWhen the shape or name don't match, you may have assigned the incorrect checkpoint weight to a randomly initialized layer. An incorrect shape may be because the `BrandNewLlama` parameters don't exactly match the original models parameters. But it could also be that the PyTorch layer implementation requires the weights to be transposed first.\n\n### Implement the forward pass\n\nThe forward pass should be implemented next if the model loads correctly. It takes some inputs and returns the model output.\n\n```py\nmodel = BrandNewLlamaModel.from_pretrained(\"/path/to/converted/checkpoint/folder\")\ninput_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\noutput = model.generate(input_ids).last_hidden_states\n```",
  "Don't be discouraged if your forward pass isn't identical with the output from the original model or if it returns an error. Check that the forward pass doesn't throw any errors. This is often because the dimensions are wrong or because the wrong data type is used ([torch.long](https://pytorch.org/docs/stable/generated/torch.Tensor.long.html) instead of [torch.float32](https://pytorch.org/docs/stable/tensors.html)).\n\nYour output should have a precision of *1e-3*. Ensure the output shapes and output values are identical. Common reasons for why the outputs aren't identical include:\n\n- Some layers were not added (activation layer or a residual connection).\n- The word embedding matrix is not tied.\n- The wrong positional embeddings are used because the original implementation includes an offset.\n- Dropout is applied during the forward pass. Fix this error by making sure `model.training` is `False` and passing `self.training` to [torch.nn.functional.dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout).",
  "Compare the forward pass of the original model and your implementation to check if there are any differences. Ideally, debug and print out the intermediate outputs of both implementations of the forward pass to pinpoint where the original implementation differs from yours.\n\n1. Make sure the hardcoded `input_ids` in both implementations are identical.\n2. Verify the outputs of the first transformation of `input_ids` (usually the word embeddings) are identical, and work your way through to the last layer.\n\nAny difference between the two implementations should point to the bug in your implementation.\n\nOne of the best strategies is to add many print statements to the same positions in both implementations, and then successively remove them when they output identical values for the intermediate outputs.\n\nWhen both implementations produce the same output, verify the outputs are within a precision of *1e-3*.\n\n```py\ntorch.allclose(original_output, output, atol=1e-3)\n```\n\nThis is typically the most difficult part of the process. Congratulations if you've made it this far!\n\nAnd if you're stuck or struggling with this step, don't hesitate to ask for help on your pull request.",
  "### Add model tests\n\nWhile the model works, you still need to add tests to ensure it is compatible with Transformers. Tests are important because they help users understand your work by looking at specific tests, and because they prevent your model from breaking in the future if any changes are made.\n\n[Cookiecutter](https://cookiecutter.readthedocs.io/en/stable/) should have added a test file for your model. Run the test file below to make sure all common tests pass.\n\n```bash\npytest tests/models/brand_new_llama/test_modeling_brand_new_llama.py\n```\n\nThe integration tests should be added first because they serve the same purpose as the debugging scripts you used earlier to implement the new model in Transformers. A template of those model tests, `BrandNewLlamaModelIntegrationTests`, was added by Cookiecutter and should be filled out. To ensure it passes, run the following command.\n\n<hfoptions id=\"integration-test\">\n<hfoption id=\"macOS\">\n\n```bash\nRUN_SLOW=1 pytest -sv tests/models/brand_new_llama/test_modeling_brand_new_llama.py::BrandNewLlamaModelIntegrationTests\n```\n\n</hfoption>\n<hfoption id=\"Windows\">\n\n```bash",
  "SET RUN_SLOW=1 pytest -sv tests/models/brand_new_llama/test_modeling_brand_new_llama.py::BrandNewLlamaModelIntegrationTests\n```\n\n</hfoption>\n</hfoptions>\n\nAll features unique to BrandNewLlama should be tested in a separate test under `BrandNewLlamaModelTester/BrandNewLlamaModelTest`. This test is often overlooked, but it is extremely important because:\n\n- it helps transfer knowledge you acquired during the process to the community by showing how the models novel features work\n- future contributors can quickly test changes to the model by running these special tests\n\n## Implement tokenizer\n\n> [!TIP]\n> We recommend adding a fast tokenizer ([`PreTrainedTokenizerFast`]) to give users the best performance. Feel free to tag [@ArthurZucker](https://github.com/ArthurZucker) or [@itazap](https://github.com/itazap) in your PR for help on how to add [`PreTrainedTokenizerFast`].\n\nWith the model out of the way, time to focus on the tokenizer. The tokenizer should be identical or very similar to an existing tokenizer in Transformers.",
  "Find and load the original tokenizer file into your implementation. Create a script in the original repository that inputs a string and returns the `input_ids`. The pseudocode should look similar to the code below.\n\n```py\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\nmodel = BrandNewLlamaModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\ninput_ids = model.tokenize(input_str)\n```\n\nYou may need to search the original repository to find the correct tokenizer function or modify the existing tokenizer in your clone of the original repository to only return the `input_ids`. The script for your tokenizer should look similar to the following.\n\n```py\nfrom transformers import BrandNewLlamaTokenizer\n\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\ntokenizer = BrandNewLlamaTokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\ninput_ids = tokenizer(input_str).input_ids\n```",
  "When both implementations have the same `input_ids`, add a tokenizer test file. This file is analogous to the modeling test files. The tokenizer test files should contain a couple of hardcoded integration tests.\n\n## Implement image processor\n\n> [!TIP]\n> Fast image processors use the [torchvision](https://pytorch.org/vision/stable/index.html) library and can perform image processing on the GPU, significantly improving processing speed.\n> We recommend adding a fast image processor ([`BaseImageProcessorFast`]) in addition to the \"slow\" image processor ([`BaseImageProcessor`]) to provide users with the best performance. Feel free to tag [@yonigozlan](https://github.com/yonigozlan) for help adding a [`BaseImageProcessorFast`].",
  "While this example doesn't include an image processor, you may need to implement one if your model requires image inputs. The image processor is responsible for converting images into a format suitable for your model. Before implementing a new one, check whether an existing image processor in the Transformers library can be reused, as many models share similar image processing techniques. Note that you can also use [modular](./modular_transformers) for image processors to reuse existing components.\n\nIf you do need to implement a new image processor, refer to an existing image processor to understand the expected structure. Slow image processors ([`BaseImageProcessor`]) and fast image processors ([`BaseImageProcessorFast`]) are designed differently, so make sure you follow the correct structure based on the processor type you're implementing.\n\nRun the following command (only if you haven't already created the fast image processor with the `transformers-cli add-new-model-like` command) to generate the necessary imports and to create a prefilled template for the fast image processor. Modify the template to fit your model.\n\n```bash",
  "transformers-cli add-fast-image-processor --model-name your_model_name\n```\n\nThis command will generate the necessary imports and provide a pre-filled template for the fast image processor. You can then modify it to fit your model's needs.\n\nAdd tests for the image processor in `tests/models/your_model_name/test_image_processing_your_model_name.py`. These tests should be similar to those for other image processors and should verify that the image processor correctly handles image inputs. If your image processor includes unique features or processing methods, ensure you add specific tests for those as well.\n\n## Implement processor\n\nIf your model accepts multiple modalities, like text and images, you need to add a processor. The processor centralizes the preprocessing of different modalities before passing them to the model.\n\nThe processor should call the appropriate modality-specific processors within its `__call__` function to handle each type of input correctly. Be sure to check existing processors in the library to understand their expected structure. Transformers uses the following convention in the `__call__` function signature.\n\n```python\ndef __call__(\nself,",
  "images: ImageInput = None,\ntext: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\naudio=None,\nvideos=None,\n**kwargs: Unpack[YourModelProcessorKwargs],\n) -> BatchFeature:\n...\n```\n\n`YourModelProcessorKwargs` is a `TypedDict` that includes all the typical processing arguments and any extra arguments a specific processor may require.\n\nAdd tests for the processor in `tests/models/your_model_name/test_processor_your_model_name.py`. These tests should be similar to those for other processors and should verify that the processor correctly handles the different modalities.\n\n## Integration tests\n\nNow that you have a model and tokenizer, add end-to-end integration tests for the model and tokenizer to `tests/models/brand_new_llama/test_modeling_brand_new_llama.py`.\n\nThe test should provide a meaningful text-to-text example to show the model works as expected. For example, you can include a source-to-target translation pair, an article-to-summary pair, or a question-to-answer pair.\n\nIf the checkpoint hasn't been fine-tuned on a downstream task, then the model tests are sufficient.",
  "Finally, try to make sure your tests can run on a GPU by adding `.to(self.device)` statements to the models internal tensors. If you don't have access to a GPU, we can take care of that for you.\n\n## Add documentation\n\nYour model is only useful if users know how to use it. This is why it's important to add documentation and docstrings. Cookiecutter added a template file, `docs/source/model_doc/brand_new_llama.md`, that you can fill out with information about your model.\n\nThis is generally a user's first interaction with a model, so the documentation should be clear and concise. It is often very useful to add examples of how the model should be used.\n\nMake sure docstrings are added to `src/transformers/models/brand_new_llama/modeling_brand_new_llama.py` and includes all necessary inputs and outputs. Review our [guide](https://github.com/huggingface/transformers/tree/main/docs#writing-documentation---specification) for writing documentation and docstrings.\n\n## Refactor\n\nTime to tidy things up and make sure the code style is consistent with the rest of the library. Run the following command to automatically fix incorrect styles.\n\n```bash\nmake style\n```",
  "To verify the code style passes quality checks, run the command below.\n\n```bash\nmake quality\n```\n\nThere may be other failing tests or checks (missing docstring or incorrect naming) on your pull request due to Transformers strict design tests. We can help you with these issues if you're stuck.\n\nAfter ensuring the code runs correctly, you may want to refactor it to make it more readable or cleaner.\n\n## Upload to the Hub\n\nConvert and upload all checkpoints to the [Hub](https://hf.co/models). Add a model card to provide more transparency and context about the model. The model card should highlight specific characteristics of a checkpoint, how the model was trained, and code examples of how to use it.\n\n> [!TIP]\n> In many cases, adding an interactive notebook users can run is a great way to showcase how to use the model for inference or fine-tune it on a downstream task. While not required, including a notebook can drive greater adoption of your model.\n\nYou should also consult with the Transformers team to decide on an appropriate name for the model, and getting the required access rights to upload the model.\n\nUse the [`~PreTrainedModel.push_to_hub`] method to upload the model.\n\n```py",
  "brand_new_bert.push_to_hub(\"brand_new_llama\")\n```\n\nRefer to the [Sharing](./model_sharing) guide for more information about uploading models to the Hub.\n\n## Merge your model\n\nYou're finally ready to merge your pull request and officially add the model to Transformers! Make sure all the tests are passing and all comments and feedback have been addressed.\n\nCongratulations on adding a new model to Transformers! 🥳\n\nThis is a very significant contribution. Your work makes Transformers more accessible to developers and researchers around the world. You should be proud of your contribution and share your accomplishment with the community!\n\n## Model addition timeline\n\nThere are four timelines for model additions depending on the model contributor and community demand for an architecture.\n\n- **day-0 integration**: If you plan on having a Transformers-first release, this is a great option because we can ensure the documentation is clear and optimize your model as much as possible (quantization, FlashAttention, KV-cache, etc.). We can also help you add the model, provide early reviews and make sure it works as expected.",
  "Reach out to transformers@huggingface.co a few days (preferably weeks) in advance, especially if an architecture is particularly novel, to ensure model integration. We'll work together on a private fork of Transformers until your checkpoint and release is ready.\n\n- **same week integration**: Models with significant requests/demand are usually added the same week if the model author doesn't reach out.\n\nUse the [issue tracker](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&projects=&template=new-model-addition.yml) to request a specific model to add. The more activity on the issue, the faster and more likely we'll integrate it.\n\n- **post-release integration**: Models without popular requests/demand or if we don't have the bandwidth to integrate it are added post-release.",
  "This is a good opportunity if you're interested in contributing a model to Transformers. Take a look at open issues tagged with [\"New model\"](https://github.com/huggingface/transformers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+model%22). Feel free to give the most requested models a try first to multiply the impact of your contribution. We'll be there to help you each step of the way!\n\n- **Hub-first release**: Transformers [remote-code](./models#custom-models) feature allows Transformers-based projects to be shared directly on the Hub. This is a good option if you don't have the bandwidth to add a model directly to Transformers.\n\nIf a model ends up being very popular, then it's very likely that we'll integrate it in Transformers ourselves to enable better support (documentation, maintenance, optimization, etc.) for it. A Hub-first release is the most frictionless way to add a model.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Web server inference\n\nA web server is a system that waits for requests and serves them as they come in. This means you can use [`Pipeline`] as an inference engine on a web server, since you can use an iterator (similar to how you would [iterate over a dataset](./pipeline_tutorial#large-datasets)) to handle each incoming request.",
  "Designing a web server with [`Pipeline`] is unique though because they're fundamentally different. Web servers are multiplexed (multithreaded, async, etc.) to handle multiple requests concurrently. [`Pipeline`] and its underlying model on the other hand are not designed for parallelism because they take a lot of memory. It's best to give a [`Pipeline`] all the available resources when they're running or for a compute intensive job.\n\nThis guide shows how to work around this difference by using a web server to handle the lighter load of receiving and sending requests, and having a single thread to handle the heavier load of running [`Pipeline`].\n\n## Create a server\n\n[Starlette](https://www.starlette.io/) is a lightweight framework for building web servers. You can use any other framework you'd like, but you may have to make some changes to the code below.\n\nBefore you begin, make sure Starlette and [uvicorn](http://www.uvicorn.org/) are installed.\n\n```py\n!pip install starlette uvicorn\n```\n\nNow you can create a simple web server in a `server.py` file. The key is to only load the model **once** to prevent unnecessary copies of it from consuming memory.",
  "Create a pipeline to fill in the masked token, `[MASK]`.\n\n```py\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\nfrom transformers import pipeline\nimport asyncio\n\nasync def homepage(request):\npayload = await request.body()\nstring = payload.decode(\"utf-8\")\nresponse_q = asyncio.Queue()\nawait request.app.model_queue.put((string, response_q))\noutput = await response_q.get()\nreturn JSONResponse(output)\n\nasync def server_loop(q):\npipeline = pipeline(task=\"fill-mask\",model=\"google-bert/bert-base-uncased\")\nwhile True:\n(string, response_q) = await q.get()\nout = pipeline(string)\nawait response_q.put(out)\n\napp = Starlette(\nroutes=[\nRoute(\"/\", homepage, methods=[\"POST\"]),\n],\n)\n\n@app.on_event(\"startup\")\nasync def startup_event():\nq = asyncio.Queue()\napp.model_queue = q\nasyncio.create_task(server_loop(q))\n```\n\nStart the server with the following command.\n\n```bash\nuvicorn server:app\n```\n\nQuery the server with a POST request.\n\n```bash\ncurl -X POST -d \"Paris is the [MASK] of France.\" http://localhost:8000/\n[{'score': 0.9969332218170166,\n'token': 3007,\n'token_str': 'capital',",
  "'sequence': 'paris is the capital of france.'},\n{'score': 0.0005914849461987615,\n'token': 2540,\n'token_str': 'heart',\n'sequence': 'paris is the heart of france.'},\n{'score': 0.00043787318281829357,\n'token': 2415,\n'token_str': 'center',\n'sequence': 'paris is the center of france.'},\n{'score': 0.0003378340043127537,\n'token': 2803,\n'token_str': 'centre',\n'sequence': 'paris is the centre of france.'},\n{'score': 0.00026995912776328623,\n'token': 2103,\n'token_str': 'city',\n'sequence': 'paris is the city of france.'}]\n```\n\n## Queuing requests\n\nThe server's queuing mechanism can be used for some interesting applications such as dynamic batching. Dynamic batching accumulates several requests first before processing them with [`Pipeline`].\n\nThe example below is written in pseudocode for readability rather than performance, in particular, you'll notice that:\n\n1. There is no batch size limit.",
  "2. The timeout is reset on every queue fetch, so you could end up waiting much longer than the `timeout` value before processing a request. This would also delay the first inference request by that amount of time. The web server always waits 1ms even if the queue is empty, which is inefficient, because that time can be used to start inference. It could make sense though if batching is essential to your use case.\n\nIt would be better to have a single 1ms deadline, instead of resetting it on every fetch.\n\n```py\n(string, rq) = await q.get()\nstrings = []\nqueues = []\nwhile True:\ntry:\n(string, rq) = await asyncio.wait_for(q.get(), timeout=0.001)\nexcept asyncio.exceptions.TimeoutError:\nbreak\nstrings.append(string)\nqueues.append(rq)\nstrings\nouts = pipeline(strings, batch_size=len(strings))\nfor rq, out in zip(queues, outs):\nawait rq.put(out)\n```\n\n## Error checking\n\nThere are many things that can go wrong in production. You could run out-of-memory, out of space, fail to load a model, have an incorrect model configuration, have an incorrect query, and so much more.",
  "Adding `try...except` statements is helpful for returning these errors to the user for debugging. Keep in mind this could be a security risk if you shouldn't be revealing certain information.\n\n## Circuit breaking\n\nTry to return a 503 or 504 error when the server is overloaded instead of forcing a user to wait indefinitely.\n\nIt is relatively simple to implement these error types since it's only a single queue. Take a look at the queue size to determine when to start returning errors before your server fails under load.\n\n## Block the main thread\n\nPyTorch is not async aware, so computation will block the main thread from running.\n\nFor this reason, it's better to run PyTorch on its own separate thread or process. When inference of a single request is especially long (more than 1s), it's even more important because it means every query during inference must wait 1s before even receiving an error.\n\n## Dynamic batching\n\nDynamic batching can be very effective when used in the correct setting, but it's not necessary when you're only passing 1 request at a time (see [batch inference](./pipeline_tutorial#batch-inference) for more details).",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Summary of the tokenizers\n\n[[open-in-colab]]\n\nOn this page, we will have a closer look at tokenization.\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nAs we saw in [the preprocessing tutorial](preprocessing), tokenizing a text is splitting it into words or\nsubwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is",
  "straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text).\nMore specifically, we will look at the three main types of tokenizers used in 🤗 Transformers: [Byte-Pair Encoding\n(BPE)](#byte-pair-encoding), [WordPiece](#wordpiece), and [SentencePiece](#sentencepiece), and show examples\nof which tokenizer type is used by which model.\n\nNote that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer\ntype was used by the pretrained model. For instance, if we look at [`BertTokenizer`], we can see\nthat the model uses [WordPiece](#wordpiece).\n\n## Introduction\n\nSplitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so.\nFor instance, let's look at the sentence `\"Don't you love 🤗 Transformers? We sure do.\"`\n\n<Youtube id=\"nhJxYji1aho\"/>\n\nA simple way of tokenizing this text is to split it by spaces, which would give:\n\n```\n[\"Don't\", \"you\", \"love\", \"🤗\", \"Transformers?\", \"We\", \"sure\", \"do.\"]\n```\n\nThis is a sensible first step, but if we look at the tokens `\"Transformers?\"` and `\"do.\"`, we notice that the",
  "punctuation is attached to the words `\"Transformer\"` and `\"do\"`, which is suboptimal. We should take the\npunctuation into account so that a model does not have to learn a different representation of a word and every possible\npunctuation symbol that could follow it, which would explode the number of representations the model has to learn.\nTaking punctuation into account, tokenizing our exemplary text would give:\n\n```\n[\"Don\", \"'\", \"t\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```\n\nBetter. However, it is disadvantageous, how the tokenization dealt with the word `\"Don't\"`. `\"Don't\"` stands for\n`\"do not\"`, so it would be better tokenized as `[\"Do\", \"n't\"]`. This is where things start getting complicated, and\npart of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a\ndifferent tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an\ninput that was tokenized with the same rules that were used to tokenize its training data.\n\n[spaCy](https://spacy.io/) and [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) are two popular",
  "rule-based tokenizers. Applying them on our example, *spaCy* and *Moses* would output something like:\n\n```\n[\"Do\", \"n't\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```\n\nAs can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and\npunctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined\nas splitting sentences into words. While it's the most intuitive way to split texts into smaller chunks, this\ntokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization\nusually generates a very big vocabulary (the set of all unique words and tokens used). *E.g.*, [Transformer XL](model_doc/transfo-xl) uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!\n\nSuch a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which\ncauses both an increased memory and time complexity. In general, transformers models rarely have a vocabulary size\ngreater than 50,000, especially if they are pretrained only on a single language.",
  "So if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters?\n\n<Youtube id=\"ssLq_EK2jLE\"/>\n\nWhile character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder\nfor the model to learn meaningful input representations. *E.g.* learning a meaningful context-independent\nrepresentation for the letter `\"t\"` is much harder than learning a context-independent representation for the word\n`\"today\"`. Therefore, character tokenization is often accompanied by a loss of performance. So to get the best of\nboth worlds, transformers models use a hybrid between word-level and character-level tokenization called **subword**\ntokenization.\n\n## Subword tokenization\n\n<Youtube id=\"zHvTiHr506c\"/>\n\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller\nsubwords, but rare words should be decomposed into meaningful subwords. For instance `\"annoyingly\"` might be\nconsidered a rare word and could be decomposed into `\"annoying\"` and `\"ly\"`. Both `\"annoying\"` and `\"ly\"` as",
  "stand-alone subwords would appear more frequently while at the same time the meaning of `\"annoyingly\"` is kept by the\ncomposite meaning of `\"annoying\"` and `\"ly\"`. This is especially useful in agglutinative languages such as Turkish,\nwhere you can form (almost) arbitrarily long complex words by stringing together subwords.\n\nSubword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful\ncontext-independent representations. In addition, subword tokenization enables the model to process words it has never\nseen before, by decomposing them into known subwords. For instance, the [`~transformers.BertTokenizer`] tokenizes\n`\"I have a new GPU!\"` as follows:\n\n```py\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> tokenizer.tokenize(\"I have a new GPU!\")\n[\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]\n```\n\nBecause we are considering the uncased model, the sentence was lowercased first. We can see that the words `[\"i\", \"have\", \"a\", \"new\"]` are present in the tokenizer's vocabulary, but the word `\"gpu\"` is not. Consequently, the",
  "tokenizer splits `\"gpu\"` into known subwords: `[\"gp\" and \"##u\"]`. `\"##\"` means that the rest of the token should\nbe attached to the previous one, without space (for decoding or reversal of the tokenization).\n\nAs another example, [`~transformers.XLNetTokenizer`] tokenizes our previously exemplary text as follows:\n\n```py\n>>> from transformers import XLNetTokenizer\n\n>>> tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n>>> tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")\n[\"▁Don\", \"'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"🤗\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"]\n```\n\nWe'll get back to the meaning of those `\"▁\"` when we look at [SentencePiece](#sentencepiece). As one can see,\nthe rare word `\"Transformers\"` has been split into the more frequent subwords `\"Transform\"` and `\"ers\"`.\n\nLet's now look at how the different subword tokenization algorithms work. Note that all of those tokenization\nalgorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained\non.\n\n<a id='byte-pair-encoding'></a>\n\n### Byte-Pair Encoding (BPE)",
  "Byte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et\nal., 2015)](https://arxiv.org/abs/1508.07909). BPE relies on a pre-tokenizer that splits the training data into\nwords. Pretokenization can be as simple as space tokenization, e.g. [GPT-2](model_doc/gpt2), [RoBERTa](model_doc/roberta). More advanced pre-tokenization include rule-based tokenization, e.g. [XLM](model_doc/xlm),\n[FlauBERT](model_doc/flaubert) which uses Moses for most languages, or [GPT](model_doc/openai-gpt) which uses\nspaCy and ftfy, to count the frequency of each word in the training corpus.\n\nAfter pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the\ntraining data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set\nof unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until\nthe vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to\ndefine before training the tokenizer.",
  "As an example, let's assume that after pre-tokenization, the following set of words including their frequency has been\ndetermined:\n\n```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nConsequently, the base vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. Splitting all words into symbols of the\nbase vocabulary, we obtain:\n\n```\n(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n```\n\nBPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In\nthe example above `\"h\"` followed by `\"u\"` is present _10 + 5 = 15_ times (10 times in the 10 occurrences of\n`\"hug\"`, 5 times in the 5 occurrences of `\"hugs\"`). However, the most frequent symbol pair is `\"u\"` followed by\n`\"g\"`, occurring _10 + 5 + 5 = 20_ times in total. Thus, the first merge rule the tokenizer learns is to group all\n`\"u\"` symbols followed by a `\"g\"` symbol together. Next, `\"ug\"` is added to the vocabulary. The set of words then\nbecomes\n\n```\n(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n```",
  "BPE then identifies the next most common symbol pair. It's `\"u\"` followed by `\"n\"`, which occurs 16 times. `\"u\"`,\n`\"n\"` is merged to `\"un\"` and added to the vocabulary. The next most frequent symbol pair is `\"h\"` followed by\n`\"ug\"`, occurring 15 times. Again the pair is merged and `\"hug\"` can be added to the vocabulary.\n\nAt this stage, the vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` and our set of unique words\nis represented as\n\n```\n(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n```\n\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied\nto new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance,\nthe word `\"bug\"` would be tokenized to `[\"b\", \"ug\"]` but `\"mug\"` would be tokenized as `[\"<unk>\", \"ug\"]` since\nthe symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the\n`\"<unk>\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\nto happen for very special characters like emojis.",
  "As mentioned earlier, the vocabulary size, *i.e.* the base vocabulary size + the number of merges, is a hyperparameter\nto choose. For instance [GPT](model_doc/openai-gpt) has a vocabulary size of 40,478 since they have 478 base characters\nand chose to stop training after 40,000 merges.\n\n#### Byte-level BPE\n\nA base vocabulary that includes all possible base characters can be quite large if *e.g.* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the <unk> symbol. [GPT-2](model_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges.\n\n<a id='wordpiece'></a>\n\n### WordPiece",
  "WordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n\nSo what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. *E.g.* `\"u\"`, followed by `\"g\"` would have only been\nmerged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol",
  "pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols\nto ensure it's _worth it_.\n\n<a id='unigram'></a>\n\n### Unigram\n\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\nconjunction with [SentencePiece](#sentencepiece).\n\nAt each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then",
  "removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, *i.e.* those\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.\n\nBecause Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n\n```\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\n```\n\n`\"hugs\"` could be tokenized both as `[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]` or `[\"h\", \"u\", \"g\", \"s\"]`. So which one\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\nthe probability of each possible tokenization can be computed after training. The algorithm simply picks the most\nlikely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their\nprobabilities.",
  "Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of\nthe words \\\\(x_{1}, \\dots, x_{N}\\\\) and that the set of all possible tokenizations for a word \\\\(x_{i}\\\\) is\ndefined as \\\\(S(x_{i})\\\\), then the overall loss is defined as\n\n$$\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )$$\n\n<a id='sentencepiece'></a>\n\n### SentencePiece\n\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, *e.g.* [XLM](model_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer.\nTo solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary.",
  "The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\"▁\"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\"▁\"` is replaced by a space.\n\nAll transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\nusing SentencePiece are [ALBERT](model_doc/albert), [XLNet](model_doc/xlnet), [Marian](model_doc/marian), and [T5](model_doc/t5).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is designed to optimize distributed training for large models with data, model, pipeline, and even a combination of all three [parallelism](./perf_train_gpu_many) strategies to provide better memory efficiency and faster training speeds. This is achieved with the [Zero Redundancy Optimizer (ZeRO)](https://hf.co/papers/1910.02054) which consists of three stages.\n\n| ZeRO stage | description |",
  "|---|---|\n| 1 | partition optimizer states |\n| 2 | partition optimizer and gradient states |\n| 3 | partition optimizer, gradient, and parameters |\n\nEach stage progressively saves more memory, allowing really large models to fit and train on a single GPU. All ZeRO stages, offloading optimizer memory and computations from the GPU to the CPU are integrated with [`Trainer`]. Provide a config file or one of the example templates to [`Trainer`] to enable DeepSpeed features.\n\nThis guide walks you through setting up a DeepSpeed config file, how to enable its features in [`Trainer`], and deploy for training.\n\nInstall DeepSpeed from either PyPI or Transformers. For more detailed installation instructions, refer to the DeepSpeed [installation](https://www.deepspeed.ai/tutorials/advanced-install/) or GitHUB [README](https://github.com/microsoft/deepspeed#installation).\n\n<hfoptions id=\"installation\">\n<hfoption id=\"PyPI\">\n\n```bash\npip install deepspeed\n```\n\n</hfoption>\n<hfoption id=\"Transformers\">\n\n```bash\npip install transformers[deepspeed]\n```\n\n</hfoption>\n</hfoptions>\n\n> [!WARNING]",
  "> Refer to the [DeepSpeed CUDA installation](./debugging#deepspeed-cuda-issues) if you're having trouble with your installation. While DeepSpeed has a pip installable package, it is highly recommended to [install it from source](https://www.deepspeed.ai/tutorials/advanced-install/#install-deepspeed-from-source) to ensure it matches your hardware and to support certain features which aren't available in the PyPI distribution.\n\nDeepSpeed provides a tool for estimating the required CPU and GPU memory for the parameters, optimizer and gradient states. You'll also to need to reserve some memory for the CUDA kernels and activations.\n\nRun the command below to check the memory requirements for [bigscience/T0_3B](https://huggingface.co/docs/transformers/main/en/bigscience/T0_3B) on a single GPU.\n\n```bash\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \\\nmodel = AutoModel.from_pretrained(\"bigscience/T0_3B\"); \\\nestimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:",
  "HW: Setup with 1 node, 1 GPU per node.\nSW: Model with 2783M total params, 65M largest layer params.\nper CPU  |  per GPU |   Options\n70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n0.37GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=1\n15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0\n```\n\n> [!TIP]\n> If you have enough GPU memory, disable CPU and NVMe offload to speed everything up.\n\n## Choosing a ZeRO stage\n\nConsider the table below to help you choose the appropriate ZeRO stage for training because there is a trade-off between training speed and memory usage. The table orders the ZeRO stages from fastest to slowest and from least memory usage to most.\n\n| fastest | least memory usage |\n|---|---|\n| ZeRO-1 | ZeRO-3 + offload |\n| ZeRO-2 | ZeRO-3 |\n| ZeRO-2 + offload | ZeRO-2 + offload |\n| ZeRO-3 | ZeRO-2 |\n| ZeRO-3 + offload | ZeRO-1 |",
  "Decide the type of performance you're optimizing for, speed or memory, and then work backwards to discover the best ZeRO stage for your use case. For example, if you're optimizing for speed, start with the fastest ZeRO stage and if you run out of memory, try the next stage which is slower but more memory efficient.\n\n## Config file\n\nOnce you've decided on a ZeRO stage, set up a config file to enable DeepSpeed with [`Trainer`]. The config file contains all the parameters for how to configure and set up your training. When the training script is executed, DeepSpeed logs the configuration from [`Trainer`] to the console so you can see exactly what's being used.\n\n> [!TIP]\n> Find a complete list of DeepSpeed configuration options on the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. There are also practical examples of various DeepSpeed configuration examples in the [DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples) main [DeepSpeed](https://github.com/microsoft/DeepSpeed) repository. Run the command below to quickly find specific examples.\n>\n> ```bash\n> git clone https://github.com/microsoft/DeepSpeedExamples\n> cd DeepSpeedExamples",
  "> find . -name '*json'\n> # find examples with the Lamb optimizer\n> grep -i Lamb $(find . -name '*json')\n> ```\n\nThe config file is passed as a path to a JSON file if you're training from the command line interface or as a nested dict object if you're using [`Trainer`] in a notebook.\n\n<hfoptions id=\"pass-config\">\n<hfoption id=\"path to file\">\n\n```py\nTrainingArguments(\ndeepspeed=\"path/to/deepspeed_config.json\",\n...,\n)\n```\n\n</hfoption>\n<hfoption id=\"nested dict\">\n\n```py\nds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)\nargs = TrainingArguments(\ndeepspeed=ds_config_dict,\n...,\n)\ntrainer = Trainer(\nmodel,\nargs,\n...,\n)\n```\n\n</hfoption>\n</hfoptions>\n\n### DeepSpeed versus Trainer parameters\n\nThere are three types of config parameters.\n\n1. Some config parameters are shared by DeepSpeed and [`Trainer`] making it difficult to identify errors when there are conflicting definitions. In this case, configure these parameters from the [`Trainer`] command line arguments.",
  "1. Some config parameters are automatically derived from the model configuration and don't need to be manually configured. [`Trainer`] uses the config value `auto` to set the most correct or efficient option. You could define these parameters explicitly, but you must take care to ensure the [`Trainer`] and DeepSpeed config parameters match. Mismatches may cause training to fail in very difficult to detect ways.\n1. Some config parameters are specific to DeepSpeed and should be manually set based on your training requirements.\n\nThere are two ways to modify the config parameters.\n\n> [!TIP]\n> Some values, such as `scheduler.params.total_num_steps`, are calculated by [`Trainer`] during training.\n\n1. Create or load a DeepSpeed config to use as the main config.\n1. Create a [`TrainingArguments`] object based on the DeepSpeed config values.\n\n### ZeRO stage\n\nEach ZeRO stage config is defined in `zero_optimization`.\n\nFor a more detailed explanation of each parameter, refer to the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. These parameters must be set up with DeepSpeed because [`Trainer`] doesn't provide equivalent command line arguments.",
  "> [!WARNING]\n> DeepSpeed doesn't validate parameter names and any typos will fallback on the parameters default setting. Observe the DeepSpeed engine startup log messages to see what values are being used.\n\n<hfoptions id=\"zero-config\">\n<hfoption id=\"ZeRO-1\">\n\nZeRO-1 shards the optimizer states across GPUs and you can expect a small speed up.\n\n```yml\n{\n\"zero_optimization\": {\n\"stage\": 1\n}\n}\n```\n\n</hfoption>\n<hfoption id=\"ZeRO-2\">\n\nZeRO-2 shards the optimizer and gradient states across GPUs. This stage is primarily used for training since its features are not relevant to inference. Some important parameters to configure for better performance include the following.\n\n* `offload_optimizer` should be enabled to reduce GPU memory usage.\n* `overlap_comm` when set to `true` uses more GPU memory in exchange for lower allreduce latency. This feature uses 4.5x the `allgather_bucket_size` and `reduce_bucket_size` values. In this example, they're set to `5e8` which means it requires 9GB of GPU memory. If your GPU memory is 8GB or less, you should reduce `overlap_comm` to lower the memory requirements and prevent an out-of-memory (OOM) error.",
  "* `allgather_bucket_size` and `reduce_bucket_size` trade-off available GPU memory for communication speed. The smaller their values, the slower communication is and the more GPU memory is available. You can balance, for example, whether a bigger batch size is more important than a slightly slower training time.\n* `round_robin_gradients` is available in DeepSpeed 0.4.4 for CPU offloading. It parallelizes gradient copying to CPU memory among ranks by fine-grained gradient partitioning. Performance benefit grows with gradient accumulation steps (more copying between optimizer steps) or GPU count (increased parallelism).\n\n```yml\n{\n\"zero_optimization\": {\n\"stage\": 2,\n\"offload_optimizer\": {\n\"device\": \"cpu\",\n\"pin_memory\": true\n},\n\"allgather_partitions\": true,\n\"allgather_bucket_size\": 5e8,\n\"overlap_comm\": true,\n\"reduce_scatter\": true,\n\"reduce_bucket_size\": 5e8,\n\"contiguous_gradients\": true\n\"round_robin_gradients\": true\n}\n}\n```\n\n</hfoption>\n<hfoption id=\"ZeRO-3\">",
  "ZeRO-3 shards the optimizer and gradient states, and parameters across GPUs. Unlike ZeRO-2, ZeRO-3 can also be used for inference in addition to training because it loads large models onto multiple GPUs. Some important parameters to configure include the following.\n\n* `device: \"cpu\"` can help if you're running out of GPU memory and if you have free CPU memory available. This offloads model parameters to the CPU.\n* `pin_memory: true` can improve throughput, but less memory becomes available for other processes because the pinned memory is reserved for the specific process that requested it and it's typically accessed much faster than normal CPU memory.\n* `stage3_max_live_parameters` is the upper limit on how many full parameters to keep on the GPU at any given time. Reduce this value if you encounter an OOM error.",
  "* `stage3_max_reuse_distance` is a value for determining when a parameter is used again in the future, and it helps decide whether to throw the parameter away or to keep it. If the parameter is going to be reused (if the value is less than `stage3_max_reuse_distance`), then it is kept to reduce communication overhead. This is helpful when activation checkpointing is enabled and you want to keep the parameter in the forward recompute until the backward pass. Reduce this value if you encounter an OOM error.\n* `stage3_gather_16bit_weights_on_model_save` consolidates fp16 weights when a model is saved. For large models and multiple GPUs, this is expensive in terms of memory and speed. You should enable it if you're planning on resuming training.",
  "* `sub_group_size` controls which parameters are updated during the optimizer step. Parameters are grouped into buckets of `sub_group_size` and each bucket is updated one at a time. When used with NVMe offload, `sub_group_size` determines when model states are moved in and out of CPU memory during the optimization step. This prevents running out of CPU memory for extremely large models. `sub_group_size` can be left to its default value if you aren't using NVMe offload, but you may want to change it if you:\n\n1. Run into an OOM error during the optimization step. In this case, reduce `sub_group_size` to reduce memory usage of the temporary buffers.\n2. The optimization step is taking a really long time. In this case, increase `sub_group_size` to improve bandwidth utilization as a result of increased data buffers.\n\n* `reduce_bucket_size`, `stage3_prefetch_bucket_size`, and `stage3_param_persistence_threshold` are dependent on a models hidden size. It is recommended to set these values to `auto` and allow [`Trainer`] to automatically assign the values.\n\n```yml\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\",\n\"pin_memory\": true\n},\n\"offload_param\": {",
  "\"device\": \"cpu\",\n\"pin_memory\": true\n},\n\"overlap_comm\": true,\n\"contiguous_gradients\": true,\n\"sub_group_size\": 1e9,\n\"reduce_bucket_size\": \"auto\",\n\"stage3_prefetch_bucket_size\": \"auto\",\n\"stage3_param_persistence_threshold\": \"auto\",\n\"stage3_max_live_parameters\": 1e9,\n\"stage3_max_reuse_distance\": 1e9,\n\"stage3_gather_16bit_weights_on_model_save\": true\n}\n}\n```\n\n### Initialize large models\n\nWith ZeRO-3, use the [deepspeed.zero.Init](https://deepspeed.readthedocs.io/en/latest/zero3.html#deepspeed.zero.Init) context manager to initialize a model faster.\n\n```py\nfrom transformers import T5ForConditionalGeneration, T5Config\nimport deepspeed\n\nwith deepspeed.zero.Init():\nconfig = T5Config.from_pretrained(\"google-t5/t5-small\")\nmodel = T5ForConditionalGeneration(config)\n```\n\nThe DeepSped config file needs to have `is_deepspeed_zero3_enabled: true` setup in [`TrainingArguments`] and it needs a ZeRO configuration enabled. The [`TrainingArguments`] object must be created **before** calling [`~PreTrainedModel.from_pretrained`].\n\n> [!TIP]",
  "> You'll need ZeRO-3 when the fp16 weights don't fit on a single GPU. But if you're able to load the fp16 weights, set `torch_dtype=torch.float16` in [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import AutoModel, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(..., deepspeed=ds_config)\nmodel = AutoModel.from_pretrained(\"google-t5/t5-small\")\ntrainer = Trainer(model=model, args=training_args, ...)\n```\n\nWhen there are multiple GPUs, no single GPU has all the parameters unless it's the parameters of the currently executing layer. To access all parameters from all the layers at once, such as loading pretrained model weights in [`~PreTrainedModel.from_pretrained`], one layer is loaded at a time and immediately partitioned to all GPUs. For very large models, it isn't possible to load the weights onto one GPU and then distribute them across the other GPUs due to memory limitations.\n\nIf you encounter a model parameter weight where `tensor([1.])` or the parameter size is 1 instead of a larger multidimensional shape, it means the parameter is partitioned and this is a ZeRO-3 placeholder.\n\n```py",
  "tensor([1.0], device=\"cuda:0\", dtype=torch.float16, requires_grad=True)\n```\n\n> [!TIP]\n> For more information about initializing large models with ZeRO-3 and accessing the parameters, take a look at the [Constructing Massive Models](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models) and [Gathering Parameters](https://deepspeed.readthedocs.io/en/latest/zero3.html#gathering-parameters) guides.\n\n</hfoption>\n</hfoptions>\n\n### NVMe\n\n[ZeRO-Infinity](https://hf.co/papers/2104.07857) offloads model states to the CPU and/or NVMe to save even more memory. Smart partitioning and tiling algorithms allow each GPU to send and receive very small amounts of data during offloading such that a modern NVMe can fit an even larger total memory pool than is available to your training process. ZeRO-Infinity requires ZeRO-3.",
  "Depending on the CPU and NVMe memory available, you can offload both the [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading), just one of them, or none of them. Make sure the `nvme_path` points to a NVMe device, because while it still works with a regular hard drive or solid state drive, it'll be significantly slower. With a modern NVMe, you can expect peak transfer speeds of ~3.5GB/s for read operations and ~3GB/s for write operations.\n\nConsider running a [benchmark](https://github.com/microsoft/DeepSpeed/issues/998) on your training setup to determine the optimal `aio` configuration.\n\nThe example ZeRO-3 and ZeRO-Infinity config below sets most of the parameter values to `auto`, but you can also manually set configure these values.\n\n```yaml\n{\n\"fp16\": {\n\"enabled\": \"auto\",\n\"loss_scale\": 0,\n\"loss_scale_window\": 1000,\n\"initial_scale_power\": 16,\n\"hysteresis\": 2,\n\"min_loss_scale\": 1\n},\n\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": \"auto\",\n\"betas\": \"auto\",\n\"eps\": \"auto\",\n\"weight_decay\": \"auto\"\n}\n},\n\n\"scheduler\": {\n\"type\": \"WarmupLR\",\n\"params\": {",
  "\"warmup_min_lr\": \"auto\",\n\"warmup_max_lr\": \"auto\",\n\"warmup_num_steps\": \"auto\"\n}\n},\n\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"nvme\",\n\"nvme_path\": \"/local_nvme\",\n\"pin_memory\": true,\n\"buffer_count\": 4,\n\"fast_init\": false\n},\n\"offload_param\": {\n\"device\": \"nvme\",\n\"nvme_path\": \"/local_nvme\",\n\"pin_memory\": true,\n\"buffer_count\": 5,\n\"buffer_size\": 1e8,\n\"max_in_cpu\": 1e9\n},\n\"aio\": {\n\"block_size\": 262144,\n\"queue_depth\": 32,\n\"thread_count\": 1,\n\"single_submit\": false,\n\"overlap_events\": true\n},\n\"overlap_comm\": true,\n\"contiguous_gradients\": true,\n\"sub_group_size\": 1e9,\n\"reduce_bucket_size\": \"auto\",\n\"stage3_prefetch_bucket_size\": \"auto\",\n\"stage3_param_persistence_threshold\": \"auto\",\n\"stage3_max_live_parameters\": 1e9,\n\"stage3_max_reuse_distance\": 1e9,\n\"stage3_gather_16bit_weights_on_model_save\": true\n},\n\n\"gradient_accumulation_steps\": \"auto\",\n\"gradient_clipping\": \"auto\",\n\"steps_per_print\": 2000,\n\"train_batch_size\": \"auto\",\n\"train_micro_batch_size_per_gpu\": \"auto\",\n\"wall_clock_breakdown\": false\n}\n```\n\n## Training features\n\nDeepSpeed supports many training features that can be configured in the config file. This section describes some of the most important features.",
  "### Gradient checkpointing\n\nGradient checkpointing saves memory by only storing *some* of the intermediate activations instead of storing *all* of them. It is useful for fitting larger models on the GPU without running out of memory or to increase the batch size for better performance. Training speed is slower though.\n\n* For a Transformers model, set `model.gradient_checkpointing_enable()` or add `--gradient_checkpointing` in the [`TrainingArguments`].\n* For a non-Transformers model, use the DeepSpeed [Activation Checkpointing API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). Replacing Transformers modeling code and [torch.utils.checkpoint](https://pytorch.org/docs/stable/checkpoint.html) with the DeepSpeed API gives you more flexibility because you can offload the forward activations to the CPU memory instead of recalculating them.\n\n### Batch size\n\nThe batch size can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets `train_micro_batch_size_per_gpu` and `train_batch_size` to the value of `world_size * per_device_train_batch_size * gradient_accumulation_steps`.\n\n```yaml\n{",
  "\"train_micro_batch_size_per_gpu\": \"auto\",\n\"train_batch_size\": \"auto\"\n}\n```\n\n### Communication data type\n\nA separate data type is used for communication collectives like reduction, gathering and scattering operations.\n\nAll gather and scatter operations are performed in the same data type the data is in. For example, if you're training in bf16, the data is also gathered in bf16 because gathering is a non-lossy operation.\n\nReduce operations are lossy, for example, when gradients are averaged across multiple GPUs. When the communication is done in fp16 or bf16, it's more likely to be lossy because adding multiple numbers in low precision isn't exact. This is especially the case with bf16 which has a lower precision than fp16. For this reason, fp16 is the default for reduction operations because the loss is minimal when averaging gradients.\n\nChoose the communication data type by setting the `communication_data_type` parameter in the config file. For example, choosing fp32 adds a small amount of overhead but ensures the reduction operation is accumulated in fp32 and when it is ready, it's downcasted to whichever half-precision data type you're training in.\n\n```yaml\n{",
  "\"communication_data_type\": \"fp32\"\n}\n```\n\n### Gradient accumulation\n\nGradient accumulation accumulates gradients over several mini-batches of data before updating parameters. It stores less gradients and enables training with a larger *effective batch size*. Training speed is slower though, but it's useful for overcoming memory constraints.\n\nGradient accumulation can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets it to the value of `gradient_accumulation_steps`.\n\n```yaml\n{\n\"gradient_accumulation_steps\": \"auto\"\n}\n```\n\n### Gradient clipping\n\nGradient clipping is useful for preventing exploding gradients which can lead to instability during training. It sets a maximum threshold value and rescales the gradients if their norm exceeds the threshold.\n\nGradient clipping can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets it to the value of `max_grad_norm`.\n\n```yaml\n{\n\"gradient_clipping\": \"auto\"\n}\n```\n\n### Mixed precision training",
  "Mixed precision accelerates training speed by performing some calculations in half-precision, but it also maintains some calculations in full-precision to preserve accuracy. DeepSpeed supports fp32, fp16, and bf16 data types.\n\n<hfoptions id=\"precision\">\n<hfoption id=\"fp32\">\n\nTrain in fp32 if a model wasn't pretrained in mixed precision because it may cause underflow or overflow errors. Disable fp16, the default, in this case.\n\n```yaml\n{\n\"fp16\": {\n\"enabled\": false\n}\n}\n```\n\nFor Ampere GPUs and PyTorch 1.7+, the more efficient [tf32](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices) mode is automatically enabled for some operations but the results are still in fp32. Configure it in [`Trainer`] by setting `--tf32` to enable it, and `--tf32 0` or `--no_tf32` to disable it.\n\n</hfoption>\n<hfoption id=\"fp16\">",
  "To configure AMP-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically enables or disables fp16 based on the value of `fp16_backend`, and the rest of the config can be set by you. fp16 is enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend amp` or `--fp16_full_eval`.\n\n```yaml\n{\n\"fp16\": {\n\"enabled\": \"auto\",\n\"loss_scale\": 0,\n\"loss_scale_window\": 1000,\n\"initial_scale_power\": 16,\n\"hysteresis\": 2,\n\"min_loss_scale\": 1\n}\n}\n```\n\nFor additional DeepSpeed fp16 training options, take a look at the [FP16 Training Options](https://www.deepspeed.ai/docs/config-json/#fp16-training-options) reference.\n\nTo configure Apex-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically configures `amp` based on the values of `fp16_backend` and `fp16_opt_level`. It can also be enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend apex` or `--fp16_opt_level 01`.\n\n```yaml\n{\n\"amp\": {\n\"enabled\": \"auto\",\n\"opt_level\": \"auto\"\n}\n}\n```\n\n</hfoption>\n<hfoption id=\"bf16\">\n\n> [!TIP]",
  "> bf16 requires DeepSpeed 0.6.0.\n\nbf16 has the same dynamic range as fp32, and doesn’t require loss scaling unlike fp16. However, if you use [gradient accumulation](#gradient-accumulation) with bf16, gradients are accumulated in bf16 which may not be desirable because the lower precision can lead to lossy accumulation.\n\nbf16 can be set up in the config file or enabled from the command line when the following arguments are passed: `--bf16` or `--bf16_full_eval`.\n\n```yaml\n{\n\"bf16\": {\n\"enabled\": \"auto\"\n}\n}\n```\n\n</hfoption>\n</hfoptions>\n\n### Optimizer and scheduler\n\nDeepSpeed and Transformers optimizers and schedulers can be mixed and matched if `offload_optimizer` isn't enabled. When `offload_optimizer` is enabled, use a non-DeepSpeed optimizer (except for LAMB) as long as it has it a CPU and GPU implementation.\n\nSet the optimizer and scheduler parameters for the config file from the command line to avoid hard to find errors. For example, if the learning rate is set to a different value in another place, you can override it from the command line.\n\n<hfoptions id=\"opt-sched\">\n<hfoption id=\"optimizer\">",
  "DeepSpeed offers several [optimizers](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters) (Adam, AdamW, OneBitAdam, and LAMB) but you can also import other optimizers from PyTorch. If you don't configure the optimizer in the config, [`Trainer`] automatically selects AdamW and either uses the supplied values or the default values for the following parameters from the command line: `lr`, `adam_beta1`, `adam_beta2`, `adam_epsilon`, `weight_decay`.\n\nYou can set the parameters to `\"auto\"` or manually input your own values.\n\n```yaml\n{\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": \"auto\",\n\"betas\": \"auto\",\n\"eps\": \"auto\",\n\"weight_decay\": \"auto\"\n}\n}\n}\n```\n\nUse an unsupported optimizer by adding the following to the top level configuration.\n\n```yaml\n{\n\"zero_allow_untested_optimizer\": true\n}\n```\n\nFrom DeepSpeed 0.8.3+, if you want to use offload, you'll also need to add the following to the top level configuration because offload works best with DeepSpeed's CPU Adam optimizer.\n\n```yaml\n{\n\"zero_force_ds_cpu_optimizer\": false\n}\n```\n\n</hfoption>\n<hfoption id=\"scheduler\">",
  "DeepSpeed supports the LRRangeTest, OneCycle, WarmupLR and WarmupDecayLR learning rate [schedulers](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters).\n\nTransformers and DeepSpeed provide two of the same schedulers:\n\n* WarmupLR is the same as `--lr_scheduler_type constant_with_warmup` in Transformers.\n* WarmupDecayLR is the same as  `--lr_scheduler_type linear` in Transformers (this is the default scheduler used in Transformers).\n\nIf you don't configure the scheduler in the config file, [`Trainer`] automatically selects WarmupDecayLR and either uses the supplied values or the default values for the following parameters from the command line: `warmup_min_lr`, `warmup_max_lr`, `warmup_num_steps`, `total_num_steps` (automatically calculated during run time if `max_steps` is not provided).\n\nYou can set the parameters to `\"auto\"` or manually input your own values.\n\n```yaml\n{\n\"scheduler\": {\n\"type\": \"WarmupDecayLR\",\n\"params\": {\n\"total_num_steps\": \"auto\",\n\"warmup_min_lr\": \"auto\",\n\"warmup_max_lr\": \"auto\",\n\"warmup_num_steps\": \"auto\"\n}\n}\n}\n```\n\n</hfoption>\n</hfoptions>\n\n### Universal checkpointing",
  "[Universal Checkpointing](https://www.deepspeed.ai/tutorials/universal-checkpointing) saves and loads model, optimizer and training scheduler states across different model architectures, parallelism techniques, and training configurations. By saving them in a Universal format, it enables easier model training continuation and fine-tuning.\n\nResume training with a Universal checkpoint by setting `load_universal` to `true` in the config file.\n\n```yaml\n{\n\"checkpoint\": {\n\"load_universal\": true\n}\n}\n```\n\n## Deploy\n\nDeepSpeed can be deployed with its native launcher, [torchrun](https://pytorch.org/docs/stable/elastic/run.html) or [Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch#using-accelerate-launch).\n\nAdd the `--deepspeed ds_config.json` argument to [`Trainer`] in the command line. It is recommended to use DeepSpeeds [add_config_arguments](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) utility to add any other command line arguments to your code.\n\n<hfoptions id=\"deploy\">\n<hfoption id=\"multi-GPU\">",
  "To deploy DeepSpeed on multiple GPUs, add `--num_gpus`. You don't need to add `--num_gpus` if you're planning on using all available GPUs.\n\n```bash\ndeepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\n</hfoption>\n<hfoption id=\"single-GPU\">\n\nDeepSpeed is still useful with just one GPU because you can:\n\n1. Offload some computations and memory to the CPU to make more GPU resources available to your model to use a larger batch size or fit a very large model that normally won't fit.\n2. Minimize memory fragmentation with its smart GPU memory management system which also allows you to fit bigger models and data batches.\n\nTo deploy DeepSpeed on a single GPU, add `--num_gpus`. You don't need to add `--num_gpus` if you only have one GPU because DeepSpeed deploys all GPUs it can see on a given node.\n\n> [!TIP]",
  "> Set the `allgather_bucket_size` and `reduce_bucket_size` values to 2e8 in the [ZeRO-2](#zero-configuration) configuration file to get better performance on a single GPU.\n\n```bash\ndeepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero2.json \\\n--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\n</hfoption>\n</hfoptions>\n\n### Multi-node\n\nA multi-node setup consists of multiple nodes, where each node has one of more GPUs running a workload. DeepSpeed expects a shared storage system, but if this is not the case, you need to adjust the config file to include a [checkpoint](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) to allow loading without access to a shared filesystem.\n\n```yaml\n{\n\"checkpoint\": {\n\"use_node_local_storage\": true\n}\n}\n```\n\nYou could also use the `--save_on_each_node` parameter in [`TrainingArguments`] to automatically add the above `checkpoint` to your config.",
  "The examples below for the torchrun and DeepSpeed launcher shows how to deploy two nodes with eight GPUs each. Access the first node with `ssh hostname1` and the second node with `ssh hostname2`. Both nodes must be able to communicate with each other locally over ssh without a password.\n\n<hfoptions id=\"multinode\">\n<hfoption id=\"torchrun\">\n\nWith [torchrun](https://pytorch.org/docs/stable/elastic/run.html), ssh to each node and run the following command on both of them. The launcher waits until both nodes are synchronized before launching the training.\n\n```bash\ntorchrun --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \\\n--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\n</hfoption>\n<hfoption id=\"DeepSpeed\">\n\nCreate a `hostfile` for the DeepSpeed launcher.\n\n```bash\nhostname1 slots=8\nhostname2 slots=8\n```\n\nThe DeepSpeed launcher automatically launches the command on both nodes at once with the command below.\n\n```bash\ndeepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \\\nyour_program.py <normal cl args> --deepspeed ds_config.json\n```",
  "Check out the [Resource Configuration (multi-node)](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) guide for more details about configuring multi-node compute resources.\n\n</hfoption>\n</hfoptions>\n\n### Slurm\n\n[Slurm](https://slurm.schedmd.com/documentation.html) is a cluster management and job scheduling system. An example Slurm script is shown below.\n\n```bash\n#SBATCH --job-name=test-nodes        # name\n#SBATCH --nodes=2                    # nodes\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=10           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n\nexport GPUS_PER_NODE=8\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nexport MASTER_PORT=9901\n\nsrun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n--nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n--master_addr $MASTER_ADDR --master_port $MASTER_PORT \\",
  "your_program.py <normal cl args> --deepspeed ds_config.json'\n```\n\nLaunch training simultaneously on all nodes with the command below.\n\n```bash\nsbatch launch.slurm\n```\n\n### Jupyter Notebook\n\nTo use DeepSpeed in a Jupyter Notebook, you need to emulate a distributed environment because the launcher doesn't support deployment from a notebook. This is only supported for one GPU. To use multiple GPUs, you must use a multi-process environment, which means you have to use the DeepSpeed launcher which can't be emulated as shown here.\n\n```py\n# emulate a launcher in the notebook\nimport os\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"LOCAL_RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\n\ntraining_args = TrainingArguments(..., deepspeed=\"ds_config_zero3.json\")\ntrainer = Trainer(...)\ntrainer.train()\n```\n\nCreate a config file on the fly in the notebook in the current directory with a dedicated cell.\n\n```py\n%%bash\ncat <<'EOT' > ds_config_zero3.json\n{\n\"fp16\": {\n\"enabled\": \"auto\",\n\"loss_scale\": 0,\n\"loss_scale_window\": 1000,\n\"initial_scale_power\": 16,\n\"hysteresis\": 2,",
  "\"min_loss_scale\": 1\n},\n\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": \"auto\",\n\"betas\": \"auto\",\n\"eps\": \"auto\",\n\"weight_decay\": \"auto\"\n}\n},\n\n\"scheduler\": {\n\"type\": \"WarmupLR\",\n\"params\": {\n\"warmup_min_lr\": \"auto\",\n\"warmup_max_lr\": \"auto\",\n\"warmup_num_steps\": \"auto\"\n}\n},\n\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\",\n\"pin_memory\": true\n},\n\"offload_param\": {\n\"device\": \"cpu\",\n\"pin_memory\": true\n},\n\"overlap_comm\": true,\n\"contiguous_gradients\": true,\n\"sub_group_size\": 1e9,\n\"reduce_bucket_size\": \"auto\",\n\"stage3_prefetch_bucket_size\": \"auto\",\n\"stage3_param_persistence_threshold\": \"auto\",\n\"stage3_max_live_parameters\": 1e9,\n\"stage3_max_reuse_distance\": 1e9,\n\"stage3_gather_16bit_weights_on_model_save\": true\n},\n\n\"gradient_accumulation_steps\": \"auto\",\n\"gradient_clipping\": \"auto\",\n\"steps_per_print\": 2000,\n\"train_batch_size\": \"auto\",\n\"train_micro_batch_size_per_gpu\": \"auto\",\n\"wall_clock_breakdown\": false\n}\nEOT\n```\n\nIf the training script is in a file and not a notebook cell, launch DeepSpeed from the shell in the notebook cell.\n\n```py\n!git clone https://github.com/huggingface/transformers",
  "!cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...\n```\n\nAnother option is to use `%%bash` to run the shell program without emulating the distributed environment. However, you won't be able to view the logs until training is complete.\n\n```py\n%%bash\n\ngit clone https://github.com/huggingface/transformers\ncd transformers\ndeepspeed examples/pytorch/translation/run_translation.py ...\n```\n\n## Save model weights\n\nDeepSpeed stores the main fp32 weights in custom checkpoint optimizer files (`global_step*/*optim_states.pt`) which are saved under the normal checkpoint.\n\n### fp16\n\nZeRO-2 saves the model weights in fp16. To save the weights in fp16 for ZeRO-3, set `\"stage3_gather_16bit_weights_on_model_save\": true` in the config file, because the weights are distributed across multiple GPUs.\n\nIf you don't, [`Trainer`] won't save the weights in fp16 and won't create a `pytorch_model.bin` file. This is because DeepSpeed's state_dict contains a placeholder instead of the real weights, so you won't be able to load it.\n\n```yaml\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"stage3_gather_16bit_weights_on_model_save\": true\n}\n}\n```\n\n### fp32",
  "Unless you have a lot of free CPU memory, fp32 weights shouldn't be saved during training because it can require a lot of memory. It is usually best to save the fp32 weights offline after training is complete.\n\n<hfoptions id=\"save\">\n<hfoption id=\"offline\">\n\nDeepSpeed provides a [zero_to_fp32.py](https://github.com/microsoft/DeepSpeed/blob/91829476a8fd4d0d9268c03c1d56795d20a51c12/deepspeed/utils/zero_to_fp32.py#L14) script at the top-level checkpoint folder for extracting weights at any point. This is a standalone script and you don't need a config file or [`Trainer`].\n\nFor example, if your checkpoint folder looks like the one shown below, then you can run the following command to create and consolidate the fp32 weights from multiple GPUs into a single `pytorch_model.bin` file. The script automatically discovers the subfolder `global_step1` which contains the checkpoint.\n\n```bash\n$ ls -l output_dir/checkpoint-1/\n-rw-rw-r-- 1 stas stas 1.4K Mar 27 20:42 config.json\ndrwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global_step1/\n-rw-rw-r-- 1 stas stas   12 Mar 27 13:16 latest\n-rw-rw-r-- 1 stas stas 827K Mar 27 20:42 optimizer.pt\n-rw-rw-r-- 1 stas stas 231M Mar 27 20:42 pytorch_model.bin",
  "-rw-rw-r-- 1 stas stas  623 Mar 27 20:42 scheduler.pt\n-rw-rw-r-- 1 stas stas 1.8K Mar 27 20:42 special_tokens_map.json\n-rw-rw-r-- 1 stas stas 774K Mar 27 20:42 spiece.model\n-rw-rw-r-- 1 stas stas 1.9K Mar 27 20:42 tokenizer_config.json\n-rw-rw-r-- 1 stas stas  339 Mar 27 20:42 trainer_state.json\n-rw-rw-r-- 1 stas stas 2.3K Mar 27 20:42 training_args.bin\n-rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero_to_fp32.py*\n```\n\n> [!TIP]\n> Run `python zero_to_fp32.py -h` for more usage details. The script requires 2x the general RAM of the final fp32 weights.\n\n```bash\npython zero_to_fp32.py . pytorch_model.bin\n```\n\n</hfoption>\n<hfoption id=\"online\">\n\nAdding the `--load_best_model_at_end` parameter in [`TrainingArguments`] tracks the best checkpoint so you can finish training first and save the final model explicitly. Reload the model as shown below.\n\n> [!WARNING]",
  "> Once [load_state_dict_from_zero_checkpoint](https://deepspeed.readthedocs.io/en/stable/model-checkpointing.html#deepspeed.utils.zero_to_fp32.load_state_dict_from_zero_checkpoint) is run, the model is no longer usable in DeepSpeed in the context of the same application. You'll need to reinitialize the DeepSpeed engine because `model.load_state_dict(state_dict)` removes all the DeepSpeed magic from it. Only use this function once training is complete.\n\n```py\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\ncheckpoint_dir = os.path.join(trainer.args.output_dir, \"checkpoint-final\")\ntrainer.deepspeed.save_checkpoint(checkpoint_dir)\nfp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n```\n\nYou must have saved at least one checkpoint to load the latest checkpoint as shown in the example below.\n\n```py\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\ncheckpoint_dir = get_last_checkpoint(trainer.args.output_dir)\nfp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n```",
  "Use `load_state_dict` to extract and load the state_dict of the fp32 weights.\n\n```py\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\nstate_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)\nmodel = model.cpu()\nmodel.load_state_dict(state_dict)\n```\n\n</hfoption>\n</hfoptions>\n\n## Non-Trainer integration\n\nDeepSpeed also works with Transformers without [`Trainer`]. The [`~integrations.HfDeepSpeedConfig`] is responsible for gathering ZeRO-3 parameters and partitioning a model across multiple GPUs when [`~PreTrainedModel.from_pretrained`] is called.\n\nYou must instantiate [`~integrations.HfDeepSpeedConfig`] before loading a model to efficiently deploy ZeRO-3.\n\n<hfoptions id=\"models\">\n<hfoption id=\"pretrained model\">\n\n```py\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom transformers import AutoModel\nimport deepspeed\n\n# DeepSpeed config object or path to the file\nds_config = {...}\n# must run before instantiating the model to detect ZeRO-3\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\nmodel = AutoModel.from_pretrained(\"openai-community/gpt2\")\nengine = deepspeed.initialize(model=model, config_params=ds_config, ...)",
  "```\n\n</hfoption>\n<hfoption id=\"non-pretrained model\">\n\n[`~integrations.HfDeepSpeedConfig`] is not required for ZeRO-1 or ZeRO-2.\n\n```py\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom transformers import AutoModel, AutoConfig\nimport deepspeed\n\n# DeepSpeed config object or path to the file\nds_config = {...}\n# must run before instantiating the model to detect zero 3\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n# randomly initialize model weights\nconfig = AutoConfig.from_pretrained(\"openai-community/gpt2\")\nmodel = AutoModel.from_config(config)\nengine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n```\n\n</hfoption>\n</hfoptions>\n\n## Troubleshoot\n\nOne of the first things to check when you encounter an error is whether DeepSpeed is the cause (because often it isn't). Retry your setup without DeepSpeed, and if the error persists, report the issue. If the issue is unrelated to the Transformers integration, please open the issue on the DeepSpeed [repository](https://github.com/microsoft/DeepSpeed).\n\nFor issues related to the Transformers integration, please provide the following information.\n\n* The full DeepSpeed config file.",
  "* The command line arguments for [`Trainer`] or the [`TrainingArguments`] if you're scripting the [`Trainer`] setup yourself (don't dump the entire [`TrainingArguments`] which contains many irrelevant entries).\n* The outputs of the following commands.\n\n```bash\npython -c 'import torch; print(f\"torch: {torch.__version__}\")'\npython -c 'import transformers; print(f\"transformers: {transformers.__version__}\")'\npython -c 'import deepspeed; print(f\"deepspeed: {deepspeed.__version__}\")'\n```\n\n* A link to a Google Colab notebook to reproduce the issue.\n* A standard or non-custom dataset or an existing example to reproduce the issue.\n\nThe following sections provide a guide for resolving two of the most common issues.\n\n### Process killed at startup\n\nWhen the DeepSpeed process is killed during launch without a traceback, that usually means the program tried to allocate more CPU memory than is available on your system. Or the process may have tried to allocate more CPU memory than allowed, leading the OS kernel to terminate the process.\n\nIn this case, check whether your config file has either `offload_optimizer`, `offlload_param`, or both configured to offload to the CPU.",
  "If you have NVM3 and ZeRO-3 set up, experiment with offloading to the NVMe ([estimate](https://deepspeed.readthedocs.io/en/latest/memory.html) the memory requirements of a model first) instead.\n\n### NaN loss\n\nNaN loss often occurs when a model is pretrained in bf16 and you try to use it with fp16 (especially relevant to TPU trained models). To resolve this, use fp32 or bf16 if your hardware (TPUs, Ampere GPUs or newer) supports it.\n\nIt is also possible that fp16 is causing overflow. For example, if your config file looks like the one below, you may see the following overflow errors in the logs.\n\n```yaml\n{\n\"fp16\": {\n\"enabled\": \"auto\",\n\"loss_scale\": 0,\n\"loss_scale_window\": 1000,\n\"initial_scale_power\": 16,\n\"hysteresis\": 2,\n\"min_loss_scale\": 1\n}\n}\n```\n\nThe `OVERFLOW!` error below is a result of the DeepSpeed loss scaler unable to find a scaling coefficient to overcome the loss overflow. Try a higher `initial_scale_power` value in this case (32 usually works).\n\n```bash\n0%|                                                                                                                             | 0/189 [00:00<?, ?it/s]",
  "[deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 262144\n1%|▌                                                                                                                    | 1/189 [00:00<01:26,  2.17it/s]\n[deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072.0\n1%|█▏\n[...]\n[deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n14%|████████████████▌                                                                                                   | 27/189 [00:14<01:13,  2.21it/s]\n[deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n15%|█████████████████▏                                                                                                  | 28/189 [00:14<01:13,  2.18it/s]\n[deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n15%|█████████████████▊                                                                                                  | 29/189 [00:15<01:13,  2.18it/s]\n[deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n[...]\n```\n\n## Resources",
  "DeepSpeed is a powerful technology for scaling large model training. To learn more about DeepSpeed, take a look at their [blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [documentation](https://www.deepspeed.ai/getting-started/), and [GitHub](https://github.com/microsoft/deepspeed).\n\nThe papers below provide additional details about ZeRO.\n\n* [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://hf.co/papers/1910.02054)\n* [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://hf.co/papers/2101.06840)\n* [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://hf.co/papers/2104.07857)",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Transformers\n\nTransformers is a library of pretrained natural language processing, computer vision, audio, and multimodal models for inference and training. Use Transformers to train models on your data, build inference applications, and generate text with large language models.\n\nExplore the [Hugging Face Hub](https://huggingface.com) today to find a model and use Transformers to help you get started right away.\n\n## Features",
  "Transformers provides everything you need for inference or training with state-of-the-art pretrained models. Some of the main features include:\n\n- [Pipeline](./pipeline_tutorial): Simple and optimized inference class for many machine learning tasks like text generation, image segmentation, automatic speech recognition, document question answering, and more.\n- [Trainer](./trainer): A comprehensive trainer that supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models.\n- [generate](./llm_tutorial): Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies.\n\n## Design\n\n> [!TIP]\n> Read our [Philosophy](./philosophy) to learn more about Transformers' design principles.\n\nTransformers is designed for developers and machine learning engineers and researchers. Its main design principles are:\n\n1. Fast and easy to use: Every model is implemented from only three main classes (configuration, model, and preprocessor) and can be quickly used for inference or training with [`Pipeline`] or [`Trainer`].",
  "2. Pretrained models: Reduce your carbon footprint, compute cost and time by using a pretrained model instead of training an entirely new one. Each pretrained model is reproduced as closely as possible to the original model and offers state-of-the-art performance.\n\n<div class=\"flex justify-center\">\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\n<img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://hf.co/datasets/huggingface/documentation-images/resolve/81d7d9201fd4ceb537fc4cebc22c29c37a2ed216/transformers/transformers-index.png\" style=\"width: 100%; max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n</a>\n</div>\n\nJoin us on the Hugging Face [Hub](https://huggingface.co/), [Discord](https://discord.com/invite/JfAtkvEtRb), or [forum](https://discuss.huggingface.co/) to collaborate and build models, datasets, and applications together.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Loading models\n\nTransformers provides many pretrained models that are ready to use with a single line of code. It requires a model class and the [`~PreTrainedModel.from_pretrained`] method.\n\nCall [`~PreTrainedModel.from_pretrained`] to download and load a models weights and configuration stored on the Hugging Face [Hub](https://hf.co/models).\n\n> [!TIP]",
  "> The [`~PreTrainedModel.from_pretrained`] method loads weights stored in the [safetensors](https://hf.co/docs/safetensors/index) file format if they're available. Traditionally, PyTorch model weights are serialized with the [pickle](https://docs.python.org/3/library/pickle.html) utility which is known to be unsecure. Safetensor files are more secure and faster to load.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=\"auto\", device_map=\"auto\")\n```\n\nThis guide explains how models are loaded, the different ways you can load a model, how to overcome memory issues for really big models, and how to load custom models.\n\n## Models and configurations",
  "All models have a `configuration.py` file with specific attributes like the number of hidden layers, vocabulary size, activation function, and more. You'll also find a `modeling.py` file that defines the layers and mathematical operations taking place inside each layer. The `modeling.py` file takes the model attributes in `configuration.py` and builds the model accordingly. At this point, you have a model with random weights that needs to be trained to output meaningful results.\n\n<!-- insert diagram of model and configuration -->\n\n> [!TIP]\n> An *architecture* refers to the model's skeleton and a *checkpoint* refers to the model's weights for a given architecture. For example, [BERT](./model_doc/bert) is an architecture while [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) is a checkpoint. You'll see the term *model* used interchangeably with architecture and checkpoint.\n\nThere are two general types of models you can load:\n\n1. A barebones model, like [`AutoModel`] or [`LlamaModel`], that outputs hidden states.\n2. A model with a specific *head* attached, like [`AutoModelForCausalLM`] or [`LlamaForCausalLM`], for performing specific tasks.",
  "For each model type, there is a separate class for each machine learning framework (PyTorch, TensorFlow, Flax). Pick the corresponding prefix for the framework you're using.\n\n<hfoptions id=\"backend\">\n<hfoption id=\"PyTorch\">\n\n```py\nfrom transformers import AutoModelForCausalLM, MistralForCausalLM\n\n# load with AutoClass or model-specific class\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", , torch_dtype=\"auto\", device_map=\"auto\")\nmodel = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", , torch_dtype=\"auto\", device_map=\"auto\")\n```\n\n</hfoption>\n<hfoption id=\"TensorFlow\">\n\n```py\nfrom transformers import TFAutoModelForCausalLM, TFMistralForCausalLM\n\n# load with AutoClass or model-specific class\nmodel = TFAutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\nmodel = TFMistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```py\nfrom transformers import FlaxAutoModelForCausalLM, FlaxMistralForCausalLM\n\n# load with AutoClass or model-specific class\nmodel = FlaxAutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")",
  "model = FlaxMistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Model classes\n\nTo get a pretrained model, you need to load the weights into the model. This is done by calling [`~PreTrainedModel.from_pretrained`] which accepts weights from the Hugging Face Hub or a local directory.\n\nThere are two model classes, the [AutoModel](./model_doc/auto) class and a model-specific class.\n\n<hfoptions id=\"model-classes\">\n<hfoption id=\"AutoModel\">\n\n<Youtube id=\"AhChOFRegn4\"/>\n\nThe [AutoModel](./model_doc/auto) class is a convenient way to load an architecture without needing to know the exact model class name because there are many models available. It automatically selects the correct model class based on the configuration file. You only need to know the task and checkpoint you want to use.\n\nEasily switch between models or tasks, as long as the architecture is supported for a given task.\n\nFor example, the same model can be used for separate tasks.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering\n\n# use the same API for 3 different tasks",
  "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n```\n\nIn other cases, you may want to quickly try out several different models for a task.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\n# use the same API to load 3 different models\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n```\n\n</hfoption>\n<hfoption id=\"model-specific class\">\n\nThe [AutoModel](./model_doc/auto) class builds on top of model-specific classes. All model classes that support a specific task are mapped to their respective `AutoModelFor` task class.\n\nIf you already know which model class you want to use, then you could use its model-specific class directly.\n\n```py\nfrom transformers import LlamaModel, LlamaForCausalLM\n\nmodel = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Large models",
  "Large pretrained models require a lot of memory to load. The loading process involves:\n\n1. creating a model with random weights\n2. loading the pretrained weights\n3. placing the pretrained weights on the model\n\nYou need enough memory to hold two copies of the model weights (random and pretrained) which may not be possible depending on your hardware. In distributed training environments, this is even more challenging because each process loads a pretrained model.\n\nTransformers reduces some of these memory-related challenges with fast initialization, sharded checkpoints, Accelerate's [Big Model Inference](https://hf.co/docs/accelerate/usage_guides/big_modeling) feature, and supporting lower bit data types.\n\n### Fast initialization\n\nA PyTorch model is instantiated with random weights, or \"empty\" tensors, that take up space in memory without filling it.\n\nTransformers boosts loading speed by skipping random weight initialization with the [_fast_init](https://github.com/huggingface/transformers/blob/c9f6e5e35156e068b227dd9b15521767f6afd4d2/src/transformers/modeling_utils.py#L2710) parameter if the pretrained weights are correctly initialized. This parameter is set to `True` by default.",
  "### Sharded checkpoints\n\nThe [`~PreTrainedModel.save_pretrained`] method automatically shards checkpoints larger than 10GB.\n\nEach shard is loaded sequentially after the previous shard is loaded, limiting memory usage to only the model size and the largest shard size.\n\nThe `max_shard_size` parameter defaults to 5GB for each shard because it is easier to run on free-tier GPU instances without running out of memory.\n\nFor example, create some shards checkpoints for [BioMistral/BioMistral-7B](https://hf.co/BioMistral/BioMistral-7B) in [`~PreTrainedModel.save_pretrained`].\n\n```py\nfrom transformers import AutoModel\nimport tempfile\nimport os\n\nmodel = AutoModel.from_pretrained(\"biomistral/biomistral-7b\")\nwith tempfile.TemporaryDirectory() as tmp_dir:\nmodel.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\nprint(sorted(os.listdir(tmp_dir)))\n```\n\nReload the sharded checkpoint with [`~PreTrainedModel.from_pretrained`].\n\n```py\nwith tempfile.TemporaryDirectory() as tmp_dir:\nmodel.save_pretrained(tmp_dir)\nnew_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nSharded checkpoints can also be directly loaded with [`~transformers.modeling_utils.load_sharded_checkpoint`].\n\n```py",
  "from transformers.modeling_utils import load_sharded_checkpoint\n\nwith tempfile.TemporaryDirectory() as tmp_dir:\nmodel.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\nload_sharded_checkpoint(model, tmp_dir)\n```\n\nThe [`~PreTrainedModel.save_pretrained`] method creates an index file that maps parameter names to the files they're stored in. The index file has two keys, `metadata` and `weight_map`.\n\n```py\nimport json\n\nwith tempfile.TemporaryDirectory() as tmp_dir:\nmodel.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\nwith open(os.path.join(tmp_dir, \"model.safetensors.index.json\"), \"r\") as f:\nindex = json.load(f)\n\nprint(index.keys())\n```\n\nThe `metadata` key provides the total model size.\n\n```py\nindex[\"metadata\"]\n{'total_size': 28966928384}\n```\n\nThe `weight_map` key maps each parameter to the shard it's stored in.\n\n```py\nindex[\"weight_map\"]\n{'lm_head.weight': 'model-00006-of-00006.safetensors',\n'model.embed_tokens.weight': 'model-00001-of-00006.safetensors',\n'model.layers.0.input_layernorm.weight': 'model-00001-of-00006.safetensors',\n'model.layers.0.mlp.down_proj.weight': 'model-00001-of-00006.safetensors',\n...\n}\n```\n\n### Big Model Inference\n\n> [!TIP]",
  "> Make sure you have Accelerate v0.9.0 and PyTorch v1.9.0 or later installed to use this feature!\n\n<Youtube id=\"MWCSGj9jEAo\"/>\n\n[`~PreTrainedModel.from_pretrained`] is supercharged with Accelerate's [Big Model Inference](https://hf.co/docs/accelerate/usage_guides/big_modeling) feature.\n\nBig Model Inference creates a *model skeleton* on the PyTorch [meta](https://pytorch.org/docs/main/meta.html) device. The meta device doesn't store any real data, only the metadata.\n\nRandomly initialized weights are only created when the pretrained weights are loaded to avoid maintaining two copies of the model in memory at the same time. The maximum memory usage is only the size of the model.\n\n> [!TIP]\n> Learn more about device placement in [Designing a device map](https://hf.co/docs/accelerate/v0.33.0/en/concept_guides/big_model_inference#designing-a-device-map).\n\nBig Model Inference's second feature relates to how weights are loaded and dispatched in the model skeleton. Model weights are dispatched across all available devices, starting with the fastest device (usually the GPU) and then offloading any remaining weights to slower devices (CPU and hard drive).",
  "Both features combined reduces memory usage and loading times for big pretrained models.\n\nSet [device_map](https://github.com/huggingface/transformers/blob/026a173a64372e9602a16523b8fae9de4b0ff428/src/transformers/modeling_utils.py#L3061) to `\"auto\"` to enable Big Model Inference. This also sets the [low_cpu_mem_usage](https://github.com/huggingface/transformers/blob/026a173a64372e9602a16523b8fae9de4b0ff428/src/transformers/modeling_utils.py#L3028) parameter to `True`, such that not more than 1x the model size is used in CPU memory.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")\n```\n\nYou can also manually assign layers to a device in `device_map`. It should map all model parameters to a device, but you don't have to detail where all the submodules of a layer go if the entire layer is on the same device.\n\nAccess the `hf_device_map` attribute to see how a model is distributed across devices.\n\n```py\ndevice_map = {\"model.layers.1\": 0, \"model.layers.14\": 1, \"model.layers.31\": \"cpu\", \"lm_head\": \"disk\"}\nmodel.hf_device_map\n```\n\n### Model data type",
  "PyTorch model weights are initialized in `torch.float32` by default. Loading a model in a different data type, like `torch.float16`, requires additional memory because the model is loaded again in the desired data type.\n\nExplicitly set the [torch_dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype) parameter to directly initialize the model in the desired data type instead of loading the weights twice (`torch.float32` then `torch.float16`). You could also set `torch_dtype=\"auto\"` to automatically load the weights in the data type they are stored in.\n\n<hfoptions id=\"dtype\">\n<hfoption id=\"specific dtype\">\n\n```py\nfrom transformers import AutoModelForCausalLM\n\ngemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.float16)\n```\n\n</hfoption>\n<hfoption id=\"auto dtype\">\n\n```py\nfrom transformers import AutoModelForCausalLM\n\ngemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=\"auto\")\n```\n\n</hfoption>\n</hfoptions>\n\nThe `torch_dtype` parameter can also be configured in [`AutoConfig`] for models instantiated from scratch.\n\n```py\nimport torch\nfrom transformers import AutoConfig, AutoModel",
  "my_config = AutoConfig.from_pretrained(\"google/gemma-2b\", torch_dtype=torch.float16)\nmodel = AutoModel.from_config(my_config)\n```\n\n## Custom models\n\nCustom models builds on Transformers' configuration and modeling classes, supports the [AutoClass](#autoclass) API, and are loaded with [`~PreTrainedModel.from_pretrained`]. The difference is that the modeling code is *not* from Transformers.\n\nTake extra precaution when loading a custom model. While the Hub includes [malware scanning](https://hf.co/docs/hub/security-malware#malware-scanning) for every repository, you should still be careful to avoid inadvertently executing malicious code.\n\nSet `trust_remote_code=True` in [`~PreTrainedModel.from_pretrained`] to load a custom model.\n\n```py\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\"sgugger/custom-resnet50d\", trust_remote_code=True)\n```\n\nAs an extra layer of security, load a custom model from a specific revision to avoid loading model code that may have changed. The commit hash can be copied from the models [commit history](https://hf.co/sgugger/custom-resnet50d/commits/main).\n\n```py",
  "commit_hash = \"ed94a7c6247d8aedce4647f00f20de6875b5b292\"\nmodel = AutoModelForImageClassification.from_pretrained(\n\"sgugger/custom-resnet50d\", trust_remote_code=True, revision=commit_hash\n)\n```\n\nRefer to the [Customize models](./custom_models) guide for more information.",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Optimizing LLMs for Speed and Memory\n\n[[open-in-colab]]\n\nLarge Language Models (LLMs) such as GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), and [Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf) are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.\nDeploying these models in real-world tasks remains challenging, however:",
  "-   To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361), [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the memory demands for inference.\n-   In many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model's capability to manage very long input sequences during inference.\n\nThe crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.\n\nIn this guide, we will go over the effective techniques for efficient LLM deployment:\n\n1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.\n\n2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.",
  "3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://arxiv.org/abs/2108.12409), [Rotary embeddings](https://arxiv.org/abs/2104.09864), [Multi-Query Attention (MQA)](https://arxiv.org/abs/1911.02150) and [Grouped-Query-Attention (GQA)]((https://arxiv.org/abs/2305.13245)).\n\nThroughout this guide, we will offer an analysis of auto-regressive generation from a tensor's perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\n\n## 1. Lower Precision",
  "Memory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition *weights* will be used to signify all model weight matrices and vectors.\n\nAt the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. `4.5689` which is usually stored in either [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), or [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) format. This allows us to easily compute the memory requirement to load the LLM into memory:\n\n> *Loading the weights of a model having X billion parameters requires roughly 4 * X GB of VRAM in float32 precision*\n\nNowadays, models are however rarely trained in full float32 precision, but usually in bfloat16 precision or less frequently in float16 precision. Therefore the rule of thumb becomes:",
  "> *Loading the weights of a model having X billion parameters requires roughly 2 * X GB of VRAM in bfloat16/float16 precision*\n\nFor shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let's assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.\n\nTo give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n\n-   **GPT3** requires 2 \\* 175 GB = **350 GB** VRAM\n-   [**Bloom**](https://huggingface.co/bigscience/bloom) requires 2 \\* 176 GB = **352 GB** VRAM\n-   [**Llama-2-70b**](https://huggingface.co/meta-llama/Llama-2-70b-hf) requires 2 \\* 70 GB = **140 GB** VRAM\n-   [**Falcon-40b**](https://huggingface.co/tiiuae/falcon-40b) requires 2 \\* 40 GB = **80 GB** VRAM\n-   [**MPT-30b**](https://huggingface.co/mosaicml/mpt-30b) requires 2 \\* 30 GB = **60 GB** VRAM\n-   [**bigcode/starcoder**](https://huggingface.co/bigcode/starcoder) requires 2 \\* 15.5 = **31 GB** VRAM",
  "As of writing this document, the largest GPU chip on the market is the A100 & H100 offering 80GB of VRAM. Most of the models listed before require more than 80GB just to be loaded and therefore necessarily require [tensor parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism) and/or [pipeline parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).\n\n🤗 Transformers now supports tensor parallelism for supported models having `base_tp_plan` in their respective config classes. Learn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism). Furthermore, if you're interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at [the text-generation-inference library](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling).",
  "Naive pipeline parallelism is supported out of the box. For this, simply load the model with `device=\"auto\"` which will automatically place the different layers on the available GPUs as explained [here](https://huggingface.co/docs/accelerate/v0.22.0/en/concept_guides/big_model_inference).\nNote, however that while very effective, this naive pipeline parallelism does not tackle the issues of GPU idling. For this more advanced pipeline parallelism is required as explained [here](https://huggingface.co/docs/transformers/en/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).\n\nIf you have access to an 8 x 80GB A100 node, you could load BLOOM as follows\n\n```bash\n!pip install transformers accelerate bitsandbytes optimum\n```\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"auto\", pad_token_id=0)\n```\n\nBy using `device_map=\"auto\"` the attention layers would be equally distributed over all available GPUs.",
  "In this guide, we will use [bigcode/octocoder](https://huggingface.co/bigcode/octocoder) as it can be run on a single 40 GB A100 GPU device chip. Note that all memory and speed optimizations that we will apply going forward, are equally applicable to models that require model or tensor parallelism.\n\nSince the model is loaded in bfloat16 precision, using our rule of thumb above, we would expect the memory requirement to run inference with `bigcode/octocoder` to be around 31 GB VRAM. Let's give it a try.\n\nWe first load the model and tokenizer and then pass both to Transformers' [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) object.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n```\n\n```python\nprompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\"",
  "result = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\nresult\n```\n\n**Output**:\n```\nHere is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_giga_bytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single\n```\n\nNice, we can now directly use the result to convert bytes into Gigabytes.\n\n```python\ndef bytes_to_giga_bytes(bytes):\nreturn bytes / 1024 / 1024 / 1024\n```\n\nLet's call [`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html) to measure the peak GPU memory allocation.\n\n```python\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```\n\n**Output**:\n```bash\n29.0260648727417\n```\n\nClose enough to our back-of-the-envelope computation! We can see the number is not exactly correct as going from bytes to kilobytes requires a multiplication of 1024 instead of 1000. Therefore the back-of-the-envelope formula can also be understood as an \"at most X GB\" computation.\nNote that if we had tried to run the model in full float32 precision, a whopping 64 GB of VRAM would have been required.",
  "> Almost all models are trained in bfloat16 nowadays, there is no reason to run the model in full float32 precision if [your GPU supports bfloat16](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5). Float32 won't give better inference results than the precision that was used to train the model.\n\nIf you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint's config under `\"torch_dtype\"`, *e.g.* [here](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). It is recommended to set the model to the same precision type as written in the config when loading with `from_pretrained(..., torch_dtype=...)` except when the original type is float32 in which case one can use both `float16` or `bfloat16` for inference.\n\n\nLet's define a `flush(...)` function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.\n\n```python\ndel pipe\ndel model\n\nimport gc\nimport torch\n\ndef flush():\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\n```\n\nLet's call it now for the next experiment.\n\n```python\nflush()\n```",
  "From the Accelerate library, you can also use a device-agnostic utility method called [release_memory](https://github.com/huggingface/accelerate/blob/29be4788629b772a3b722076e433b5b3b5c85da3/src/accelerate/utils/memory.py#L63), which takes various hardware backends like XPU, MLU, NPU, MPS, and more into account.\n\n```python\nfrom accelerate.utils import release_memory\n# ...\n\nrelease_memory(model)\n```\n\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see [Dettmers et al.](https://arxiv.org/abs/2208.07339)).\nModel can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent [GPTQ paper](https://arxiv.org/abs/2210.17323) 🤯.\n\nWithout going into too many details, quantization schemes aim at reducing the precision of weights while trying to keep the model's inference results as accurate as possible (*a.k.a* as close as possible to bfloat16).",
  "Note that quantization works especially well for text generation since all we care about is choosing the *set of most likely next tokens* and don't really care about the exact values of the next token *logit* distribution.\nAll that matters is that the next token *logit* distribution stays roughly the same so that an `argmax` or `topk` operation gives the same results.\n\nThere are various quantization techniques, which we won't discuss in detail here, but in general, all quantization techniques work as follows:\n\n-   1.  Quantize all weights to the target precision\n-   2.  Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision\n-   3.  Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision\n\nIn a nutshell, this means that *inputs-weight matrix* multiplications, with \\\\( X \\\\) being the *inputs*, \\\\( W \\\\) being a weight matrix and \\\\( Y \\\\) being the output:\n\n$$ Y = X * W $$\n\nare changed to\n\n$$ Y = X * \\text{dequantize}(W) $$\n\nfor every matrix multiplication. Dequantization and re-quantization is performed sequentially for all weight matrices as the inputs run through the network graph.",
  "Therefore, inference time is often **not** reduced when using quantized weights, but rather increases.\nEnough theory, let's give it a try! To quantize the weights with Transformers, you need to make sure that\nthe [`bitsandbytes`](https://github.com/bitsandbytes-foundation/bitsandbytes) library is installed.\n\n```bash\n!pip install bitsandbytes\n```\n\nWe can then load models in 8-bit quantization by simply adding a `load_in_8bit=True` flag to `from_pretrained`.\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\n```\n\nNow, let's run our example again and measure the memory usage.\n\n```python\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\nresult\n```\n\n**Output**:\n```\nHere is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_giga_bytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single\n```\n\nNice, we're getting the same result as before, so no loss in accuracy! Let's look at how much memory was used this time.\n\n```python",
  "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```\n\n**Output**:\n```\n15.219234466552734\n```\n\nSignificantly less! We're down to just a bit over 15 GBs and could therefore run this model on consumer GPUs like the 4090.\nWe're seeing a very nice gain in memory efficiency and more or less no degradation to the model's output. However, we can also notice a slight slow-down during inference.\n\n\nWe delete the models and flush the memory again.\n```python\ndel model\ndel pipe\n```\n\n```python\nflush()\n```\n\nLet's see what peak GPU memory consumption 4-bit quantization gives. Quantizing the model to 4-bit can be done with the same API as before - this time by passing `load_in_4bit=True` instead of `load_in_8bit=True`.\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\nresult\n```\n\n**Output**:\n```",
  "Here is a Python function that transforms bytes to Giga bytes:\\n\\n```\\ndef bytes_to_gigabytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single argument\n```\n\nWe're almost seeing the same output text as before - just the `python` is missing just before the code snippet. Let's see how much memory was required.\n\n```python\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```\n\n**Output**:\n```\n9.543574333190918\n```\n\nJust 9.5GB! That's really not a lot for a >15 billion parameter model.\n\nWhile we see very little degradation in accuracy for our model here, 4-bit quantization can in practice often lead to different results compared to 8-bit quantization or full `bfloat16` inference. It is up to the user to try it out.\n\nAlso note that inference here was again a bit slower compared to 8-bit quantization which is due to the more aggressive quantization method used for 4-bit quantization leading to \\\\( \\text{quantize} \\\\) and \\\\( \\text{dequantize} \\\\) taking longer during inference.\n\n```python\ndel model\ndel pipe\n```\n```python\nflush()\n```",
  "Overall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32G GPU VRAM to only 15GB and running the model in 4-bit precision further reduces the required GPU VRAM to just a bit over 9GB.\n\n4-bit quantization allows the model to be run on GPUs such as RTX3090, V100, and T4 which are quite accessible for most people.\n\nFor more information on quantization and to see how one can quantize models to require even less GPU VRAM memory than 4-bit, we recommend looking into the [`AutoGPTQ`](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60) implementation.\n\n> As a conclusion, it is important to remember that model quantization trades improved memory efficiency against accuracy and in some cases inference time.\n\nIf GPU memory is not a constraint for your use case, there is often no need to look into quantization. However many GPUs simply can't run LLMs without quantization methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful tools.",
  "For more in-detail usage information, we strongly recommend taking a look at the [Transformers Quantization Docs](https://huggingface.co/docs/transformers/main_classes/quantization#general-usage).\nNext, let's look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture.\n\n## 2. Flash Attention\n\nToday's top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers.\n\nSelf-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens.\nHowever, the peak GPU memory consumption for self-attention layers grows *quadratically* both in compute and memory complexity with number of input tokens (also called *sequence length*) that we denote in the following by \\\\( N \\\\) .\nWhile this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens).",
  "Let's take a closer look. The formula to compute the output \\\\( \\mathbf{O} \\\\) of a self-attention layer for an input \\\\( \\mathbf{X} \\\\) of length \\\\( N \\\\) is:\n\n$$ \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T) \\text{ with } \\mathbf{Q} = \\mathbf{W}_q \\mathbf{X}, \\mathbf{V} = \\mathbf{W}_v \\mathbf{X}, \\mathbf{K} = \\mathbf{W}_k \\mathbf{X} $$\n\n\\\\(  \\mathbf{X} = (\\mathbf{x}_1, ... \\mathbf{x}_{N}) \\\\) is thereby the input sequence to the attention layer. The projections \\\\( \\mathbf{Q} \\\\) and \\\\( \\mathbf{K} \\\\) will each consist of \\\\( N \\\\) vectors resulting in the \\\\( \\mathbf{QK}^T \\\\) being of size \\\\( N^2 \\\\) .\n\nLLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel.\nAssuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\\\( \\mathbf{QK^T} \\\\) matrices to be \\\\( 40 * 2 * N^2 \\\\) bytes. For \\\\( N=1000 \\\\) only around 50 MB of VRAM are needed, however, for \\\\( N=16000 \\\\) we would need 19 GB of VRAM, and for \\\\( N=100,000 \\\\) we would need almost 1TB just to store the \\\\( \\mathbf{QK}^T \\\\) matrices.",
  "Long story short, the default self-attention algorithm quickly becomes prohibitively memory-expensive for large input contexts.\n\nAs LLMs improve in text comprehension and generation, they are applied to increasingly complex tasks. While models once handled the translation or summarization of a few sentences, they now manage entire pages, demanding the capability to process extensive input lengths.\n\nHow can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the \\\\( QK^T \\\\) matrix. [Tri Dao et al.](https://arxiv.org/abs/2205.14135) developed exactly such a new algorithm and called it **Flash Attention**.\n\nIn a nutshell, Flash Attention breaks the  \\\\(\\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T\\\\)) computation apart and instead computes smaller chunks of the output by iterating over multiple softmax computation steps:\n\n$$ \\textbf{O}_i \\leftarrow s^a_{ij} * \\textbf{O}_i + s^b_{ij} * \\mathbf{V}_{j} \\times \\text{Softmax}(\\mathbf{QK}^T_{i,j}) \\text{ for multiple } i, j \\text{ iterations} $$",
  "with \\\\( s^a_{ij} \\\\) and \\\\( s^b_{ij} \\\\) being some softmax normalization statistics that need to be recomputed for every \\\\( i \\\\) and \\\\( j \\\\) .\n\nPlease note that the whole Flash Attention is a bit more complex and is greatly simplified here as going in too much depth is out of scope for this guide. The reader is invited to take a look at the well-written [Flash Attention paper](https://arxiv.org/abs/2205.14135) for more details.\n\nThe main takeaway here is:\n\n> By keeping track of softmax normalization statistics and by using some smart mathematics, Flash Attention gives **numerical identical** outputs compared to the default self-attention layer at a memory cost that only increases linearly with \\\\( N \\\\) .\n\nLooking at the formula, one would intuitively say that Flash Attention must be much slower compared to the default self-attention formula as more computation needs to be done. Indeed Flash Attention requires more FLOPs compared to normal attention as the softmax normalization statistics have to constantly be recomputed (see [paper](https://arxiv.org/abs/2205.14135) for more details if interested)",
  "> However, Flash Attention is much faster in inference compared to default attention which comes from its ability to significantly reduce the demands on the slower, high-bandwidth memory of the GPU (VRAM), focusing instead on the faster on-chip memory (SRAM).\n\nEssentially, Flash Attention makes sure that all intermediate write and read operations can be done using the fast *on-chip* SRAM memory instead of having to access the slower VRAM memory to compute the output vector \\\\( \\mathbf{O} \\\\) .\n\nIn practice, there is currently absolutely no reason to **not** use Flash Attention if available. The algorithm gives mathematically the same outputs, and is both faster and more memory-efficient.\n\nLet's look at a practical example.\n\nOur OctoCoder model now gets a significantly longer input prompt which includes a so-called *system prompt*. System prompts are used to steer the LLM into a better assistant that is tailored to the users' task.\nIn the following, we use a system prompt that will make OctoCoder a better coding assistant.\n\n```python\nsystem_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.",
  "The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\nThe assistant is happy to help with code questions and will do their best to understand exactly what is needed.\nIt also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.\nThat said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.\n\nThe Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\nThe model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\n\n-----\n\nQuestion: Write a function that takes two lists and returns a list that has alternating elements from each input list.\n\nAnswer: Sure. Here is a function that does that.\n\ndef alternating(list1, list2):\nresults = []\nfor i in range(len(list1)):\nresults.append(list1[i])\nresults.append(list2[i])\nreturn results\n\nQuestion: Can you write some test cases for this function?",
  "Answer: Sure, here are some tests.\n\nassert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\nassert alternating([True, False], [4, 5]) == [True, 4, False, 5]\nassert alternating([], []) == []\n\nQuestion: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\n\nAnswer: Here is the modified function.\n\ndef alternating(list1, list2):\nresults = []\nfor i in range(min(len(list1), len(list2))):\nresults.append(list1[i])\nresults.append(list2[i])\nif len(list1) > len(list2):\nresults.extend(list1[i+1:])\nelse:\nresults.extend(list2[i+1:])\nreturn results\n\n-----\n\"\"\"\n```\nFor demonstration purposes, we duplicate the system prompt by ten so that the input length is long enough to observe Flash Attention's memory savings.\nWe append the original text prompt `\"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"`\n\n```python\nlong_prompt = 10 * system_prompt + prompt\n```\n\nWe instantiate our model again in bfloat16 precision.\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")",
  "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n```\n\nLet's now run the model just like before *without Flash Attention* and measure the peak GPU memory requirement and inference time.\n\n```python\nimport time\n\nstart_time = time.time()\nresult = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n\nprint(f\"Generated in {time.time() - start_time} seconds.\")\nresult\n```\n\n**Output**:\n```\nGenerated in 10.96854019165039 seconds.\nSure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n````\n\nWe're getting the same output as before, however this time, the model repeats the answer multiple times until it's 60 tokens cut-off. This is not surprising as we've repeated the system prompt ten times for demonstration purposes and thus cued the model to repeat itself.\n\n**Note** that the system prompt should not be repeated ten times in real-world applications - one time is enough!\n\nLet's measure the peak GPU memory requirement.\n\n```python",
  "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```\n\n**Output**:\n```bash\n37.668193340301514\n```\n\nAs we can see the peak GPU memory requirement is now significantly higher than in the beginning, which is largely due to the longer input sequence. Also the generation takes a little over a minute now.\n\nWe call `flush()` to free GPU memory for our next experiment.\n\n```python\nflush()\n```\n\nFor comparison, let's run the same function, but enable Flash Attention instead.\nTo do so, we convert the model to [BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) and by doing so enabling PyTorch's [SDPA self-attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) which in turn is able to use Flash Attention.\n\n```python\nmodel.to_bettertransformer()\n```\n\nNow we run the exact same code snippet as before and under the hood Transformers will make use of Flash Attention.\n\n```py\nstart_time = time.time()\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\nresult = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]",
  "print(f\"Generated in {time.time() - start_time} seconds.\")\nresult\n```\n\n**Output**:\n```\nGenerated in 3.0211617946624756 seconds.\nSure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n```\n\nWe're getting the exact same result as before, but can observe a very significant speed-up thanks to Flash Attention.\n\nLet's measure the memory consumption one last time.\n\n```python\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```\n\n**Output**:\n```\n32.617331981658936\n```\n\nAnd we're almost back to our original 29GB peak GPU memory from the beginning.\n\nWe can observe that we only use roughly 100MB more GPU memory when passing a very long input sequence with Flash Attention compared to passing a short input sequence as done in the beginning.\n\n```py\nflush()\n```\n\nFor more information on how to use Flash Attention, please have a look at [this doc page](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2).\n\n## 3. Architectural Innovations\n\nSo far we have looked into improving computational and memory efficiency by:",
  "-   Casting the weights to a lower precision format\n-   Replacing the self-attention algorithm with a more memory- and compute efficient version\n\nLet's now look into how we can change the architecture of an LLM so that it is most effective and efficient for task that require long text inputs, *e.g.*:\n-   Retrieval augmented Questions Answering,\n-   Summarization,\n-   Chat\n\nNote that *chat* not only requires the LLM to handle long text inputs, but it also necessitates that the LLM is able to efficiently handle the back-and-forth dialogue between user and assistant (such as ChatGPT).\n\nOnce trained, the fundamental LLM architecture is difficult to change, so it is important to make considerations about the LLM's tasks beforehand and accordingly optimize the model's architecture.\nThere are two important components of the model architecture that quickly become memory and/or performance bottlenecks for large input sequences.\n\n-   The positional embeddings\n-   The key-value cache\n\nLet's go over each component in more detail\n\n### 3.1 Improving positional embeddings of LLMs\n\nSelf-attention puts each token in relation to each other's tokens.",
  "As an example, the \\\\( \\text{Softmax}(\\mathbf{QK}^T) \\\\) matrix of the text input sequence *\"Hello\", \"I\", \"love\", \"you\"* could look as follows:\n\n![](/blog/assets/163_optimize_llm/self_attn_tokens.png)\n\nEach word token is given a probability mass at which it attends all other word tokens and, therefore is put into relation with all other word tokens. E.g. the word *\"love\"* attends to the word *\"Hello\"* with 5%, to *\"I\"* with 30%, and to itself with 65%.\n\nA LLM based on self-attention, but without position embeddings would have great difficulties in understanding the positions of the text inputs to each other.\nThis is because the probability score computed by \\\\( \\mathbf{QK}^T \\\\) relates each word token to each other word token in \\\\( O(1) \\\\) computations regardless of their relative positional distance to each other.\nTherefore, for the LLM without position embeddings each token appears to have the same distance to all other tokens, *e.g.* differentiating between *\"Hello I love you\"* and *\"You love I hello\"* would be very challenging.",
  "For the LLM to understand sentence order, an additional *cue* is needed and is usually applied in the form of *positional encodings* (or also called *positional embeddings*).\nPositional encodings, encode the position of each token into a numerical presentation that the LLM can leverage to better understand sentence order.\n\nThe authors of the [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) paper introduced sinusoidal positional embeddings \\\\( \\mathbf{P} = \\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\\\) .\nwhere each vector \\\\( \\mathbf{p}_i \\\\) is computed as a sinusoidal function of its position \\\\( i \\\\) .\nThe positional encodings are then simply added to the input sequence vectors \\\\( \\mathbf{\\hat{X}} = \\mathbf{\\hat{x}}_1, \\ldots, \\mathbf{\\hat{x}}_N \\\\) = \\\\( \\mathbf{x}_1 + \\mathbf{p}_1, \\ldots, \\mathbf{x}_N + \\mathbf{p}_N \\\\) thereby cueing the model to better learn sentence order.\n\nInstead of using fixed position embeddings, others (such as [Devlin et al.](https://arxiv.org/abs/1810.04805)) used learned positional encodings for which the positional embeddings\n\\\\( \\mathbf{P} \\\\) are learned during training.",
  "Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence order into LLMs, but a couple of problems related to these positional encodings were found:\n\n1. Sinusoidal and learned position embeddings are both absolute positional embeddings, *i.e.* encoding a unique embedding for each position id: \\\\( 0, \\ldots, N \\\\) . As shown by [Huang et al.](https://arxiv.org/abs/2009.13658) and [Su et al.](https://arxiv.org/abs/2104.09864), absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position.\n2. When using learned position embeddings, the LLM has to be trained on a fixed input length \\\\( N \\\\), which makes it difficult to extrapolate to an input length longer than what it was trained on.\n\nRecently, relative positional embeddings that can tackle the above mentioned problems have become more popular, most notably:\n\n-   [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864)\n-   [ALiBi](https://arxiv.org/abs/2108.12409)",
  "Both *RoPE* and *ALiBi* argue that it's best to cue the LLM about sentence order directly in the self-attention algorithm as it's there that word tokens are put into relation with each other. More specifically, sentence order should be cued by modifying the \\\\( \\mathbf{QK}^T \\\\) computation.\n\nWithout going into too many details, *RoPE* notes that positional information can be encoded into query-key pairs, *e.g.* \\\\( \\mathbf{q}_i \\\\) and \\\\( \\mathbf{x}_j \\\\) by rotating each vector by an angle \\\\( \\theta * i \\\\) and \\\\( \\theta * j \\\\) respectively with \\\\( i, j \\\\) describing each vectors sentence position:\n\n$$ \\mathbf{\\hat{q}}_i^T \\mathbf{\\hat{x}}_j = \\mathbf{{q}}_i^T \\mathbf{R}_{\\theta, i -j} \\mathbf{{x}}_j. $$\n\n\\\\( \\mathbf{R}_{\\theta, i - j} \\\\) thereby represents a rotational matrix. \\\\( \\theta \\\\) is *not* learned during training, but instead set to a pre-defined value that depends on the maximum input sequence length during training.",
  "> By doing so, the probability score between \\\\( \\mathbf{q}_i \\\\) and \\\\( \\mathbf{q}_j \\\\) is only affected if \\\\( i \\ne j \\\\) and solely depends on the relative distance \\\\( i - j \\\\) regardless of each vector's specific positions \\\\( i \\\\) and \\\\( j \\\\) .\n\n*RoPE* is used in multiple of today's most important LLMs, such as:\n\n-   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)\n-   [**Llama**](https://arxiv.org/abs/2302.13971)\n-   [**PaLM**](https://arxiv.org/abs/2204.02311)\n\nAs an alternative, *ALiBi* proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value `m` to each query-key entry of the \\\\( \\mathbf{QK}^T \\\\) matrix right before the softmax computation.\n\n![](/blog/assets/163_optimize_llm/alibi.png)\n\nAs shown in the [ALiBi](https://arxiv.org/abs/2108.12409) paper, this simple relative positional encoding allows the model to retain a high performance even at very long text input sequences.\n\n*ALiBi* is used in multiple of today's most important LLMs, such as:\n\n-   [**MPT**](https://huggingface.co/mosaicml/mpt-30b)",
  "-   [**BLOOM**](https://huggingface.co/bigscience/bloom)\n\nBoth *RoPE* and *ALiBi* position encodings can extrapolate to input lengths not seen during training whereas it has been shown that extrapolation works much better out-of-the-box for *ALiBi* as compared to *RoPE*.\nFor ALiBi, one simply increases the values of the lower triangular position matrix to match the length of the input sequence.\nFor *RoPE*, keeping the same \\\\( \\theta \\\\) that was used during training leads to poor results when passing text inputs much longer than those seen during training, *c.f* [Press et al.](https://arxiv.org/abs/2108.12409). However, the community has found a couple of effective tricks that adapt \\\\( \\theta \\\\), thereby allowing *RoPE* position embeddings to work well for extrapolated text input sequences (see [here](https://github.com/huggingface/transformers/pull/24653)).\n\n> Both RoPE and ALiBi are relative positional embeddings that are *not* learned during training, but instead are based on the following intuitions:\n-   Positional cues about the text inputs should be given directly to the \\\\( QK^T \\\\) matrix of the self-attention layer",
  "-   The LLM should be incentivized to learn a constant *relative* distance positional encodings have to each other\n-   The further text input tokens are from each other, the lower the probability of their query-value probability. Both RoPE and ALiBi lower the query-key probability of tokens far away from each other. RoPE by decreasing their vector product by increasing the angle between the query-key vectors. ALiBi by adding large negative numbers to the vector product\n\nIn conclusion, LLMs that are intended to be deployed in tasks that require handling large text inputs are better trained with relative positional embeddings, such as RoPE and ALiBi. Also note that even if an LLM with RoPE and ALiBi has been trained only on a fixed length of say \\\\( N_1 = 2048 \\\\) it can still be used in practice with text inputs much larger than \\\\( N_1 \\\\), like \\\\( N_2 = 8192 > N_1 \\\\) by extrapolating the positional embeddings.\n\n### 3.2 The key-value cache",
  "Auto-regressive text generation with LLMs works by iteratively putting in an input sequence, sampling the next token, appending the next token to the input sequence, and continuing to do so until the LLM produces a token that signifies that the generation has finished.\n\nPlease have a look at [Transformer's Generate Text Tutorial](https://huggingface.co/docs/transformers/llm_tutorial#generate-text) to get a more visual explanation of how auto-regressive generation works.\n\nLet's run a quick code snippet to show how auto-regressive works in practice. We will simply take the most likely next token via `torch.argmax`.\n\n```python\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\nnext_logits = model(input_ids)[\"logits\"][:, -1:]\nnext_token_id = torch.argmax(next_logits,dim=-1)\n\ninput_ids = torch.cat([input_ids, next_token_id], dim=-1)\nprint(\"shape of input_ids\", input_ids.shape)\n\ngenerated_text = tokenizer.batch_decode(input_ids[:, -5:])\ngenerated_text\n```\n\n**Output**:\n```\nshape of input_ids torch.Size([1, 21])\nshape of input_ids torch.Size([1, 22])\nshape of input_ids torch.Size([1, 23])\nshape of input_ids torch.Size([1, 24])",
  "shape of input_ids torch.Size([1, 25])\n[' Here is a Python function']\n```\n\nAs we can see every time we increase the text input tokens by the just sampled token.\n\nWith very few exceptions, LLMs are trained using the [causal language modeling objective](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling) and therefore mask the upper triangle matrix of the attention score - this is why in the two diagrams above the attention scores are left blank (*a.k.a* have 0 probability). For a quick recap on causal language modeling you can refer to the [*Illustrated Self Attention blog*](https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention).",
  "As a consequence, tokens *never* depend on previous tokens, more specifically the \\\\( \\mathbf{q}_i \\\\) vector is never put in relation with any key, values vectors \\\\( \\mathbf{k}_j, \\mathbf{v}_j \\\\) if \\\\( j > i \\\\) . Instead \\\\( \\mathbf{q}_i \\\\) only attends to previous key-value vectors \\\\( \\mathbf{k}_{m < i}, \\mathbf{v}_{m < i} \\text{ , for } m \\in \\{0, \\ldots i - 1\\} \\\\). In order to reduce unnecessary computation, one can therefore cache each layer's key-value vectors for all previous timesteps.\n\nIn the following, we will tell the LLM to make use of the key-value cache by retrieving and forwarding it for each forward pass.\nIn Transformers, we can retrieve the key-value cache by passing the `use_cache` flag to the `forward` call and can then pass it with the current token.\n\n```python\npast_key_values = None # past_key_values is the key-value cache\ngenerated_tokens = []\nnext_token_id = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\nnext_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\nnext_logits = next_logits[:, -1:]\nnext_token_id = torch.argmax(next_logits, dim=-1)",
  "print(\"shape of input_ids\", next_token_id.shape)\nprint(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\ngenerated_tokens.append(next_token_id.item())\n\ngenerated_text = tokenizer.batch_decode(generated_tokens)\ngenerated_text\n```\n\n**Output**:\n```\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 20\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 21\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 22\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 23\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 24\n[' Here', ' is', ' a', ' Python', ' function']\n```\n\nAs one can see, when using the key-value cache the text input tokens are *not* increased in length, but remain a single input vector. The length of the key-value cache on the other hand is increased by one at every decoding step.",
  "> Making use of the key-value cache means that the \\\\( \\mathbf{QK}^T \\\\) is essentially reduced to \\\\( \\mathbf{q}_c\\mathbf{K}^T \\\\) with \\\\( \\mathbf{q}_c \\\\) being the query projection of the currently passed input token which is *always* just a single vector.\n\nUsing the key-value cache has two advantages:\n-   Significant increase in computational efficiency as less computations are performed compared to computing the full \\\\( \\mathbf{QK}^T \\\\) matrix. This leads to an increase in inference speed\n-   The maximum required memory is not increased quadratically with the number of generated tokens, but only increases linearly.\n\n> One should *always* make use of the key-value cache as it leads to identical results and a significant speed-up for longer input sequences. Transformers has the key-value cache enabled by default when making use of the text pipeline or the [`generate` method](https://huggingface.co/docs/transformers/main_classes/text_generation). We have an entire guide dedicated to caches [here](./kv_cache).\n\n<Tip warning={true}>",
  "Note that, despite our advice to use key-value caches, your LLM output may be slightly different when you use them. This is a property of the matrix multiplication kernels themselves -- you can read more about it [here](https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535).\n\n</Tip>\n\n#### 3.2.1 Multi-round conversation\n\nThe key-value cache is especially useful for applications such as chat where multiple passes of auto-regressive decoding are required. Let's look at an example.\n\n```\nUser: How many people live in France?\nAssistant: Roughly 75 million people live in France\nUser: And how many are in Germany?\nAssistant: Germany has ca. 81 million inhabitants\n```\n\nIn this chat, the LLM runs auto-regressive decoding twice:\n1. The first time, the key-value cache is empty and the input prompt is `\"User: How many people live in France?\"` and the model auto-regressively generates the text `\"Roughly 75 million people live in France\"` while increasing the key-value cache at every decoding step.",
  "2. The second time the input prompt is `\"User: How many people live in France? \\n Assistant: Roughly 75 million people live in France \\n User: And how many in Germany?\"`. Thanks to the cache, all key-value vectors for the first two sentences are already computed. Therefore the input prompt only consists of `\"User: And how many in Germany?\"`. While processing the shortened input prompt, its computed key-value vectors are concatenated to the key-value cache of the first decoding. The second Assistant's answer `\"Germany has ca. 81 million inhabitants\"` is then auto-regressively generated with the key-value cache consisting of encoded key-value vectors of `\"User: How many people live in France? \\n Assistant: Roughly 75 million people live in France \\n User: And how many are in Germany?\"`.\n\nTwo things should be noted here:\n1. Keeping all the context is crucial for LLMs deployed in chat so that the LLM understands all the previous context of the conversation. E.g. for the example above the LLM needs to understand that the user refers to the population when asking `\"And how many are in Germany\"`.",
  "2. The key-value cache is extremely useful for chat as it allows us to continuously grow the encoded chat history instead of having to re-encode the chat history again from scratch (as e.g. would be the case when using an encoder-decoder architecture).\n\nIn `transformers`, a `generate` call will return `past_key_values` when `return_dict_in_generate=True` is passed, in addition to the default `use_cache=True`. Note that it is not yet available through the `pipeline` interface.\n\n```python\n# Generation as usual\nprompt = system_prompt + \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"\nmodel_inputs = tokenizer(prompt, return_tensors='pt')\ngeneration_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)\ndecoded_output = tokenizer.batch_decode(generation_output.sequences)[0]\n\n# Piping the returned `past_key_values` to speed up the next conversation round\nprompt = decoded_output + \"\\nQuestion: How can I modify the function above to return Mega bytes instead?\\n\\nAnswer: Here\"\nmodel_inputs = tokenizer(prompt, return_tensors='pt')\ngeneration_output = model.generate(\n**model_inputs,",
  "past_key_values=generation_output.past_key_values,\nmax_new_tokens=60,\nreturn_dict_in_generate=True\n)\ntokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]\n```\n\n**Output**:\n```\nis a modified version of the function that returns Mega bytes instead.\n\ndef bytes_to_megabytes(bytes):\nreturn bytes / 1024 / 1024\n\nAnswer: The function takes a number of bytes as input and returns the number of\n```\n\nGreat, no additional time is spent recomputing the same key and values for the attention layer! There is however one catch. While the required peak memory for the \\\\( \\mathbf{QK}^T \\\\) matrix is significantly reduced, holding the key-value cache in memory can become very memory expensive for long input sequences or multi-turn chat. Remember that the key-value cache needs to store the key-value vectors for all previous input vectors \\\\( \\mathbf{x}_i \\text{, for } i \\in \\{1, \\ldots, c - 1\\} \\\\) for all self-attention layers and for all attention heads.\n\nLet's compute the number of float values that need to be stored in the key-value cache for the LLM `bigcode/octocoder` that we used before.",
  "The number of float values amounts to two times the sequence length times the number of attention heads times the attention head dimension and times the number of layers.\nComputing this for our LLM at a hypothetical input sequence length of 16000 gives:\n\n```python\nconfig = model.config\n2 * 16_000 * config.n_layer * config.n_head * config.n_embd // config.n_head\n```\n\n**Output**:\n```\n7864320000\n```\n\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires around 15 GB of RAM which is circa half as much as the model weights themselves!\nResearchers have proposed two methods that allow to significantly reduce the memory cost of storing the key-value cache, which are explored in the next subsections.\n\n#### 3.2.2 Multi-Query-Attention (MQA)\n\n[Multi-Query-Attention](https://arxiv.org/abs/1911.02150) was proposed in Noam Shazeer's *Fast Transformer Decoding: One Write-Head is All You Need* paper. As the title says, Noam found out that instead of using `n_head` key-value projections weights, one can use a single head-value projection weight pair that is shared across all attention heads without that the model's performance significantly degrades.",
  "> By using a single head-value projection weight pair, the key value vectors \\\\( \\mathbf{k}_i, \\mathbf{v}_i \\\\) have to be identical across all attention heads which in turn means that we only need to store 1 key-value projection pair in the cache instead of `n_head` ones.\n\nAs most LLMs use between 20 and 100 attention heads, MQA significantly reduces the memory consumption of the key-value cache. For the LLM used in this notebook we could therefore reduce the required memory consumption from 15 GB to less than 400 MB at an input sequence length of 16000.\n\nIn addition to memory savings, MQA also leads to improved computational efficiency as explained in the following.",
  "In auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the \\\\( \\mathbf{q}_c\\mathbf{K}^T \\\\) computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at [Noam's paper](https://arxiv.org/abs/1911.02150).\n\nThe important part to understand here is that reducing the number of key-value attention heads to 1 only makes sense if a key-value cache is used. The peak memory consumption of the model for a single forward pass without key-value cache stays unchanged as every attention head still has a unique query vector so that each attention head still has a different \\\\( \\mathbf{QK}^T \\\\) matrix.\n\nMQA has seen wide adoption by the community and is now used by many of the most popular LLMs:\n\n-   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)\n-   [**PaLM**](https://arxiv.org/abs/2204.02311)",
  "-   [**MPT**](https://huggingface.co/mosaicml/mpt-30b)\n-   [**BLOOM**](https://huggingface.co/bigscience/bloom)\n\nAlso, the checkpoint used in this notebook - `bigcode/octocoder` - makes use of MQA.\n\n#### 3.2.3 Grouped-Query-Attention (GQA)\n\n[Grouped-Query-Attention](https://arxiv.org/abs/2305.13245), as proposed by Ainslie et al. from Google, found that using MQA can often lead to quality degradation compared to using vanilla multi-key-value head projections. The paper argues that more model performance can be kept by less drastically reducing the number of query head projection weights. Instead of using just a single key-value projection weight, `n < n_head` key-value projection weights should be used. By choosing `n` to a significantly smaller value than `n_head`, such as 2,4 or 8 almost all of the memory and speed gains from MQA can be kept while sacrificing less model capacity and thus arguably less performance.",
  "Moreover, the authors of GQA found out that existing model checkpoints can be *uptrained* to have a GQA architecture with as little as 5% of the original pre-training compute. While 5% of the original pre-training compute can still be a massive amount, GQA *uptraining* allows existing checkpoints to be useful for longer input sequences.\n\nGQA was only recently proposed which is why there is less adoption at the time of writing this notebook.\nThe most notable application of GQA is [Llama-v2](https://huggingface.co/meta-llama/Llama-2-70b-hf).\n\n> As a conclusion, it is strongly recommended to make use of either GQA or MQA if the LLM is deployed with auto-regressive decoding and is required to handle large input sequences as is the case for example for chat.\n\n\n## Conclusion",
  "The research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs. As an example, one such promising research direction is [speculative decoding](https://arxiv.org/abs/2211.17192) where \"easy tokens\" are generated by smaller, faster language models and only \"hard tokens\" are generated by the LLM itself. Going into more detail is out of the scope of this notebook, but can be read upon in this [nice blog post](https://huggingface.co/blog/assisted-generation).\n\nThe reason massive LLMs such as GPT3/4, Llama-2-70b, Claude, PaLM can run so quickly in chat-interfaces such as [Hugging Face Chat](https://huggingface.co/chat/) or ChatGPT is to a big part thanks to the above-mentioned improvements in precision, algorithms, and architecture.\nGoing forward, accelerators such as GPUs, TPUs, etc... will only get faster and allow for more memory, but one should nevertheless always make sure to use the best available algorithms and architectures to get the most bang for your buck 🤗",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Pipeline\n\nThe [`Pipeline`] is a simple but powerful inference API that is readily available for a variety of machine learning tasks with any model from the Hugging Face [Hub](https://hf.co/models).",
  "Tailor the [`Pipeline`] to your task with task specific parameters such as adding timestamps to an automatic speech recognition (ASR) pipeline for transcribing meeting notes. [`Pipeline`] supports GPUs, Apple Silicon, and half-precision weights to accelerate inference and save memory.\n\n<Youtube id=tiZFewofSLM/>\n\nTransformers has two pipeline classes, a generic [`Pipeline`] and many individual task-specific pipelines like [`TextGenerationPipeline`] or [`VisualQuestionAnsweringPipeline`]. Load these individual pipelines by setting the task identifier in the `task` parameter in [`Pipeline`]. You can find the task identifier for each pipeline in their API documentation.\n\nEach task is configured to use a default pretrained model and preprocessor, but this can be overridden with the `model` parameter if you want to use a different model.\n\nFor example, to use the [`TextGenerationPipeline`] with [Gemma 2](./model_doc/gemma2), set `task=\"text-generation\"` and `model=\"google/gemma-2-2b\"`.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\")\npipeline(\"the secret to baking a really good cake is \")",
  "[{'generated_text': 'the secret to baking a really good cake is 1. the right ingredients 2. the'}]\n```\n\nWhen you have more than one input, pass them as a list.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=\"cuda\")\npipeline([\"the secret to baking a really good cake is \", \"a baguette is \"])\n[[{'generated_text': 'the secret to baking a really good cake is 1. the right ingredients 2. the'}],\n[{'generated_text': 'a baguette is 100% bread.\\n\\na baguette is 100%'}]]\n```\n\nThis guide will introduce you to the [`Pipeline`], demonstrate its features, and show how to configure its various parameters.\n\n## Tasks\n\n[`Pipeline`] is compatible with many machine learning tasks across different modalities. Pass an appropriate input to the pipeline and it will handle the rest.\n\nHere are some examples of how to use [`Pipeline`] for different tasks and modalities.\n\n<hfoptions id=\"tasks\">\n<hfoption id=\"summarization\">\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"summarization\", model=\"google/pegasus-billsum\")",
  "pipeline(\"Section was formerly set out as section 44 of this title. As originally enacted, this section contained two further provisions that 'nothing in this act shall be construed as in any wise affecting the grant of lands made to the State of California by virtue of the act entitled 'An act authorizing a grant to the State of California of the Yosemite Valley, and of the land' embracing the Mariposa Big-Tree Grove, approved June thirtieth, eighteen hundred and sixty-four; or as affecting any bona-fide entry of land made within the limits above described under any law of the United States prior to the approval of this act.' The first quoted provision was omitted from the Code because the land, granted to the state of California pursuant to the Act cite, was receded to the United States. Resolution June 11, 1906, No. 27, accepted the recession.\")\n[{'summary_text': 'Instructs the Secretary of the Interior to convey to the State of California all right, title, and interest of the United States in and to specified lands which are located within the Yosemite and Mariposa National Forests, California.'}]\n```\n\n</hfoption>\n<hfoption id=\"automatic speech recognition\">\n\n```py",
  "from transformers import pipeline\n\npipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\npipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n</hfoption>\n<hfoption id=\"image classification\">\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"image-classification\", model=\"google/vit-base-patch16-224\")\npipeline(images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n[{'label': 'lynx, catamount', 'score': 0.43350091576576233},\n{'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n'score': 0.034796204417943954},\n{'label': 'snow leopard, ounce, Panthera uncia',\n'score': 0.03240183740854263},\n{'label': 'Egyptian cat', 'score': 0.02394474856555462},\n{'label': 'tiger cat', 'score': 0.02288915030658245}]\n```\n\n</hfoption>\n<hfoption id=\"visual question answering\">\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\npipeline(",
  "image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\nquestion=\"What is in the image?\",\n)\n[{'answer': 'statue of liberty'}]\n```\n\n</hfoption>\n</hfoptions>\n\n## Parameters\n\nAt a minimum, [`Pipeline`] only requires a task identifier, model, and the appropriate input. But there are many parameters available to configure the pipeline with, from task-specific parameters to optimizing performance.\n\nThis section introduces you to some of the more important parameters.\n\n### Device\n\n[`Pipeline`] is compatible with many hardware types, including GPUs, CPUs, Apple Silicon, and more. Configure the hardware type with the `device` parameter. By default, [`Pipeline`] runs on a CPU which is given by `device=-1`.\n\n<hfoptions id=\"device\">\n<hfoption id=\"GPU\">\n\nTo run [`Pipeline`] on a GPU, set `device` to the associated CUDA device id. For example, `device=0` runs on the first GPU.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=0)\npipeline(\"the secret to baking a really good cake is \")\n```",
  "You could also let [Accelerate](https://hf.co/docs/accelerate/index), a library for distributed training, automatically choose how to load and store the model weights on the appropriate device. This is especially useful if you have multiple devices. Accelerate loads and stores the model weights on the fastest device first, and then moves the weights to other devices (CPU, hard drive) as needed. Set `device_map=\"auto\"` to let Accelerate choose the device.\n\n> [!TIP]\n> Make sure have [Accelerate](https://hf.co/docs/accelerate/basic_tutorials/install) is installed.\n>\n> ```py\n> !pip install -U accelerate\n> ```\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device_map=\"auto\")\npipeline(\"the secret to baking a really good cake is \")\n```\n\n</hfoption>\n<hfoption id=\"Apple silicon\">\n\nTo run [`Pipeline`] on Apple silicon, set `device=\"mps\"`.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=\"mps\")\npipeline(\"the secret to baking a really good cake is \")\n```\n\n</hfoption>\n</hfoptions>\n\n### Batch inference",
  "[`Pipeline`] can also process batches of inputs with the `batch_size` parameter. Batch inference may improve speed, especially on a GPU, but it isn't guaranteed. Other variables such as hardware, data, and the model itself can affect whether batch inference improves speed. For this reason, batch inference is disabled by default.\n\nIn the example below, when there are 4 inputs and `batch_size` is set to 2, [`Pipeline`] passes a batch of 2 inputs to the model at a time.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=\"cuda\", batch_size=2)\npipeline([\"the secret to baking a really good cake is\", \"a baguette is\", \"paris is the\", \"hotdogs are\"])\n[[{'generated_text': 'the secret to baking a really good cake is to use a good cake mix.\\n\\ni’'}],\n[{'generated_text': 'a baguette is'}],\n[{'generated_text': 'paris is the most beautiful city in the world.\\n\\ni’ve been to paris 3'}],\n[{'generated_text': 'hotdogs are a staple of the american diet. they are a great source of protein and can'}]]\n```\n\nAnother good use case for batch inference is for streaming data in [`Pipeline`].\n\n```py\nfrom transformers import pipeline",
  "from transformers.pipelines.pt_utils import KeyDataset\nimport datasets\n\n# KeyDataset is a utility that returns the item in the dict returned by the dataset\ndataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\npipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=\"cuda\")\nfor out in pipeline(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\nprint(out)\n```\n\nKeep the following general rules of thumb in mind for determining whether batch inference can help improve performance.\n\n1. The only way to know for sure is to measure performance on your model, data, and hardware.\n2. Don't batch inference if you're constrained by latency (a live inference product for example).\n3. Don't batch inference if you're using a CPU.\n4. Don't batch inference if you don't know the `sequence_length` of your data. Measure performance, iteratively add to `sequence_length`, and include out-of-memory (OOM) checks to recover from failures.",
  "5. Do batch inference if your `sequence_length` is regular, and keep pushing it until you reach an OOM error. The larger the GPU, the more helpful batch inference is.\n6. Do make sure you can handle OOM errors if you decide to do batch inference.\n\n### Task-specific parameters\n\n[`Pipeline`] accepts any parameters that are supported by each individual task pipeline. Make sure to check out each individual task pipeline to see what type of parameters are available. If you can't find a parameter that is useful for your use case, please feel free to open a GitHub [issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml) to request it!\n\nThe examples below demonstrate some of the task-specific parameters available.\n\n<hfoptions id=\"task-specific-parameters\">\n<hfoption id=\"automatic speech recognition\">\n\nPass the `return_timestamps=\"word\"` parameter to [`Pipeline`] to return when each word was spoken.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")",
  "pipeline(audio=\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\", return_timestamp=\"word\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.',\n'chunks': [{'text': ' I', 'timestamp': (0.0, 1.1)},\n{'text': ' have', 'timestamp': (1.1, 1.44)},\n{'text': ' a', 'timestamp': (1.44, 1.62)},\n{'text': ' dream', 'timestamp': (1.62, 1.92)},\n{'text': ' that', 'timestamp': (1.92, 3.7)},\n{'text': ' one', 'timestamp': (3.7, 3.88)},\n{'text': ' day', 'timestamp': (3.88, 4.24)},\n{'text': ' this', 'timestamp': (4.24, 5.82)},\n{'text': ' nation', 'timestamp': (5.82, 6.78)},\n{'text': ' will', 'timestamp': (6.78, 7.36)},\n{'text': ' rise', 'timestamp': (7.36, 7.88)},\n{'text': ' up', 'timestamp': (7.88, 8.46)},\n{'text': ' and', 'timestamp': (8.46, 9.2)},\n{'text': ' live', 'timestamp': (9.2, 10.34)},\n{'text': ' out', 'timestamp': (10.34, 10.58)},\n{'text': ' the', 'timestamp': (10.58, 10.8)},\n{'text': ' true', 'timestamp': (10.8, 11.04)},\n{'text': ' meaning', 'timestamp': (11.04, 11.4)},\n{'text': ' of', 'timestamp': (11.4, 11.64)},\n{'text': ' its', 'timestamp': (11.64, 11.8)},\n{'text': ' creed.', 'timestamp': (11.8, 12.3)}]}\n```",
  "</hfoption>\n<hfoption id=\"text generation\">\n\nPass `return_full_text=False` to [`Pipeline`] to only return the generated text instead of the full text (prompt and generated text).\n\n[`~TextGenerationPipeline.__call__`] also supports additional keyword arguments from the [`~GenerationMixin.generate`] method. To return more than one generated sequence, set `num_return_sequences` to a value greater than 1.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\")\npipeline(\"the secret to baking a good cake is\", num_return_sequences=4, return_full_text=False)\n[{'generated_text': ' how easy it is for me to do it with my hands. You must not go nuts, or the cake is going to fall out.'},\n{'generated_text': ' to prepare the cake before baking. The key is to find the right type of icing to use and that icing makes an amazing frosting cake.\\n\\nFor a good icing cake, we give you the basics'},\n{'generated_text': \" to remember to soak it in enough water and don't worry about it sticking to the wall. In the meantime, you could remove the top of the cake and let it dry out with a paper towel.\\n\"},",
  "{'generated_text': ' the best time to turn off the oven and let it stand 30 minutes. After 30 minutes, stir and bake a cake in a pan until fully moist.\\n\\nRemove the cake from the heat for about 12'}]\n```\n\n</hfoption>\n</hfoptions>\n\n## Chunk batching\n\nThere are some instances where you need to process data in chunks.\n\n- for some data types, a single input (for example, a really long audio file) may need to be chunked into multiple parts before it can be processed\n- for some tasks, like zero-shot classification or question answering, a single input may need multiple forward passes which can cause issues with the `batch_size` parameter",
  "The [ChunkPipeline](https://github.com/huggingface/transformers/blob/99e0ab6ed888136ea4877c6d8ab03690a1478363/src/transformers/pipelines/base.py#L1387) class is designed to handle these use cases. Both pipeline classes are used in the same way, but since [ChunkPipeline](https://github.com/huggingface/transformers/blob/99e0ab6ed888136ea4877c6d8ab03690a1478363/src/transformers/pipelines/base.py#L1387) can automatically handle batching, you don't need to worry about the number of forward passes your inputs trigger. Instead, you can optimize `batch_size` independently of the inputs.\n\nThe example below shows how it differs from [`Pipeline`].\n\n```py\n# ChunkPipeline\nall_model_outputs = []\nfor preprocessed in pipeline.preprocess(inputs):\nmodel_outputs = pipeline.model_forward(preprocessed)\nall_model_outputs.append(model_outputs)\noutputs =pipeline.postprocess(all_model_outputs)\n\n# Pipeline\npreprocessed = pipeline.preprocess(inputs)\nmodel_outputs = pipeline.forward(preprocessed)\noutputs = pipeline.postprocess(model_outputs)\n```\n\n## Large datasets",
  "For inference with large datasets, you can iterate directly over the dataset itself. This avoids immediately allocating memory for the entire dataset, and you don't need to worry about creating batches yourself. Try [Batch inference](#batch-inference) with the `batch_size` parameter to see if it improves performance.\n\n```py\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom transformers import pipeline\nfrom datasets import load_dataset\n\ndataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\npipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=\"cuda\")\nfor out in pipeline(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\nprint(out)\n```\n\nOther ways to run inference on large datasets with [`Pipeline`] include using an iterator or generator.\n\n```py\ndef data():\nfor i in range(1000):\nyield f\"My example {i}\"\n\npipeline = pipeline(model=\"openai-community/gpt2\", device=0)\ngenerated_characters = 0\nfor out in pipeline(data()):\ngenerated_characters += len(out[0][\"generated_text\"])\n```\n\n## Large models",
  "[Accelerate](https://hf.co/docs/accelerate/index) enables a couple of optimizations for running large models with [`Pipeline`]. Make sure Accelerate is installed first.\n\n```py\n!pip install -U accelerate\n```\n\nThe `device_map=\"auto\"` setting is useful for automatically distributing the model across the fastest devices (GPUs) first before dispatching to other slower devices if available (CPU, hard drive).\n\n[`Pipeline`] supports half-precision weights (torch.float16), which can be significantly faster and save memory. Performance loss is negligible for most models, especially for larger ones. If your hardware supports it, you can enable torch.bfloat16 instead for more range.\n\n> [!TIP]\n> Inputs are internally converted to torch.float16 and it only works for models with a PyTorch backend.\n\nLastly, [`Pipeline`] also accepts quantized models to reduce memory usage even further. Make sure you have the [bitsandbytes](https://hf.co/docs/bitsandbytes/installation) library installed first, and then add `load_in_8bit=True` to `model_kwargs` in the pipeline.\n\n```py\nimport torch\nfrom transformers import pipeline, BitsAndBytesConfig",
  "pipeline = pipeline(model=\"google/gemma-7b\", torch_dtype=torch.bfloat16, device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\npipeline(\"the secret to baking a good cake is \")\n[{'generated_text': 'the secret to baking a good cake is 1. the right ingredients 2. the right'}]\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FullyShardedDataParallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a [parallelism](./perf_train_gpu_many) method that combines the advantages of data and model parallelism for distributed training.",
  "Unlike [DistributedDataParallel (DDP)](./perf_train_gpu_many#distributeddataparallel), FSDP saves more memory because it doesn't replicate a model on each GPU. It shards the models parameters, gradients and optimizer states across GPUs. Each model shard processes a portion of the data and the results are synchronized to speed up training.\n\nThis guide covers how to set up training a model with FSDP and [Accelerate](https://hf.co/docs/accelerate/index), a library for managing distributed training.\n\n```bash\npip install accelerate\n```\n\n## Configuration options\n\nAlways start by running the [accelerate config](https://hf.co/docs/accelerate/package_reference/cli#accelerate-config) command to help Accelerate set up the correct distributed training environment.\n\n```bash\naccelerate config\n```\n\nThe section below discusses some of the more important FSDP configuration options. Learn more about other available options in the [fsdp_config](https://hf.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config) parameter.\n\n### Sharding strategy",
  "FSDP offers several sharding strategies to distribute a model. Refer to the table below to help you choose the best strategy for your setup. Specify a strategy with the `fsdp_sharding_strategy` parameter in the configuration file.\n\n| sharding strategy | description | parameter value |\n|---|---|---|\n| `FULL_SHARD` | shards model parameters, gradients, and optimizer states | `1` |\n| `SHARD_GRAD_OP` | shards gradients and optimizer states | `2` |\n| `NO_SHARD` | don't shard the model | `3` |\n| `HYBRID_SHARD` | shards model parameters, gradients, and optimizer states within each GPU | `4` |\n| `HYBRID_SHARD_ZERO2` | shards gradients and optimizer states within each GPU | `5` |\n\n### CPU offload\n\nOffload model parameters and gradients when they aren't being used to the CPU to save additional GPU memory. This is useful for scenarios where a model is too large even with FSDP.\n\nSpecify `fsdp_offload_params: true` in the configuration file to enable offloading.\n\n### Wrapping policy\n\nFSDP is applied by wrapping each layer in the network. The wrapping is usually applied in a nested way where the full weights are discarded after each forward pass to save memory for the next layer.",
  "There are several wrapping policies available, but the *auto wrapping* policy is the simplest and doesn't require any changes to your code. Specify `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` to wrap a Transformer layer and `fsdp_transformer_layer_cls_to_wrap` to determine which layer to wrap (for example, `BertLayer`).\n\nSize-based wrapping is also available. If a layer exceeds a certain number of parameters, it is wrapped. Specify `fsdp_wrap_policy: SIZED_BASED_WRAP` and `min_num_param` to set the minimum number of parameters for a layer to be wrapped.\n\n### Checkpoints\n\nIntermediate checkpoints should be saved as a sharded state dict because saving the full state dict - even with CPU offloading - is time consuming and can cause `NCCL Timeout` errors due to indefinite hanging during broadcasting.\n\nSpecify `fsdp_state_dict_type: SHARDED_STATE_DICT` in the configuration file to save the sharded state dict. Now you can resume training from the sharded state dict with [`~accelerate.Accelerator.load_state`].\n\n```py\naccelerator.load_state(\"directory/containing/checkpoints\")\n```",
  "Once training is complete though, you should save the full state dict because the sharded state dict is only compatible with FSDP.\n\n```py\nif trainer.is_fsdp_enabled:\ntrainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n\ntrainer.save_model(script_args.output_dir)\n```\n\n### TPU\n\n[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html), a package for running PyTorch on XLA devices, enables FSDP on TPUs. Modify the configuration file to include the parameters below. Refer to the [xla_fsdp_settings](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128) parameter for additional XLA-specific parameters you can configure for FSDP.\n\n```yaml\nxla: True # must be set to True to enable PyTorch/XLA\nxla_fsdp_settings: # XLA specific FSDP parameters\nxla_fsdp_grad_ckpt: True # enable gradient checkpointing\n```\n\n## Training",
  "After running [accelerate config](https://hf.co/docs/accelerate/package_reference/cli#accelerate-config), your configuration file should be ready. An example configuration file is shown below that fully shards the parameter, gradient and optimizer states on two GPUs. Your file may look different depending on how you set up your configuration.\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\nfsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\nfsdp_backward_prefetch_policy: BACKWARD_PRE\nfsdp_cpu_ram_efficient_loading: true\nfsdp_forward_prefetch: false\nfsdp_offload_params: true\nfsdp_sharding_strategy: 1\nfsdp_state_dict_type: SHARDED_STATE_DICT\nfsdp_sync_module_states: true\nfsdp_transformer_layer_cls_to_wrap: BertLayer\nfsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```",
  "Run the [accelerate launch](https://hf.co/docs/accelerate/package_reference/cli#accelerate-launch) command to launch a training script with the FSDP configurations you chose in the configuration file.\n\n```bash\naccelerate launch my-training-script.py\n```\n\nIt is also possible to directly specify some of the FSDP arguments in the command line.\n\n```bash\naccelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/\" my-training-script.py\n```\n\n## Resources\n\nFSDP is a powerful tool for training large models with fewer GPUs compared to other parallelism strategies. Refer to the following resources below to learn even more about FSDP.\n\n- Follow along with the more in-depth Accelerate guide for [FSDP](https://hf.co/docs/accelerate/usage_guides/fsdp).\n- Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n- Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# The Transformer model family",
  "Since its introduction in 2017, the [original Transformer](https://arxiv.org/abs/1706.03762) model (see the [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post for a gentle technical introduction) has inspired many new and exciting models that extend beyond natural language processing (NLP) tasks. There are models for [predicting the folded structure of proteins](https://huggingface.co/blog/deep-learning-with-proteins), [training a cheetah to run](https://huggingface.co/blog/train-decision-transformers), and [time series forecasting](https://huggingface.co/blog/time-series-transformers). With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before.",
  "If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.\n\n<div align=\"center\">\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H39Z_720T5s\" title=\"YouTube video player\"\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\npicture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Computer vision\n\n<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1\" allowfullscreen></iframe>\n\n### Convolutional network",
  "For a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers!\n\n### Encoder[[cv-encoder]]",
  "The [Vision Transformer (ViT)](model_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection.",
  "One of these models is the [Swin](model_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN 👀 and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction.",
  "Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model_doc/beit) is pretrained by *masked image modeling (MIM)*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens. What's unusual is 75% of the image patches are masked! The decoder reconstructs the pixels from the masked tokens and encoded patches. After pretraining, the decoder is thrown away, and the encoder is ready to be used in downstream tasks.\n\n### Decoder[[cv-decoder]]",
  "Decoder-only vision models are rare because most vision models rely on an encoder to learn an image representation. But for use cases like image generation, the decoder is a natural fit, as we've seen from text generation models like GPT-2. [ImageGPT](model_doc/imagegpt) uses the same architecture as GPT-2, but instead of predicting the next token in a sequence, it predicts the next pixel in an image. In addition to image generation, ImageGPT could also be finetuned for image classification.\n\n### Encoder-decoder[[cv-encoder-decoder]]\n\nVision models commonly use an encoder (also known as a backbone) to extract important image features before passing them to a Transformer decoder. [DETR](model_doc/detr) has a pretrained backbone, but it also uses the complete Transformer encoder-decoder architecture for object detection. The encoder learns image representations and combines them with object queries (each object query is a learned embedding that focuses on a region or object in an image) in the decoder. DETR predicts the bounding box coordinates and class label for each object query.\n\n## Natural language processing",
  "<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1\" allowfullscreen></iframe>\n\n### Encoder[[nlp-encoder]]\n\n[BERT](model_doc/bert) is an encoder-only Transformer that randomly masks certain tokens in the input to avoid seeing other tokens, which would allow it to \"cheat\". The pretraining objective is to predict the masked token based on the context. This allows BERT to fully use the left and right contexts to help it learn a deeper and richer representation of the inputs. However, there was still room for improvement in BERT's pretraining strategy. [RoBERTa](model_doc/roberta) improved upon this by introducing a new pretraining recipe that includes training for longer and on larger batches, randomly masking tokens at each epoch instead of just once during preprocessing, and removing the next-sentence prediction objective.",
  "The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities.",
  "However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n\n### Decoder[[nlp-decoder]]",
  "[GPT-2](model_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence. It masks tokens to the right so the model can't \"cheat\" by looking ahead. By pretraining on a massive body of text, GPT-2 became really good at generating text, even if the text is only sometimes accurate or true. But GPT-2 lacked the bidirectional context from BERT's pretraining, which made it unsuitable for certain tasks. [XLNET](model_doc/xlnet) combines the best of both BERT and GPT-2's pretraining objectives by using a permutation language modeling objective (PLM) that allows it to learn bidirectionally.",
  "After GPT-2, language models grew even bigger and are now known as *large language models (LLMs)*. LLMs demonstrate few- or even zero-shot learning if pretrained on a large enough dataset. [GPT-J](model_doc/gptj) is an LLM with 6B parameters and trained on 400B tokens. GPT-J was followed by [OPT](model_doc/opt), a family of decoder-only models, the largest of which is 175B and trained on 180B tokens. [BLOOM](model_doc/bloom) was released around the same time, and the largest model in the family has 176B parameters and is trained on 366B tokens in 46 languages and 13 programming languages.\n\n### Encoder-decoder[[nlp-encoder-decoder]]",
  "[BART](model_doc/bart) keeps the original Transformer architecture, but it modifies the pretraining objective with *text infilling* corruption, where some text spans are replaced with a single `mask` token. The decoder predicts the uncorrupted tokens (future tokens are masked) and uses the encoder's hidden states to help it. [Pegasus](model_doc/pegasus) is similar to BART, but Pegasus masks entire sentences instead of text spans. In addition to masked language modeling, Pegasus is pretrained by gap sentence generation (GSG). The GSG objective masks whole sentences important to a document, replacing them with a `mask` token. The decoder must generate the output from the remaining sentences. [T5](model_doc/t5) is a more unique model that casts all NLP tasks into a text-to-text problem using specific prefixes. For example, the prefix `Summarize:` indicates a summarization task. T5 is pretrained by supervised (GLUE and SuperGLUE) training and self-supervised training (randomly sample and drop out 15% of tokens).\n\n## Audio",
  "<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1\" allowfullscreen></iframe>\n\n### Encoder[[audio-encoder]]\n\n[Wav2Vec2](model_doc/wav2vec2) uses a Transformer encoder to learn speech representations directly from raw audio waveforms. It is pretrained with a contrastive task to determine the true speech representation from a set of false ones. [HuBERT](model_doc/hubert) is similar to Wav2Vec2 but has a different training process. Target labels are created by a clustering step in which segments of similar audio are assigned to a cluster which becomes a hidden unit. The hidden unit is mapped to an embedding to make a prediction.\n\n### Encoder-decoder[[audio-encoder-decoder]]",
  "[Speech2Text](model_doc/speech_to_text) is a speech model designed for automatic speech recognition (ASR) and speech translation. The model accepts log mel-filter bank features extracted from the audio waveform and pretrained autoregressively to generate a transcript or translation. [Whisper](model_doc/whisper) is also an ASR model, but unlike many other speech models, it is pretrained on a massive amount of ✨ labeled ✨ audio transcription data for zero-shot performance. A large chunk of the dataset also contains non-English languages, meaning Whisper can also be used for low-resource languages. Structurally, Whisper is similar to Speech2Text. The audio signal is converted to a log-mel spectrogram encoded by the encoder. The decoder generates the transcript autoregressively from the encoder's hidden states and the previous tokens.\n\n## Multimodal\n\n<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n\n### Encoder[[mm-encoder]]",
  "[VisualBERT](model_doc/visual_bert) is a multimodal model for vision-language tasks released shortly after BERT. It combines BERT and a pretrained object detection system to extract image features into visual embeddings, passed alongside text embeddings to BERT. VisualBERT predicts the masked text based on the unmasked text and the visual embeddings, and it also has to predict whether the text is aligned with the image. When ViT was released, [ViLT](model_doc/vilt) adopted ViT in its architecture because it was easier to get the image embeddings this way. The image embeddings are jointly processed with the text embeddings. From there, ViLT is pretrained by image text matching, masked language modeling, and whole word masking.",
  "[CLIP](model_doc/clip) takes a different approach and makes a pair prediction of (`image`, `text`) . An image encoder (ViT) and a text encoder (Transformer) are jointly trained on a 400 million (`image`, `text`) pair dataset to maximize the similarity between the image and text embeddings of the (`image`, `text`) pairs. After pretraining, you can use natural language to instruct CLIP to predict the text given an image or vice versa. [OWL-ViT](model_doc/owlvit) builds on top of CLIP by using it as its backbone for zero-shot object detection. After pretraining, an object detection head is added to make a set prediction over the (`class`, `bounding box`) pairs.\n\n### Encoder-decoder[[mm-encoder-decoder]]",
  "Optical character recognition (OCR) is a long-standing text recognition task that typically involves several components to understand the image and generate the text. [TrOCR](model_doc/trocr) simplifies the process using an end-to-end Transformer. The encoder is a ViT-style model for image understanding and processes the image as fixed-size patches. The decoder accepts the encoder's hidden states and autoregressively generates text. [Donut](model_doc/donut) is a more general visual document understanding model that doesn't rely on OCR-based approaches. It uses a Swin Transformer as the encoder and multilingual BART as the decoder. Donut is pretrained to read text by predicting the next word based on the image and text annotations. The decoder generates a token sequence given a prompt. The prompt is represented by a special token for each downstream task. For example, document parsing has a special `parsing` token that is combined with the encoder hidden states to parse the document into a structured output format (JSON).\n\n## Reinforcement learning",
  "<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n\n### Decoder[[rl-decoder]]\n\nThe Decision and Trajectory Transformer casts the state, action, and reward as a sequence modeling problem. The [Decision Transformer](model_doc/decision_transformer) generates a series of actions that lead to a future desired return based on returns-to-go, past states, and actions. For the last *K* timesteps, each of the three modalities are converted into token embeddings and processed by a GPT-like model to predict a future action token. [Trajectory Transformer](model_doc/trajectory_transformer) also tokenizes the states, actions, and rewards and processes them with a GPT architecture. Unlike the Decision Transformer, which is focused on reward conditioning, the Trajectory Transformer generates future actions with beam search.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Build your own machine\n\nOne of the most important consideration when building a machine for deep learning is the GPU choice. GPUs are the standard workhorse for deep learning owing to their tensor cores for performing very efficient matrix multiplication and high memory bandwidth. To train large models, you either need a more powerful GPU, multiple GPUs, or take advantage of techniques that offload some of the load to the CPU or NVMe.",
  "This guide provides some practical tips for setting up a GPU for deep learning. For a more detailed discussion and comparison of GPUs, take a look at the [Which GPU(s) to Get for Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/) blog post.\n\n## Power\n\nHigh-end consumer GPUs may have two or three PCIe 8-pin power sockets, and you should make sure you have the same number of 12V PCIe 8-pin cables connected to each socket. Don't use a *pigtail cable*, a single cable with two splits at one end, to connect two sockets or else you won't get full performance from your GPU.\n\nEach PCIe 8-pin power cable should be connected to a 12V rail on the power supply unit (PSU) and can deliver up to 150W. Other GPUs may use a PCIe 12-pin connector which can deliver up to 500-600W. Lower-end GPUs may only use a PCIe 6-pin connector which supplies up to 75W.\n\nIt is important the PSU has stable voltage otherwise it may not be able to supply the GPU with enough power to function properly during peak usage.\n\n## Cooling",
  "An overheated GPU throttles its performance and can even shutdown if it's too hot to prevent damage. Keeping the GPU temperature low, anywhere between 158 - 167F, is essential for delivering full performance and maintaining its lifespan. Once temperatures reach 183 - 194F, the GPU may begin to throttle performance.\n\n## Multi-GPU connectivity\n\nWhen your setup uses multiple GPUs, it is important to consider how they're connected. [NVLink](https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/) connections are faster than PCIe bridges, but you should also consider the [parallelism](./perf_train_gpu_many) strategy you're using. For example, in DistributedDataParallel, GPUs communicate less frequently compared to ZeRO-DP. In this case, a slower connection is not as important.\n\nRun the command below to check how your GPUs are connected.\n\n```bash\nnvidia-smi topo -m\n```\n\n<hfoptions id=\"nvlink\">\n<hfoption id=\"NVLink\">",
  "[NVLink](https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/) is a high-speed communication system designed by NVIDIA for connecting multiple NVIDIA GPUs. Training [openai-community/gpt2](https://huggingface.co/openai-community/gpt2) on a small sample of the [wikitext](https://huggingface.co/datasets/Salesforce/wikitext) dataset is ~23% faster with NVLink.\n\nOn a machine with two GPUs connected with NVLink, an example output of `nvidia-smi topo -m` is shown below.\n\n```bash\nGPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      NV2     0-23            N/A\nGPU1    NV2      X      0-23            N/A\n```\n\n`NV2` indicates `GPU0` and `GPU1` are connected by 2 NVLinks.\n\n</hfoption>\n<hfoption id=\"without NVLink\">\n\nOn a machine with two GPUs connected with a PCIe bridge, an example output of `nvidia-smi topo -m` is shown below.\n\n```bash\nGPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      PHB     0-11            N/A\nGPU1    PHB      X      0-11            N/A\n```\n\n`PHB` indicates `GPU0` and `GPU1` are connected by a PCIe bridge.\n\n</hfoption>\n</hfoptions>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GGUF\n\n[GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) is a file format used to store models for inference with [GGML](https://github.com/ggerganov/ggml), a fast and lightweight inference framework written in C and C++. GGUF is a single-file format containing the model metadata and tensors.\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png\"/>\n</div>\n\nThe GGUF format also supports many quantized data types (refer to [quantization type table](https://hf.co/docs/hub/en/gguf#quantization-types) for a complete list of supported quantization types) which saves a significant amount of memory, making inference with large models like Whisper and Llama feasible on local and edge devices.\n\nTransformers supports loading models stored in the GGUF format for further training or finetuning. The GGUF checkpoint is **dequantized to fp32** where the full model weights are available and compatible with PyTorch.\n\n> [!TIP]\n> Models that support GGUF include Llama, Mistral, Qwen2, Qwen2Moe, Phi3, Bloom, Falcon, StableLM, GPT2, Starcoder2, and [more](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/ggml.py)\n\nAdd the `gguf_file` parameter to [`~PreTrainedModel.from_pretrained`] to specify the GGUF file to load.\n\n```py\n# pip install gguf\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\nfilename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"",
  "torch_dtype = torch.float32 # could be torch.float16 or torch.bfloat16 too\ntokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, torch_dtype=torch_dtype)\n```\n\nOnce you're done tinkering with the model, save and convert it back to the GGUF format with the [convert-hf-to-gguf.py](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) script.\n\n```py\ntokenizer.save_pretrained(\"directory\")\nmodel.save_pretrained(\"directory\")\n\n!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}\n```",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Attention mechanisms\n\nMost transformer models use full attention in the sense that the attention matrix is square. It can be a big\ncomputational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and\nuse a sparse version of the attention matrix to speed up training.\n\n## LSH attention",
  "[Reformer](model_doc/reformer) uses LSH attention. In the softmax(QK^t), only the biggest elements (in the softmax\ndimension) of the matrix QK^t are going to give useful contributions. So for each query q in Q, we can consider only\nthe keys k in K that are close to q. A hash function is used to determine if q and k are close. The attention mask is\nmodified to mask the current token (except at the first position), because it will give a query and a key equal (so\nvery similar to each other). Since the hash can be a bit random, several hash functions are used in practice\n(determined by a n_rounds parameter) and then are averaged together.\n\n## Local attention\n\n[Longformer](model_doc/longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access",
  "all tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n\n<div class=\"flex justify-center\">\n<img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>\n\nUsing those attention matrices with less parameters then allows the model to have inputs having a bigger sequence\nlength.\n\n## Other tricks\n\n### Axial positional encodings\n\n[Reformer](model_doc/reformer) uses axial positional encodings: in traditional transformer models, the positional encoding\nE is a matrix of size \\\\(l\\\\) by \\\\(d\\\\), \\\\(l\\\\) being the sequence length and \\\\(d\\\\) the dimension of the\nhidden state. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate\nthat, axial positional encodings consist of factorizing that big matrix E in two smaller matrices E1 and E2, with\ndimensions \\\\(l_{1} \\times d_{1}\\\\) and \\\\(l_{2} \\times d_{2}\\\\), such that \\\\(l_{1} \\times l_{2} = l\\\\) and",
  "\\\\(d_{1} + d_{2} = d\\\\) (with the product for the lengths, this ends up being way smaller). The embedding for time\nstep \\\\(j\\\\) in E is obtained by concatenating the embeddings for timestep \\\\(j \\% l1\\\\) in E1 and \\\\(j // l1\\\\)\nin E2.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Tokenizers\n\nTokenizers convert text into an array of numbers known as tensors, the inputs to a text model. There are several tokenizer algorithms, but they all share the same purpose. Split text into smaller words or subwords (tokens) according to some rules, and convert them into numbers (input ids). A Transformers tokenizer also returns an attention mask to indicate which tokens should be attended to.\n\n> [!TIP]",
  "> Learn about the most popular tokenization algorithms on the [Summary of the tokenizers](./tokenizer_summary) doc.\n\nCall [`~PreTrainedTokenizer.from_pretrained`] to load a tokenizer and its configuration from the Hugging Face [Hub](https://hf.co) or a local directory. The pretrained tokenizer is saved in a [tokenizer.model](https://huggingface.co/google/gemma-2-2b/blob/main/tokenizer.model) file with all its associated vocabulary files.\n\nPass a string of text to the tokenizer to return the input ids and attention mask, and set the framework tensor type to return with the `return_tensors` parameter.\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\ntokenizer(\"We are very happy to show you the 🤗 Transformers library\", return_tensors=\"pt\")\n{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n156808, 128149,   9581, 235265]]),\n'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n}\n```",
  "Whichever tokenizer you use, make sure the tokenizer vocabulary is the same as the pretrained models tokenizer vocabulary. This is especially important if you're using a custom tokenizer with a different vocabulary from the pretrained models tokenizer.\n\nThis guide provides a brief overview of the tokenizer classes and how to preprocess text with it.\n\n## Tokenizer classes\n\nAll tokenizers inherit from a [`PreTrainedTokenizerBase`] class that provides common methods for all tokenizers like [`~PreTrainedTokenizerBase.from_pretrained`] and [`~PreTrainedTokenizerBase.batch_decode`]. There are two main tokenizer classes that build on top of the base class.\n\n- [`PreTrainedTokenizer`] is a Python implementation, for example [`LlamaTokenizer`].\n- [`PreTrainedTokenizerFast`] is a fast Rust-based implementation from the [Tokenizers](https://hf.co/docs/tokenizers/index) library, for example [`LlamaTokenizerFast`].\n\nThere are two ways you can load a tokenizer, with [`AutoTokenizer`] or a model-specific tokenizer.\n\n<hfoptions id=\"tokenizer-classes\">\n<hfoption id=\"AutoTokenizer\">",
  "The [AutoClass](./model_doc/auto) API is a fast and easy way to load a tokenizer without needing to know whether a Python or Rust-based implementation is available. By default, [`AutoTokenizer`] tries to load a fast tokenizer if it's available, otherwise, it loads the Python implementation.\n\nUse [`~PreTrainedTokenizer.from_pretrained`] to load a tokenizer.\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\ntokenizer(\"We are very happy to show you the 🤗 Transformers library.\", return_tensors=\"pt\")\n{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n156808, 128149,   9581, 235265]]),\n'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n}\n```\n\nLoad your own tokenizer by passing its vocabulary file to [`~AutoTokenizer.from_pretrained`].\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"./model_directory/my_vocab_file.txt\")\n```\n\n</hfoption>\n<hfoption id=\"model-specific tokenizer\">",
  "Each pretrained model is associated with a tokenizer and the specific vocabulary it was trained on. A tokenizer can be loaded directly from the model-specific class.\n\n> [!TIP]\n> Refer to a models API documentation to check whether a fast tokenizer is supported.\n\n```py\nfrom transformers import GemmaTokenizer\n\ntokenizer = GemmaTokenizer.from_pretrained(\"google/gemma-2-2b\")\ntokenizer(\"We are very happy to show you the 🤗 Transformers library.\", return_tensors=\"pt\")\n```\n\nTo load a fast tokenizer, use the fast implementation class.\n\n```py\nfrom transformers import GemmaTokenizerFast\n\ntokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-2-2b\")\ntokenizer(\"We are very happy to show you the 🤗 Transformers library.\", return_tensors=\"pt\")\n```\n\nLoad your own tokenizer by passing its vocabulary file to the `vocab_file` parameter.\n\n```py\nfrom transformers import GemmaTokenizerFast\n\ntokenizer = GemmaTokenizerFast(vocab_file=\"my_vocab_file.txt\")\n```\n\n</hfoption>\n</hfoptions>\n\n## Multimodal tokenizers\n\nIn addition to text tokens, multimodal tokenizers also holds tokens from other modalities as a part of its attributes for easy access.",
  "To add these special tokens to a tokenizer, pass them as a dictionary to the `extra_special_tokens` parameter in [`~AutoTokenizer.from_pretrained`]. The example below adds the `image_token` to a vision-language model.\n\nSave the tokenizer so you can reuse it with direct access to the `image_token`, `boi_token`, and `eoi_token`.\n\n```py\nvision_tokenizer = AutoTokenizer.from_pretrained(\n\"llava-hf/llava-1.5-7b-hf\",\nextra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n)\nprint(vision_tokenizer.image_token, vision_tokenizer.image_token_id)\n(\"<image>\", 32000)\n\nvision_tokenizer.save_pretrained(\"./path/to/tokenizer\")\n```\n\n## Fast tokenizers\n\n<Youtube id=\"3umI3tm27Vw\"/>\n\n[`PreTrainedTokenizerFast`] or *fast tokenizers* are Rust-based tokenizers from the [Tokenizers](https://hf.co/docs/tokenizers) library. It is significantly faster at batched tokenization and provides additional alignment methods compared to the Python-based tokenizers.\n\n[`AutoTokenizer`] automatically loads a fast tokenizer if it's supported. Otherwise, you need to explicitly load the fast tokenizer.",
  "This section will show you how to train a fast tokenizer and reuse it in Transformers.\n\nTo train a Byte-Pair Encoding (BPE) tokenizer, create a [`~tokenizers.Tokenizer`] and [`~tokenizers.trainers.BpeTrainer`] class and define the unknown token and special tokens.\n\n```py\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\n\ntokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n```\n\nSplit the tokens on [`~tokenizers.pre_tokenizers.Whitespace`] to create tokens that don't overlap with each other.\n\n```py\nfrom tokenizers.pre_tokenizers import Whitespace\n\ntokenizer.pre_tokenizer = Whitespace()\n```\n\nCall [`~tokenizers.Tokenizer.train`] on the text files and trainer to start training.\n\n```py\nfiles = [...]\ntokenizer.train(files, trainer)\n```\n\nUse [`~tokenizers.Tokenizer.save`] to save the tokenizers configuration and vocabulary to a JSON file.\n\n```py\ntokenizer.save(\"tokenizer.json\")\n```\n\nNow you can load and reuse the tokenizer object in Transformers by passing it to the `tokenizer_object` parameter in [`PreTrainedTokenizerFast`].\n\n```py",
  "from transformers import PreTrainedTokenizerFast\n\nfast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n```\n\nTo load a saved tokenizer from its JSON file, pass the file path to the `tokenizer_file` parameter in [`PreTrainedTokenizerFast`].\n\n```py\nfrom transformers import PreTrainedTokenizerFast\n\nfast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n```\n\n## tiktoken\n\n[tiktoken](https://github.com/openai/tiktoken) is a [byte-pair encoding (BPE)](./tokenizer_summary#byte-pair-encoding-bpe) tokenizer by OpenAI. It includes several tokenization schemes or encodings for how text should be tokenized.\n\nThere are currently two models trained and released with tiktoken, GPT2 and Llama3. Transformers supports models with a [tokenizer.model](https://hf.co/meta-llama/Meta-Llama-3-8B/blob/main/original/tokenizer.model) tiktoken file. The tiktoken file is automatically converted into Transformers Rust-based [`PreTrainedTokenizerFast`].\n\nAdd the `subfolder` parameter to [`~PreTrainedModel.from_pretrained`] to specify where the `tokenizer.model` tiktoken file is located.\n\n```py\nfrom transformers import AutoTokenizer",
  "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", subfolder=\"original\")\n```\n\n### Create a tiktoken tokenizer\n\nThe tiktoken `tokenizer.model` file contains no information about additional tokens or pattern strings. If these are important, convert the tokenizer to `tokenizer.json` (the appropriate format for [`PreTrainedTokenizerFast`]).\n\nGenerate the tiktoken `tokenizer.model` file with the [tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63) function, and convert it to `tokenizer.json` with [convert_tiktoken_to_fast](https://github.com/huggingface/transformers/blob/99e0ab6ed888136ea4877c6d8ab03690a1478363/src/transformers/integrations/tiktoken.py#L8).\n\n```py\nfrom transformers.integrations.tiktoken import convert_tiktoken_to_fast\nfrom tiktoken import get_encoding\n\n# Load your custom encoding or the one provided by OpenAI\nencoding = get_encoding(\"gpt2\")\nconvert_tiktoken_to_fast(encoding, \"config/save/dir\")\n```\n\nThe resulting `tokenizer.json` file is saved to the specified directory and loaded with [`~PreTrainedTokenizerFast.from_pretrained`].\n\n```py",
  "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"config/save/dir\")\n```\n\n## Preprocess\n\n<Youtube id=\"Yffk5aydLzg\"/>\n\nA Transformers model expects the input to be a PyTorch, TensorFlow, or NumPy tensor. A tokenizers job is to preprocess text into those tensors. Specify the framework tensor type to return with the `return_tensors` parameter.\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\ntokenizer(\"We are very happy to show you the 🤗 Transformers library.\", return_tensors=\"pt\")\n{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,\n156808, 128149,   9581, 235265]]),\n'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n}\n```\n\nThe tokenization process of converting text into input ids is completed in two steps.\n\n<hfoptions id=\"steps\">\n<hfoption id=\"1. tokenize\">\n\nIn the first step, a string of text is split into tokens by the [`~PreTrainedTokenizer.tokenize`] function. How the text is split depends on the tokenization algorithm.\n\n```py\ntokens = tokenizer.tokenize(\"We are very happy to show you the 🤗 Transformers library\")\nprint(tokens)",
  "['We', '▁are', '▁very', '▁happy', '▁to', '▁show', '▁you', '▁the', '▁🤗', '▁Transformers', '▁library']\n```\n\nGemma uses a [SentencePiece](./tokenizer_summary#sentencepiece) tokenizer which replaces spaces with an underscore `_`.\n\n</hfoption>\n<hfoption id=\"2. convert tokens to ids\">\n\nIn the second step, the tokens are converted into ids with [`~PreTrainedTokenizer.convert_tokens_to_ids`].\n\n```py\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\n[1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]\n```\n\n</hfoption>\n<hfoption id=\"3. decode ids to text\">\n\nLastly, the model prediction typically generates numerical outputs which are converted back to text with [`~PreTrainedTokenizer.decode`].\n\n```py\ndecoded_string = tokenizer.decode(ids)\nprint(decoded_string)\n'We are very happy to show you the 🤗 Transformers library'\n```\n\n</hfoption>\n</hfoptions>\n\n> [!TIP]\n> Visualize how different tokenizers work in the [Tokenizer Playground](https://xenova-the-tokenizer-playground.static.hf.space).\n\n### Special tokens\n\nSpecial tokens provide the model with some additional information about the text.",
  "For example, if you compare the tokens obtained from passing text directly to the tokenizer and from [`~PreTrainedTokenizer.convert_tokens_to_ids`], you'll notice some additional tokens are added.\n\n```py\nmodel_inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n[2, 1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]\ntokenizer.convert_tokens_to_ids(tokens)\n[1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]\n```\n\nWhen you [`~PreTrainedTokenizer.decode`] the ids, you'll see `<bos>` at the beginning of the string. This is used to indicate the beginning of a sentence to the model.\n\n```py\nprint(tokenizer.decode(model_inputs[\"input_ids\"]))\nprint(tokenizer.decode(ids))\n'<bos>We are very happy to show you the 🤗 Transformers library.'\n'We are very happy to show you the 🤗 Transformers library'\n```\n\nNot all models need special tokens, but if they do, a tokenizer automatically adds them.\n\n### Batch tokenization\n\nIt is faster and more efficient to preprocess *batches* of text instead of a single sentence at a time. Fast tokenizers are especially good at parallelizing tokenization.\n\nPass a list of string text to the tokenizer.\n\n```py",
  "batch_sentences = [\n\"But what about second breakfast?\",\n\"Don't think he knows about second breakfast, Pip.\",\n\"What about elevensies?\",\n]\nencoded_inputs = tokenizer(batch_sentences, return_tensors=\"pt\")\nprint(encoded_inputs)\n{\n'input_ids':\n[[2, 1860, 1212, 1105, 2257, 14457, 235336],\n[2, 4454, 235303, 235251, 1742, 693, 9242, 1105, 2257, 14457, 235269, 48782, 235265],\n[2, 1841, 1105, 29754, 37453, 235336]],\n'attention_mask': [[1, 1, 1, 1, 1, 1, 1],\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n[1, 1, 1, 1, 1, 1]]\n}\n```\n\n### Padding\n\n> [!TIP]\n> Learn about additional padding strategies in the [Padding and truncation](./pad_truncation) guide.\n\nIn the output above, the `input_ids` have different lengths. This is an issue because Transformers expects them to have the same lengths so it can pack them into a batch. Sequences with uneven lengths can't be batched.\n\nPadding adds a special *padding token* to ensure all sequences have the same length. Set `padding=True` to pad the sequences to the longest sequence length in the batch.\n\n```py\nencoded_inputs = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")\nprint(encoded_inputs)\n```",
  "The tokenizer added the special padding token `0` to the left side (*left padding*) because Gemma and LLMs in general are not trained to continue generation from a padding token.\n\n### Truncation\n\n> [!TIP]\n> Learn about additional truncation strategies in the [Padding and truncation](./pad_truncation) guide.\n\nModels are only able to process sequences up to a certain length. If you try to process a sequence longer than a model can handle, it crashes.\n\nTruncation removes tokens from a sequence to ensure it doesn't exceed the maximum length. Set `truncation=True` to truncate a sequence to the maximum length accepted by the model. You can also set the maximum length yourself with the `max_length` parameter.\n\n```py\nencoded_inputs = tokenizer(batch_sentences, max_length=8, truncation=True, return_tensors=\"pt\")\nprint(encoded_inputs)\n```",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPU selection",
  "During distributed training, you can specify the number of GPUs to use and in what order. This can be useful when you have GPUs with different computing power and you want to use the faster GPU first. Or you could only use a subset of the available GPUs. The selection process works for both [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) and [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html). You don't need Accelerate or [DeepSpeed integration](./main_classes/deepspeed).\n\nThis guide will show you how to select the number of GPUs to use and the order to use them in.\n\n## Number of GPUs\n\nFor example, if there are 4 GPUs and you only want to use the first 2, run the command below.\n\n<hfoptions id=\"select-gpu\">\n<hfoption id=\"torchrun\">\n\nUse the `--nproc_per_node` to select how many GPUs to use.\n\n```bash\ntorchrun --nproc_per_node=2  trainer-program.py ...\n```\n\n</hfoption>\n<hfoption id=\"Accelerate\">\n\nUse `--num_processes` to select how many GPUs to use.\n\n```bash\naccelerate launch --num_processes 2 trainer-program.py ...\n```\n\n</hfoption>\n<hfoption id=\"DeepSpeed\">",
  "Use `--num_gpus` to select how many GPUs to use.\n\n```bash\ndeepspeed --num_gpus 2 trainer-program.py ...\n```\n\n</hfoption>\n</hfoptions>\n\n### Order of GPUs\n\nTo select specific GPUs to use and their order, configure the the `CUDA_VISIBLE_DEVICES` environment variable. It is easiest to set the environment variable in `~/bashrc` or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs are used. For example, if there are 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2:\n\n```bash\nCUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...\n```\n\nOnly the 2 physical GPUs (0 and 2) are \"visible\" to PyTorch and these are mapped to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs to use 2 first. The mapping becomes `cuda:1` for GPU 0 and `cuda:0` for GPU 2.\n\n```bash\nCUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n```\n\nYou can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without GPUs.\n\n```bash\nCUDA_VISIBLE_DEVICES= python trainer-program.py ...\n```\n\n> [!WARNING]",
  "> As with any environment variable, they can be exported instead of being added to the command line. However, this is not recommended because it can be confusing if you forget how the environment variable was set up and you end up using the wrong GPUs. Instead, it is common practice to set the environment variable for a specific training run on the same command line.\n\n`CUDA_DEVICE_ORDER` is an alternative environment variable you can use to control how the GPUs are ordered. You can order according to the following.\n\n1. PCIe bus IDs that matches the order of [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface) and [`rocm-smi`](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html) for NVIDIA and AMD GPUs respectively.\n\n```bash\nexport CUDA_DEVICE_ORDER=PCI_BUS_ID\n```\n\n2. GPU compute ability.\n\n```bash\nexport CUDA_DEVICE_ORDER=FASTEST_FIRST\n```",
  "The `CUDA_DEVICE_ORDER` is especially useful if your training setup consists of an older and newer GPU, where the older GPU appears first, but you cannot physically swap the cards to make the newer GPU appear first. In this case, set `CUDA_DEVICE_ORDER=FASTEST_FIRST` to always use the newer and faster GPU first (`nvidia-smi` or `rocm-smi` still reports the GPUs in their PCIe order). Or you could also set `export CUDA_VISIBLE_DEVICES=1,0`.",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Multimodal templates\n\nMultimodal model chat templates expect a similar [template](./chat_templating) as text-only models. It needs `messages` that includes a dictionary of the `role` and `content`.\n\nMultimodal templates are included in the [Processor](./processors) class and requires an additional `type` key for specifying whether the included content is an image, video, or text.",
  "This guide will show you how to format chat templates for multimodal models as well as some best practices for configuring the template\n\n## ImageTextToTextPipeline\n\n[`ImageTextToTextPipeline`] is a high-level image and text generation class with a “chat mode”. Chat mode is enabled when a conversational model is detected and the chat prompt is [properly formatted](./llm_tutorial#wrong-prompt-format).\n\nStart by building a chat history with the following two roles.\n\n- `system` describes how the model should behave and respond when you’re chatting with it. This role isn’t supported by all chat models.\n- `user` is where you enter your first message to the model.\n\n```py\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n{\"type\": \"text\", \"text\": \"What are these?\"},\n],\n},\n]\n```",
  "Create a [`ImageTextToTextPipeline`] and pass the chat to it. For large models, setting [device_map=“auto”](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Changing the data type to [torch.bfloat16](./models#model-data-type) also helps save memory.\n\n> [!TIP]\n> The [`ImageTextToTextPipeline`] accepts chats in the OpenAI format to make inference easier and more accessible.\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device=\"cuda\", torch_dtype=torch.float16)\npipeline(text=messages, max_new_tokens=50, return_full_text=False)\n[{'input_text': [{'role': 'system',\n'content': [{'type': 'text',\n'text': 'You are a friendly chatbot who always responds in the style of a pirate'}]},\n{'role': 'user',\n'content': [{'type': 'image',\n'url': 'http://images.cocodataset.org/val2017/000000039769.jpg'},\n{'type': 'text', 'text': 'What are these?'}]}],",
  "'generated_text': 'The image shows two cats lying on a pink surface, which appears to be a cushion or a soft blanket. The cat on the left has a striped coat, typical of tabby cats, and is lying on its side with its head resting on the'}]\n```\n\n## Image inputs\n\nFor multimodal models that accept images like [LLaVA](./model_doc/llava), include the following in `content` as shown below.\n\n- The content `\"type\"` can be an `\"image\"` or `\"text\"`.\n- For images, it can be a link to the image (`\"url\"`), a file path (`\"path\"`), or `\"base64\"`. Images are automatically loaded, processed, and prepared into pixel values as inputs to the model.\n\n```python\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},",
  "{\"type\": \"text\", \"text\": \"What are these?\"},\n],\n},\n]\n```\n\nPass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content and return the `input_ids` and `pixel_values`.\n\n```py\nprocessed_chat = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\nprint(processed_chat.keys())\n```\n\nThese inputs are now ready to be used in [`~GenerationMixin.generate`].\n\n## Video inputs\n\nSome vision models also support video inputs. The message format is very similar to the format for [image inputs](#image-inputs).\n\n- The content `\"type\"` should be `\"video\"` to indicate the the content is a video.\n- For videos, it can be a link to the video (`\"url\"`) or it could be a file path (`\"path\"`). Videos loaded from a URL can only be decoded with [PyAV](https://pyav.basswood-io.com/docs/stable/) or [Decord](https://github.com/dmlc/decord).\n\n> [!WARNING]\n> Loading a video from `\"url\"` is only supported by the PyAV or Decord backends.\n\n```python\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n\nmodel_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"",
  "model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"},\n{\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n],\n},\n]\n```\n\nPass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content. There are a few extra parameters to include in [`~ProcessorMixin.apply_chat_template`] that controls the sampling process.\n\nThe `video_load_backend` parameter refers to a specific framework to load a video. It supports [PyAV](https://pyav.basswood-io.com/docs/stable/), [Decord](https://github.com/dmlc/decord), [OpenCV](https://github.com/opencv/opencv), and [torchvision](https://pytorch.org/vision/stable/index.html).\n\nThe examples below uses Decord as the backend because it is a bit faster than PyAV.\n\n<hfoptions id=\"sampling\">\n<hfoption id=\"fixed number of frames\">",
  "The `num_frames` parameter controls how many frames to uniformly sample from the video. Each checkpoint has a maximum frame count it was pretrained with and exceeding this count can significantly lower generation quality. It's important to choose a frame count that fits both the model capacity and your hardware resources. If `num_frames` isn't specified, the entire video is loaded without any frame sampling.\n\n\n```python\nprocessed_chat = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\nnum_frames=32,\nvideo_load_backend=\"decord\",\n)\nprint(processed_chat.keys())\n```\n\nThese inputs are now ready to be used in [`~GenerationMixin.generate`].\n\n</hfoption>\n<hfoption id=\"fps\">\n\nFor longer videos, it may be better to sample more frames for better representation with the `video_fps` parameter. This determines how many frames per second to extract. As an example, if a video is 10 seconds long and `video_fps=2`, then the model samples 20 frames. In other words, 2 frames are uniformly sampled every 10 seconds.\n\n```py\nprocessed_chat = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,",
  "return_dict=True,\nvideo_fps=32,\nvideo_load_backend=\"decord\",\n)\nprint(processed_chat.keys())\n```\n\n</hfoption>\n<hfoption id=\"custom frame sampling\">\n\nSome models don't sample frames *uniformly* and require more complex logic to determine which frames to use. For example, the model may have an *adaptive frame selection* or if the model prioritizes *key moments* in a video rather than evenly spaced frames.\n\nIf a model has a different sampling strategy, you can write a function that customizes frame selection. The function should include the following requirements.\n\n- Use the `sample_indices_fn` parameter to pass a callable function for sampling.\n- If provided, this function *overrides* the standard `num_frames` and `fps` parameters.\n- The function receives all the parameters passed to `load_video` and must return valid frame indices to sample from.\n\nAn example function is shown below. This gives you full control over frame selection, making the model more adaptable to different video scenarios.\n\n```py\ndef sample_indices_fn(metadata, **kwargs):\n# samples only the first and the second frame\nreturn [0, 1]\n\nprocessed_chat = processor.apply_chat_template(\nmessages,",
  "add_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nsample_indices_fn=sample_indices_fn,\nvideo_load_backend=\"decord\",\n)\nprint(processed_chat.keys())\n```\n\n</hfoption>\n<hfoption id=\"list of image frames\">\n\nVideos may also exist as a set of sampled frames stored as images rather than the full video file.\n\nIn this case, pass a list of image file paths and the processor automatically concatenates them into a video. Make sure all images are the same size since they are assumed to be from the same video.\n\n```py\nframes_paths = [\"/path/to/frame0.png\", \"/path/to/frame5.png\", \"/path/to/frame10.png\"]\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"path\": frames_paths},\n{\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n],\n},\n]\n\nprocessed_chat = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\n)\nprint(processed_chat.keys())\n```\n\n</hfoption>\n</hfoptions>\n\n## Template configuration",
  "You can create a custom chat template with [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) and set it with [`~ProcessorMixin.apply_chat_template`]. Refer to the [Template writing](./chat_templating_writing) guide for more details.\n\nFor example, to enable a template to handle a *list of content* from multiple modalities while still supporting plain strings for text-only inference, specify how to handle the `content['type']` if it is an image or text as shown below in the Llama 3.2 Vision Instruct [template](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/blob/main/chat_template.json).\n\n```jinja\n{% for message in messages %}\n{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n{% if message['content'] is string %}\n{{ message['content'] }}\n{% else %}\n{% for content in message['content'] %}\n{% if content['type'] == 'image' %}\n{{ '<|image|>' }}\n{% elif content['type'] == 'text' %}\n{{ content['text'] }}\n{% endif %}\n{% endfor %}\n{% endif %}\n{{ '<|eot_id|>' }}\n{% endfor %}\n{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CPU\n\nA modern CPU is capable of efficiently training large models by leveraging the underlying optimizations built into the hardware and training on fp16 or bf16 data types.\n\nThis guide focuses on how to train large models on an Intel CPU using mixed precision and the [Intel Extension for PyTorch (IPEX)](https://intel.github.io/intel-extension-for-pytorch/index.html) library.\n\nYou can Find your PyTorch version by running the command below.\n\n```bash\npip list | grep torch\n```",
  "Install IPEX with the PyTorch version from above.\n\n```bash\npip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n> [!TIP]\n> Refer to the IPEX [installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation) guide for more details.\n\nIPEX provides additional performance optimizations for Intel CPUs. These include additional CPU instruction level architecture (ISA) support such as [Intel AVX512-VNNI](https://en.wikichip.org/wiki/x86/avx512_vnni) and [Intel AMX](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/what-is-intel-amx.html). Both of these features are designed to accelerate matrix multiplication. Older AMD and Intel CPUs with only Intel AVX2, however, aren't guaranteed better performance with IPEX.",
  "IPEX also supports [Auto Mixed Precision (AMP)](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html) training with the fp16 and bf16 data types. Reducing precision speeds up training and reduces memory usage because it requires less computation. The loss in accuracy from using full-precision is minimal. 3rd, 4th, and 5th generation Intel Xeon Scalable processors natively support bf16, and the 6th generation processor also natively supports fp16 in addition to bf16.\n\nAMP is enabled for CPU backends training with PyTorch.\n\n[`Trainer`] supports AMP training with a CPU by adding the `--use_cpu`, `--use_ipex`, and `--bf16` parameters. The example below demonstrates the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) script.\n\n```bash\npython run_qa.py \\\n--model_name_or_path google-bert/bert-base-uncased \\\n--dataset_name squad \\\n--do_train \\\n--do_eval \\\n--per_device_train_batch_size 12 \\\n--learning_rate 3e-5 \\\n--num_train_epochs 2 \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/debug_squad/ \\\n--use_ipex \\\n--bf16 \\\n--use_cpu\n```",
  "These parameters can also be added to [`TrainingArguments`] as shown below.\n\n```py\ntraining_args = TrainingArguments(\noutput_dir=\"./outputs\",\nbf16=True,\nuse_ipex=True,\nuse_cpu=True,\n)\n```\n\n## Resources\n\nLearn more about training on Intel CPUs in the [Accelerating PyTorch Transformers with Intel Sapphire Rapids](https://huggingface.co/blog/intel-sapphire-rapids) blog post.",
  "<!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Model training anatomy\n\nTo understand performance optimization techniques that one can apply to improve efficiency of model training\nspeed and memory utilization, it's helpful to get familiar with how GPU is utilized during training, and how compute\nintensity varies depending on an operation performed.\n\nLet's start by exploring a motivating example of GPU utilization and the training run of a model. For the demonstration,\nwe'll need to install a few libraries:\n\n```bash\npip install transformers datasets accelerate nvidia-ml-py3\n```",
  "The `nvidia-ml-py3` library allows us to monitor the memory usage of the models from within Python. You might be familiar\nwith the `nvidia-smi` command in the terminal - this library allows to access the same information in Python directly.\n\nThen, we create some dummy data: random token IDs between 100 and 30000 and binary labels for a classifier.\nIn total, we get 512 sequences each with length 512 and store them in a [`~datasets.Dataset`] with PyTorch format.\n\n\n```py\n>>> import numpy as np\n>>> from datasets import Dataset\n\n\n>>> seq_len, dataset_size = 512, 512\n>>> dummy_data = {\n...     \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n...     \"labels\": np.random.randint(0, 2, (dataset_size)),\n... }\n>>> ds = Dataset.from_dict(dummy_data)\n>>> ds.set_format(\"pt\")\n```\n\nTo print summary statistics for the GPU utilization and the training run with the [`Trainer`] we define two helper functions:\n\n```py\n>>> from pynvml import *\n\n\n>>> def print_gpu_utilization():\n...     nvmlInit()\n...     handle = nvmlDeviceGetHandleByIndex(0)\n...     info = nvmlDeviceGetMemoryInfo(handle)\n...     print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\n>>> def print_summary(result):",
  "...     print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n...     print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n...     print_gpu_utilization()\n```\n\nLet's verify that we start with a free GPU memory:\n\n```py\n>>> print_gpu_utilization()\nGPU memory occupied: 0 MB.\n```\n\nThat looks good: the GPU memory is not occupied as we would expect before we load any models. If that's not the case on\nyour machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by\nthe user. When a model is loaded to the GPU the kernels are also loaded, which can take up 1-2GB of memory. To see how\nmuch it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well.\n\n```py\n>>> import torch\n\n\n>>> torch.ones((1, 1)).to(\"cuda\")\n>>> print_gpu_utilization()\nGPU memory occupied: 1343 MB.\n```\n\nWe see that the kernels alone take up 1.3GB of GPU memory. Now let's see how much space the model uses.\n\n## Load Model\n\nFirst, we load the `google-bert/bert-large-uncased` model. We load the model weights directly to the GPU so that we can check\nhow much space just the weights use.\n\n\n```py",
  ">>> from transformers import AutoModelForSequenceClassification\n\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-large-uncased\").to(\"cuda\")\n>>> print_gpu_utilization()\nGPU memory occupied: 2631 MB.\n```\n\nWe can see that the model weights alone take up 1.3 GB of GPU memory. The exact number depends on the specific\nGPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an\noptimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result\nas with `nvidia-smi` CLI:\n\n\n```bash\nnvidia-smi\n```\n\n```bash\nTue Jan 11 08:58:05 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |",
  "|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |\n+-----------------------------------------------------------------------------+\n```\n\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can\nstart training the model and see how the GPU memory consumption changes. First, we set up a few standard training",
  "arguments:\n\n```py\ndefault_args = {\n\"output_dir\": \"tmp\",\n\"eval_strategy\": \"steps\",\n\"num_train_epochs\": 1,\n\"log_level\": \"error\",\n\"report_to\": \"none\",\n}\n```\n\n<Tip>\n\nIf you plan to run multiple experiments, in order to properly clear the memory between experiments, restart the Python\nkernel between experiments.\n\n</Tip>\n\n## Memory utilization at vanilla training\n\nLet's use the [`Trainer`] and train the model without using any GPU performance optimization techniques and a batch size of 4:\n\n```py\n>>> from transformers import TrainingArguments, Trainer, logging\n\n>>> logging.set_verbosity_error()\n\n\n>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n>>> result = trainer.train()\n>>> print_summary(result)\n```\n\n```\nTime: 57.82\nSamples/second: 8.86\nGPU memory occupied: 14949 MB.\n```\n\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. However, a larger batch size\ncan often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our",
  "model's needs and not to the GPU limitations. What's interesting is that we use much more memory than the size of the model.\nTo understand a bit better why this is the case let's have a look at a model's operations and memory needs.\n\n## Anatomy of Model's Operations\n\nTransformers architecture includes 3 main groups of operations grouped below by compute-intensity.\n\n1. **Tensor Contractions**\n\nLinear layers and components of Multi-Head Attention all do batched **matrix-matrix multiplications**. These operations are the most compute-intensive part of training a transformer.\n\n2. **Statistical Normalizations**\n\nSoftmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more **reduction operations**, the result of which is then applied via a map.\n\n3. **Element-wise Operators**\n\nThese are the remaining operators: **biases, dropout, activations, and residual connections**. These are the least compute-intensive operations.\n\nThis knowledge can be helpful to know when analyzing performance bottlenecks.\n\nThis summary is derived from [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://arxiv.org/abs/2007.00072)",
  "## Anatomy of Model's Memory\n\nWe've seen that training the model uses much more memory than just putting the model on the GPU. This is because there\nare many components during training that use GPU memory. The components on GPU memory are the following:\n\n1. model weights\n2. optimizer states\n3. gradients\n4. forward activations saved for gradient computation\n5. temporary buffers\n6. functionality-specific memory\n\nA typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For\ninference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per\nmodel parameter for mixed precision inference, plus activation memory.\n\nLet's look at the details.\n\n**Model Weights:**\n\n- 4 bytes * number of parameters for fp32 training\n- 6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)\n\n**Optimizer States:**\n\n- 8 bytes * number of parameters for normal AdamW (maintains 2 states)\n- 2 bytes * number of parameters for 8-bit AdamW optimizers like [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes)",
  "- 4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)\n\n**Gradients**\n\n- 4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)\n\n**Forward Activations**\n\n- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n\nThere are the input and output that are being passed and returned by the forward and the backward functions and the\nforward activations saved for gradient computation.\n\n**Temporary Memory**\n\nAdditionally, there are all kinds of temporary variables which get released once the calculation is done, but in the\nmoment these could require additional memory and could push to OOM. Therefore, when coding it's crucial to think\nstrategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.\n\n**Functionality-specific memory**\n\nThen, your software could have special memory needs. For example, when generating text using beam search, the software\nneeds to maintain multiple copies of inputs and outputs.\n\n**`forward` vs `backward` Execution Speed**",
  "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates\ninto ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually\nbandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\nand writes once, gradInput).\n\nAs you can see, there are potentially a few places where we could save GPU memory or speed up operations.\nNow that you understand what affects GPU utilization and computation speed, refer to\nthe [Methods and tools for efficient training on a single GPU](perf_train_gpu_one) documentation page to learn about\nperformance optimization techniques.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Machine learning apps\n\n[Gradio](https://www.gradio.app/), a fast and easy library for building and sharing machine learning apps, is integrated with [`Pipeline`] to quickly create a simple interface for inference.\n\nBefore you begin, make sure Gradio is installed.\n\n```py\n!pip install gradio\n```",
  "Create a pipeline for your task, and then pass it to Gradio's [Interface.from_pipeline](https://www.gradio.app/docs/gradio/interface#interface-from_pipeline) function to create the interface. Gradio automatically determines the appropriate input and output components for a [`Pipeline`].\n\nAdd [launch](https://www.gradio.app/main/docs/gradio/blocks#blocks-launch) to create a web server and start up the app.\n\n```py\nfrom transformers import pipeline\nimport gradio as gr\n\npipeline = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\ngr.Interface.from_pipeline(pipeline).launch()\n```\n\nThe web app runs on a local server by default. To share the app with other users, set `share=True` in [launch](https://www.gradio.app/main/docs/gradio/blocks#blocks-launch) to generate a temporary public link. For a more permanent solution, host the app on Hugging Face [Spaces](https://hf.co/spaces).\n\n```py\ngr.Interface.from_pipeline(pipeline).launch(share=True)\n```\n\nThe Space below is created with the code above and hosted on Spaces.\n\n<iframe\nsrc=\"https://stevhliu-gradio-pipeline-demo.hf.space\"\nframeborder=\"0\"\nwidth=\"850\"\nheight=\"850\"\n></iframe>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Perplexity of fixed-length models\n\n[[open-in-colab]]\n\nPerplexity (PPL) is one of the most common metrics for evaluating language models. Before diving in, we should note\nthat the metric applies specifically to classical language models (sometimes called autoregressive or causal language\nmodels) and is not well defined for masked language models like BERT (see [summary of the models](model_summary)).",
  "Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. If we have a tokenized\nsequence \\\\(X = (x_0, x_1, \\dots, x_t)\\\\), then the perplexity of \\\\(X\\\\) is,\n\n$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$\n\nwhere \\\\(\\log p_\\theta (x_i|x_{<i})\\\\) is the log-likelihood of the ith token conditioned on the preceding tokens \\\\(x_{<i}\\\\) according to our model. Intuitively, it can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus. Importantly, this means that the tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.\n\nThis is also equivalent to the exponentiation of the cross-entropy between the data and model predictions. For more\nintuition about perplexity and its relationship to Bits Per Character (BPC) and data compression, check out this\n[fantastic blog post on The Gradient](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/).\n\n## Calculating PPL with fixed-length models",
  "If we weren't limited by a model's context size, we would evaluate the model's perplexity by autoregressively\nfactorizing a sequence and conditioning on the entire preceding subsequence at each step, as shown below.\n\n<img width=\"600\" alt=\"Full decomposition of a sequence with unlimited context length\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif\"/>\n\nWhen working with approximate models, however, we typically have a constraint on the number of tokens the model can\nprocess. The largest version of [GPT-2](model_doc/gpt2), for example, has a fixed length of 1024 tokens, so we\ncannot calculate \\\\(p_\\theta(x_t|x_{<t})\\\\) directly when \\\\(t\\\\) is greater than 1024.\n\nInstead, the sequence is typically broken into subsequences equal to the model's maximum input size. If a model's max\ninput size is \\\\(k\\\\), we then approximate the likelihood of a token \\\\(x_t\\\\) by conditioning only on the\n\\\\(k-1\\\\) tokens that precede it rather than the entire context. When evaluating the model's perplexity of a\nsequence, a tempting but suboptimal approach is to break the sequence into disjoint chunks and add up the decomposed",
  "log-likelihoods of each segment independently.\n\n<img width=\"600\" alt=\"Suboptimal PPL not taking advantage of full available context\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif\"/>\n\nThis is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor\napproximation of the fully-factorized perplexity and will typically yield a higher (worse) PPL because the model will\nhave less context at most of the prediction steps.\n\nInstead, the PPL of fixed-length models should be evaluated with a sliding-window strategy. This involves repeatedly\nsliding the context window so that the model has more context when making each prediction.\n\n<img width=\"600\" alt=\"Sliding window PPL taking advantage of all available context\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif\"/>\n\nThis is a closer approximation to the true decomposition of the sequence probability and will typically yield a more\nfavorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good",
  "practical compromise is to employ a strided sliding window, moving the context by larger strides rather than sliding by\n1 token a time. This allows computation to proceed much faster while still giving the model a large context to make\npredictions at each step.\n\n## Example: Calculating perplexity with GPT-2 in 🤗 Transformers\n\nLet's demonstrate this process with GPT-2.\n\n```python\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom accelerate.test_utils.testing import get_backend\n\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\nmodel_id = \"openai-community/gpt2-large\"\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ntokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n```\n\nWe'll load in the WikiText-2 dataset and evaluate the perplexity using a few different sliding-window strategies. Since\nthis dataset is small and we're just doing one forward pass over the set, we can just load and encode the entire\ndataset in memory.\n\n```python\nfrom datasets import load_dataset\n\ntest = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")",
  "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n```\n\nWith 🤗 Transformers, we can simply pass the `input_ids` as the `labels` to our model, and the average negative\nlog-likelihood for each token is returned as the loss. With our sliding window approach, however, there is overlap in\nthe tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\navailable to condition on).\n\n```python\nimport torch\nfrom tqdm import tqdm\n\nmax_length = model.config.n_positions\nstride = 512\nseq_len = encodings.input_ids.size(1)\n\nnll_sum = 0.0\nn_tokens = 0\nprev_end_loc = 0\nfor begin_loc in tqdm(range(0, seq_len, stride)):\nend_loc = min(begin_loc + max_length, seq_len)\ntrg_len = end_loc - prev_end_loc  # may be different from stride on last loop",
  "input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\ntarget_ids = input_ids.clone()\ntarget_ids[:, :-trg_len] = -100\n\nwith torch.no_grad():\noutputs = model(input_ids, labels=target_ids)\n\n# loss is calculated using CrossEntropyLoss which averages over valid labels\n# N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n# to the left by 1.\nneg_log_likelihood = outputs.loss\n\n# Accumulate the total negative log-likelihood and the total number of tokens\nnum_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\nbatch_size = target_ids.size(0)\nnum_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\nnll_sum += neg_log_likelihood * num_loss_tokens\nn_tokens += num_loss_tokens\n\nprev_end_loc = end_loc\nif end_loc == seq_len:\nbreak\n\navg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\nppl = torch.exp(avg_nll)\n```\n\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window",
  "strategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\nand the better the reported perplexity will typically be.\n\nWhen we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\nstrategy, this jumps down to `16.44`. This is not only a more favorable score, but is calculated in a way that is\ncloser to the true autoregressive decomposition of a sequence likelihood.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n> [!WARNING]\n> Agents and tools are being spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. These docs will be deprecated in the future!\n\n# Agents\n\n[[open-in-colab]]\n\nAn agent is a system where a large language model (LLM) can execute more complex tasks through *planning* and using *tools*.",
  "- Planning helps a LLM reason its way through a task by breaking it down into smaller subtasks. For example, [`CodeAgent`] plans a series of actions to take and then generates Python code to execute all the actions at once.\n\nAnother planning method is by self-reflection and refinement of its previous actions to improve its performance. The [`ReactJsonAgent`] is an example of this type of planning, and it's based on the [ReAct](https://hf.co/papers/2210.03629) framework. This agent plans and executes actions one at a time based on the feedback it receives from each action.\n\n- Tools give a LLM access to external functions or APIs that it can use to help it complete a task. For example, [gradio-tools](https://github.com/freddyaboulton/gradio-tools) gives a LLM access to any of the [Gradio](https://www.gradio.app/) apps available on Hugging Face [Spaces](https://hf.co/spaces). These apps can be used for a wide range of tasks such as image generation, video generation, audio transcription, and more.\n\nTo use agents in Transformers, make sure you have the extra `agents` dependencies installed.\n\n```bash\n!pip install transformers[agents]\n```",
  "Create an agent instance (refer to the [Agents](./main_classes/agent#agents) API for supported agents in Transformers) and a list of tools available for it to use, then [`~ReactAgent.run`] the agent on your task. The example below demonstrates how a ReAct agent reasons through a task.\n\n```py\nfrom transformers import ReactCodeAgent\n\nagent = ReactCodeAgent(tools=[])\nagent.run(\n\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\",\n)\n```\n\n```bash\n======== New task ========\nHow many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\n==== Agent is executing the code below:\nbert_layers = 12  # BERT base encoder has 12 layers\nattention_layers = 6  # Encoder in Attention is All You Need has 6 layers\nlayer_diff = bert_layers - attention_layers\nprint(\"The difference in layers between BERT base encoder and Attention is All You Need is\", layer_diff)\n====\nPrint outputs:\nThe difference in layers between BERT base encoder and Attention is All You Need is 6\n\n==== Agent is executing the code below:",
  "final_answer(\"BERT base encoder has {} more layers than the encoder from Attention is All You Need.\".format(layer_diff))\n====\nPrint outputs:\n\n>>> Final answer:\nBERT base encoder has 6 more layers than the encoder from Attention is All You Need.\n```\n\nThis guide will walk you through in more detail how to initialize an agent.\n\n## LLM\n\nAn agent uses a LLM to plan and execute a task; it is the engine that powers the agent. To choose and build your own LLM engine, you need a method that:\n\n1. the input uses the [chat template](./chat_templating) format, `List[Dict[str, str]]`, and it returns a string\n2. the LLM stops generating outputs when it encounters the sequences in `stop_sequences`\n\n```py\ndef llm_engine(messages, stop_sequences=[\"Task\"]) -> str:\nresponse = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\nanswer = response.choices[0].message.content\nreturn answer\n```\n\nNext, initialize an engine to load a model. To run an agent locally, create a [`TransformersEngine`] to load a preinitialized [`Pipeline`].",
  "However, you could also leverage Hugging Face's powerful inference infrastructure, [Inference API](https://hf.co/docs/api-inference/index) or [Inference Endpoints](https://hf.co/docs/inference-endpoints/index), to run your model. This is useful for loading larger models that are typically required for agentic behavior. In this case, load the [`HfApiEngine`] to run the agent.\n\nThe agent requires a list of tools it can use to complete a task. If you aren't using any additional tools, pass an empty list. The default tools provided by Transformers are loaded automatically, but you can optionally set `add_base_tools=True` to explicitly enable them.\n\n<hfoptions id=\"engine\">\n<hfoption id=\"TransformersEngine\">\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine, CodeAgent\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\").to(\"cuda\")\npipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nllm_engine = TransformersEngine(pipeline)\nagent = CodeAgent(tools=[], llm_engine=llm_engine)\nagent.run(",
  "\"What causes bread to rise?\",\n)\n```\n\n</hfoption>\n<hfoption id=\"HfApiEngine\">\n\n```py\nfrom transformers import CodeAgent, HfApiEngine\n\nllm_engine = HfApiEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\nagent = CodeAgent(tools=[], llm_engine=llm_engine)\nagent.run(\n\"Could you translate this sentence from French, say it out loud and return the audio.\",\nsentence=\"Où est la boulangerie la plus proche?\",\n)\n```\n\n</hfoption>\n</hfoptions>\n\nThe agent supports [constrained generation](https://hf.co/docs/text-generation-inference/conceptual/guidance) for generating outputs according to a specific structure with the `grammar` parameter. The `grammar` parameter should be specified in the `llm_engine` method or you can set it when initializing an agent.\n\nLastly, an agent accepts additional inputs such as text and audio. In the [`HfApiEngine`] example above, the agent accepted a sentence to translate. But you could also pass a path to a local or remote file for the agent to access. The example below demonstrates how to pass a path to an audio file.\n\n```py\nfrom transformers import ReactCodeAgent\n\nagent = ReactCodeAgent(tools=[], llm_engine=llm_engine)",
  "agent.run(\"Why doesn't he know many people in New York?\", audio=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3\")\n```\n\n## System prompt\n\nA system prompt describes how an agent should behave, a description of the available tools, and the expected output format.\n\nTools are defined by the `<<tool_descriptions>>` token which is dynamically replaced during runtime with the actual tool. The tool description is derived from the tool name, description, inputs, output type, and a Jinja2 template. Refer to the [Tools](./tools) guide for more information about how to describe tools.\n\nThe example below is the system prompt for [`ReactCodeAgent`].\n\n```py\nYou will be given a task to solve as best you can.\nYou have access to the following tools:\n<<tool_descriptions>>\n\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n\nAt each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task, then the tools that you want to use.",
  "Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '/End code' sequence.\nDuring each intermediate step, you can use 'print()' to save whatever important information you will then need.\nThese print outputs will then be available in the 'Observation:' field, for using this information as input for the next step.\n\nIn the end you have to return a final answer using the `final_answer` tool.\n\nHere are a few examples using notional tools:\n---\n{examples}\n\nAbove example were using notional tools that might not exist for you. You only have access to those tools:\n<<tool_names>>\nYou also can perform computations in the python code you generate.\n\nAlways provide a 'Thought:' and a 'Code:\\n```py' sequence ending with '```<end_code>' sequence. You MUST provide at least the 'Code:' sequence to move forward.\n\nRemember to not perform too many operations in a single code block! You should split the task into intermediate code blocks.\nPrint results at the end of each step to save the intermediate results. Then use final_answer() to return the final result.\n\nRemember to make sure that variables you use are all defined.\n\nNow Begin!\n```",
  "The system prompt can be tailored to the intended task. For example, you can add a better explanation of the output format or you can overwrite the system prompt template entirely with your own custom system prompt as shown below.\n\n> [!WARNING]\n> If you're writing a custom system prompt, make sure to include `<<tool_descriptions>>` in the template so the agent is aware of the available tools.\n\n```py\nfrom transformers import ReactJsonAgent\nfrom transformers.agents import PythonInterpreterTool\n\nagent = ReactJsonAgent(tools=[PythonInterpreterTool()], system_prompt=\"{your_custom_prompt}\")\n```\n\n## Code execution\n\nFor safety, only the tools you provide (and the default Transformers tools) and the `print` function are executed. The interpreter doesn't allow importing modules that aren't on a safe list.\n\nTo import modules that aren't on the list, add them as a list to the `additional_authorized_imports` parameter when initializing an agent.\n\n```py\nfrom transformers import ReactCodeAgent\n\nagent = ReactCodeAgent(tools=[], additional_authorized_imports=['requests', 'bs4'])\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n```",
  "Code execution stops if a tool isn't on the safe list, it isn't authorized, or if the code generated by the agent returns a Python error.\n\n> [!WARNING]\n> A LLM can generate any arbitrary code that can be executed, so don't add any unsafe imports!\n\n## Multi-agent\n\n[Multi-agent](https://hf.co/papers/2308.08155) refers to multiple agents working together to solve a task. Performance is typically better because each agent is specialized for a particular subtask.\n\nMulti-agents are created through a [`ManagedAgent`] class, where a *manager agent* oversees how other agents work together. The manager agent requires an agent and their name and description. These are added to the manager agents system prompt which lets it know how to call and use them.\n\nThe multi-agent example below creates a web search agent that is managed by another [`ReactCodeAgent`].\n\n```py\nfrom transformers.agents import ReactCodeAgent, HfApiEngine, DuckDuckGoSearchTool, ManagedAgent\n\nllm_engine = HfApiEngine()\nweb_agent = ReactCodeAgent(tools=[DuckDuckGoSearchTool()], llm_engine=llm_engine)\nmanaged_web_agent = ManagedAgent(\nagent=web_agent,\nname=\"web_search\",",
  "description=\"Runs web searches for you. Give it your query as an argument.\"\n)\nmanager_agent = ReactCodeAgent(\ntools=[], llm_engine=llm_engine, managed_agents=[managed_web_agent]\n)\nmanager_agent.run(\"Who is the CEO of Hugging Face?\")\n```\n\n## Gradio integration\n\n[Gradio](https://www.gradio.app/) is a library for quickly creating and sharing machine learning apps. The [gradio.Chatbot](https://www.gradio.app/docs/gradio/chatbot) supports chatting with a Transformers agent with the [`stream_to_gradio`] function.\n\nLoad a tool and LLM with an agent, and then create a Gradio app. The key is to use [`stream_to_gradio`] to stream the agents messages and display how it's reasoning through a task.\n\n```py\nimport gradio as gr\nfrom transformers import (\nload_tool,\nReactCodeAgent,\nHfApiEngine,\nstream_to_gradio,\n)\n\n# Import tool from Hub\nimage_generation_tool = load_tool(\"m-ric/text-to-image\")\nllm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n\n# Initialize the agent with the image generation tool\nagent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n\ndef interact_with_agent(task):\nmessages = []\nmessages.append(gr.ChatMessage(role=\"user\", content=task))",
  "yield messages\nfor msg in stream_to_gradio(agent, task):\nmessages.append(msg)\nyield messages + [\ngr.ChatMessage(role=\"assistant\", content=\"⏳ Task not finished yet!\")\n]\nyield messages\n\nwith gr.Blocks() as demo:\ntext_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\nsubmit = gr.Button(\"Run illustrator agent!\")\nchatbot = gr.Chatbot(\nlabel=\"Agent\",\ntype=\"messages\",\navatar_images=(\nNone,\n\"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n),\n)\nsubmit.click(interact_with_agent, [text_input], [chatbot])\n\nif __name__ == \"__main__\":\ndemo.launch()\n```\n\n## Troubleshoot\n\nFor a better idea of what is happening when you call an agent, it is always a good idea to check the system prompt template first.\n\n```py\nprint(agent.system_prompt_template)\n```\n\nIf the agent is behaving unexpectedly, remember to explain the task you want to perform as clearly as possible. Every [`~Agent.run`] is different and minor variations in your system prompt may yield completely different results.\n\nTo find out what happened after a run, check the following agent attributes.",
  "- `agent.logs` stores the finegrained agent logs. At every step of the agents run, everything is stored in a dictionary and appended to `agent.logs`.\n- `agent.write_inner_memory_from_logs` only stores a high-level overview of the agents run. For example, at each step, it stores the LLM output as a message and the tool call output as a separate message. Not every detail from a step is transcripted by `write_inner_memory_from_logs`.\n\n## Resources\n\nLearn more about ReAct agents in the [Open-source LLMs as LangChain Agents](https://hf.co/blog/open-source-llms-as-agents) blog post.",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Philosophy\n\n🤗 Transformers is an opinionated library built for:\n\n- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models.\n- hands-on practitioners who want to fine-tune those models or serve them in production, or both.\n- engineers who just want to download a pretrained model and use it to solve a given machine learning task.\n\nThe library was designed with two strong goals in mind:",
  "1. Be as easy and fast to use as possible:\n\n- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,\njust three standard classes required to use each model: [configuration](main_classes/configuration),\n[models](main_classes/model), and a preprocessing class ([tokenizer](main_classes/tokenizer) for NLP, [image processor](main_classes/image_processor) for vision, [feature extractor](main_classes/feature_extractor) for audio, and [processor](main_classes/processors) for multimodal inputs).\n- All of these classes can be initialized in a simple and unified way from pretrained instances by using a common\n`from_pretrained()` method which downloads (if needed), caches and\nloads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,\nand models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint.\n- On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly",
  "using a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with `Keras.fit`).\n- As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\nextend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base\nclasses of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post.\n\n2. Provide state-of-the-art models with performances as close as possible to the original models:\n\n- We provide at least one example for each architecture which reproduces a result provided by the official authors\nof said architecture.\n- The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n*pytorchic* as it could be as a result of being converted TensorFlow code and vice versa.\n\nA few other goals:\n\n- Expose the models' internals as consistently as possible:",
  "- We give access, using a single API, to the full hidden-states and attention weights.\n- The preprocessing classes and base model APIs are standardized to easily switch between models.\n\n- Incorporate a subjective selection of promising tools for fine-tuning and investigating these models:\n\n- A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning.\n- Simple ways to mask and prune Transformer heads.\n\n- Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and inference with another.\n\n## Main concepts\n\nThe library is built around three types of classes for each model:\n\n- **Model classes** can be PyTorch models ([torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)), Keras models ([tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)) or JAX/Flax models ([flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)) that work with the pretrained weights provided in the library.",
  "- **Configuration classes** store the hyperparameters required to build a model (such as the number of layers and hidden size). You don't always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).\n- **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. [Image processors](main_classes/image_processor) preprocess vision inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and a [processor](main_classes/processors) handles multimodal inputs.\n\nAll these classes can be instantiated from pretrained instances, saved locally, and shared on the Hub with three methods:\n\n- `from_pretrained()` lets you instantiate a model, configuration, and preprocessing class from a pretrained version either",
  "provided by the library itself (the supported models can be found on the [Model Hub](https://huggingface.co/models)) or\nstored locally (or on a server) by the user.\n- `save_pretrained()` lets you save a model, configuration, and preprocessing class locally so that it can be reloaded using\n`from_pretrained()`.\n- `push_to_hub()` lets you share a model, configuration, and a preprocessing class to the Hub, so it is easily accessible to everyone.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Adding a new pipeline\n\nMake [`Pipeline`] your own by subclassing it and implementing a few methods. Share the code with the community on the [Hub](https://hf.co) and register the pipeline with Transformers so that everyone can quickly and easily use it.\n\nThis guide will walk you through the process of adding a new pipeline to Transformers.\n\n## Design choices\n\nAt a minimum, you only need to provide [`Pipeline`] with an appropriate input for a task. This is also where you should begin when designing your pipeline.",
  "Decide what input types [`Pipeline`] can accept. It can be strings, raw bytes, dictionaries, and so on. Try to keep the inputs in pure Python where possible because it's more compatible. Next, decide on the output [`Pipeline`] should return. Again, keeping the output in Python is the simplest and best option because it's easier to work with.\n\nKeeping the inputs and outputs simple, and ideally JSON-serializable, makes it easier for users to run your [`Pipeline`] without needing to learn new object types. It's also common to support many different input types for even greater ease of use. For example, making an audio file acceptable from a filename, URL, or raw bytes gives the user more flexibility in how they provide the audio data.\n\n## Create a pipeline\n\nWith an input and output decided, you can start implementing [`Pipeline`]. Your pipeline should inherit from the base [`Pipeline`] class and include 4 methods.\n\n```py\nfrom transformers import Pipeline\n\nclass MyPipeline(Pipeline):\ndef _sanitize_parameters(self, **kwargs):\n\ndef preprocess(self, inputs, args=2):\n\ndef _forward(self, model_inputs):\n\ndef postprocess(self, model_outputs):\n```",
  "1. `preprocess` takes the inputs and transforms them into the appropriate input format for the model.\n\n```py\ndef preprocess(self, inputs, maybe_arg=2):\nmodel_input = Tensor(inputs[\"input_ids\"])\nreturn {\"model_input\": model_input}\n```\n\n2. `_forward` shouldn't be called directly. `forward` is the preferred method because it includes safeguards to make sure everything works correctly on the expected device. Anything linked to the model belongs in `_forward` and everything else belongs in either `preprocess` or `postprocess`.\n\n```py\ndef _forward(self, model_inputs):\noutputs = self.model(**model_inputs)\nreturn outputs\n```\n\n3. `postprocess` generates the final output from the models output in `_forward`.\n\n```py\ndef postprocess(self, model_outputs, top_k=5):\nbest_class = model_outputs[\"logits\"].softmax(-1)\nreturn best_class\n```",
  "4. `_sanitize_parameters` lets users pass additional parameters to [`Pipeline`]. This could be during initialization or when [`Pipeline`] is called. `_sanitize_parameters` returns 3 dicts of additional keyword arguments that are passed directly to `preprocess`, `_forward`, and `postprocess`. Don't add anything if a user didn't call the pipeline with extra parameters. This keeps the default arguments in the function definition which is always more natural.\n\nFor example, add a `top_k` parameter in `postprocess` to return the top 5 most likely classes. Then in `_sanitize_parameters`, check if the user passed in `top_k` and add it to `postprocess_kwargs`.\n\n```py\ndef _sanitize_parameters(self, **kwargs):\npreprocess_kwargs = {}\nif \"maybe_arg\" in kwargs:\npreprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n\npostprocess_kwargs = {}\nif \"top_k\" in kwargs:\npostprocess_kwargs[\"top_k\"] = kwargs[\"top_k\"]\nreturn preprocess_kwargs, {}, postprocess_kwargs\n```\n\nNow the pipeline can return the top most likely labels if a user chooses to.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(\"my-task\")\n# returns 3 most likely labels\npipeline(\"This is the best meal I've ever had\", top_k=3)",
  "# returns 5 most likely labels by default\npipeline(\"This is the best meal I've ever had\")\n```\n\n## Register a pipeline\n\nRegister the new task your pipeline supports in the `PIPELINE_REGISTRY`. The registry defines:\n\n- the machine learning framework the pipeline supports with either `pt_model` or `tf_model` (add both to ensure it works with either frameworks)\n- a default model which should come from a specific revision (branch, or commit hash) where the model works as expected with `default`\n- the expected input with `type`\n\n```py\nfrom transformers.pipelines import PIPELINE_REGISTRY\nfrom transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n\nPIPELINE_REGISTRY.register_pipeline(\n\"new-task\",\npipeline_class=MyPipeline,\npt_model=AutoModelForSequenceClassification,\ntf_model=TFAutoModelForSequenceClassification,\ndefault={\"pt\": (\"user/awesome-model\", \"branch-name\")},\ntype=\"text\",\n)\n```\n\n## Share your pipeline\n\nShare your pipeline with the community on the [Hub](https://hf.co) or you can add it directly to Transformers.",
  "It's faster to upload your pipeline code to the Hub because it doesn't require a review from the Transformers team. Adding the pipeline to Transformers may be slower because it requires a review and you need to add tests to ensure your [`Pipeline`] works.\n\n### Upload to the Hub\n\nAdd your pipeline code to the Hub in a Python file.\n\nFor example, a custom pipeline for sentence pair classification might look like the following code below. The implementation works for PyTorch and TensorFlow models.\n\n```py\nimport numpy as np\nfrom transformers import Pipeline\n\ndef softmax(outputs):\nmaxes = np.max(outputs, axis=-1, keepdims=True)\nshifted_exp = np.exp(outputs - maxes)\nreturn shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n\nclass PairClassificationPipeline(Pipeline):\ndef _sanitize_parameters(self, **kwargs):\npreprocess_kwargs = {}\nif \"second_text\" in kwargs:\npreprocess_kwargs[\"second_text\"] = kwargs[\"second_text\"]\nreturn preprocess_kwargs, {}, {}\n\ndef preprocess(self, text, second_text=None):\nreturn self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)\n\ndef _forward(self, model_inputs):\nreturn self.model(**model_inputs)\n\ndef postprocess(self, model_outputs):",
  "logits = model_outputs.logits[0].numpy()\nprobabilities = softmax(logits)\n\nbest_class = np.argmax(probabilities)\nlabel = self.model.config.id2label[best_class]\nscore = probabilities[best_class].item()\nlogits = logits.tolist()\nreturn {\"label\": label, \"score\": score, \"logits\": logits}\n```\n\nSave the code in a file named `pair_classification.py`, and import and register it as shown below.\n\n```py\nfrom pair_classification import PairClassificationPipeline\nfrom transformers.pipelines import PIPELINE_REGISTRY\nfrom transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n\nPIPELINE_REGISTRY.register_pipeline(\n\"pair-classification\",\npipeline_class=PairClassificationPipeline,\npt_model=AutoModelForSequenceClassification,\ntf_model=TFAutoModelForSequenceClassification,\n)\n```\n\nThe [register_pipeline](https://github.com/huggingface/transformers/blob/9feae5fb0164e89d4998e5776897c16f7330d3df/src/transformers/pipelines/base.py#L1387) function registers the pipeline details (task type, pipeline class, supported backends) to a models `config.json` file.\n\n```json\n\"custom_pipelines\": {\n\"pair-classification\": {",
  "\"impl\": \"pair_classification.PairClassificationPipeline\",\n\"pt\": [\n\"AutoModelForSequenceClassification\"\n],\n\"tf\": [\n\"TFAutoModelForSequenceClassification\"\n],\n}\n},\n```\n\nCall [`~Pipeline.push_to_hub`] to push the pipeline to the Hub. The Python file containing the code is copied to the Hub, and the pipelines model and tokenizer are also saved and pushed to the Hub. Your pipeline should now be available on the Hub under your namespace.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"pair-classification\", model=\"sgugger/finetuned-bert-mrpc\")\npipeline.push_to_hub(\"pair-classification-pipeline\")\n```\n\nTo use the pipeline, add `trust_remote_code=True` when loading the pipeline.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"pair-classification\", trust_remote_code=True)\n```\n\n### Add to Transformers\n\nAdding a custom pipeline to Transformers requires adding tests to make sure everything works as expected, and requesting a review from the Transformers team.",
  "Add your pipeline code as a new module to the [pipelines](https://github.com/huggingface/transformers/tree/main/src/transformers/pipelines) submodule, and add it to the list of tasks defined in [pipelines/__init__.py](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/__init__.py).\n\nNext, add a new test for the pipeline in [transformers/tests/pipelines](https://github.com/huggingface/transformers/tree/main/tests/pipelines). You can look at the other tests for examples of how to test your pipeline.\n\nThe [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function should be very generic and run on the models defined in [model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L48) and [tf_model_mapping](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L49). This is important for testing future compatibility with new models.",
  "You'll also notice `ANY` is used throughout the [run_pipeline_test](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L186) function. The models are random, so you can't check the actual values. Using `ANY` allows the test to match the output of the pipeline type instead.\n\nFinally, you should also implement the following 4 tests.\n\n1. [test_small_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L59) and [test_small_model_tf](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_text_classification.py#L150), use a small model for these pipelines to make sure they return the correct outputs. The results don't have to make sense. Each pipeline should return the same result.",
  "1. [test_large_model_pt](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L187) nad [test_large_model_tf](https://github.com/huggingface/transformers/blob/db70426854fe7850f2c5834d633aff637f14772e/tests/pipelines/test_pipelines_zero_shot_image_classification.py#L220), use a realistic model for these pipelines to make sure they return meaningful results. These tests are slow and should be marked as slow.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Multi-GPU debugging\n\nDistributed training can be tricky because you have to ensure you're using the correct CUDA version across your system. You may encounter inter-communication issues between GPUs, and there may be underflow or overflow problems in your model.\n\nThis guide covers how to debug these issues, especially as it relates to DeepSpeed and PyTorch.\n\n## DeepSpeed CUDA",
  "DeepSpeed compiles CUDA C++ which can be a potential source of errors when building PyTorch extensions that require CUDA. These errors depend on how CUDA is installed on your system. This section focuses on PyTorch built with *CUDA 10.2*\n\n```bash\npip install deepspeed\n```\n\n> [!TIP]\n> For any other installation issues, please [open an issue](https://github.com/microsoft/DeepSpeed/issues) with the DeepSpeed team.\n\n### Non-identical toolkits\n\nPyTorch comes with its own CUDA toolkit, but to use DeepSpeed with PyTorch, you need to have an identical version of CUDA installed system-wide. For example, if you installed PyTorch with `cudatoolkit==10.2` in your Python environment, then you'll also need to have CUDA 10.2 installed everywhere.\n\nThe exact location can vary from system to system, but `usr/local/cuda-10.2` is the most common location on many Unix systems. When CUDA is correctly set up and added to your `PATH` environment variable, you can find the installation location with the following command.\n\n```bash\nwhich nvcc\n```\n\n### Multiple toolkits\n\nYou may also have more than one CUDA toolkit installed on your system.\n\n```bash\n/usr/local/cuda-10.2\n/usr/local/cuda-11.0\n```",
  "Typically, package installers set the paths to whatever the last version was installed. If the package build fails because it can't find the right CUDA version (despite it being installed already), then you need to configure the `PATH` and `LD_LIBRARY_PATH` environment variables to point to the correct path.\n\nTake a look at the contents of the following environment variables first.\n\n```bash\necho $PATH\necho $LD_LIBRARY_PATH\n```\n\n`PATH` lists the locations of the executables and `LD_LIBRARY_PATH` lists where to look for shared libraries. Earlier entries are prioritized over later ones, and `:` is used to separate multiple entries. To find a specific CUDA toolkit, insert the correct path to list first. This command prepends rather than overwrites the existing values.\n\n```bash\n# adjust the version and full path if needed\nexport PATH=/usr/local/cuda-10.2/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n```",
  "In addition, you should also check that the assigned directories actually exist. The `lib64` sub-directory contains various CUDA `.so` objects (like `libcudart.so`), and while it is unlikely your system names them differently, you should check the actual names and change them accordingly.\n\n### Older versions\n\nSometimes, older CUDA versions may refuse to build with newer compilers. For example, if you have `gcc-9` but CUDA wants `gcc-7`. Usually, installing the latest CUDA toolkit enables support for the newer compiler.\n\nYou could also install an older version of the compiler in addition to the one you're currently using (or it may already be installed but it's not used by default and the build system can't see it). To resolve this, create a symlink to give the build system visibility to the older compiler.\n\n```bash\n# adjust the path to your system\nsudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc\nsudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++\n```\n\n### Prebuild",
  "If you're still having issues with installing DeepSpeed or if you're building DeepSpeed at run time, try to prebuild the DeepSpeed modules before installing them. Run the commands below to make a local build for DeepSpeed.\n\n```bash\ngit clone https://github.com/deepspeedai/DeepSpeed/\ncd DeepSpeed\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n--global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v \\\n--disable-pip-version-check 2>&1 | tee build.log\n```\n\n> [!TIP]\n> Add the `DS_BUILD_AIO=1` parameter to the build command to use NVMe offload. Make sure you install the libaio-dev package across your system.\n\nNext, specify your GPUs architecture by editing the `TORCH_CUDA_ARCH_LIST` variable (find a complete list of NVIDIA GPUs and their corresponding architectures on this [page](https://developer.nvidia.com/cuda-gpus)). To check the PyTorch version that corresponds to your architecture, run the following command.\n\n```bash\npython -c \"import torch; print(torch.cuda.get_arch_list())\"\n```\n\nFind the architecture for a GPU with the following command.\n\n<hfoptions id=\"arch\">\n<hfoption id=\"same GPUs\">\n\n```bash",
  "CUDA_VISIBLE_DEVICES=0 python -c \"import torch; print(torch.cuda.get_device_capability())\"\n```\n\n</hfoption>\n<hfoption id=\"specific GPU\">\n\nRun the following command to find the architecture for GPU `0`. The results will show a value for `major` and `minor`, which is your GPU architecture. The GPU architecture below is `8.6`.\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python -c \"import torch; \\\nprint(torch.cuda.get_device_properties(torch.device('cuda')))\n\"_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)\"\n```\n\n</hfoption>\n</hfoptions>\n\nIf you get `8, 6`, then you can set `TORCH_CUDA_ARCH_LIST=\"8.6\"`. For multiple GPUs with different architectures, list them like `TORCH_CUDA_ARCH_LIST=\"6.1;8.6\"`.\n\nIt is also possible to not specify `TORCH_CUDA_ARCH_LIST` and the build program automatically queries the GPU architecture of the build. However, it may or may not match the actual GPU on the target machine which is why it is better to explicitly specify the correct architecture.\n\nFor training on multiple machines with the same setup, you'll need to make a binary wheel as shown below.\n\n```bash",
  "git clone https://github.com/deepspeedai/DeepSpeed/\ncd DeepSpeed\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\\npython setup.py build_ext -j8 bdist_wheel\n```\n\nThis command generates a binary wheel that'll look something like `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`. Install this wheel locally or on another machine.\n\n```bash\npip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl\n```\n\n## Communication\n\nDistributed training involves communication between processes and or nodes and this can be a potential source of errors.\n\nDownload the script below to diagnose network issues, and then run it to test GPU communication. The example command below tests how two GPUs communicate. Adjust the `--nproc_per_node` and `--nnodes` parameters to adapt it to your system.\n\n```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\npython -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```",
  "The script prints an `OK` status if both GPUs are able to communicate and allocate memory. Take a closer look at the diagnostic script for more details and a recipe for running it in a SLURM environment.\n\nAdd the `NCCL_DEBUG=INFO` environment variable to report more NCCL-related debugging information.\n\n```bash\nNCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\n\n## Underflow and overflow detection\n\nUnderflow and overflow can occur when activations or weights are `inf`, `nan`, and when `loss=NaN`. This may indicate an underflow or overflow issue. To detect these issues, activate the `DebugUnderflowOverflow` module in [`TrainingArguments.debug`] or import and add the module to your own training loop or another trainer class.\n\n<hfoptions id=\"overflow\">\n<hfoption id=\"Trainer\">\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\ndebug=\"underflow_overflow\",\n...\n)\n```\n\n</hfoption>\n<hfoption id=\"PyTorch training loop\">\n\n```py\nfrom transformers.debug_utils import DebugUnderflowOverflow\n\ndebug_overflow = DebugUnderflowOverflow(model)\n```\n\n</hfoption>\n</hfoptions>",
  "The [`~debug_utils.DebugUnderflowOverflow`] module inserts hooks into the model to test the input and output variables and the corresponding model weights after each forward call. If `inf` or `nan` is detected in at least one element of the activations or weights, the module prints a report like the one shown below.\n\nThe example below is for fp16 mixed precision training with [google/mt5-small](https://huggingface.co/google/mt5-small).\n\n```shell\nDetected inf/nan during batch_number=0\nLast 21 forward frames:\nabs min  abs max  metadata\nencoder.block.1.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 2.57e+02 input[0]\n0.00e+00 2.85e+02 output\n[...]\nencoder.block.2.layer.0 T5LayerSelfAttention\n6.78e-04 3.15e+03 input[0]\n2.65e-04 3.42e+03 output[0]\nNone output[1]\n2.25e-01 1.00e+04 output[2]\nencoder.block.2.layer.1.layer_norm T5LayerNorm\n8.69e-02 4.18e-01 weight\n2.65e-04 3.42e+03 input[0]\n1.79e-06 4.65e+00 output\nencoder.block.2.layer.1.DenseReluDense.wi_0 Linear\n2.17e-07 4.50e+00 weight\n1.79e-06 4.65e+00 input[0]\n2.68e-06 3.70e+01 output\nencoder.block.2.layer.1.DenseReluDense.wi_1 Linear\n8.08e-07 2.66e+01 weight\n1.79e-06 4.65e+00 input[0]\n1.27e-04 2.37e+02 output",
  "encoder.block.2.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 8.76e+03 input[0]\n0.00e+00 9.74e+03 output\nencoder.block.2.layer.1.DenseReluDense.wo Linear\n1.01e-06 6.44e+00 weight\n0.00e+00 9.74e+03 input[0]\n3.18e-04 6.27e+04 output\nencoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n1.79e-06 4.65e+00 input[0]\n3.18e-04 6.27e+04 output\nencoder.block.2.layer.1.dropout Dropout\n3.18e-04 6.27e+04 input[0]\n0.00e+00      inf output\n```\n\nAt the start of the report, you can see which batch number the error occurred. In this case, it occurred on the first batch.\n\nEach frame describes the module it is reporting on. For example, the frame below inspected `encoder.block.2.layer.1.layer_norm`. This indicates the layer norm in the first layer of the second block of the encoder. The forward calls are to `T5LayerNorm`.\n\n```shell\nencoder.block.2.layer.1.layer_norm T5LayerNorm\n8.69e-02 4.18e-01 weight\n2.65e-04 3.42e+03 input[0]\n1.79e-06 4.65e+00 output\n```",
  "The last frame reports on the `Dropout.forward` function. It called the `dropout` attribute from inside the `DenseReluDense` class. You can observe that the overflow (`inf`) occurred in the first layer of the encoders second block in the first batch. The absolute largest input element was 6.27e+04.\n\n```shell\nencoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n1.79e-06 4.65e+00 input[0]\n3.18e-04 6.27e+04 output\nencoder.block.2.layer.1.dropout Dropout\n3.18e-04 6.27e+04 input[0]\n0.00e+00      inf output\n```\n\nThe `T5DenseGatedGeluDense.forward` function output activations had an absolute maximum value of 6.27e+04 which is close to fp16s maximum limit of 6.4e+04. In the next step, `Dropout` renormalizes the weights, after zeroing some elements, which pushes the absolute maximum value to greater than 6.4e+04 resulting in an overflow.\n\nNow that you know where the error is happening, you can investigate the modeling code in [modeling_t5.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py).\n\n```py\nclass T5DenseGatedGeluDense(nn.Module):\ndef __init__(self, config):\nsuper().__init__()",
  "self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\nself.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\nself.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\nself.dropout = nn.Dropout(config.dropout_rate)\nself.gelu_act = ACT2FN[\"gelu_new\"]\n\ndef forward(self, hidden_states):\nhidden_gelu = self.gelu_act(self.wi_0(hidden_states))\nhidden_linear = self.wi_1(hidden_states)\nhidden_states = hidden_gelu * hidden_linear\nhidden_states = self.dropout(hidden_states)\nhidden_states = self.wo(hidden_states)\nreturn hidden_states\n```\n\nOne solution is to go back a few steps before the values started growing too large and switch to fp32 so the numbers don't overflow when multiplied or summed. Another potential solution is to temporarily disable mixed precision training (`amp`).\n\n```py\nimport torch\n\ndef forward(self, hidden_states):\nif torch.is_autocast_enabled():\nwith torch.cuda.amp.autocast(enabled=False):\nreturn self._forward(hidden_states)\nelse:\nreturn self._forward(hidden_states)\n```",
  "The report only returns inputs and outputs of full frames, so you may also want to analyze the intermediate values of any `forward` function as well. Add the `detect_overflow` function after the forward calls to track `inf` or `nan` values in the intermediate `forwarded_states`.\n\n```py\nfrom debug_utils import detect_overflow\n\nclass T5LayerFF(nn.Module):\n[...]\n\ndef forward(self, hidden_states):\nforwarded_states = self.layer_norm(hidden_states)\ndetect_overflow(forwarded_states, \"after layer_norm\")\nforwarded_states = self.DenseReluDense(forwarded_states)\ndetect_overflow(forwarded_states, \"after DenseReluDense\")\nreturn hidden_states + self.dropout(forwarded_states)\n```\n\nFinally, you can configure the number of frames printed by [`~debug_utils.DebugUnderflowOverflow`].\n\n```py\nfrom transformers.debug_utils import DebugUnderflowOverflow\n\ndebug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)\n```\n\n### Batch tracing\n\n[`~debug_utils.DebugUnderflowOverflow`] is able to trace the absolute minimum and maximum values in each batch with the underflow and overflow feature disabled. This is useful for identifying where errors are occurring in the model.",
  "The example below shows how to trace the minimum and maximum values in batches 1 and 3 (batches are zero-indexd).\n\n```py\ndebug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])\n```\n\n```shell\n*** Starting batch number=1 ***\nabs min  abs max  metadata\nshared Embedding\n1.01e-06 7.92e+02 weight\n0.00e+00 2.47e+04 input[0]\n5.36e-05 7.92e+02 output\n[...]\ndecoder.dropout Dropout\n1.60e-07 2.27e+01 input[0]\n0.00e+00 2.52e+01 output\ndecoder T5Stack\nnot a tensor output\nlm_head Linear\n1.01e-06 7.92e+02 weight\n0.00e+00 1.11e+00 input[0]\n6.06e-02 8.39e+01 output\nT5ForConditionalGeneration\nnot a tensor output\n\n*** Starting batch number=3 ***\nabs min  abs max  metadata\nshared Embedding\n1.01e-06 7.92e+02 weight\n0.00e+00 2.78e+04 input[0]\n5.36e-05 7.92e+02 output\n[...]\n```\n\n[`~debug_utils.DebugUnderflowOverflow`] reports on a large number of frames which is easier for debugging. Once you know where a problem is occurring, say batch 150, then you can focus the trace for batches 149 and 150 and compare where the numbers are diverging.\n\nIt is also possible to abort the trace after a certain batch number, for example, batch 3.\n\n```py",
  "debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Backbones\n\nHigher-level computer visions tasks, such as object detection or image segmentation, use several models together to generate a prediction. A separate model is used for the *backbone*, neck, and head. The backbone extracts useful features from an input image into a feature map, the neck combines and processes the feature maps, and the head uses them to make a prediction.\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Backbone.png\"/>\n</div>\n\nLoad a backbone with [`~PretrainedConfig.from_pretrained`] and use the `out_indices` parameter to determine which layer, given by the index, to extract a feature map from.\n\n```py\nfrom transformers import AutoBackbone\n\nmodel = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\n```\n\nThis guide describes the backbone class, backbones from the [timm](https://hf.co/docs/timm/index) library, and how to extract features with them.\n\n## Backbone classes\n\nThere are two backbone classes.\n\n- [`~transformers.utils.BackboneMixin`] allows you to load a backbone and includes functions for extracting the feature maps and indices.\n- [`~transformers.utils.BackboneConfigMixin`] allows you to set the feature map and indices of a backbone configuration.\n\nRefer to the [Backbone](./main_classes/backbones) API documentation to check which models support a backbone.\n\nThere are two ways to load a Transformers backbone, [`AutoBackbone`] and a model-specific backbone class.\n\n<hfoptions id=\"backbone-classes\">\n<hfoption id=\"AutoBackbone\">",
  "The [AutoClass](./model_doc/auto) API automatically loads a pretrained vision model with [`~PretrainedConfig.from_pretrained`] as a backbone if it's supported.\n\nSet the `out_indices` parameter to the layer you'd like to get the feature map from. If you know the name of the layer, you could also use `out_features`. These parameters can be used interchangeably, but if you use both, make sure they refer to the same layer.\n\nWhen `out_indices` or `out_features` isn't used, the backbone returns the feature map from the last layer. The example code below uses `out_indices=(1,)` to get the feature map from the first layer.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Swin%20Stage%201.png\"/>\n</div>\n\n```py\nfrom transformers import AutoImageProcessor, AutoBackbone\n\nmodel = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\n```\n\n</hfoption>\n<hfoption id=\"model-specific backbone\">\n\nWhen you know a model supports a backbone, you can load the backbone and neck directly into the models configuration. Pass the configuration to the model to initialize it for a task.",
  "The example below loads a [ResNet](./model_doc/resnet) backbone and neck for use in a [MaskFormer](./model_doc/maskformer) instance segmentation head.\n\nSet `backbone` to a pretrained model and  `use_pretrained_backbone=True` to use pretrained weights instead of randomly initialized weights.\n\n```py\nfrom transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n\nconfig = MaskFormerConfig(backbone=\"microsoft/resnet-50\", use_pretrained_backbone=True)\nmodel = MaskFormerForInstanceSegmentation(config)\n```\n\nAnother option is to separately load the backbone configuration and then pass it to `backbone_config` in the model configuration.\n\n```py\nfrom transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, ResNetConfig\n\n# instantiate backbone configuration\nbackbone_config = ResNetConfig()\n# load backbone in model\nconfig = MaskFormerConfig(backbone_config=backbone_config)\n# attach backbone to model head\nmodel = MaskFormerForInstanceSegmentation(config)\n```\n\n</hfoption>\n</hfoptions>\n\n## timm backbones",
  "[timm](https://hf.co/docs/timm/index) is a collection of vision models for training and inference. Transformers supports timm models as backbones with the [`TimmBackbone`] and [`TimmBackboneConfig`] classes.\n\nSet `use_timm_backbone=True` to load pretrained timm weights, and `use_pretrained_backbone` to use pretrained or randomly initialized weights.\n\n```py\nfrom transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n\nconfig = MaskFormerConfig(backbone=\"resnet50\", use_timm_backbone=True, use_pretrained_backbone=True)\nmodel = MaskFormerForInstanceSegmentation(config)\n```\n\nYou could also explicitly call the [`TimmBackboneConfig`] class to load and create a pretrained timm backbone.\n\n```py\nfrom transformers import TimmBackboneConfig\n\nbackbone_config = TimmBackboneConfig(\"resnet50\", use_pretrained_backbone=True)\n```\n\nPass the backbone configuration to the model configuration and instantiate the model head, [`MaskFormerForInstanceSegmentation`], with the backbone.\n\n```py\nfrom transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n\nconfig = MaskFormerConfig(backbone_config=backbone_config)\nmodel = MaskFormerForInstanceSegmentation(config)\n```",
  "## Feature extraction\n\nThe backbone is used to extract image features. Pass an image through the backbone to get the feature maps.\n\nLoad and preprocess an image and pass it to the backbone. The example below extracts the feature maps from the first layer.\n\n```py\nfrom transformers import AutoImageProcessor, AutoBackbone\nimport torch\nfrom PIL import Image\nimport requests\n\nmodel = AutoBackbone.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", out_indices=(1,))\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(image, return_tensors=\"pt\")\noutputs = model(**inputs)\n```\n\nThe features are stored and accessed from the outputs `feature_maps` attribute.\n\n```py\nfeature_maps = outputs.feature_maps\nlist(feature_maps[0].shape)\n[1, 96, 56, 56]\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPU\n\nGPUs are commonly used to train deep learning models due to their high memory bandwidth and parallel processing capabilities. Depending on your GPU and model size, it is possible to even train models with billions of parameters. The key is to find the right balance between GPU memory utilization (data throughput/training time) and training speed.",
  "This guide will show you the features available in Transformers and PyTorch for efficiently training a model on GPUs. In many cases, you'll want to use a combination of these features to optimize training.\n\nRefer to the table below to quickly help you identify the features relevant to your training scenario.\n\n| Feature | Training speed | Memory usage |\n|---|---|---|\n| batch size | yes | yes |\n| gradient accumulation | no | yes |\n| gradient checkpointing | no | yes |\n| mixed precision | yes | depends |\n| optimizers | yes | yes |\n| data preloading | yes | no |\n| torch_empty_cache_steps | no | yes |\n| torch.compile | yes | no |\n| PEFT | no | yes |\n\n## Trainer\n\n[Trainer](./trainer) supports many useful training features that can be configured through [`TrainingArguments`]. This section highlights some of the more important features for optimizing training.\n\n### Batch size",
  "Batch size is one of the most important hyperparameters for efficient GPU training because it affects memory usage and training speed. Larger batch sizes lead to faster training because it takes advantage of a GPUs parallel processing power. It is recommended to use batch sizes that are powers of 2, such as 8, 64, 128, 256, 512, etc. The batch size depends on your GPU and the models data type.\n\nConfigure [`~TrainingArguments.per_device_train_batch_size`] in [`TrainingArguments`].\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=256,\nper_device_eval_batch_size=256,\n)\n```\n\nRefer to the NVIDIA [Performance](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) guide to learn more about how input features and output neuron counts and batch size affect performance. These are involved in the General Matrix Multiplications (GEMMs) performed by the GPU. Larger parameters are better for parallelization and efficiency.",
  "The [Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) section is also useful for selecting a batch size that maximizes the speed of tensor multiplication based on the data type and GPU. For example, multiples of 8 are recommended for fp16, unless it's an A100 GPU, in which case use multiples of 64.\n\nFinally, consider [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization) for smaller parameters. Tile quantization results when matrix dimensions aren't divisible by a GPUs thread block tile size, causing the GPU to underutilize its resources. Selecting the correct batch size multiplier, such that the matrix is divisible by the tile size, can significantly speed up training.\n\n### Gradient accumulation",
  "Gradient accumulation overcomes memory constraints - useful for fitting a very large model that otherwise wouldn't fit on a single GPU - by accumulating gradients over multiple mini-batches before updating the parameters. This reduces memory by storing fewer gradients and enables training with a larger *effective batch size* because usually, the parameters are updated from a single batch of data. Training can slow down though due to the additional forward and backward passes introduced by gradient accumulation.\n\nConfigure [`~TrainingArguments.per_device_train_batch_size`] in [`TrainingArguments`] to enable gradient accumulation.\n\n```py\nfrom transformers import TrainingArguments\n\n# effective batch size of 64\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\n)\n```\n\nTry to avoid too many gradient accumulation steps because it can really slow down training. Consider the example below, where the maximum batch size that'll fit on your GPU is 4. You should keep your batch size at 4 to better utilize the GPU.\n\n| batch size | gradient accumulation steps | effective batch size |  |\n|---|---|---|---|\n| 1 | 64 | 64 | 👎 |\n| 4 | 16 | 64 | 👍 |",
  "### Gradient checkpointing\n\nGradient checkpointing reduces memory usage by only storing some of the intermediate activations during the backward pass and recomputing the remaining activations. This avoids storing *all* of the intermediate activations from the forward pass, which can require a lot of memory overhead. However, it comes at the cost of slower training speed (~20%).\n\nConfigure [`~TrainingArguments.gradient_checkpointing`] in [`TrainingArguments`] to enable gradient checkpointing.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\n)\n```\n\n### Mixed precision\n\nMixed precision accelerates training speed by performing some calculations in half-precision (fp16) and some in full-precision (fp32). The half-precision calculations boosts training speed because it's not as computationally expensive as performing the calculations in full-precision. Meanwhile, preserving some of the calculations in full-precision maintains accuracy.\n\nThere are several data types available for mixed precision training.\n\n<hfoptions id=\"mixed-precision\">\n<hfoption id=\"fp16\">",
  "The main advantage of mixed precision training is saving the activations in fp16.\n\nConfigure [`~TrainingArguments.fp16`] in [`TrainingArguments`] to enable mixed precision training with the fp16 data type.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nfp16=True.\n)\n```\n\nfp16 isn't memory-optimized because the gradients that are computed in fp16 are converted back to fp32 during the optimization step. You may end up using more GPU memory, especially for small batch sizes, because there are now two versions (fp16 and fp32) of the model on the GPU.\n\n</hfoption>\n<hfoption id=\"bf16\">\n\n[bf16](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus) trades off some precision for a much larger dynamic range, which is helpful for avoiding overflow and underflow errors. You can use bf16 without adding any loss scaling methods like you would with fp16. bf16 is supported by NVIDIAs Ampere architecture or newer.",
  "Configure [`~TrainingArguments.fp16`] in [`TrainingArguments`] to enable mixed precision training with the bf16 data type.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nbf16=True,\n)\n```\n\n</hfoption>\n<hfoption id=\"tf32\">\n\n[tf32](https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/) is a mode on NVIDIA Ampere GPUs that convert the convolution and matrix multiplication inputs to tf32. All other storage and operations are kept in fp32. This allows tf32 to maintain the same range as fp32, the same precision as fp16 and more precision than bf16. Combining tf32 with fp16 or bf16 mixed precision training can improve throughput by 16x.\n\ntf32 is enabled by default on NVIDIA Ampere GPUs, but you can also add the code below to your fp32 training or inference code to explicitly enable it.\n\n```py\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n```",
  "Configure [tf32()](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.tf32) in [`TrainingArguments`] to enable mixed precision training with tf32 mode.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nbf16=True.\ntf32=True,\n)\n```\n\n</hfoption>\n</hfoptions>\n\n### Optimizers\n\nTransformers implements the [AdamW (adamw_torch)](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer from PyTorch by default. But because it stores a weighted average of past gradients, it requires additional memory proportional to the number of model parameters to store the past gradients. This can be an issue when training very large models, and in such cases, you should consider choosing a different optimizer. For example, if you have [Apex](https://nvidia.github.io/apex/index.html) installed on either [NVIDIA](https://github.com/NVIDIA/apex) or [AMD](https://github.com/ROCm/apex), then using the `adamw_apex_fused` optimizer provides the fastest training for all AdamW optimizers.",
  "Configure [`~TrainingArguments.optim`] in [`TrainingArguments`] to choose an optimizer.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nbf16=True,\noptim=\"adamw_bnb_8bit\"\n)\n```\n\nThere are many optimizers to choose from (refer to [OptimizerNames](https://github.com/huggingface/transformers/blob/34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478/src/transformers/training_args.py#L145) for a full supported list) depending on your training scenario. For example, Adafactor can significantly reduce memory requirements by storing a weighted average of a row or column instead of each element in the matrix at the cost of slower convergence. Another example is using a [8-bit AdamW optimizer](https://huggingface.co/docs/bitsandbytes) from bitsandbytes to quantize optimizer states. The optimizer state is stored in a lower precision and dequantized before being used in the optimizer step.\n\nRefer to the [optimizer](./optimizers) guide for to learn about more specialized optimizers.\n\n### Data preloading",
  "Data preloading loads and prepares batches of data in advance on the CPU to ensure the GPU is continuously working, reducing GPU idling and increasing utilization. There are two ways to preload data to ensure the GPU is always working.\n\n1. Allocate pinned memory on the CPU to store the data and transfer it directly to the GPU.\n2. Increase the number of CPU threads or workers to preload the data faster.\n\nConfigure [`~TrainingArguments.dataloader_pin_memory`] and [`~TrainingArguments.dataloader_num_workers`] in [`TrainingArguments`] to allocate pinned memory and increase the number of workers.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nbf16=True,\noptim=\"adamw_bnb_8bit\",\ndataloader_pin_memory=True,\ndataloader_num_workers=4,\n)\n```\n\n## PyTorch\n\nPyTorch provides several features for reducing memory requirements and increasing training speed. These features can often be enabled in Transformers by only adding a few lines of code.\n\n### torch.empty_cache_steps",
  "The [torch.cuda.empty_cache](https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache) function releases unused cached memory, which can help avoid out-of-memory (OOM) errors at the cost of ~10% slower training.\n\nUse [torch_empty_cache_steps()](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.torch_empty_cache_steps) in [`TrainingArguments`] to enable it after a certain number of training steps.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nbf16=True,\noptim=\"adamw_bnb_8bit\",\ndataloader_pin_memory=True,\ndataloader_num_workers=4,\ntorch_empty_cache_steps=4,\n)\n```\n\n### torch.compile\n\n[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) compiles PyTorch code into optimized kernels that significantly speed up training. This feature relies on TorchDynamo to capture PyTorch graphs with the Frame Evaluation API. The graph can be further compiled into optimized kernels for different backends.",
  "Configure [`~TrainingArguments.torch_compile`] in [`TrainingArguments`] to enable it, and configure [torch_compile_backend()](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.torch_compile_backend) to select a backend to use.\n\n```py\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\nper_device_train_batch_size=4,\ngradient_accumulation_steps=16,\ngradient_checkpointing=True,\nbf16=True,\noptim=\"adamw_bnb_8bit\",\ndataloader_pin_memory=True,\ndataloader_num_workers=4,\ntorch_empty_cache_steps=4,\ntorch_compile=True,\ntorch_compile_backend=\"inductor\"\n)\n```\n\nRefer to the table below to help you choose the right backend for your training scenario.\n\n| backend | description | goal |\n|---|---|---|\n| eager | uses PyTorch to run extracted GraphModule | debugging |\n| aot_eager | uses PyTorch eager mode for AOTAutograd's extracted forward and backward graphs | debugging |\n| inductor | uses TorchInductor with AOTAutograd and CUDA Graphs by leveraging Triton kernels | training and inference |\n| nvfuser | uses nvFuser with TorchScript | training and inference |\n| aot_nvfuser | uses nvFuser with AOTAutograd | training and inference |",
  "| aot_cudagraphs | uses CUDA Graphs with AOTAutograd | training and inference |\n| ofi | uses TorchScripts [optimize_for_inference](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html#torch-jit-optimize-for-inference) | inference |\n| fx2trt | uses [Torch-TensorRT](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html) | inference |\n| onnxrt | uses [ONNX-RT](https://onnxruntime.ai/) for CPU and GPU inference | inference |\n| ipex | uses [IPEX](https://github.com/intel/intel-extension-for-pytorch) for CPU inference | inference |\n\n### Scaled dot production attention\n\n[torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) is a native PyTorch implementation of the scaled dot product attention mechanism. SDPA is more efficient and optimized than the original attention mechanism in transformer models. It supports three types of scaled dot product attention.",
  "- [FlashAttention2](https://github.com/Dao-AILab/flash-attention) is automatically enabled for models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate type first.\n- [xFormers](https://github.com/facebookresearch/xformers) or Memory-Efficient Attention supports models with the fp32 torch type.\n- C++ implementation of scaled dot product attention.\n\nSDPA is enabled by default for PyTorch 2.1.1+, but it can be explicitly enabled by setting `attn_implementation=\"sdpa\"` in [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", attn_implementation=\"sdpa\")\n```",
  "<!---\nCopyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Installation\n\nTransformers works with [PyTorch](https://pytorch.org/get-started/locally/), [TensorFlow 2.0](https://www.tensorflow.org/install/pip), and [Flax](https://flax.readthedocs.io/en/latest/). It has been tested on Python 3.9+, PyTorch 2.0+, TensorFlow 2.6+, and Flax 0.4.1+.\n\n## Virtual environment",
  "A virtual environment helps manage different projects and avoids compatibility issues between dependencies. Take a look at the [Install packages in a virtual environment using pip and venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) guide if you're unfamiliar with Python virtual environments.\n\n<hfoptions id=\"virtual\">\n<hfoption id=\"venv\">\n\nCreate and activate a virtual environment in your project directory with [venv](https://docs.python.org/3/library/venv.html).\n\n```bash\npython -m venv .env\nsource ./env/bin/activate\n```\n\n</hfoption>\n<hfoption id=\"uv\">\n\n[uv](https://docs.astral.sh/uv/) is a fast Rust-based Python package and project manager.\n\n```bash\nuv venv .env\nsource ./env/bin/activate\n```\n\n</hfoption>\n</hfoptions>\n\n## Python\n\nYou can install Transformers with pip or uv.\n\n<hfoptions id=\"install\">\n<hfoption id=\"pip\">\n\n[pip](https://pip.pypa.io/en/stable/) is a package installer for Python. Install Transformers with pip in your newly created virtual environment.\n\n```bash\npip install transformers\n```\n\n</hfoption>\n<hfoption id=\"uv\">\n\n[uv](https://docs.astral.sh/uv/) is a fast Rust-based Python package and project manager.\n\n```bash",
  "uv pip install transformers\n```\n\n</hfoption>\n</hfoptions>\n\nFor GPU acceleration, install the appropriate CUDA drivers for [PyTorch](https://pytorch.org/get-started/locally) and [TensorFlow](https://www.tensorflow.org/install/pip).\n\nRun the command below to check if your system detects an NVIDIA GPU.\n\n```bash\nnvidia-smi\n```\n\nTo install a CPU-only version of Transformers and a machine learning framework, run the following command.\n\n<hfoptions id=\"cpu-only\">\n<hfoption id=\"PyTorch\">\n\n```bash\npip install 'transformers[torch]'\nuv pip install 'transformers[torch]'\n```\n\n</hfoption>\n<hfoption id=\"TensorFlow\">\n\nFor Apple M1 hardware, you need to install CMake and pkg-config first.\n\n```bash\nbrew install cmake\nbrew install pkg-config\n```\n\nInstall TensorFlow 2.0.\n\n```bash\npip install 'transformers[tf-cpu]'\nuv pip install 'transformers[tf-cpu]'\n```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```bash\npip install 'transformers[flax]'\nuv pip install 'transformers[flax]'\n```\n\n</hfoption>\n</hfoptions>\n\nTest whether the install was successful with the following command. It should return a label and score for the provided text.\n\n```bash",
  "python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n### Source install\n\nInstalling from source installs the *latest* version rather than the *stable* version of the library. It ensures you have the most up-to-date changes in Transformers and it's useful for experimenting with the latest features or fixing a bug that hasn't been officially released in the stable version yet.\n\nThe downside is that the latest version may not always be stable. If you encounter any problems, please open a [GitHub Issue](https://github.com/huggingface/transformers/issues) so we can fix it as soon as possible.\n\nInstall from source with the following command.\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nCheck if the install was successful with the command below. It should return a label and score for the provided text.\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n### Editable install",
  "An [editable install](https://pip.pypa.io/en/stable/topics/local-project-installs/#editable-installs) is useful if you're developing locally with Transformers. It links your local copy of Transformers to the Transformers [repository](https://github.com/huggingface/transformers) instead of copying the files. The files are added to Python's import path.\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\n> [!WARNING]\n> You must keep the local Transformers folder to keep using it.\n\nUpdate your local version of Transformers with the latest changes in the main repository with the following command.\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\n## conda\n\n[conda](https://docs.conda.io/projects/conda/en/stable/#) is a language-agnostic package manager. Install Transformers from the [conda-forge](https://anaconda.org/conda-forge/transformers) channel in your newly created virtual environment.\n\n```bash\nconda install conda-forge::transformers\n```\n\n## Set up\n\nAfter installation, you can configure the Transformers cache location or set up the library for offline usage.\n\n### Cache directory",
  "When you load a pretrained model with [`~PreTrainedModel.from_pretrained`], the model is downloaded from the Hub and locally cached.\n\nEvery time you load a model, it checks whether the cached model is up-to-date. If it's the same, then the local model is loaded. If it's not the same, the newer model is downloaded and cached.\n\nThe default directory given by the shell environment variable `TRANSFORMERS_CACHE` is `~/.cache/huggingface/hub`. On Windows, the default directory is `C:\\Users\\username\\.cache\\huggingface\\hub`.\n\nCache a model in a different directory by changing the path in the following shell environment variables (listed by priority).\n\n1. [HF_HUB_CACHE](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhubcache) or `TRANSFORMERS_CACHE` (default)\n2. [HF_HOME](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhome)\n3. [XDG_CACHE_HOME](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#xdgcachehome) + `/huggingface` (only if `HF_HOME` is not set)",
  "Older versions of Transformers uses the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE`. You should keep these unless you specify the newer shell environment variable `TRANSFORMERS_CACHE`.\n\n### Offline mode\n\nTo use Transformers in an offline or firewalled environment requires the downloaded and cached files ahead of time. Download a model repository from the Hub with the [`~huggingface_hub.snapshot_download`] method.\n\n> [!TIP]\n> Refer to the [Download files from the Hub](https://hf.co/docs/huggingface_hub/guides/download) guide for more options for downloading files from the Hub. You can download files from specific revisions, download from the CLI, and even filter which files to download from a repository.\n\n```py\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(repo_id=\"meta-llama/Llama-2-7b-hf\", repo_type=\"model\")\n```\n\nSet the environment variable `HF_HUB_OFFLINE=1` to prevent HTTP calls to the Hub when loading a model.\n\n```bash\nHF_HUB_OFFLINE=1 \\\npython examples/pytorch/language-modeling/run_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name wikitext ...\n```",
  "Another option for only loading cached files is to set `local_files_only=True` in [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import LlamaForCausalLM\n\nmodel = LlamaForCausalLM.from_pretrained(\"./path/to/local/directory\", local_files_only=True)\n```",
  "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Checks on a Pull Request\n\nWhen you open a pull request on 🤗 Transformers, a fair number of checks will be run to make sure the patch you are adding is not breaking anything existing. Those checks are of four types:\n- regular tests\n- documentation build\n- code and documentation style\n- general repository consistency",
  "In this document, we will take a stab at explaining what those various checks are and the reason behind them, as well as how to debug them locally if one of them fails on your PR.\n\nNote that, ideally, they require you to have a dev install:\n\n```bash\npip install transformers[dev]\n```\n\nor for an editable install:\n\n```bash\npip install -e .[dev]\n```\n\ninside the Transformers repo. Since the number of optional dependencies of Transformers has grown a lot, it's possible you don't manage to get all of them. If the dev install fails, make sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow and/or Flax) then do\n\n```bash\npip install transformers[quality]\n```\n\nor for an editable install:\n\n```bash\npip install -e .[quality]\n```\n\n\n## Tests\n\nAll the jobs that begin with `ci/circleci: run_tests_` run parts of the Transformers testing suite. Each of those jobs focuses on a part of the library in a certain environment: for instance `ci/circleci: run_tests_pipelines_tf` runs the pipelines test in an environment where TensorFlow only is installed.",
  "Note that to avoid running tests when there is no real change in the modules they are testing, only part of the test suite is run each time: a utility is run to determine the differences in the library between before and after the PR (what GitHub shows you in the \"Files changes\" tab) and picks the tests impacted by that diff. That utility can be run locally with:\n\n```bash\npython utils/tests_fetcher.py\n```\n\nfrom the root of the Transformers repo. It will:\n\n1. Check for each file in the diff if the changes are in the code or only in comments or docstrings. Only the files with real code changes are kept.\n2. Build an internal map that gives for each file of the source code of the library all the files it recursively impacts. Module A is said to impact module B if module B imports module A. For the recursive impact, we need a chain of modules going from module A to module B in which each module imports the previous one.\n3. Apply this map on the files gathered in step 1, which  gives us the list of model files impacted by the PR.\n4. Map each of those files to their corresponding test file(s) and get the list of tests to run.",
  "When executing the script locally, you should get the results of step 1, 3 and 4 printed and thus know which tests are run. The script will also create a file named `test_list.txt` which contains the list of tests to run, and you can run them locally with the following command:\n\n```bash\npython -m pytest -n 8 --dist=loadfile -rA -s $(cat test_list.txt)\n```\n\nJust in case anything slipped through the cracks, the full test suite is also run daily.\n\n## Documentation build\n\nThe `build_pr_documentation` job builds and generates a preview of the documentation to make sure everything looks okay once your PR is merged. A bot will add a link to preview the documentation in your PR. Any changes you make to the PR are automatically updated in the preview. If the documentation fails to build, click on **Details** next to the failed job to see where things went wrong. Often, the error is as simple as a missing file in the `toctree`.\n\nIf you're interested in building or previewing the documentation locally, take a look at the [`README.md`](https://github.com/huggingface/transformers/tree/main/docs) in the docs folder.\n\n## Code and documentation style",
  "Code formatting is applied to all the source files, the examples and the tests using `black` and `ruff`. We also have a custom tool taking care of the formatting of docstrings and `rst` files (`utils/style_doc.py`), as well as the order of the lazy imports performed in the Transformers `__init__.py` files (`utils/custom_init_isort.py`). All of this can be launched by executing\n\n```bash\nmake style\n```\n\nThe CI checks those have been applied inside the `ci/circleci: check_code_quality` check. It also runs `ruff`, that will have a basic look at your code and will complain if it finds an undefined variable, or one that is not used. To run that check locally, use\n\n```bash\nmake quality\n```\n\nThis can take a lot of time, so to run the same thing on only the files you modified in the current branch, run\n\n```bash\nmake fixup\n```\n\nThis last command will also run all the additional checks for the repository consistency. Let's have a look at them.\n\n## Repository consistency\n\nThis regroups all the tests to make sure your PR leaves the repository in a good state, and is performed by the `ci/circleci: check_repository_consistency` check. You can locally run that check by executing the following:",
  "```bash\nmake repo-consistency\n```\n\nThis checks that:\n\n- All objects added to the init are documented (performed by `utils/check_repo.py`)\n- All `__init__.py` files have the same content in their two sections (performed by `utils/check_inits.py`)\n- All code identified as a copy from another module is consistent with the original (performed by `utils/check_copies.py`)\n- All configuration classes have at least one valid checkpoint mentioned in their docstrings (performed by `utils/check_config_docstrings.py`)\n- All configuration classes only contain attributes that are used in corresponding modeling files (performed by `utils/check_config_attributes.py`)\n- The translations of the READMEs and the index of the doc have the same model list as the main README (performed by `utils/check_copies.py`)\n- The auto-generated tables in the documentation are up to date (performed by `utils/check_table.py`)\n- The library has all objects available even if not all optional dependencies are installed (performed by `utils/check_dummies.py`)\n- All docstrings properly document the arguments in the signature of the object (performed by `utils/check_docstrings.py`)",
  "Should this check fail, the first two items require manual fixing, the last four can be fixed automatically for you by running the command\n\n```bash\nmake fix-copies\n```\n\nAdditional checks concern PRs that add new models, mainly that:\n\n- All models added are in an Auto-mapping (performed by `utils/check_repo.py`)\n<!-- TODO Sylvain, add a check that makes sure the common tests are implemented.-->\n- All models are properly tested (performed by `utils/check_repo.py`)\n\n<!-- TODO Sylvain, add the following\n- All models are added to the main README, inside the main doc\n- All checkpoints used actually exist on the Hub\n\n-->\n\n### Check copies\n\nSince the Transformers library is very opinionated with respect to model code, and each model should fully be implemented in a single file without relying on other models, we have added a mechanism that checks whether a copy of the code of a layer of a given model stays consistent with the original. This way, when there is a bug fix, we can see all other impacted models and choose to trickle down the modification or break the copy.\n\n<Tip>",
  "If a file is a full copy of another file, you should register it in the constant `FULL_COPIES` of `utils/check_copies.py`.\n\n</Tip>\n\nThis mechanism relies on comments of the form `# Copied from xxx`. The `xxx` should contain the whole path to the class of function which is being copied below. For instance, `RobertaSelfOutput` is a direct copy of the `BertSelfOutput` class, so you can see [here](https://github.com/huggingface/transformers/blob/2bd7a27a671fd1d98059124024f580f8f5c0f3b5/src/transformers/models/roberta/modeling_roberta.py#L289) it has a comment:\n\n```py\n# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n```\n\nNote that instead of applying this to a whole class, you can apply it to the relevant methods that are copied from. For instance [here](https://github.com/huggingface/transformers/blob/2bd7a27a671fd1d98059124024f580f8f5c0f3b5/src/transformers/models/roberta/modeling_roberta.py#L598) you can see how `RobertaPreTrainedModel._init_weights` is copied from the same method in `BertPreTrainedModel` with the comment:\n\n```py\n# Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n```",
  "Sometimes the copy is exactly the same except for names: for instance in `RobertaAttention`, we use `RobertaSelfAttention` instead of `BertSelfAttention` but other than that, the code is exactly the same. This is why `# Copied from` supports simple string replacements with the following syntax: `Copied from xxx with foo->bar`. This means the code is copied with all instances of `foo` being replaced by `bar`. You can see how it used [here](https://github.com/huggingface/transformers/blob/2bd7a27a671fd1d98059124024f580f8f5c0f3b5/src/transformers/models/roberta/modeling_roberta.py#L304C1-L304C86) in `RobertaAttention` with the comment:\n\n```py\n# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n```\n\nNote that there shouldn't be any spaces around the arrow (unless that space is part of the pattern to replace of course).",
  "You can add several patterns separated by a comma. For instance here `CamemberForMaskedLM` is a direct copy of `RobertaForMaskedLM` with two replacements: `Roberta` to `Camembert` and `ROBERTA` to `CAMEMBERT`. You can see [here](https://github.com/huggingface/transformers/blob/15082a9dc6950ecae63a0d3e5060b2fc7f15050a/src/transformers/models/camembert/modeling_camembert.py#L929) this is done with the comment:\n\n```py\n# Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT\n```\n\nIf the order matters (because one of the replacements might conflict with a previous one), the replacements are executed from left to right.\n\n<Tip>\n\nIf the replacements change the formatting (if you replace a short name by a very long name for instance), the copy is checked after applying the auto-formatter.\n\n</Tip>",
  "Another way when the patterns are just different casings of the same replacement (with an uppercased and a lowercased variants) is just to add the option `all-casing`. [Here](https://github.com/huggingface/transformers/blob/15082a9dc6950ecae63a0d3e5060b2fc7f15050a/src/transformers/models/mobilebert/modeling_mobilebert.py#L1237) is an example in `MobileBertForSequenceClassification` with the comment:\n\n```py\n# Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing\n```\n\nIn this case, the code is copied from `BertForSequenceClassification` by replacing:\n- `Bert` by `MobileBert` (for instance when using `MobileBertModel` in the init)\n- `bert` by `mobilebert` (for instance when defining `self.mobilebert`)\n- `BERT` by `MOBILEBERT` (in the constant `MOBILEBERT_INPUTS_DOCSTRING`)",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Distributed GPU inference\n\n[Tensor parallelism](./perf_train_gpu_many#tensor-parallelism) shards a model onto multiple GPUs and parallelizes computations such as matrix multiplication. It enables fitting larger model sizes into memory and is faster because each GPU can process a tensor slice.\n\n> [!TIP]\n> Expand the list below to see which models support tensor parallelism. Open a GitHub issue or pull request to add support for a model not currently below.\n\n<details>\n<summary>Supported models</summary>",
  "* [Cohere](./model_doc/cohere) and [Cohere 2](./model_doc/cohere2)\n* [Gemma](./model_doc/gemma) and [Gemma 2](./model_doc/gemma2)\n* [GLM](./model_doc/glm)\n* [Granite](./model_doc/granite)\n* [Llama](./model_doc/llama)\n* [Mistral](./model_doc/mistral)\n* [Mixtral](./model_doc/mixtral)\n* [OLMo](./model_doc/olmo) and [OLMo2](./model_doc/olmo2)\n* [Phi](./model_doc/phi) and [Phi-3](./model_doc/phi3)\n* [Qwen2](./model_doc/qwen2), [Qwen2Moe](./model_doc/qwen2_moe), and [Qwen2-VL](./model_doc/qwen2_5_vl)\n* [Starcoder2](./model_doc/starcoder2)\n\n</details>\n\nSet `tp_plan=\"auto\"` in [`~AutoModel.from_pretrained`] to enable tensor parallelism for inference.\n\n```py\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# initialize distributed environment\nrank = int(os.environ[\"RANK\"])\ndevice = torch.device(f\"cuda:{rank}\")\ntorch.cuda.set_device(device)\ntorch.distributed.init_process_group(\"nccl\", device_id=device)\n\n# enable tensor parallelism\nmodel = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B-Instruct\",\ntp_plan=\"auto\",\n)\n\n# prepare input tokens\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")",
  "prompt = \"Can I help\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# distributed run\noutputs = model(inputs)\n```\n\nLaunch the inference script above on [torchrun](https://pytorch.org/docs/stable/elastic/run.html) with 4 processes per GPU.\n\n```bash\ntorchrun --nproc-per-node 4 demo.py\n```\n\nYou can benefit from considerable speed ups for inference, especially for inputs with large batch size or long sequences.\n\nFor a single forward pass on [Llama](./model_doc/llama) with a sequence length of 512 and various batch sizes, you can expect the following speed ups.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n</div>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LiteRT\n\n[LiteRT](https://ai.google.dev/edge/litert) (previously known as TensorFlow Lite) is a high-performance runtime designed for on-device machine learning.\n\nThe [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to LiteRT for [many architectures]((https://huggingface.co/docs/optimum/exporters/onnx/overview)).\n\nThe benefits of exporting to LiteRT include the following.",
  "- Low-latency, privacy-focused, no internet connectivity required, and reduced model size and power consumption for on-device machine learning.\n- Broad platform, model framework, and language support.\n- Hardware acceleration for GPUs and Apple Silicon.\n\nExport a Transformers model to LiteRT with the Optimum CLI.\n\nRun the command below to install Optimum and the [exporters](https://huggingface.co/docs/optimum/exporters/overview) module for LiteRT.\n\n```bash\npip install optimum[exporters-tf]\n```\n\n> [!TIP]\n> Refer to the [Export a model to TFLite with optimum.exporters.tflite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model) guide for all available arguments or with the command below.\n> ```bash\n> optimum-cli export tflite --help\n> ```\n\nSet the `--model` argument to export a from the Hub.\n\n```bash\noptimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n```\n\nYou should see logs indicating the progress and showing where the resulting `model.tflite` is saved.\n\n```bash\nValidating TFLite model...\n-[✓] TFLite model output names match reference model (logits)\n- Validating TFLite Model output \"logits\":",
  "-[✓] (1, 128, 30522) matches (1, 128, 30522)\n-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\nThe TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n- logits: max diff = 5.817413330078125e-05.\nThe exported model was saved at: bert_tflite\n```\n\nFor local models, make sure the model weights and tokenizer files are saved in the same directory, for example `local_path`. Pass the directory to the `--model` argument and use `--task` to indicate the [task](https://huggingface.co/docs/optimum/exporters/task_manager) a model can perform. If `--task` isn't provided, the model architecture without a task-specific head is used.\n\n```bash\noptimum-cli export tflite --model local_path --task question-answering google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# torch.compile",
  "[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) compiles PyTorch code into optimized kernels that significantly speed up inference. This feature relies on [TorchDynamo](https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html) to compile the code into graphs and [TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747) to further compile the graphs into optimized kernels. It is a powerful optimization tool, and in many cases, only requires adding a single line of code.\n\nWrap a model with torch.compile to compile and return an optimized model.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\ncompiled_model = torch.compile(model)\n```\n\n> [!TIP]\n> The initial call to torch.compile is slow because the model needs to be compiled. Subsequent calls to the compiled model are much faster because it doesn't need to compile again.",
  "There are several parameters to customize the compilation process. Two of the more important ones are listed below. For a full list of parameters, refer to the torch.compile [documentation](https://pytorch.org/docs/stable/generated/torch.compile.html).\n\n## Modes\n\nThe `mode` parameter offers several performance options for compiling. Try different modes to see which one works best for your use case.\n\n- `default` is a balanced option between speed and memory.\n- `reduce-overhead` reduces the Python overhead at the expense of a little more memory, but it can be faster.\n- `max-autotune` offers the fastest speed, but compilation takes longer.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\ncompiled_model = torch.compile(model, mode=\"reduce-overhead\")\n```\n\n## Fullgraph\n\nFullgraph attempts to compile the entire model into a single graph to maximize performance. torch.compile raises an error if it encounters a graph break, which means it can't compile the model into a single graph.\n\n```py\nfrom transformers import AutoModelForCausalLM",
  "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\ncompiled_model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n## Benchmarks\n\nRefer to the table below for performance benchmarks comparing the mean inference time in milliseconds with torch.compile enabled and disabled across various GPUs and batch sizes on the same image for different vision tasks.\n\nSelect **Subset** in the table below to switch between different GPUs, as well as benchmarks on [PyTorch nightly](https://download.pytorch.org/whl/nightly/cu118) 2.1.0dev and torch.compile with `reduce-overhead` mode enabled.\n\n<iframe\nsrc=\"https://huggingface.co/datasets/stevhliu/compile-benchmarks/embed/viewer/t4/train\"\nframeborder=\"0\"\nwidth=\"100%\"\nheight=\"560px\"\n></iframe>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Apple Silicon\n\nApple Silicon (M series) features a unified memory architecture, making it possible to efficiently train large models locally and improves performance by reducing latency associated with data retrieval. You can take advantage of Apple Silicon for training with PyTorch due to its integration with [Metal Performance Shaders (MPS)](https://pytorch.org/docs/stable/notes/mps.html).\n\nThe `mps` backend requires macOS 12.3 or later.\n\n> [!WARNING]",
  "> Some PyTorch operations are not implemented in MPS yet. To avoid an error, set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to fallback on the CPU kernels. Please open an issue in the [PyTorch](https://github.com/pytorch/pytorch/issues) repository if you encounter any other issues.\n\n[`TrainingArguments`] and [`Trainer`] detects and sets the backend device to `mps` if an Apple Silicon device is available. No additional changes are required to enable training on your device.\n\nThe `mps` backend doesn't support [distributed training](https://pytorch.org/docs/stable/distributed.html#backends).\n\n## Resources\n\nLearn more about the MPS backend in the [Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/) blog post.",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Generation features\n\nThe [`~GenerationMixin.generate`] API supports a couple features for building applications on top of it.\n\nThis guide will show you how to use these features.\n\n## Streaming",
  "Streaming starts returning text as soon as it is generated so you don't have to wait to see the entire generated response all at once. It is important in user-facing applications because it reduces perceived latency and allows users to see the generation progression.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tgi/streaming-generation-visual-dark_360.gif\"/>\n</div>\n\n> [!TIP]\n> Learn more about streaming in the [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/conceptual/streaming) docs.\n\nCreate an instance of [`TextStreamer`] with the tokenizer. Pass [`TextStreamer`] to the `streamer` parameter in [`~GenerationMixin.generate`] to stream the output one word at a time.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ninputs = tokenizer([\"The secret to baking a good cake is \"], return_tensors=\"pt\")\nstreamer = TextStreamer(tokenizer)\n\n_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\n```",
  "The `streamer` parameter is compatible with any class with a [`~TextStreamer.put`] and [`~TextStreamer.end`] method. [`~TextStreamer.put`] pushes new tokens and [`~TextStreamer.end`] flags the end of generation. You can create your own streamer class as long as they include these two methods, or you can use Transformers' basic streamer classes.\n\n## Watermarking\n\nWatermarking is useful for detecting whether text is generated. The [watermarking strategy](https://hf.co/papers/2306.04634) in Transformers randomly \"colors\" a subset of the tokens green. When green tokens are generated, they have a small bias added to their logits, and a higher probability of being generated. You can detect generated text by comparing the proportion of green tokens to the amount of green tokens typically found in human-generated text.\n\nWatermarking is supported for any generative model in Transformers and doesn't require an extra classification model to detect the watermarked text.",
  "Create a [`WatermarkingConfig`] with the bias value to add to the logits and watermarking algorithm. The example below uses the `\"selfhash\"` algorithm, where the green token selection only depends on the current token. Pass the [`WatermarkingConfig`] to [`~GenerationMixin.generate`].\n\n> [!TIP]\n> The [`WatermarkDetector`] class detects the proportion of green tokens in generated text, which is why it is recommended to strip the prompt text, if it is much longer than the generated text. Padding can also have an effect on [`WatermarkDetector`].\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.padding_side = \"left\"\n\ninputs = tokenizer([\"This is the beginning of a long story\", \"Alice and Bob are\"], padding=True, return_tensors=\"pt\")\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwatermarking_config = WatermarkingConfig(bias=2.5, seeding_scheme=\"selfhash\")",
  "out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n```\n\nCreate an instance of [`WatermarkDetector`] and pass the model output to it to detect whether the text is machine-generated. The [`WatermarkDetector`] must have the same [`WatermarkingConfig`] used during generation.\n\n```py\ndetector = WatermarkDetector(model_config=model.config, device=\"cpu\", watermarking_config=watermarking_config)\ndetection_out = detector(out, return_dict=True)\ndetection_out.prediction\narray([True, True])\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n> [!WARNING]\n> Agents and tools are being spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. These docs will be deprecated in the future!\n\n# Tools\n\nA tool is a function an agent can use to complete a task. Depending on your task, a tool can perform a web search, answer questions about a document, transcribe speech to text, and much more.",
  "Transformers provides a default set of tools for agents. These include the tools mentioned above as well as image question answering, text-to-speech, translation, and a Python code interpreter that executes the Python code generated by a LLM in a secure environment.\n\nSet `add_base_tools=True` to enable this default set of tools. The `tools` parameter is for adding additional tools. Leave the list empty if you aren't planning on adding any other tools to the toolbox.\n\n```py\nfrom transformers import ReactCodeAgent\n\nagent = ReactCodeAgent(tools=[], add_base_tools=True)\n```\n\nYou could also manually load a tool with [`load_tool`].\n\n```py\nfrom transformers import load_tool, ReactCodeAgent\n\ntool = load_tool(\"text-to-speech\")\naudio = tool(\"This is a text-to-speech tool\")\nagent = ReactCodeAgent(tools=[audio])\n```\n\nThis guide will help you learn how to create your own tools and manage an agents toolbox.\n\n## Create a new tool\n\nYou can create any tool you can dream of to empower an agent. The example in this section creates a tool that returns the most downloaded model for a task from the Hub, and the code for it is shown below.\n\n```py\nfrom huggingface_hub import list_models",
  "task = \"text-classification\"\nmodel = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\nprint(model.id)\n```\n\nThere are two ways you can create a tool, using a decorator or a superclass.\n\n### Tool decorator\n\nA fast and simple way to create a tool is to add the `@tool` decorator.\n\nConvert the code above into a tool by wrapping it in a function and adding the `@tool` decorator. The function needs:\n\n- A clear name that describes what the tool does, `model_download_counter`.\n- Type hints for the input and output (`str`).\n- A description that describes the tool in more detail and its arguments. This description is incorporated in the agents system prompt. It tells the agent *how* to use the tool, so try to make it as clear as possible!\n\n```py\nfrom transformers import tool\n\n@tool\ndef model_download_counter(task: str) -> str:\n\"\"\"\nThis is a tool that returns the checkpoint name of the most downloaded model for a task from the Hugging Face Hub.\n\nArgs:\ntask: The task to retrieve the most downloaded model from.\n\"\"\"\nmodel = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\nreturn model.id\n```",
  "Pass the `model_download_counter` tool to the agents `tools` parameter to use it.\n\n```py\nfrom transformers import CodeAgent\n\nagent = CodeAgent(tools=[model_download_counter], add_base_tools=True)\nagent.run(\n\"Can you give me the name of the model that has the most downloads on the 'text-to-video' task on the Hugging Face Hub?\"\n)\n```\n\n### Tool superclass\n\nInheritance allows you to customize the [`Tool`] superclass or build a tool much more flexibly and comprehensively. This example will show you how to build the same `model_download_counter` tool as a [`Tool`] class.\n\nThe [`Tool`] class needs:\n\n- A clear name that describes what the tool does, `model_download_counter`.\n- A description that describes the tool in more detail and its arguments. This description is incorporated in the agents system prompt. It tells the agent *how* to use the tool, so try to make it as clear as possible!\n- An `inputs` attribute that describes the input type. This is a dictionary with the keys, `type` and `description`.\n- An `outputs` attribute that describes the output type.\n- A `forward` method containing the code to be executed when the tool is called.",
  "Write the following code below to a file named `model_download.py`.\n\n```py\nfrom transformers import Tool\nfrom huggingface_hub import list_models\n\nclass HFModelDownloadsTool(Tool):\nname = \"model_download_counter\"\ndescription = \"\"\"\nThis is a tool that returns the checkpoint name of the most downloaded model for a task from the Hugging Face Hub.\"\"\"\n\ninputs = {\n\"task\": {\n\"type\": \"string\",\n\"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n}\n}\noutput_type = \"string\"\n\ndef forward(self, task: str):\nmodel = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\nreturn model.id\n```\n\nImport the tool from `model_download.py` and use [`load_tool`] to load it into the agent.\n\n```py\nfrom model_download import HFModelDownloadsTool\nfrom transformers import load_tool, CodeAgent\n\ntool = HFModelDownloadsTool()\nmodel_counter = load_tool(tool)\nagent = CodeAgent(tools=[model_counter], add_base_tools=True)\n```\n\nAlso consider sharing your tool to the Hub with [`~Tool.push_to_hub`] so that everyone can use it!\n\n```py\nfrom model_download import HFModelDownloadsTool\nfrom transformers import load_tool, CodeAgent\n\ntool = HFModelDownloadsTool()",
  "tool.push_to_hub(\"{your_username}/hf-model-downloads\")\nmodel_counter = load_tool(\"m-ric/hf-model-downloads\")\nagent = CodeAgent(tools=[model_counter], add_base_tools=True)\n```\n\n## Add and replace tools\n\nOnce an agent is initialized, add or replace its available tools without reinitializing the agent from scratch.\n\nUse [`add_tool`] to add a tool to an existing agent.\n\n```py\nfrom transformers import CodeAgent\n\nagent = CodeAgent(tools=[], add_base_tools=True)\nagent.toolbox.add_tool(model_download_counter)\n```\n\nNow you can use the default text-to-speech tool to read aloud the most downloaded model for the text-to-video task.\n\n```py\nagent.run(\n\"Can you read out loud the name of the model that has the most downloads on the 'text-to-video' task on the Hugging Face Hub and return the audio?\"\n)\n```\n\n> [!WARNING]\n> When adding tools to an agent that already works well, it can bias the agent towards your tool or a tool other than the one currently defined.",
  "Use [`update_tool`] to replace an agents existing tool. This is useful if the new tool is a one-to-one replacement of the existing tool because the agent already knows how to perform the task. The new tool should follow the same API as the tool it replaced or the system prompt template should be adapted to ensure all examples using the replaced tool are updated.\n\n```py\nagent.toolbox.update_tool(new_model_download_counter)\n```\n\n## ToolCollection\n\nA [`ToolCollection`] is a collection of Hugging Face [Spaces](https://hf.co/spaces) that can be quickly loaded and used by an agent.\n\n> [!TIP]\n> Learn more about creating collections on the Hub.\n\nCreate a [`ToolCollection`] object and specify the `collection_slug` of the collection you want to use, and then pass it to the agent. To speed up the starting process, tools are only loaded if they're called by the agent.\n\nThe example loads a collection of image generation tools.\n\n```py\nfrom transformers import ToolCollection, ReactCodeAgent\n\nimage_tool_collection = ToolCollection(collection_slug=\"\")\nagent = ReactCodeAgent(tools=[*image_tool_collection], add_base_tools=True)\nagent.run(\n\"Please draw me a picture of rivers and lakes.\"\n)\n```",
  "## Tool integrations\n\nTransformers supports tools from several other libraries, such as [gradio-tools](https://github.com/freddyaboulton/gradio-tools) and [LangChain](https://python.langchain.com/docs/introduction/).\n\n### gradio-tools\n\ngradio-tools is a library that enables [Gradio](https://www.gradio.app/) apps to be used as tools. With the wide variety of Gradio apps available, you can enhance your agent with a range of tools like generating images and videos or transcribing audio and summarizing it.\n\nImport and instantiate a tool from gradio-tools, for example, the [StableDiffusionPromptGeneratorTool](https://github.com/freddyaboulton/gradio-tools/blob/main/gradio_tools/tools/prompt_generator.py). This tool can help improve prompts to generate better images.\n\n> [!WARNING]\n> gradio-tools require text inputs and outputs even when working with different modalities like images and audio, which are currently incompatible.\n\nUse [`~Tool.from_gradio`] to load the prompt generator tool.\n\n```py\nfrom gradio_tools import StableDiffusionPromptGeneratorTool\nfrom transformers import Tool, load_tool, CodeAgent\n\ngradio_prompt_generator_tool = StableDiffusionPromptGeneratorTool()",
  "prompt_generator_tool = Tool.from_gradio(gradio_prompt_generator_tool)\n```\n\nNow pass it to the agent along with a text-to-image tool.\n\n```py\nimage_generation_tool = load_tool(\"huggingface-tools/text-to-image\")\nagent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)\nagent.run(\n\"Improve this prompt, then generate an image of it.\", prompt=\"A rabbit wearing a space suit\"\n)\n```\n\n### LangChain\n\nLangChain is a library for working with LLMs which includes agents and tools. Use the [`~Tool.from_langchain`] method to load any LangChain tool into an agent.\n\nThe example below demonstrates how to use LangChains web search tool.\n\n```py\nfrom langchain.agents import load_tools\nfrom transformers import Tool, ReactCodeAgent\n\nsearch_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\nagent = ReactCodeAgent(tools=[search_tool])\nagent.run(\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\")\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# PEFT\n\n[[open-in-colab]]",
  "[PEFT](https://huggingface.co/docs/peft/index), a library of parameter-efficient fine-tuning methods, enables training and storing large models on consumer GPUs. These methods only fine-tune a small number of extra model parameters, also known as adapters, on top of the pretrained model. A significant amount of memory is saved because the GPU doesn't need to store the optimizer states and gradients for the pretrained base model. Adapters are very lightweight, making it convenient to share, store, and load them.\n\nThis guide provides a short introduction to the PEFT library and how to use it for training with Transformers. For more details, refer to the PEFT [documentation](https://huggingface.co/docs/peft/index).\n\nInstall PEFT with the command below.\n\n<hfoptions id=\"install\">\n<hfoption id=\"pip\">\n\n```bash\npip install -U peft\n```\n\n</hfoption>\n<hfoption id=\"source\">\n\n```bash\npip install git+https://github.com/huggingface/peft.git\n```\n\n</hfoption>\n</hfoptions>\n\n> [!TIP]\n> PEFT currently supports the LoRA, IA3, and AdaLoRA methods for Transformers. To use another PEFT method, such as prompt learning or prompt tuning, use the PEFT library directly.",
  "[Low-Rank Adaptation (LoRA)](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora) is a very common PEFT method that decomposes the weight matrix into two smaller trainable matrices. Start by defining a [LoraConfig](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig) object with the parameters shown below.\n\n```py\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# create LoRA configuration object\nlora_config = LoraConfig(\ntask_type=TaskType.CAUSAL_LM, # type of task to train on\ninference_mode=False, # set to False for training\nr=8, # dimension of the smaller matrices\nlora_alpha=32, # scaling factor\nlora_dropout=0.1 # dropout of LoRA layers\n)\n```\n\nAdd [LoraConfig](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig) to the model with [`~integrations.PeftAdapterMixin.add_adapter`]. The model is now ready to be passed to [`Trainer`] for training.\n\n```py\nmodel.add_adapter(lora_config, adapter_name=\"lora_1\")\ntrainer = Trainer(model=model, ...)\ntrainer.train()\n```",
  "To add an additional trainable adapter on top of a model with an existing adapter attached, specify the modules you want to train in [modules_to_save()](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig.modules_to_save).\n\nFor example, to train the `lm_head` module on top of a causal language model with a LoRA adapter attached, set `modules_to_save=[\"lm_head\"]`. Add the adapter to the model as shown below, and then pass it to [`Trainer`].\n\n```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoraConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n\nlora_config = LoraConfig(\ntarget_modules=[\"q_proj\", \"k_proj\"],\nmodules_to_save=[\"lm_head\"],\n)\n\nmodel.add_adapter(lora_config)\ntrainer = Trainer(model=model, ...)\ntrainer.train()\n```\n\nSave your adapter with [`~PreTrainedModel.save_pretrained`] to reuse it.\n\n## Load adapter\n\nTo load an adapter with Transformers, the Hub repository or local directory must contain an `adapter_config.json` file and the adapter weights. Load the adapter with [`~PreTrainedModel.from_pretrained`] or with [`~integrations.PeftAdapterMixin.load_adapter`].\n\n<hfoptions id=\"load\">",
  "<hfoption id=\"from_pretrained\">\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"klcsp/gemma7b-lora-alpaca-11-v1\")\n```\n\n</hfoption>\n<hfoption id=\"load_adapter\">\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\nmodel.load_adapter(\"klcsp/gemma7b-lora-alpaca-11-v1\")\n```\n\n</hfoption>\n</hfoptions>\n\nFor very large models, it is helpful to load a quantized version of the model in 8 or 4-bit precision to save memory. Transformers supports quantization with its [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index) integration. Specify in [`BitsAndBytesConfig`] whether you want to load a model in 8 or 4-bit precision.\n\nFor multiple devices, add `device_map=\"auto\"` to automatically distribute the model across your hardware.\n\n```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"klcsp/gemma7b-lora-alpaca-11-v1\",\nquantization_config=BitsAndBytesConfig(load_in_8bit=True),\ndevice_map=\"auto\",\n)\n```\n\n## Set adapter",
  "[`~integrations.PeftAdapterMixin.add_adapter`] adds a new adapter to a model. To add a second adapter, the new adapter must be the same type as the first adapter. Use the `adapter_name` parameter to assign a name to the adapter.\n\n```py\nmodel.add_adapter(lora_config, adapter_name=\"lora_2\")\n```\n\nOnce added, use [`~integrations.PeftAdapterMixin.set_adapter`] to force a model to use the specified adapter and disable the other adapters.\n\n```py\nmodel.set_adapter(\"lora_2\")\n```\n\n## Enable and disable adapter\n\n[`~integrations.PeftAdapterMixin.enable_adapters`] is a broader function that enables *all* adapters attached to a model, and [`~integrations.PeftAdapterMixin.disable_adapters`] disables *all* attached adapters.\n\n```py\nmodel.add_adapter(lora_1)\nmodel.add_adapter(lora_2)\nmodel.enable_adapters()\n\n# disable all adapters\nmodel.disable_adapters()\n```",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Translation\n\n[[open-in-colab]]\n\n<Youtube id=\"1JvfrvZgi6c\"/>",
  "Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework for returning some output from an input, like translation or summarization. Translation systems are commonly used for translation between different language texts, but it can also be used for speech or some combination in between like text-to-speech or speech-to-text.\n\nThis guide will show you how to:\n\n1. Finetune [T5](https://huggingface.co/google-t5/t5-small) on the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset to translate English text to French.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/translation).\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate sacrebleu\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py",
  ">>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load OPUS Books dataset\n\nStart by loading the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset from the 🤗 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> books = load_dataset(\"opus_books\", \"en-fr\")\n```\n\nSplit the dataset into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> books = books[\"train\"].train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:\n\n```py\n>>> books[\"train\"][0]\n{'id': '90560',\n'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',\n'fr': 'Mais ce plateau élevé ne mesurait que quelques toises, et bientôt nous fûmes rentrés dans notre élément.'}}\n```\n\n`translation`: an English and French translation of the text.\n\n## Preprocess\n\n<Youtube id=\"XAR8jnZZuUs\"/>\n\nThe next step is to load a T5 tokenizer to process the English-French language pairs:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> checkpoint = \"google-t5/t5-small\"\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```",
  "The preprocessing function you want to create needs to:\n\n1. Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n2. Set the target language (French) in the `text_target` parameter to ensure the tokenizer processes the target text correctly. If you don't set `text_target`, the tokenizer processes the target text as English.\n3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter.\n\n```py\n>>> source_lang = \"en\"\n>>> target_lang = \"fr\"\n>>> prefix = \"translate English to French: \"\n\n\n>>> def preprocess_function(examples):\n...     inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n...     targets = [example[target_lang] for example in examples[\"translation\"]]\n...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n...     return model_inputs\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\n\n```py",
  ">>> tokenized_books = books.map(preprocess_function, batched=True)\n```\n\nNow create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\n<frameworkcontent>\n<pt>\n```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n```\n</pt>\n<tf>\n\n```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py\n>>> import evaluate",
  ">>> metric = evaluate.load(\"sacrebleu\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the SacreBLEU score:\n\n```py\n>>> import numpy as np\n\n\n>>> def postprocess_text(preds, labels):\n...     preds = [pred.strip() for pred in preds]\n...     labels = [[label.strip()] for label in labels]\n\n...     return preds, labels\n\n\n>>> def compute_metrics(eval_preds):\n...     preds, labels = eval_preds\n...     if isinstance(preds, tuple):\n...         preds = preds[0]\n...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n...     result = {\"bleu\": result[\"score\"]}\n\n...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n...     result[\"gen_len\"] = np.mean(prediction_lens)\n...     result = {k: round(v, 4) for k, v in result.items()}",
  "...     return result\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load T5 with [`AutoModelForSeq2SeqLM`]:\n\n```py\n>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`Seq2SeqTrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the SacreBLEU metric and save the training checkpoint.\n2. Pass the training arguments to [`Seq2SeqTrainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.",
  "3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = Seq2SeqTrainingArguments(\n...     output_dir=\"my_awesome_opus_books_model\",\n...     eval_strategy=\"epoch\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     weight_decay=0.01,\n...     save_total_limit=3,\n...     num_train_epochs=2,\n...     predict_with_generate=True,\n...     fp16=True, #change to bf16=True for XPU\n...     push_to_hub=True,\n... )\n\n>>> trainer = Seq2SeqTrainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_books[\"train\"],\n...     eval_dataset=tokenized_books[\"test\"],\n...     processing_class=tokenizer,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>",
  "To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import AdamWeightDecay\n\n>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n```\n\nThen you can load T5 with [`TFAutoModelForSeq2SeqLM`]:\n\n```py\n>>> from transformers import TFAutoModelForSeq2SeqLM\n\n>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_books[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_test_set = model.prepare_tf_dataset(\n...     tokenized_books[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> import tensorflow as tf",
  ">>> model.compile(optimizer=optimizer)  # No loss argument!\n```\n\nThe last two things to setup before you start training is to compute the SacreBLEU metric from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n\nPass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_opus_books_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py",
  ">>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with some text you'd like to translate to another language. For T5, you need to prefix your input depending on the task you're working on. For translation from English to French, you should prefix your input as shown below:\n\n```py\n>>> text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for translation with your model, and pass your text to it:",
  "```py\n>>> from transformers import pipeline\n\n# Change `xx` to the language of the input and `yy` to the language of the desired output.\n# Examples: \"en\" for English, \"fr\" for French, \"de\" for German, \"es\" for Spanish, \"zh\" for Chinese, etc; translation_en_to_fr translates English to French\n# You can view all the lists of languages here - https://huggingface.co/languages\n>>> translator = pipeline(\"translation_xx_to_yy\", model=\"username/my_awesome_opus_books_model\")\n>>> translator(text)\n[{'translation_text': 'Legumes partagent des ressources avec des bactéries azotantes.'}]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nTokenize the text and return the `input_ids` as PyTorch tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n```",
  "Use the [`~generation.GenerationMixin.generate`] method to create the translation. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](../main_classes/text_generation) API.\n\n```py\n>>> from transformers import AutoModelForSeq2SeqLM\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'Les lignées partagent des ressources avec des bactéries enfixant l'azote.'\n```\n</pt>\n<tf>\nTokenize the text and return the `input_ids` as TensorFlow tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n>>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n```",
  "Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the translation. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](../main_classes/text_generation) API.\n\n```py\n>>> from transformers import TFAutoModelForSeq2SeqLM\n\n>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'Les lugumes partagent les ressources avec des bactéries fixatrices d'azote.'\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Document Question Answering\n\n[[open-in-colab]]\n\nDocument Question Answering, also referred to as Document Visual Question Answering, is a task that involves providing\nanswers to questions posed about document images. The input to models supporting this task is typically a combination of an image and\na question, and the output is an answer expressed in natural language. These models utilize multiple modalities, including",
  "text, the positions of words (bounding boxes), and the image itself.\n\nThis guide illustrates how to:\n\n- Fine-tune [LayoutLMv2](../model_doc/layoutlmv2) on the [DocVQA dataset](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut).\n- Use your fine-tuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/image-to-text)\n\n</Tip>\n\nLayoutLMv2 solves the document question-answering task by adding a question-answering head on top of the final hidden\nstates of the tokens, to predict the positions of the start and end tokens of the\nanswer. In other words, the problem is treated as extractive question answering: given the context, extract which piece\nof information answers the question. The context comes from the output of an OCR engine, here it is Google's Tesseract.\n\nBefore you begin, make sure you have all the necessary libraries installed. LayoutLMv2 depends on detectron2, torchvision and tesseract.\n\n```bash\npip install -q transformers datasets\n```\n\n```bash\npip install 'git+https://github.com/facebookresearch/detectron2.git'\npip install torchvision\n```\n\n```bash",
  "sudo apt install tesseract-ocr\npip install -q pytesseract\n```\n\nOnce you have installed all of the dependencies, restart your runtime.\n\nWe encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the 🤗 Hub.\nWhen prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\nLet's define some global variables.\n\n```py\n>>> model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n>>> batch_size = 4\n```\n\n## Load the data\n\nIn this guide we use a small sample of preprocessed DocVQA that you can find on 🤗 Hub. If you'd like to use the full\nDocVQA dataset, you can register and download it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to\nproceed with this guide check out [how to load files into a 🤗 dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files).\n\n```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"nielsr/docvqa_1200_examples\")\n>>> dataset\nDatasetDict({\ntrain: Dataset({\nfeatures: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\nnum_rows: 1000\n})\ntest: Dataset({",
  "features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\nnum_rows: 200\n})\n})\n```\n\nAs you can see, the dataset is split into train and test sets already. Take a look at a random example to familiarize\nyourself with the features.\n\n```py\n>>> dataset[\"train\"].features\n```\n\nHere's what the individual fields represent:\n* `id`: the example's id\n* `image`: a PIL.Image.Image object containing the document image\n* `query`: the question string - natural language asked question, in several languages\n* `answers`: a list of correct answers provided by human annotators\n* `words` and `bounding_boxes`: the results of OCR, which we will not use here\n* `answer`: an answer matched by a different model which we will not use here\n\nLet's leave only English questions, and drop the `answer` feature which appears to contain predictions by another model.\nWe'll also take the first of the answers from the set provided by the annotators. Alternatively, you can randomly sample it.\n\n```py\n>>> updated_dataset = dataset.map(lambda example: {\"question\": example[\"query\"][\"en\"]}, remove_columns=[\"query\"])\n>>> updated_dataset = updated_dataset.map(",
  "...     lambda example: {\"answer\": example[\"answers\"][0]}, remove_columns=[\"answer\", \"answers\"]\n... )\n```\n\nNote that the LayoutLMv2 checkpoint that we use in this guide has been trained with `max_position_embeddings = 512` (you can\nfind this information in the [checkpoint's `config.json` file](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)).\nWe can truncate the examples but to avoid the situation where the answer might be at the end of a large document and end up truncated,\nhere we'll remove the few examples where the embedding is likely to end up longer than 512.\nIf most of the documents in your dataset are long, you can implement a sliding window strategy - check out [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb) for details.\n\n```py\n>>> updated_dataset = updated_dataset.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)\n```\n\nAt this point let's also remove the OCR features from this dataset. These are a result of OCR for fine-tuning a different\nmodel. They would still require some processing if we wanted to use them, as they do not match the input requirements",
  "of the model we use in this guide. Instead, we can use the [`LayoutLMv2Processor`] on the original data for both OCR and\ntokenization. This way we'll get the inputs that match model's expected input. If you want to process images manually,\ncheck out the [`LayoutLMv2` model documentation](../model_doc/layoutlmv2) to learn what input format the model expects.\n\n```py\n>>> updated_dataset = updated_dataset.remove_columns(\"words\")\n>>> updated_dataset = updated_dataset.remove_columns(\"bounding_boxes\")\n```\n\nFinally, the data exploration won't be complete if we don't peek at an image example.\n\n```py\n>>> updated_dataset[\"train\"][11][\"image\"]\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg\" alt=\"DocVQA Image Example\"/>\n</div>\n\n## Preprocess the data\n\nThe Document Question Answering task is a multimodal task, and you need to make sure that the inputs from each modality",
  "are preprocessed according to the model's expectations. Let's start by loading the [`LayoutLMv2Processor`], which internally combines an image processor that can handle image data and a tokenizer that can encode text data.\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n```\n\n### Preprocessing document images\n\nFirst, let's prepare the document images for the model with the help of the `image_processor` from the processor.\nBy default, image processor resizes the images to 224x224, makes sure they have the correct order of color channels,\napplies OCR with tesseract to get words and normalized bounding boxes. In this tutorial, all of these defaults are exactly what we need.\nWrite a function that applies the default image processing to a batch of images and returns the results of OCR.\n\n```py\n>>> image_processor = processor.image_processor\n\n\n>>> def get_ocr_words_and_boxes(examples):\n...     images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n...     encoded_inputs = image_processor(images)\n\n...     examples[\"image\"] = encoded_inputs.pixel_values\n...     examples[\"words\"] = encoded_inputs.words",
  "...     examples[\"boxes\"] = encoded_inputs.boxes\n\n...     return examples\n```\n\nTo apply this preprocessing to the entire dataset in a fast way, use [`~datasets.Dataset.map`].\n\n```py\n>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)\n```\n\n### Preprocessing text data\n\nOnce we have applied OCR to the images, we need to encode the text part of the dataset to prepare it for the model.\nThis involves converting the words and boxes that we got in the previous step to token-level `input_ids`, `attention_mask`,\n`token_type_ids` and `bbox`. For preprocessing text, we'll need the `tokenizer` from the processor.\n\n```py\n>>> tokenizer = processor.tokenizer\n```\n\nOn top of the preprocessing mentioned above, we also need to add the labels for the model. For `xxxForQuestionAnswering` models\nin 🤗 Transformers, the labels consist of the `start_positions` and `end_positions`, indicating which token is at the\nstart and which token is at the end of the answer.\n\nLet's start with that. Define a helper function that can find a sublist (the answer split into words) in a larger list (the words list).",
  "This function will take two lists as input, `words_list` and `answer_list`. It will then iterate over the `words_list` and check\nif the current word in the `words_list` (words_list[i]) is equal to the first word of answer_list (answer_list[0]) and if\nthe sublist of `words_list` starting from the current word and of the same length as `answer_list` is equal `to answer_list`.\nIf this condition is true, it means that a match has been found, and the function will record the match, its starting index (idx),\nand its ending index (idx + len(answer_list) - 1). If more than one match was found, the function will return only the first one.\nIf no match is found, the function returns (`None`, 0, and 0).\n\n```py\n>>> def subfinder(words_list, answer_list):\n...     matches = []\n...     start_indices = []\n...     end_indices = []\n...     for idx, i in enumerate(range(len(words_list))):\n...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:\n...             matches.append(answer_list)\n...             start_indices.append(idx)\n...             end_indices.append(idx + len(answer_list) - 1)\n...     if matches:",
  "...         return matches[0], start_indices[0], end_indices[0]\n...     else:\n...         return None, 0, 0\n```\n\nTo illustrate how this function finds the position of the answer, let's use it on an example:\n\n```py\n>>> example = dataset_with_ocr[\"train\"][1]\n>>> words = [word.lower() for word in example[\"words\"]]\n>>> match, word_idx_start, word_idx_end = subfinder(words, example[\"answer\"].lower().split())\n>>> print(\"Question: \", example[\"question\"])\n>>> print(\"Words:\", words)\n>>> print(\"Answer: \", example[\"answer\"])\n>>> print(\"start_index\", word_idx_start)\n>>> print(\"end_index\", word_idx_end)\nQuestion:  Who is in  cc in this letter?",
  "Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', '«short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', '«extremely', 'fast', 'buming', 'cigarette.', '«novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', '«more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.', '499150498']",
  "Answer:  T.F. Riehl\nstart_index 17\nend_index 18\n```\n\nOnce examples are encoded, however, they will look like this:\n\n```py\n>>> encoding = tokenizer(example[\"question\"], example[\"words\"], example[\"boxes\"])\n>>> tokenizer.decode(encoding[\"input_ids\"])\n[CLS] who is in cc in this letter? [SEP] wie baw brown & williamson tobacco corporation research & development ...\n```\n\nWe'll need to find the position of the answer in the encoded input.\n* `token_type_ids` tells us which tokens are part of the question, and which ones are part of the document's words.\n* `tokenizer.cls_token_id` will help find the special token at the beginning of the input.\n* `word_ids` will help match the answer found in the original `words` to the same answer in the full encoded input and determine\nthe start/end position of the answer in the encoded input.\n\nWith that in mind, let's create a function to encode a batch of examples in the dataset:\n\n```py\n>>> def encode_dataset(examples, max_length=512):\n...     questions = examples[\"question\"]\n...     words = examples[\"words\"]\n...     boxes = examples[\"boxes\"]\n...     answers = examples[\"answer\"]",
  "...     # encode the batch of examples and initialize the start_positions and end_positions\n...     encoding = tokenizer(questions, words, boxes, max_length=max_length, padding=\"max_length\", truncation=True)\n...     start_positions = []\n...     end_positions = []\n\n...     # loop through the examples in the batch\n...     for i in range(len(questions)):\n...         cls_index = encoding[\"input_ids\"][i].index(tokenizer.cls_token_id)\n\n...         # find the position of the answer in example's words\n...         words_example = [word.lower() for word in words[i]]\n...         answer = answers[i]\n...         match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())\n\n...         if match:\n...             # if match is found, use `token_type_ids` to find where words start in the encoding\n...             token_type_ids = encoding[\"token_type_ids\"][i]\n...             token_start_index = 0\n...             while token_type_ids[token_start_index] != 1:\n...                 token_start_index += 1\n\n...             token_end_index = len(encoding[\"input_ids\"][i]) - 1\n...             while token_type_ids[token_end_index] != 1:\n...                 token_end_index -= 1",
  "...             word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]\n...             start_position = cls_index\n...             end_position = cls_index\n\n...             # loop over word_ids and increase `token_start_index` until it matches the answer position in words\n...             # once it matches, save the `token_start_index` as the `start_position` of the answer in the encoding\n...             for id in word_ids:\n...                 if id == word_idx_start:\n...                     start_position = token_start_index\n...                 else:\n...                     token_start_index += 1\n\n...             # similarly loop over `word_ids` starting from the end to find the `end_position` of the answer\n...             for id in word_ids[::-1]:\n...                 if id == word_idx_end:\n...                     end_position = token_end_index\n...                 else:\n...                     token_end_index -= 1\n\n...             start_positions.append(start_position)\n...             end_positions.append(end_position)\n\n...         else:\n...             start_positions.append(cls_index)\n...             end_positions.append(cls_index)",
  "...     encoding[\"image\"] = examples[\"image\"]\n...     encoding[\"start_positions\"] = start_positions\n...     encoding[\"end_positions\"] = end_positions\n\n...     return encoding\n```\n\nNow that we have this preprocessing function, we can encode the entire dataset:\n\n```py\n>>> encoded_train_dataset = dataset_with_ocr[\"train\"].map(\n...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr[\"train\"].column_names\n... )\n>>> encoded_test_dataset = dataset_with_ocr[\"test\"].map(\n...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr[\"test\"].column_names\n... )\n```\n\nLet's check what the features of the encoded dataset look like:\n\n```py\n>>> encoded_train_dataset.features\n{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),",
  "'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n'start_positions': Value(dtype='int64', id=None),\n'end_positions': Value(dtype='int64', id=None)}\n```\n\n## Evaluation\n\nEvaluation for document question answering requires a significant amount of postprocessing. To avoid taking up too much\nof your time, this guide skips the evaluation step. The [`Trainer`] still calculates the evaluation loss during training so\nyou're not completely in the dark about your model's performance. Extractive question answering is typically evaluated using F1/exact match.\nIf you'd like to implement it yourself, check out the [Question Answering chapter](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)\nof the Hugging Face course for inspiration.\n\n## Train\n\nCongratulations! You've successfully navigated the toughest part of this guide and now you are ready to train your own model.\nTraining involves the following steps:\n* Load the model with [`AutoModelForDocumentQuestionAnswering`] using the same checkpoint as in the preprocessing.\n* Define your training hyperparameters in [`TrainingArguments`].",
  "* Define a function to batch examples together, here the [`DefaultDataCollator`] will do just fine\n* Pass the training arguments to [`Trainer`] along with the model, dataset, and data collator.\n* Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> from transformers import AutoModelForDocumentQuestionAnswering\n\n>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)\n```\n\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, and configure hyperparameters as you see fit.\nIf you wish to share your model with the community, set `push_to_hub` to `True` (you must be signed in to Hugging Face to upload your model).\nIn this case the `output_dir` will also be the name of the repo where your model checkpoint will be pushed.\n\n```py\n>>> from transformers import TrainingArguments\n\n>>> # REPLACE THIS WITH YOUR REPO ID\n>>> repo_id = \"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\"\n\n>>> training_args = TrainingArguments(\n...     output_dir=repo_id,\n...     per_device_train_batch_size=4,\n...     num_train_epochs=20,\n...     save_steps=200,\n...     logging_steps=50,\n...     eval_strategy=\"steps\",\n...     learning_rate=5e-5,",
  "...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n```\n\nDefine a simple data collator to batch examples together.\n\n```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator()\n```\n\nFinally, bring everything together, and call [`~Trainer.train`]:\n\n```py\n>>> from transformers import Trainer\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     data_collator=data_collator,\n...     train_dataset=encoded_train_dataset,\n...     eval_dataset=encoded_test_dataset,\n...     processing_class=processor,\n... )\n\n>>> trainer.train()\n```\n\nTo add the final model to 🤗 Hub, create a model card and call `push_to_hub`:\n\n```py\n>>> trainer.create_model_card()\n>>> trainer.push_to_hub()\n```\n\n## Inference\n\nNow that you have finetuned a LayoutLMv2 model, and uploaded it to the 🤗 Hub, you can use it for inference. The simplest\nway to try out your finetuned model for inference is to use it in a [`Pipeline`].\n\nLet's take an example:\n```py\n>>> example = dataset[\"test\"][2]\n>>> question = example[\"query\"][\"en\"]\n>>> image = example[\"image\"]\n>>> print(question)\n>>> print(example[\"answers\"])",
  "'Who is ‘presiding’ TRRF GENERAL SESSION (PART 1)?'\n['TRRF Vice President', 'lee a. waller']\n```\n\nNext, instantiate a pipeline for\ndocument question answering with your model, and pass the image + question combination to it.\n\n```py\n>>> from transformers import pipeline\n\n>>> qa_pipeline = pipeline(\"document-question-answering\", model=\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n>>> qa_pipeline(image, question)\n[{'score': 0.9949808120727539,\n'answer': 'Lee A. Waller',\n'start': 55,\n'end': 57}]\n```\n\nYou can also manually replicate the results of the pipeline if you'd like:\n1. Take an image and a question, prepare them for the model using the processor from your model.\n2. Forward the result or preprocessing through the model.\n3. The model returns `start_logits` and `end_logits`, which indicate which token is at the start of the answer and\nwhich token is at the end of the answer. Both have shape (batch_size, sequence_length).\n4. Take an argmax on the last dimension of both the `start_logits` and `end_logits` to get the predicted `start_idx` and `end_idx`.\n5. Decode the answer with the tokenizer.\n\n```py\n>>> import torch\n>>> from transformers import AutoProcessor",
  ">>> from transformers import AutoModelForDocumentQuestionAnswering\n\n>>> processor = AutoProcessor.from_pretrained(\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n\n>>> with torch.no_grad():\n...     encoding = processor(image.convert(\"RGB\"), question, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     start_logits = outputs.start_logits\n...     end_logits = outputs.end_logits\n...     predicted_start_idx = start_logits.argmax(-1).item()\n...     predicted_end_idx = end_logits.argmax(-1).item()\n\n>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])\n'lee a. waller'\n```",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning.\nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more.",
  "This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS.\n\n[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub. For each variant, you can also find fine-tuned instructed\nversions of the model adapted for conversational use cases.\n\nThis model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However,",
  "being a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether\nthis approach suits your use case better than fine-tuning specialized models for each individual task.\n\nIn this guide, you'll learn how to:\n- [Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#quantized-model)\n- Use IDEFICS for:\n- [Image captioning](#image-captioning)\n- [Prompted image captioning](#prompted-image-captioning)\n- [Few-shot prompting](#few-shot-prompting)\n- [Visual question answering](#visual-question-answering)\n- [Image classification](#image-classification)\n- [Image-guided text generation](#image-guided-text-generation)\n- [Run inference in batch mode](#running-inference-in-batch-mode)\n- [Run IDEFICS instruct for conversational use](#idefics-instruct-for-conversational-use)\n\nBefore you begin, make sure you have all the necessary libraries installed.\n\n```bash\npip install -q bitsandbytes sentencepiece accelerate transformers\n```\n\n<Tip>\nTo run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.\n</Tip>\n\n## Loading the model",
  "Let's start by loading the model's 9 billion parameters checkpoint:\n\n```py\n>>> checkpoint = \"HuggingFaceM4/idefics-9b\"\n```\n\nJust like for other Transformers models, you need to load a processor and the model itself from the checkpoint.\nThe IDEFICS processor wraps a [`LlamaTokenizer`] and IDEFICS image processor into a single processor to take care of\npreparing text and image inputs for the model.\n\n```py\n>>> import torch\n\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\n\n>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n```\n\nSetting `device_map` to `\"auto\"` will automatically determine how to load and store the model weights in the most optimized\nmanner given existing devices.\n\n### Quantized model\n\nIf high-memory GPU availability is an issue, you can load the quantized version of the model. To load the model and the\nprocessor in 4bit precision, pass a `BitsAndBytesConfig` to the `from_pretrained` method and the model will be compressed\non the fly while loading.\n\n```py\n>>> import torch",
  ">>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\n\n>>> quantization_config = BitsAndBytesConfig(\n...     load_in_4bit=True,\n...     bnb_4bit_compute_dtype=torch.float16,\n... )\n\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\n\n>>> model = IdeficsForVisionText2Text.from_pretrained(\n...     checkpoint,\n...     quantization_config=quantization_config,\n...     device_map=\"auto\"\n... )\n```\n\nNow that you have the model loaded in one of the suggested ways, let's move on to exploring tasks that you can use IDEFICS for.\n\n## Image captioning\nImage captioning is the task of predicting a caption for a given image. A common application is to aid visually impaired\npeople navigate through different situations, for instance, explore image content online.\n\nTo illustrate the task, get an image to be captioned, e.g.:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-im-captioning.jpg\" alt=\"Image of a puppy in a flower bed\"/>\n</div>\n\nPhoto by [Hendo Wang](https://unsplash.com/@hendoo).",
  "IDEFICS accepts text and image prompts. However, to caption an image, you do not have to provide a text prompt to the\nmodel, only the preprocessed input image. Without a text prompt, the model will start generating text from the\nBOS (beginning-of-sequence) token thus creating a caption.\n\nAs image input to the model, you can use either an image object (`PIL.Image`) or a url from which the image can be retrieved.\n\n```py\n>>> prompt = [\n...     \"https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80\",\n... ]\n\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> print(generated_text[0])\nA puppy in a flower bed\n```\n\n<Tip>\n\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid errors arising when increasing",
  "the `max_new_tokens`: the model will want to generate a new `<image>` or `<fake_token_around_image>` token when there\nis no image being generated by the model.\nYou can set it on-the-fly as in this guide, or store in the `GenerationConfig` as described in the [Text generation strategies](../generation_strategies) guide.\n</Tip>\n\n## Prompted image captioning\n\nYou can extend image captioning by providing a text prompt, which the model will continue given the image. Let's take\nanother image to illustrate:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-prompted-im-captioning.jpg\" alt=\"Image of the Eiffel Tower at night\"/>\n</div>\n\nPhoto by [Denys Nevozhai](https://unsplash.com/@dnevozhai).\n\nTextual and image prompts can be passed to the model's processor as a single list to create appropriate inputs.\n\n```py\n>>> prompt = [\n...     \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\n...     \"This is an image of \",\n... ]",
  ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> print(generated_text[0])\nThis is an image of the Eiffel Tower in Paris, France.\n```\n\n## Few-shot prompting\n\nWhile IDEFICS demonstrates great zero-shot results, your task may require a certain format of the caption, or come with\nother restrictions or requirements that increase task's complexity. Few-shot prompting can be used to enable in-context learning.\nBy providing examples in the prompt, you can steer the model to generate results that mimic the format of given examples.\n\nLet's use the previous image of the Eiffel Tower as an example for the model and build a prompt that demonstrates to the model\nthat in addition to learning what the object in an image is, we would also like to get some interesting information about it.\nThen, let's see, if we can get the same response format for an image of the Statue of Liberty:",
  "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\" alt=\"Image of the Statue of Liberty\"/>\n</div>\n\nPhoto by [Juan Mayobre](https://unsplash.com/@jmayobres).\n\n```py\n>>> prompt = [\"User:\",\n...            \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\n...            \"Describe this image.\\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\\n\",\n...            \"User:\",\n...            \"https://images.unsplash.com/photo-1524099163253-32b7f0256868?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3387&q=80\",\n...            \"Describe this image.\\nAssistant:\"\n...            ]\n\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)",
  ">>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> print(generated_text[0])\nUser: Describe this image.\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\nUser: Describe this image.\nAssistant: An image of the Statue of Liberty. Fun fact: the Statue of Liberty is 151 feet tall.\n```\n\nNotice that just from a single example (i.e., 1-shot) the model has learned how to perform the task. For more complex tasks,\nfeel free to experiment with a larger number of examples (e.g., 3-shot, 5-shot, etc.).\n\n## Visual question answering\n\nVisual Question Answering (VQA) is the task of answering open-ended questions based on an image. Similar to image\ncaptioning it can be used in accessibility applications, but also in education (reasoning about visual materials), customer\nservice (questions about products based on images), and image retrieval.\n\nLet's get a new image for this task:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-vqa.jpg\" alt=\"Image of a couple having a picnic\"/>\n</div>",
  "Photo by [Jarritos Mexican Soda](https://unsplash.com/@jarritos).\n\nYou can steer the model from image captioning to visual question answering by prompting it with appropriate instructions:\n\n```py\n>>> prompt = [\n...     \"Instruction: Provide an answer to the question. Use the image to answer.\\n\",\n...     \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n...     \"Question: Where are these people and what's the weather like? Answer:\"\n... ]\n\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> print(generated_text[0])\nInstruction: Provide an answer to the question. Use the image to answer.\nQuestion: Where are these people and what's the weather like? Answer: They're in a park in New York City, and it's a beautiful day.\n```\n\n## Image classification",
  "IDEFICS is capable of classifying images into different categories without being explicitly trained on data containing\nlabeled examples from those specific categories. Given a list of categories and using its image and text understanding\ncapabilities, the model can infer which category the image likely belongs to.\n\nSay, we have this image of a vegetable stand:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-classification.jpg\" alt=\"Image of a vegetable stand\"/>\n</div>\n\nPhoto by [Peter Wendt](https://unsplash.com/@peterwendt).\n\nWe can instruct the model to classify the image into one of the categories that we have:\n\n```py\n>>> categories = ['animals','vegetables', 'city landscape', 'cars', 'office']\n>>> prompt = [f\"Instruction: Classify the following image into a single category from the following list: {categories}.\\n\",\n...     \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n...     \"Category: \"\n... ]",
  ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> print(generated_text[0])\nInstruction: Classify the following image into a single category from the following list: ['animals', 'vegetables', 'city landscape', 'cars', 'office'].\nCategory: Vegetables\n```\n\nIn the example above we instruct the model to classify the image into a single category, however, you can also prompt the model to do rank classification.\n\n## Image-guided text generation\n\nFor more creative applications, you can use image-guided text generation to generate text based on an image. This can be\nuseful to create descriptions of products, ads, descriptions of a scene, etc.\n\nLet's prompt IDEFICS to write a story based on a simple image of a red door:\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-story-generation.jpg\" alt=\"Image of a red door with a pumpkin on the steps\"/>\n</div>\n\nPhoto by [Craig Tidball](https://unsplash.com/@devonshiremedia).\n\n```py\n>>> prompt = [\"Instruction: Use the image to write a story. \\n\",\n...     \"https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80\",\n...     \"Story: \\n\"]\n\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=200, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> print(generated_text[0])\nInstruction: Use the image to write a story.\nStory:\nOnce upon a time, there was a little girl who lived in a house with a red door.  She loved her red door.  It was the prettiest door in the whole world.",
  "One day, the little girl was playing in her yard when she noticed a man standing on her doorstep.  He was wearing a long black coat and a top hat.\n\nThe little girl ran inside and told her mother about the man.\n\nHer mother said, “Don’t worry, honey.  He’s just a friendly ghost.”\n\nThe little girl wasn’t sure if she believed her mother, but she went outside anyway.\n\nWhen she got to the door, the man was gone.\n\nThe next day, the little girl was playing in her yard again when she noticed the man standing on her doorstep.\n\nHe was wearing a long black coat and a top hat.\n\nThe little girl ran\n```\n\nLooks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story about a ghost.\n\n<Tip>\n\nFor longer outputs like this, you will greatly benefit from tweaking the text generation strategy. This can help\nyou significantly improve the quality of the generated output. Check out [Text generation strategies](../generation_strategies)\nto learn more.\n</Tip>\n\n## Running inference in batch mode\n\nAll of the earlier sections illustrated IDEFICS for a single example. In a very similar fashion, you can run inference\nfor a batch of examples by passing a list of prompts:\n\n```py",
  ">>> prompts = [\n...     [   \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\n...         \"This is an image of \",\n...     ],\n...     [   \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n...         \"This is an image of \",\n...     ],\n...     [   \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n...         \"This is an image of \",\n...     ],\n... ]\n\n>>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> for i,t in enumerate(generated_text):\n...     print(f\"{i}:\\n{t}\\n\")\n0:\nThis is an image of the Eiffel Tower in Paris, France.",
  "1:\nThis is an image of a couple on a picnic blanket.\n\n2:\nThis is an image of a vegetable stand.\n```\n\n## IDEFICS instruct for conversational use\n\nFor conversational use cases, you can find fine-tuned instructed versions of the model on the 🤗 Hub:\n`HuggingFaceM4/idefics-80b-instruct` and `HuggingFaceM4/idefics-9b-instruct`.\n\nThese checkpoints are the result of fine-tuning the respective base models on a mixture of supervised and instruction\nfine-tuning datasets, which boosts the downstream performance while making the models more usable in conversational settings.\n\nThe use and prompting for the conversational use is very similar to using the base models:\n\n```py\n>>> import torch\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n>>> from accelerate.test_utils.testing import get_backend\n\n>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n>>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\n\n>>> prompts = [\n...     [",
  "...         \"User: What is in this image?\",\n...         \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n...         \"<end_of_utterance>\",\n\n...         \"\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\n\n...         \"\\nUser:\",\n...         \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\n...         \"And who is that?<end_of_utterance>\",\n\n...         \"\\nAssistant:\",\n...     ],\n... ]\n\n>>> # --batched mode\n>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\n>>> # --single sample mode\n>>> # inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n>>> # Generation args\n>>> exit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)",
  ">>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> for i, t in enumerate(generated_text):\n...     print(f\"{i}:\\n{t}\\n\")\n```",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Summarization\n\n[[open-in-colab]]\n\n<Youtube id=\"yHnr5Dk2zCI\"/>\n\nSummarization creates a shorter version of a document or an article that captures all the important information. Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task. Summarization can be:\n\n- Extractive: extract the most relevant information from a document.",
  "- Abstractive: generate new text that captures the most relevant information.\n\nThis guide will show you how to:\n\n1. Finetune [T5](https://huggingface.co/google-t5/t5-small) on the California state bill subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset for abstractive summarization.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/summarization)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate rouge_score\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load BillSum dataset\n\nStart by loading the smaller California state bill subset of the BillSum dataset from the 🤗 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> billsum = load_dataset(\"billsum\", split=\"ca_test\")\n```",
  "Split the dataset into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> billsum = billsum.train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:\n\n```py\n>>> billsum[\"train\"][0]",
  "{'summary': 'Existing law authorizes state agencies to enter into contracts for the acquisition of goods or services upon approval by the Department of General Services. Existing law sets forth various requirements and prohibitions for those contracts, including, but not limited to, a prohibition on entering into contracts for the acquisition of goods or services of $100,000 or more with a contractor that discriminates between spouses and domestic partners or same-sex and different-sex couples in the provision of benefits. Existing law provides that a contract entered into in violation of those requirements and prohibitions is void and authorizes the state or any person acting on behalf of the state to bring a civil action seeking a determination that a contract is in violation and therefore void. Under existing law, a willful violation of those requirements and prohibitions is a misdemeanor.\\nThis bill would also prohibit a state agency from entering into contracts for the acquisition of goods or services of $100,000 or more with a contractor that discriminates between employees on the basis of gender identity in the provision of benefits, as specified. By expanding the scope of a crime, this bill would impose a state-mandated local program.\\nThe California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\\nThis bill would provide that no reimbursement is required by this act for a specified reason.',",
  "'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 10295.35 is added to the Public Contract Code, to read:\\n10295.35.\\n(a) (1) Notwithstanding any other law, a state agency shall not enter into any contract for the acquisition of goods or services in the amount of one hundred thousand dollars ($100,000) or more with a contractor that, in the provision of benefits, discriminates between employees on the basis of an employee’s or dependent’s actual or perceived gender identity, including, but not limited to, the employee’s or dependent’s identification as transgender.\\n(2) For purposes of this section, “contract” includes contracts with a cumulative amount of one hundred thousand dollars ($100,000) or more per contractor in each fiscal year.\\n(3) For purposes of this section, an employee health plan is discriminatory if the plan is not consistent with Section 1365.5 of the Health and Safety Code and Section 10140 of the Insurance Code.\\n(4) The requirements of this section shall apply only to those portions of a contractor’s operations that occur under any of the following conditions:\\n(A) Within the state.\\n(B) On real property outside the state if the property is owned by the state or if the state has a right to occupy the property, and if the contractor’s presence at that location is connected to a contract with the state.\\n(C) Elsewhere in the United States where work related to a state contract is being performed.\\n(b) Contractors shall treat as confidential, to the maximum extent allowed by law or by the requirement of the contractor’s insurance provider, any request by an employee or applicant for employment benefits or any documentation of eligibility for benefits submitted by an employee or applicant for employment.\\n(c) After taking all reasonable measures to find a contractor that complies with this section, as determined by the state agency, the requirements of this section may be waived under any of the following circumstances:\\n(1) There is only one prospective contractor willing to enter into a specific contract with the state agency.\\n(2) The contract is necessary to respond to an emergency, as determined by the state agency, that endangers the public health, welfare, or safety, or the contract is necessary for the provision of essential services, and no entity that complies with the requirements of this section capable of responding to the emergency is immediately available.\\n(3) The requirements of this section violate, or are inconsistent with, the terms or conditions of a grant, subvention, or agreement, if the agency has made a good faith attempt to change the terms or conditions of any grant, subvention, or agreement to authorize application of this section.\\n(4) The contractor is providing wholesale or bulk water, power, or natural gas, the conveyance or transmission of the same, or ancillary services, as required for ensuring reliable services in accordance with good utility practice, if the purchase of the same cannot practically be accomplished through the standard competitive bidding procedures and the contractor is not providing direct retail services to end users.\\n(d) (1) A contractor shall not be deemed to discriminate in the provision of benefits if the contractor, in providing the benefits, pays the actual costs incurred in obtaining the benefit.\\n(2) If a contractor is unable to provide a certain benefit, despite taking reasonable measures to do so, the contractor shall not be deemed to discriminate in the provision of benefits.\\n(e) (1) Every contract subject to this chapter shall contain a statement by which the contractor certifies that the contractor is in compliance with this section.\\n(2) The department or other contracting agency shall enforce this section pursuant to its existing enforcement powers.\\n(3) (A) If a contractor falsely certifies that it is in compliance with this section, the contract with that contractor shall be subject to Article 9 (commencing with Section 10420), unless, within a time period specified by the department or other contracting agency, the contractor provides to the department or agency proof that it has complied, or is in the process of complying, with this section.\\n(B) The application of the remedies or penalties contained in Article 9 (commencing with Section 10420) to a contract subject to this chapter shall not preclude the application of any existing remedies otherwise available to the department or other contracting agency under its existing enforcement powers.\\n(f) Nothing in this section is intended to regulate the contracting practices of any local jurisdiction.\\n(g) This section shall be construed so as not to conflict with applicable federal laws, rules, or regulations. In the event that a court or agency of competent jurisdiction holds that federal law, rule, or regulation invalidates any clause, sentence, paragraph, or section of this code or the application thereof to any person or circumstances, it is the intent of the state that the court or agency sever that clause, sentence, paragraph, or section so that the remainder of this section shall remain in effect.\\nSEC. 2.\\nSection 10295.35 of the Public Contract Code shall not be construed to create any new enforcement authority or responsibility in the Department of General Services or any other contracting agency.\\nSEC. 3.\\nNo reimbursement is required by this act pursuant to Section 6 of Article XIII\\u2009B of the California Constitution because the only costs that may be incurred by a local agency or school district will be incurred because this act creates a new crime or infraction, eliminates a crime or infraction, or changes the penalty for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIII\\u2009B of the California Constitution.',",
  "'title': 'An act to add Section 10295.35 to the Public Contract Code, relating to public contracts.'}\n```\n\nThere are two fields that you'll want to use:\n\n- `text`: the text of the bill which'll be the input to the model.\n- `summary`: a condensed version of `text` which'll be the model target.\n\n## Preprocess\n\nThe next step is to load a T5 tokenizer to process `text` and `summary`:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> checkpoint = \"google-t5/t5-small\"\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```\n\nThe preprocessing function you want to create needs to:\n\n1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n2. Use the keyword `text_target` argument when tokenizing labels.\n3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter.\n\n```py\n>>> prefix = \"summarize: \"\n\n\n>>> def preprocess_function(examples):\n...     inputs = [prefix + doc for doc in examples[\"text\"]]\n...     model_inputs = tokenizer(inputs, max_length=1024, truncation=True)",
  "...     labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n\n...     model_inputs[\"labels\"] = labels[\"input_ids\"]\n...     return model_inputs\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\n\n```py\n>>> tokenized_billsum = billsum.map(preprocess_function, batched=True)\n```\n\nNow create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\n<frameworkcontent>\n<pt>\n\n```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n```\n</pt>\n<tf>\n\n```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Evaluate",
  "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py\n>>> import evaluate\n\n>>> rouge = evaluate.load(\"rouge\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the ROUGE metric:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n...     result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]",
  "...     result[\"gen_len\"] = np.mean(prediction_lens)\n\n...     return {k: round(v, 4) for k, v in result.items()}\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load T5 with [`AutoModelForSeq2SeqLM`]:\n\n```py\n>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`Seq2SeqTrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the ROUGE metric and save the training checkpoint.",
  "2. Pass the training arguments to [`Seq2SeqTrainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = Seq2SeqTrainingArguments(\n...     output_dir=\"my_awesome_billsum_model\",\n...     eval_strategy=\"epoch\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     weight_decay=0.01,\n...     save_total_limit=3,\n...     num_train_epochs=4,\n...     predict_with_generate=True,\n...     fp16=True, #change to bf16=True for XPU\n...     push_to_hub=True,\n... )\n\n>>> trainer = Seq2SeqTrainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_billsum[\"train\"],\n...     eval_dataset=tokenized_billsum[\"test\"],\n...     processing_class=tokenizer,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>",
  "If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer, AdamWeightDecay\n\n>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n```\n\nThen you can load T5 with [`TFAutoModelForSeq2SeqLM`]:\n\n```py\n>>> from transformers import TFAutoModelForSeq2SeqLM\n\n>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_billsum[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_test_set = model.prepare_tf_dataset(\n...     tokenized_billsum[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```",
  "Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```\n\nThe last two things to setup before you start training is to compute the ROUGE score from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n\nPass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_billsum_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py",
  ">>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for summarization, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with some text you'd like to summarize. For T5, you need to prefix your input depending on the task you're working on. For summarization you should prefix your input as shown below:\n\n```py",
  ">>> text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for summarization with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> summarizer = pipeline(\"summarization\", model=\"username/my_awesome_billsum_model\")\n>>> summarizer(text)\n[{\"summary_text\": \"The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country.\"}]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n\n<frameworkcontent>",
  "<pt>\nTokenize the text and return the `input_ids` as PyTorch tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n```\n\nUse the [`~generation.GenerationMixin.generate`] method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](../main_classes/text_generation) API.\n\n```py\n>>> from transformers import AutoModelForSeq2SeqLM\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n```\n</pt>\n<tf>",
  "Tokenize the text and return the `input_ids` as TensorFlow tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n>>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n```\n\nUse the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](../main_classes/text_generation) API.\n\n```py\n>>> from transformers import TFAutoModelForSeq2SeqLM\n\n>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mask Generation\n\nMask generation is the task of generating semantically meaningful masks for an image.\nThis task is very similar to [image segmentation](semantic_segmentation), but many differences exist. Image segmentation models are trained on labeled datasets and are limited to the classes they have seen during training; they return a set of masks and corresponding classes, given an image.",
  "Mask generation models are trained on large amounts of data and operate in two modes.\n- Prompting mode: In this mode, the model takes in an image and a prompt, where a prompt can be a 2D point location (XY coordinates) in the image within an object or a bounding box surrounding an object. In prompting mode, the model only returns the mask over the object\nthat the prompt is pointing out.\n- Segment Everything mode: In segment everything, given an image, the model generates every mask in the image. To do so, a grid of points is generated and overlaid on the image for inference.\n\nMask generation task is supported by [Segment Anything Model (SAM)](model_doc/sam). It's a powerful model that consists of a Vision Transformer-based image encoder, a prompt encoder, and a two-way transformer mask decoder. Images and prompts are encoded, and the decoder takes these embeddings and generates valid masks.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam.png\" alt=\"SAM Architecture\"/>\n</div>\n\nSAM serves as a powerful foundation model for segmentation as it has large data coverage. It is trained on",
  "[SA-1B](https://ai.meta.com/datasets/segment-anything/), a dataset with 1 million images and 1.1 billion masks.\n\nIn this guide, you will learn how to:\n- Infer in segment everything mode with batching,\n- Infer in point prompting mode,\n- Infer in box prompting mode.\n\nFirst, let's install `transformers`:\n\n```bash\npip install -q transformers\n```\n\n## Mask Generation Pipeline\n\nThe easiest way to infer mask generation models is to use the `mask-generation` pipeline.\n\n```python\n>>> from transformers import pipeline\n\n>>> checkpoint = \"facebook/sam-vit-base\"\n>>> mask_generator = pipeline(model=checkpoint, task=\"mask-generation\")\n```\n\nLet's see the image.\n\n```python\nfrom PIL import Image\nimport requests\n\nimg_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" alt=\"Example Image\"/>\n</div>",
  "Let's segment everything. `points-per-batch` enables parallel inference of points in segment everything mode. This enables faster inference, but consumes more memory. Moreover, SAM only enables batching over points and not the images. `pred_iou_thresh` is the IoU confidence threshold where only the masks above that certain threshold are returned.\n\n```python\nmasks = mask_generator(image, points_per_batch=128, pred_iou_thresh=0.88)\n```\n\nThe `masks` looks like the following:\n\n```bash\n{'masks': [array([[False, False, False, ...,  True,  True,  True],\n[False, False, False, ...,  True,  True,  True],\n[False, False, False, ...,  True,  True,  True],\n...,\n[False, False, False, ..., False, False, False],\n[False, False, False, ..., False, False, False],\n[False, False, False, ..., False, False, False]]),\narray([[False, False, False, ..., False, False, False],\n[False, False, False, ..., False, False, False],\n[False, False, False, ..., False, False, False],\n...,\n'scores': tensor([0.9972, 0.9917,\n...,\n}\n```\n\nWe can visualize them like this:\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.imshow(image, cmap='gray')\n\nfor i, mask in enumerate(masks[\"masks\"]):",
  "plt.imshow(mask, cmap='viridis', alpha=0.1, vmin=0, vmax=1)\n\nplt.axis('off')\nplt.show()\n```\n\nBelow is the original image in grayscale with colorful maps overlaid. Very impressive.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee_segmented.png\" alt=\"Visualized\"/>\n</div>\n\n\n## Model Inference\n\n### Point Prompting\n\nYou can also use the model without the pipeline. To do so, initialize the model and\nthe processor.\n\n```python\nfrom transformers import SamModel, SamProcessor\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\ndevice, _, _ = get_backend()\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n```\n\nTo do point prompting, pass the input point to the processor, then take the processor output\nand pass it to the model for inference. To post-process the model output, pass the outputs and\n`original_sizes` and `reshaped_input_sizes` we take from the processor's initial output. We need to pass these",
  "since the processor resizes the image, and the output needs to be extrapolated.\n\n```python\ninput_points = [[[2592, 1728]]] # point location of the bee\n\ninputs = processor(image, input_points=input_points, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = model(**inputs)\nmasks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n```\nWe can visualize the three masks in the `masks` output.\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(1, 4, figsize=(15, 5))\n\naxes[0].imshow(image)\naxes[0].set_title('Original Image')\nmask_list = [masks[0][0][0].numpy(), masks[0][0][1].numpy(), masks[0][0][2].numpy()]\n\nfor i, mask in enumerate(mask_list, start=1):\noverlayed_image = np.array(image).copy()\n\noverlayed_image[:,:,0] = np.where(mask == 1, 255, overlayed_image[:,:,0])\noverlayed_image[:,:,1] = np.where(mask == 1, 0, overlayed_image[:,:,1])\noverlayed_image[:,:,2] = np.where(mask == 1, 0, overlayed_image[:,:,2])\n\naxes[i].imshow(overlayed_image)\naxes[i].set_title(f'Mask {i}')\nfor ax in axes:\nax.axis('off')\n\nplt.show()\n```",
  "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/masks.png\" alt=\"Visualized\"/>\n</div>\n\n### Box Prompting\n\nYou can also do box prompting in a similar fashion to point prompting. You can simply pass the input box in the format of a list\n`[x_min, y_min, x_max, y_max]` format along with the image to the `processor`. Take the processor output and directly pass it\nto the model, then post-process the output again.\n\n\n```python\n# bounding box around the bee\nbox = [2350, 1600, 2850, 2100]\n\ninputs = processor(\nimage,\ninput_boxes=[[[box]]],\nreturn_tensors=\"pt\"\n).to(\"cuda\")\n\nwith torch.no_grad():\noutputs = model(**inputs)\n\nmask = processor.image_processor.post_process_masks(\noutputs.pred_masks.cpu(),\ninputs[\"original_sizes\"].cpu(),\ninputs[\"reshaped_input_sizes\"].cpu()\n)[0][0][0].numpy()\n```\n\nYou can visualize the bounding box around the bee as shown below.\n\n```python\nimport matplotlib.patches as patches\n\nfig, ax = plt.subplots()\nax.imshow(image)\n\nrectangle = patches.Rectangle((2350, 1600), 500, 500, linewidth=2, edgecolor='r', facecolor='none')\nax.add_patch(rectangle)\nax.axis(\"off\")\nplt.show()\n```",
  "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bbox.png\" alt=\"Visualized Bbox\"/>\n</div>\n\nYou can see the inference output below.\n\n```python\nfig, ax = plt.subplots()\nax.imshow(image)\nax.imshow(mask, cmap='viridis', alpha=0.4)\n\nax.axis(\"off\")\nplt.show()\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/box_inference.png\" alt=\"Visualized Inference\"/>\n</div>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Zero-shot object detection\n\n[[open-in-colab]]\n\nTraditionally, models used for [object detection](object_detection) require labeled image datasets for training,\nand are limited to detecting the set of classes from the training data.\n\nZero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit) model which uses a different approach. OWL-ViT",
  "is an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without\nthe need to fine-tune the model on labeled datasets.\n\nOWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\nlightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads,\nwhich associate images with their corresponding textual descriptions, while ViT processes image patches as inputs. The authors\nof OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\na bipartite matching loss.\n\nWith this approach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\n\nIn this guide, you will learn how to use OWL-ViT:\n- to detect objects based on text prompts\n- for batch object detection\n- for image-guided object detection\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash",
  "pip install -q transformers\n```\n\n## Zero-shot object detection pipeline\n\nThe simplest way to try out inference with OWL-ViT is to use it in a [`pipeline`]. Instantiate a pipeline\nfor zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit):\n\n```python\n>>> from transformers import pipeline\n\n>>> checkpoint = \"google/owlv2-base-patch16-ensemble\"\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\n```\n\nNext, choose an image you'd like to detect objects in. Here we'll use the image of astronaut Eileen Collins that is\na part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\n\n```py\n>>> import skimage\n>>> import numpy as np\n>>> from PIL import Image\n\n>>> image = skimage.data.astronaut()\n>>> image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\" alt=\"Astronaut Eileen Collins\"/>\n</div>\n\nPass the image and the candidate object labels to look for to the pipeline.",
  "Here we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for.\n\n```py\n>>> predictions = detector(\n...     image,\n...     candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n... )\n>>> predictions\n[{'score': 0.3571370542049408,\n'label': 'human face',\n'box': {'xmin': 180, 'ymin': 71, 'xmax': 271, 'ymax': 178}},\n{'score': 0.28099656105041504,\n'label': 'nasa badge',\n'box': {'xmin': 129, 'ymin': 348, 'xmax': 206, 'ymax': 427}},\n{'score': 0.2110239565372467,\n'label': 'rocket',\n'box': {'xmin': 350, 'ymin': -1, 'xmax': 468, 'ymax': 288}},\n{'score': 0.13790413737297058,\n'label': 'star-spangled banner',\n'box': {'xmin': 1, 'ymin': 1, 'xmax': 105, 'ymax': 509}},\n{'score': 0.11950037628412247,\n'label': 'nasa badge',\n'box': {'xmin': 277, 'ymin': 338, 'xmax': 327, 'ymax': 380}},\n{'score': 0.10649408400058746,\n'label': 'rocket',\n'box': {'xmin': 358, 'ymin': 64, 'xmax': 424, 'ymax': 280}}]\n```\n\nLet's visualize the predictions:\n\n```py\n>>> from PIL import ImageDraw\n\n>>> draw = ImageDraw.Draw(image)\n\n>>> for prediction in predictions:",
  "...     box = prediction[\"box\"]\n...     label = prediction[\"label\"]\n...     score = prediction[\"score\"]\n\n...     xmin, ymin, xmax, ymax = box.values()\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n...     draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png\" alt=\"Visualized predictions on NASA image\"/>\n</div>\n\n## Text-prompted zero-shot object detection by hand\n\nNow that you've seen how to use the zero-shot object detection pipeline, let's replicate the same\nresult manually.\n\nStart by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\nHere we'll use the same checkpoint as before:\n\n```py\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\n```\n\nLet's take a different image to switch things up.\n\n```py\n>>> import requests",
  ">>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n>>> im = Image.open(requests.get(url, stream=True).raw)\n>>> im\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\n</div>\n\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\n\n```py\n>>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n>>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\n```\n\nPass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\nboxes have the correct coordinates relative to the original image:\n\n```py\n>>> import torch\n\n>>> with torch.no_grad():",
  "...     outputs = model(**inputs)\n...     target_sizes = torch.tensor([im.size[::-1]])\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n\n>>> draw = ImageDraw.Draw(im)\n\n>>> scores = results[\"scores\"].tolist()\n>>> labels = results[\"labels\"].tolist()\n>>> boxes = results[\"boxes\"].tolist()\n\n>>> for box, score, label in zip(boxes, scores, labels):\n...     xmin, ymin, xmax, ymax = box\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n...     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")\n\n>>> im\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\n</div>\n\n## Batch processing\n\nYou can pass multiple sets of images and text queries to search for different (or same) objects in several images.\nLet's use both an astronaut image and the beach image together.\nFor batch processing, you should pass text queries as a nested list to the processor and images as lists of PIL images,\nPyTorch tensors, or NumPy arrays.",
  "```py\n>>> images = [image, im]\n>>> text_queries = [\n...     [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n...     [\"hat\", \"book\", \"sunglasses\", \"camera\"],\n... ]\n>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")\n```\n\nPreviously for post-processing you passed the single image's size as a tensor, but you can also pass a tuple, or, in case\nof several images, a list of tuples. Let's create predictions for the two examples, and visualize the second one (`image_idx = 1`).\n\n```py\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n...     target_sizes = [x.size[::-1] for x in images]\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\n\n>>> image_idx = 1\n>>> draw = ImageDraw.Draw(images[image_idx])\n\n>>> scores = results[image_idx][\"scores\"].tolist()\n>>> labels = results[image_idx][\"labels\"].tolist()\n>>> boxes = results[image_idx][\"boxes\"].tolist()\n\n>>> for box, score, label in zip(boxes, scores, labels):\n...     xmin, ymin, xmax, ymax = box\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)",
  "...     draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")\n\n>>> images[image_idx]\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\n</div>\n\n## Image-guided object detection\n\nIn addition to zero-shot object detection with text queries, OWL-ViT offers image-guided object detection. This means\nyou can use an image query to find similar objects in the target image.\nUnlike text queries, only a single example image is allowed.\n\nLet's take an image with two cats on a couch as a target image, and an image of a single cat\nas a query:\n\n```py\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image_target = Image.open(requests.get(url, stream=True).raw)\n\n>>> query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)\n```\n\nLet's take a quick look at the images:\n\n```py\n>>> import matplotlib.pyplot as plt\n\n>>> fig, ax = plt.subplots(1, 2)\n>>> ax[0].imshow(image_target)",
  ">>> ax[1].imshow(query_image)\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png\" alt=\"Cats\"/>\n</div>\n\nIn the preprocessing step, instead of text queries, you now need to use `query_images`:\n\n```py\n>>> inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")\n```\n\nFor predictions, instead of passing the inputs to the model, pass them to [`~OwlViTForObjectDetection.image_guided_detection`]. Draw the predictions\nas before except now there are no labels.\n\n```py\n>>> with torch.no_grad():\n...     outputs = model.image_guided_detection(**inputs)\n...     target_sizes = torch.tensor([image_target.size[::-1]])\n...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\n\n>>> draw = ImageDraw.Draw(image_target)\n\n>>> scores = results[\"scores\"].tolist()\n>>> boxes = results[\"boxes\"].tolist()\n\n>>> for box, score in zip(boxes, scores):\n...     xmin, ymin, xmax, ymax = box\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\n\n>>> image_target\n```",
  "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\n</div>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image Segmentation\n\n[[open-in-colab]]\n\n<Youtube id=\"dKE8SIt9C-w\"/>\n\nImage segmentation models separate areas corresponding to different areas of interest in an image. These models work by assigning a label to each pixel. There are several types of segmentation: semantic segmentation, instance segmentation, and panoptic segmentation.\n\nIn this guide, we will:\n1. [Take a look at different types of segmentation](#types-of-segmentation).",
  "2. [Have an end-to-end fine-tuning example for semantic segmentation](#fine-tuning-a-model-for-segmentation).\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```py\n# uncomment to install the necessary libraries\n!pip install -q datasets transformers evaluate accelerate\n```\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Types of Segmentation\n\nSemantic segmentation assigns a label or class to every single pixel in an image. Let's take a look at a semantic segmentation model output. It will assign the same class to every instance of an object it comes across in an image, for example, all cats will be labeled as \"cat\" instead of \"cat-1\", \"cat-2\".\nWe can use transformers' image segmentation pipeline to quickly infer a semantic segmentation model. Let's take a look at the example image.\n\n```python\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests",
  "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg\" alt=\"Segmentation Input\"/>\n</div>\n\nWe will use [nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024).\n\n```python\nsemantic_segmentation = pipeline(\"image-segmentation\", \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\")\nresults = semantic_segmentation(image)\nresults\n```\n\nThe segmentation pipeline output includes a mask for every predicted class.\n```bash\n[{'score': None,\n'label': 'road',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'sidewalk',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'building',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'wall',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'pole',",
  "'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'traffic sign',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'vegetation',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'terrain',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'sky',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': None,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\n\nTaking a look at the mask for the car class, we can see every car is classified with the same mask.\n\n```python\nresults[-1][\"mask\"]\n```\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/semantic_segmentation_output.png\" alt=\"Semantic Segmentation Output\"/>\n</div>",
  "In instance segmentation, the goal is not to classify every pixel, but to predict a mask for **every instance of an object** in a given image. It works very similar to object detection, where there is a bounding box for every instance, there's a segmentation mask instead. We will use [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance) for this.\n\n```python\ninstance_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-instance\")\nresults = instance_segmentation(image)\nresults\n```\n\nAs you can see below, there are multiple cars classified, and there's no classification for pixels other than pixels that belong to car and person instances.\n\n```bash\n[{'score': 0.999944,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999945,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999652,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.903529,\n'label': 'person',\n'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\nChecking out one of the car masks below.\n\n```python",
  "results[2][\"mask\"]\n```\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png\" alt=\"Semantic Segmentation Output\"/>\n</div>\n\nPanoptic segmentation combines semantic segmentation and instance segmentation, where every pixel is classified into a class and an instance of that class, and there are multiple masks for each instance of a class. We can use [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) for this.\n\n```python\npanoptic_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-panoptic\")\nresults = panoptic_segmentation(image)\nresults\n```\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is classified into one of the classes.\n\n```bash\n[{'score': 0.999981,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999958,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.99997,\n'label': 'vegetation',",
  "'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999575,\n'label': 'pole',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999958,\n'label': 'building',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999634,\n'label': 'road',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.996092,\n'label': 'sidewalk',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.999221,\n'label': 'car',\n'mask': <PIL.Image.Image image mode=L size=612x415>},\n{'score': 0.99987,\n'label': 'sky',\n'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\n\nLet's have a side by side comparison for all types of segmentation.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation-comparison.png\" alt=\"Segmentation Maps Compared\"/>\n</div>\n\nSeeing all types of segmentation, let's have a deep dive on fine-tuning a model for semantic segmentation.",
  "Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery.\n\n## Fine-tuning a Model for Segmentation\n\nWe will now:\n\n1. Finetune [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) on the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.\n2. Use your fine-tuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/image-segmentation)\n\n</Tip>\n\n\n### Load SceneParse150 dataset\n\nStart by loading a smaller subset of the SceneParse150 dataset from the 🤗 Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset\n\n>>> ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")\n```",
  "Split the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> ds = ds.train_test_split(test_size=0.2)\n>>> train_ds = ds[\"train\"]\n>>> test_ds = ds[\"test\"]\n```\n\nThen take a look at an example:\n\n```py\n>>> train_ds[0]\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,\n'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,\n'scene_category': 368}\n\n# view the image\n>>> train_ds[0][\"image\"]\n```\n\n- `image`: a PIL image of the scene.\n- `annotation`: a PIL image of the segmentation map, which is also the model's target.\n- `scene_category`: a category id that describes the image scene like \"kitchen\" or \"office\". In this guide, you'll only need `image` and `annotation`, both of which are PIL images.\n\nYou'll also want to create a dictionary that maps a label id to a label class which will be useful when you set up the model later. Download the mappings from the Hub and create the `id2label` and `label2id` dictionaries:\n\n```py\n>>> import json\n>>> from pathlib import Path\n>>> from huggingface_hub import hf_hub_download",
  ">>> repo_id = \"huggingface/label-files\"\n>>> filename = \"ade20k-id2label.json\"\n>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n>>> id2label = {int(k): v for k, v in id2label.items()}\n>>> label2id = {v: k for k, v in id2label.items()}\n>>> num_labels = len(id2label)\n```\n\n#### Custom dataset\n\nYou could also create and use your own dataset if you prefer to train with the [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py) script instead of a notebook instance. The script requires:\n\n1. a [`~datasets.DatasetDict`] with two [`~datasets.Image`] columns, \"image\" and \"label\"\n\n```py\nfrom datasets import Dataset, DatasetDict, Image\n\nimage_paths_train = [\"path/to/image_1.jpg/jpg\", \"path/to/image_2.jpg/jpg\", ..., \"path/to/image_n.jpg/jpg\"]\nlabel_paths_train = [\"path/to/annotation_1.png\", \"path/to/annotation_2.png\", ..., \"path/to/annotation_n.png\"]\n\nimage_paths_validation = [...]\nlabel_paths_validation = [...]\n\ndef create_dataset(image_paths, label_paths):\ndataset = Dataset.from_dict({\"image\": sorted(image_paths),",
  "\"label\": sorted(label_paths)})\ndataset = dataset.cast_column(\"image\", Image())\ndataset = dataset.cast_column(\"label\", Image())\nreturn dataset\n\n# step 1: create Dataset objects\ntrain_dataset = create_dataset(image_paths_train, label_paths_train)\nvalidation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n\n# step 2: create DatasetDict\ndataset = DatasetDict({\n\"train\": train_dataset,\n\"validation\": validation_dataset,\n}\n)\n\n# step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)\ndataset.push_to_hub(\"your-name/dataset-repo\")\n\n# optionally, you can push to a private repo on the Hub\n# dataset.push_to_hub(\"name of repo on the hub\", private=True)\n```\n\n2. an id2label dictionary mapping the class integers to their class names\n\n```py\nimport json\n# simple example\nid2label = {0: 'cat', 1: 'dog'}\nwith open('id2label.json', 'w') as fp:\njson.dump(id2label, fp)\n```\n\nAs an example, take a look at this [example dataset](https://huggingface.co/datasets/nielsr/ade20k-demo) which was created with the steps shown above.\n\n### Preprocess",
  "The next step is to load a SegFormer image processor to prepare the images and annotations for the model. Some datasets, like this one, use the zero-index as the background class. However, the background class isn't actually included in the 150 classes, so you'll need to set `do_reduce_labels=True` to subtract one from all the labels. The zero-index is replaced by `255` so it's ignored by SegFormer's loss function:\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> checkpoint = \"nvidia/mit-b0\"\n>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)\n```\n\n<frameworkcontent>\n<pt>\n\nIt is common to apply some data augmentations to an image dataset to make a model more robust against overfitting. In this guide, you'll use the [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) function from [torchvision](https://pytorch.org/vision/stable/index.html) to randomly change the color properties of an image, but you can also use any image library you like.\n\n```py\n>>> from torchvision.transforms import ColorJitter\n\n>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n```",
  "Now create two preprocessing functions to prepare the images and annotations for the model. These functions convert the images into `pixel_values` and annotations to `labels`. For the training set, `jitter` is applied before providing the images to the image processor. For the test set, the image processor crops and normalizes the `images`, and only crops the `labels` because no data augmentation is applied during testing.\n\n```py\n>>> def train_transforms(example_batch):\n...     images = [jitter(x) for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n\n\n>>> def val_transforms(example_batch):\n...     images = [x for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n```\n\nTo apply the `jitter` over the entire dataset, use the 🤗 Datasets [`~datasets.Dataset.set_transform`] function. The transform is applied on the fly which is faster and consumes less disk space:\n\n```py\n>>> train_ds.set_transform(train_transforms)\n>>> test_ds.set_transform(val_transforms)\n```\n\n</pt>",
  "</frameworkcontent>\n\n<frameworkcontent>\n<tf>\nIt is common to apply some data augmentations to an image dataset to make a model more robust against overfitting.\nIn this guide, you'll use [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) to randomly change the color properties of an image, but you can also use any image\nlibrary you like.\nDefine two separate transformation functions:\n- training data transformations that include image augmentation\n- validation data transformations that only transpose the images, since computer vision models in 🤗 Transformers expect channels-first layout\n\n```py\n>>> import tensorflow as tf\n\n\n>>> def aug_transforms(image):\n...     image = tf.keras.utils.img_to_array(image)\n...     image = tf.image.random_brightness(image, 0.25)\n...     image = tf.image.random_contrast(image, 0.5, 2.0)\n...     image = tf.image.random_saturation(image, 0.75, 1.25)\n...     image = tf.image.random_hue(image, 0.1)\n...     image = tf.transpose(image, (2, 0, 1))\n...     return image\n\n\n>>> def transforms(image):\n...     image = tf.keras.utils.img_to_array(image)\n...     image = tf.transpose(image, (2, 0, 1))\n...     return image\n```",
  "Next, create two preprocessing functions to prepare batches of images and annotations for the model. These functions apply\nthe image transformations and use the earlier loaded `image_processor` to convert the images into `pixel_values` and\nannotations to `labels`. `ImageProcessor` also takes care of resizing and normalizing the images.\n\n```py\n>>> def train_transforms(example_batch):\n...     images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n\n\n>>> def val_transforms(example_batch):\n...     images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n```\n\nTo apply the preprocessing transformations over the entire dataset, use the 🤗 Datasets [`~datasets.Dataset.set_transform`] function.\nThe transform is applied on the fly which is faster and consumes less disk space:\n\n```py\n>>> train_ds.set_transform(train_transforms)\n>>> test_ds.set_transform(val_transforms)\n```\n</tf>\n</frameworkcontent>",
  "### Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load an evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py\n>>> import evaluate\n\n>>> metric = evaluate.load(\"mean_iou\")\n```\n\nThen create a function to [`~evaluate.EvaluationModule.compute`] the metrics. Your predictions need to be converted to\nlogits first, and then reshaped to match the size of the labels before you can call [`~evaluate.EvaluationModule.compute`]:\n\n<frameworkcontent>\n<pt>\n\n```py\n>>> import numpy as np\n>>> import torch\n>>> from torch import nn\n\n>>> def compute_metrics(eval_pred):\n...     with torch.no_grad():\n...         logits, labels = eval_pred\n...         logits_tensor = torch.from_numpy(logits)\n...         logits_tensor = nn.functional.interpolate(\n...             logits_tensor,\n...             size=labels.shape[-2:],",
  "...             mode=\"bilinear\",\n...             align_corners=False,\n...         ).argmax(dim=1)\n\n...         pred_labels = logits_tensor.detach().cpu().numpy()\n...         metrics = metric.compute(\n...             predictions=pred_labels,\n...             references=labels,\n...             num_labels=num_labels,\n...             ignore_index=255,\n...             reduce_labels=False,\n...         )\n...         for key, value in metrics.items():\n...             if isinstance(value, np.ndarray):\n...                 metrics[key] = value.tolist()\n...         return metrics\n```\n\n</pt>\n</frameworkcontent>\n\n\n<frameworkcontent>\n<tf>\n\n```py\n>>> def compute_metrics(eval_pred):\n...     logits, labels = eval_pred\n...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n...     logits_resized = tf.image.resize(\n...         logits,\n...         size=tf.shape(labels)[1:],\n...         method=\"bilinear\",\n...     )\n\n...     pred_labels = tf.argmax(logits_resized, axis=-1)\n...     metrics = metric.compute(\n...         predictions=pred_labels,\n...         references=labels,\n...         num_labels=num_labels,\n...         ignore_index=-1,\n...         reduce_labels=image_processor.do_reduce_labels,",
  "...     )\n\n...     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n...     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n\n...     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n...     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n...     return {\"val_\" + k: v for k, v in metrics.items()}\n```\n\n</tf>\n</frameworkcontent>\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n### Train\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#finetune-with-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load SegFormer with [`AutoModelForSemanticSegmentation`], and pass the model the mapping between label ids and label classes:\n\n```py\n>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n\n>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)\n```\n\nAt this point, only three steps remain:",
  "1. Define your training hyperparameters in [`TrainingArguments`]. It is important you don't remove unused columns because this'll drop the `image` column. Without the `image` column, you can't create `pixel_values`. Set `remove_unused_columns=False` to prevent this behavior! The only other required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the IoU metric and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"segformer-b0-scene-parse-150\",\n...     learning_rate=6e-5,\n...     num_train_epochs=50,\n...     per_device_train_batch_size=2,\n...     per_device_eval_batch_size=2,\n...     save_total_limit=3,\n...     eval_strategy=\"steps\",\n...     save_strategy=\"steps\",\n...     save_steps=20,\n...     eval_steps=20,\n...     logging_steps=1,",
  "...     eval_accumulation_steps=5,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=train_ds,\n...     eval_dataset=test_ds,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n</frameworkcontent>\n\n<frameworkcontent>\n<tf>\n<Tip>\n\nIf you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](./training#train-a-tensorflow-model-with-keras) first!\n\n</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pretrained model.\n3. Convert a 🤗 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks to calculate metrics and upload your model to 🤗 Hub\n6. Use the `fit()` method to run the training.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py",
  ">>> from transformers import create_optimizer\n\n>>> batch_size = 2\n>>> num_epochs = 50\n>>> num_train_steps = len(train_ds) * num_epochs\n>>> learning_rate = 6e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```\n\nThen, load SegFormer with [`TFAutoModelForSemanticSegmentation`] along with the label mappings, and compile it with the\noptimizer. Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> from transformers import TFAutoModelForSemanticSegmentation\n\n>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```\n\nConvert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and the [`DefaultDataCollator`]:\n\n```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator(return_tensors=\"tf\")",
  ">>> tf_train_dataset = train_ds.to_tf_dataset(\n...     columns=[\"pixel_values\", \"label\"],\n...     shuffle=True,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_eval_dataset = test_ds.to_tf_dataset(\n...     columns=[\"pixel_values\", \"label\"],\n...     shuffle=True,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n```\n\nTo compute the accuracy from the predictions and push your model to the 🤗 Hub, use [Keras callbacks](../main_classes/keras_callbacks).\nPass your `compute_metrics` function to [`KerasMetricCallback`],\nand use the [`PushToHubCallback`] to upload the model:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n\n>>> metric_callback = KerasMetricCallback(\n...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n... )\n\n>>> push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", tokenizer=image_processor)\n\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you are ready to train your model! Call `fit()` with your training and validation datasets, the number of epochs,",
  "and your callbacks to fine-tune the model:\n\n```py\n>>> model.fit(\n...     tf_train_dataset,\n...     validation_data=tf_eval_dataset,\n...     callbacks=callbacks,\n...     epochs=num_epochs,\n... )\n```\n\nCongratulations! You have fine-tuned your model and shared it on the 🤗 Hub. You can now use it for inference!\n</tf>\n</frameworkcontent>\n\n### Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nReload the dataset and load an image for inference.\n\n```py\n>>> from datasets import load_dataset\n\n>>> ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")\n>>> ds = ds.train_test_split(test_size=0.2)\n>>> test_ds = ds[\"test\"]\n>>> image = ds[\"test\"][0][\"image\"]\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png\" alt=\"Image of bedroom\"/>\n</div>\n\n<frameworkcontent>\n<pt>\n\nWe will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel_values` on a GPU:\n\n```py\n>>> from accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)",
  ">>> device, _, _ = get_backend()\n>>> encoding = image_processor(image, return_tensors=\"pt\")\n>>> pixel_values = encoding.pixel_values.to(device)\n```\n\nPass your input to the model and return the `logits`:\n\n```py\n>>> outputs = model(pixel_values=pixel_values)\n>>> logits = outputs.logits.cpu()\n```\n\nNext, rescale the logits to the original image size:\n\n```py\n>>> upsampled_logits = nn.functional.interpolate(\n...     logits,\n...     size=image.size[::-1],\n...     mode=\"bilinear\",\n...     align_corners=False,\n... )\n\n>>> pred_seg = upsampled_logits.argmax(dim=1)[0]\n```\n\n</pt>\n</frameworkcontent>\n\n<frameworkcontent>\n<tf>\nLoad an image processor to preprocess the image and return the input as TensorFlow tensors:\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n>>> inputs = image_processor(image, return_tensors=\"tf\")\n```\n\nPass your input to the model and return the `logits`:\n\n```py\n>>> from transformers import TFAutoModelForSemanticSegmentation\n\n>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n>>> logits = model(**inputs).logits\n```",
  "Next, rescale the logits to the original image size and apply argmax on the class dimension:\n```py\n>>> logits = tf.transpose(logits, [0, 2, 3, 1])\n\n>>> upsampled_logits = tf.image.resize(\n...     logits,\n...     # We reverse the shape of `image` because `image.size` returns width and height.\n...     image.size[::-1],\n... )\n\n>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\n```\n\n</tf>\n</frameworkcontent>\n\nTo visualize the results, load the [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) as `ade_palette()` that maps each class to their RGB values.\n\n```py\ndef ade_palette():\nreturn np.asarray([\n[0, 0, 0],\n[120, 120, 120],\n[180, 120, 120],\n[6, 230, 230],\n[80, 50, 50],\n[4, 200, 3],\n[120, 120, 80],\n[140, 140, 140],\n[204, 5, 255],\n[230, 230, 230],\n[4, 250, 7],\n[224, 5, 255],\n[235, 255, 7],\n[150, 5, 61],\n[120, 120, 70],\n[8, 255, 51],\n[255, 6, 82],\n[143, 255, 140],\n[204, 255, 4],\n[255, 51, 7],\n[204, 70, 3],\n[0, 102, 200],\n[61, 230, 250],\n[255, 6, 51],\n[11, 102, 255],\n[255, 7, 71],\n[255, 9, 224],\n[9, 7, 230],\n[220, 220, 220],\n[255, 9, 92],\n[112, 9, 255],",
  "[8, 255, 214],\n[7, 255, 224],\n[255, 184, 6],\n[10, 255, 71],\n[255, 41, 10],\n[7, 255, 255],\n[224, 255, 8],\n[102, 8, 255],\n[255, 61, 6],\n[255, 194, 7],\n[255, 122, 8],\n[0, 255, 20],\n[255, 8, 41],\n[255, 5, 153],\n[6, 51, 255],\n[235, 12, 255],\n[160, 150, 20],\n[0, 163, 255],\n[140, 140, 140],\n[250, 10, 15],\n[20, 255, 0],\n[31, 255, 0],\n[255, 31, 0],\n[255, 224, 0],\n[153, 255, 0],\n[0, 0, 255],\n[255, 71, 0],\n[0, 235, 255],\n[0, 173, 255],\n[31, 0, 255],\n[11, 200, 200],\n[255, 82, 0],\n[0, 255, 245],\n[0, 61, 255],\n[0, 255, 112],\n[0, 255, 133],\n[255, 0, 0],\n[255, 163, 0],\n[255, 102, 0],\n[194, 255, 0],\n[0, 143, 255],\n[51, 255, 0],\n[0, 82, 255],\n[0, 255, 41],\n[0, 255, 173],\n[10, 0, 255],\n[173, 255, 0],\n[0, 255, 153],\n[255, 92, 0],\n[255, 0, 255],\n[255, 0, 245],\n[255, 0, 102],\n[255, 173, 0],\n[255, 0, 20],\n[255, 184, 184],\n[0, 31, 255],\n[0, 255, 61],\n[0, 71, 255],\n[255, 0, 204],\n[0, 255, 194],\n[0, 255, 82],\n[0, 10, 255],\n[0, 112, 255],\n[51, 0, 255],\n[0, 194, 255],\n[0, 122, 255],\n[0, 255, 163],\n[255, 153, 0],\n[0, 255, 10],\n[255, 112, 0],\n[143, 255, 0],\n[82, 0, 255],\n[163, 255, 0],\n[255, 235, 0],\n[8, 184, 170],\n[133, 0, 255],\n[0, 255, 92],\n[184, 0, 255],\n[255, 0, 31],\n[0, 184, 255],\n[0, 214, 255],",
  "[255, 0, 112],\n[92, 255, 0],\n[0, 224, 255],\n[112, 224, 255],\n[70, 184, 160],\n[163, 0, 255],\n[153, 0, 255],\n[71, 255, 0],\n[255, 0, 163],\n[255, 204, 0],\n[255, 0, 143],\n[0, 255, 235],\n[133, 255, 0],\n[255, 0, 235],\n[245, 0, 255],\n[255, 0, 122],\n[255, 245, 0],\n[10, 190, 212],\n[214, 255, 0],\n[0, 204, 255],\n[20, 0, 255],\n[255, 255, 0],\n[0, 153, 255],\n[0, 41, 255],\n[0, 255, 204],\n[41, 0, 255],\n[41, 255, 0],\n[173, 0, 255],\n[0, 245, 255],\n[71, 0, 255],\n[122, 0, 255],\n[0, 255, 184],\n[0, 92, 255],\n[184, 255, 0],\n[0, 133, 255],\n[255, 214, 0],\n[25, 194, 194],\n[102, 255, 0],\n[92, 0, 255],\n])\n```\n\nThen you can combine and plot your image and the predicted segmentation map:\n\n```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n\n>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n>>> palette = np.array(ade_palette())\n>>> for label, color in enumerate(palette):\n...     color_seg[pred_seg == label, :] = color\n>>> color_seg = color_seg[..., ::-1]  # convert to BGR\n\n>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n>>> img = img.astype(np.uint8)\n\n>>> plt.figure(figsize=(15, 10))\n>>> plt.imshow(img)\n>>> plt.show()",
  "```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png\" alt=\"Image of bedroom overlaid with segmentation map\"/>\n</div>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n\n# Image captioning\n\n[[open-in-colab]]\n\nImage captioning is the task of predicting a caption for a given image. Common real world applications of it include\naiding visually impaired people that can help them navigate through different situations. Therefore, image captioning\nhelps to improve content accessibility for people by describing images to them.\n\nThis guide will show you how to:\n\n* Fine-tune an image captioning model.",
  "* Use the fine-tuned model for inference.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate -q\npip install jiwer -q\n```\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\n\n\n```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n## Load the Pokémon BLIP captions dataset\n\nUse the 🤗 Dataset library to load a dataset that consists of {image-caption} pairs. To create your own image captioning dataset\nin PyTorch, you can follow [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb).\n\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"lambdalabs/pokemon-blip-captions\")\nds\n```\n```bash\nDatasetDict({\ntrain: Dataset({\nfeatures: ['image', 'text'],\nnum_rows: 833\n})\n})\n```\n\nThe dataset has two features, `image` and `text`.\n\n<Tip>",
  "Many image captioning datasets contain multiple captions per image. In those cases, a common strategy is to randomly sample a caption amongst the available ones during training.\n\n</Tip>\n\nSplit the dataset’s train split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n\n```python\nds = ds[\"train\"].train_test_split(test_size=0.1)\ntrain_ds = ds[\"train\"]\ntest_ds = ds[\"test\"]\n```\n\nLet's visualize a couple of samples from the training set.\n\n\n```python\nfrom textwrap import wrap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_images(images, captions):\nplt.figure(figsize=(20, 20))\nfor i in range(len(images)):\nax = plt.subplot(1, len(images), i + 1)\ncaption = captions[i]\ncaption = \"\\n\".join(wrap(caption, 12))\nplt.title(caption)\nplt.imshow(images[i])\nplt.axis(\"off\")\n\n\nsample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\nsample_captions = [train_ds[i][\"text\"] for i in range(5)]\nplot_images(sample_images_to_visualize, sample_captions)\n```\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png\" alt=\"Sample training images\"/>\n</div>\n\n## Preprocess the dataset\n\nSince the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions.\n\nTo do so, load the processor class associated with the model you are about to fine-tune.\n\n```python\nfrom transformers import AutoProcessor\n\ncheckpoint = \"microsoft/git-base\"\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n```\n\nThe processor will internally pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption.\n\n```python\ndef transforms(example_batch):\nimages = [x for x in example_batch[\"image\"]]\ncaptions = [x for x in example_batch[\"text\"]]\ninputs = processor(images=images, text=captions, padding=\"max_length\")\ninputs.update({\"labels\": inputs[\"input_ids\"]})\nreturn inputs\n\n\ntrain_ds.set_transform(transforms)\ntest_ds.set_transform(transforms)\n```\n\nWith the dataset ready, you can now set up the model for fine-tuning.\n\n## Load a base model",
  "Load the [\"microsoft/git-base\"](https://huggingface.co/microsoft/git-base) into a [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) object.\n\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n```\n\n## Evaluate\n\nImage captioning models are typically evaluated with the [Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) or [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer). For this guide, you will use the Word Error Rate (WER).\n\nWe use the 🤗 Evaluate library to do so. For potential limitations and other gotchas of the WER, refer to [this guide](https://huggingface.co/spaces/evaluate-metric/wer).\n\n\n```python\nfrom evaluate import load\nimport torch\n\nwer = load(\"wer\")\n\n\ndef compute_metrics(eval_pred):\nlogits, labels = eval_pred\npredicted = logits.argmax(-1)\ndecoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\ndecoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)\nwer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)",
  "return {\"wer_score\": wer_score}\n```\n\n## Train!\n\nNow, you are ready to start fine-tuning the model. You will use the 🤗 [`Trainer`] for this.\n\nFirst, define the training arguments using [`TrainingArguments`].\n\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\nmodel_name = checkpoint.split(\"/\")[1]\n\ntraining_args = TrainingArguments(\noutput_dir=f\"{model_name}-pokemon\",\nlearning_rate=5e-5,\nnum_train_epochs=50,\nfp16=True,\nper_device_train_batch_size=32,\nper_device_eval_batch_size=32,\ngradient_accumulation_steps=2,\nsave_total_limit=3,\neval_strategy=\"steps\",\neval_steps=50,\nsave_strategy=\"steps\",\nsave_steps=50,\nlogging_steps=50,\nremove_unused_columns=False,\npush_to_hub=True,\nlabel_names=[\"labels\"],\nload_best_model_at_end=True,\n)\n```\n\nThen pass them along with the datasets and the model to 🤗 Trainer.\n\n```python\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=train_ds,\neval_dataset=test_ds,\ncompute_metrics=compute_metrics,\n)\n```\n\nTo start training, simply call [`~Trainer.train`] on the [`Trainer`] object.\n\n```python\ntrainer.train()\n```\n\nYou should see the training loss drop smoothly as training progresses.",
  "Once training is completed, share your model to the Hub with the [`~Trainer.push_to_hub`] method so everyone can use your model:\n\n\n```python\ntrainer.push_to_hub()\n```\n\n## Inference\n\nTake a sample image from `test_ds` to test the model.\n\n\n```python\nfrom PIL import Image\nimport requests\n\nurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png\" alt=\"Test image\"/>\n</div>\n\nPrepare image for the model.\n\n```python\nfrom accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\ndevice, _, _ = get_backend()\ninputs = processor(images=image, return_tensors=\"pt\").to(device)\npixel_values = inputs.pixel_values\n```\n\nCall [`generate`] and decode the predictions.\n\n```python\ngenerated_ids = model.generate(pixel_values=pixel_values, max_length=50)\ngenerated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(generated_caption)\n```\n```bash",
  "a drawing of a pink and blue pokemon\n```\n\nLooks like the fine-tuned model generated a pretty good caption!",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image-text-to-text\n\n[[open-in-colab]]",
  "Image-text-to-text models, also known as vision language models (VLMs), are language models that take an image input. These models can tackle various tasks, from visual question answering to image segmentation. This task shares many similarities with image-to-text, but with some overlapping use cases like image captioning. Image-to-text models only take image inputs and often accomplish a specific task, whereas VLMs take open-ended text and image inputs and are more generalist models.\n\nIn this guide, we provide a brief overview of VLMs and show how to use them with Transformers for inference.\n\nTo begin with, there are multiple types of VLMs:\n- base models used for fine-tuning\n- chat fine-tuned models for conversation\n- instruction fine-tuned models\n\nThis guide focuses on inference with an instruction-tuned model.\n\nLet's begin installing the dependencies.\n\n```bash\npip install -q transformers accelerate flash_attn\n```\n\nLet's initialize the model and the processor.\n\n```python\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\ndevice = torch.device(\"cuda\")\nmodel = AutoModelForImageTextToText.from_pretrained(\n\"HuggingFaceM4/idefics2-8b\",",
  "torch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\n).to(device)\n\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n```\n\nThis model has a [chat template](./chat_templating) that helps user parse chat outputs. Moreover, the model can also accept multiple images as input in a single conversation or message. We will now prepare the inputs.\n\nThe image inputs look like the following.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\" alt=\"Two cats sitting on a net\"/>\n</div>\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" alt=\"A bee on a pink flower\"/>\n</div>\n\n\n```python\nfrom PIL import Image\nimport requests\n\nimg_urls =[\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\",\n\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"]\nimages = [Image.open(requests.get(img_urls[0], stream=True).raw),\nImage.open(requests.get(img_urls[1], stream=True).raw)]\n```",
  "Below is an example of the chat template. We can feed conversation turns and the last message as an input by appending it at the end of the template.\n\n\n```python\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n]\n},\n{\n\"role\": \"assistant\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"In this image we can see two cats on the nets.\"},\n]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"And how about this image?\"},\n]\n},\n]\n```\n\nWe will now call the processors' [`~ProcessorMixin.apply_chat_template`] method to preprocess its output along with the image inputs.\n\n```python\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[images[0], images[1]], return_tensors=\"pt\").to(device)\n```\n\nWe can now pass the preprocessed inputs to the model.\n\n```python\nwith torch.no_grad():\ngenerated_ids = model.generate(**inputs, max_new_tokens=500)\ngenerated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n\nprint(generated_texts)",
  "## ['User: What do we see in this image? \\nAssistant: In this image we can see two cats on the nets. \\nUser: And how about this image? \\nAssistant: In this image we can see flowers, plants and insect.']\n```\n\n## Pipeline\n\nThe fastest way to get started is to use the [`Pipeline`] API. Specify the `\"image-text-to-text\"` task and the model you want to use.\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n```\n\nThe example below uses chat templates to format the text inputs.\n\n```python\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"There's a pink flower\"},\n],\n},\n]\n```\n\nPass the chat template formatted text and image to [`Pipeline`] and set `return_full_text=False` to remove the input from the generated output.\n\n```python\noutputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)\noutputs[0][\"generated_text\"]",
  "#  with a yellow center in the foreground. The flower is surrounded by red and white flowers with green stems\n```\n\n## Streaming\n\nWe can use [text streaming](./generation_strategies#streaming) for a better generation experience. Transformers supports streaming with the [`TextStreamer`] or [`TextIteratorStreamer`] classes. We will use the [`TextIteratorStreamer`] with IDEFICS-8B.\n\nAssume we have an application that keeps chat history and takes in the new user input. We will preprocess the inputs as usual and initialize [`TextIteratorStreamer`] to handle the generation in a separate thread. This allows you to stream the generated text tokens in real-time. Any generation arguments can be passed to [`TextIteratorStreamer`].\n\n\n```python\nimport time\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n\ndef model_inference(\nuser_prompt,\nchat_history,\nmax_new_tokens,\nimages\n):\nuser_prompt = {\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": user_prompt},\n]\n}\nchat_history.append(user_prompt)\nstreamer = TextIteratorStreamer(\nprocessor.tokenizer,\nskip_prompt=True,\ntimeout=5.0,\n)\n\ngeneration_args = {\n\"max_new_tokens\": max_new_tokens,",
  "\"streamer\": streamer,\n\"do_sample\": False\n}\n\n# add_generation_prompt=True makes model generate bot response\nprompt = processor.apply_chat_template(chat_history, add_generation_prompt=True)\ninputs = processor(\ntext=prompt,\nimages=images,\nreturn_tensors=\"pt\",\n).to(device)\ngeneration_args.update(inputs)\n\nthread = Thread(\ntarget=model.generate,\nkwargs=generation_args,\n)\nthread.start()\n\nacc_text = \"\"\nfor text_token in streamer:\ntime.sleep(0.04)\nacc_text += text_token\nif acc_text.endswith(\"<end_of_utterance>\"):\nacc_text = acc_text[:-18]\nyield acc_text\n\nthread.join()\n```\n\nNow let's call the `model_inference` function we created and stream the values.\n\n```python\ngenerator = model_inference(\nuser_prompt=\"And what is in this image?\",\nchat_history=messages[:2],\nmax_new_tokens=100,\nimages=images\n)\n\nfor value in generator:\nprint(value)\n\n# In\n# In this\n# In this image ...\n```\n\n## Fit models in smaller hardware",
  "VLMs are often large and need to be optimized to fit on smaller hardware. Transformers supports many model quantization libraries, and here we will only show int8 quantization with [Quanto](./quantization/quanto#quanto). int8 quantization offers memory improvements up to 75 percent (if all weights are quantized). However it is no free lunch, since 8-bit is not a CUDA-native precision, the weights are quantized back and forth on the fly, which adds up to latency.\n\nFirst, install dependencies.\n\n```bash\npip install -U quanto bitsandbytes\n```\n\nTo quantize a model during loading, we need to first create [`QuantoConfig`]. Then load the model as usual, but pass `quantization_config` during model initialization.\n\n```python\nfrom transformers import AutoModelForImageTextToText, QuantoConfig\n\nmodel_id = \"HuggingFaceM4/idefics2-8b\"\nquantization_config = QuantoConfig(weights=\"int8\")\nquantized_model = AutoModelForImageTextToText.from_pretrained(\nmodel_id, device_map=\"cuda\", quantization_config=quantization_config\n)\n```\n\nAnd that's it, we can use the model the same way with no changes.\n\n## Further Reading\n\nHere are some more resources for the image-text-to-text task.",
  "- [Image-text-to-text task page](https://huggingface.co/tasks/image-text-to-text) covers model types, use cases, datasets, and more.\n- [Vision Language Models Explained](https://huggingface.co/blog/vlms) is a blog post that covers everything about vision language models and supervised fine-tuning using [TRL](https://huggingface.co/docs/trl/en/index).",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Object detection\n\n[[open-in-colab]]\n\nObject detection is the computer vision task of detecting instances (such as humans, buildings, or cars) in an image. Object detection models receive an image as input and output\ncoordinates of the bounding boxes and associated labels of the detected objects. An image can contain multiple objects,\neach with its own bounding box and a label (e.g. it can have a car and a building), and each object can",
  "be present in different parts of an image (e.g. the image can have several cars).\nThis task is commonly used in autonomous driving for detecting things like pedestrians, road signs, and traffic lights.\nOther applications include counting objects in images, image search, and more.\n\nIn this guide, you will learn how to:\n\n1. Finetune [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a model that combines a convolutional\nbackbone with an encoder-decoder Transformer, on the [CPPE-5](https://huggingface.co/datasets/cppe-5)\ndataset.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/object-detection)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install -q datasets transformers accelerate timm\npip install -q -U albumentations>=1.4.5 torchmetrics pycocotools\n```\n\nYou'll use 🤗 Datasets to load a dataset from the Hugging Face Hub, 🤗 Transformers to train your model,\nand `albumentations` to augment the data.",
  "We encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the Hub.\nWhen prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\nTo get started, we'll define global constants, namely the model name and image size. For this tutorial, we'll use the conditional DETR model due to its faster convergence. Feel free to select any object detection model available in the `transformers` library.\n\n```py\n>>> MODEL_NAME = \"microsoft/conditional-detr-resnet-50\"  # or \"facebook/detr-resnet-50\"\n>>> IMAGE_SIZE = 480\n```\n\n## Load the CPPE-5 dataset\n\nThe [CPPE-5 dataset](https://huggingface.co/datasets/cppe-5) contains images with\nannotations identifying medical personal protective equipment (PPE) in the context of the COVID-19 pandemic.\n\nStart by loading the dataset and creating a `validation` split from `train`:\n\n```py\n>>> from datasets import load_dataset\n\n>>> cppe5 = load_dataset(\"cppe-5\")\n\n>>> if \"validation\" not in cppe5:\n...     split = cppe5[\"train\"].train_test_split(0.15, seed=1337)\n...     cppe5[\"train\"] = split[\"train\"]\n...     cppe5[\"validation\"] = split[\"test\"]\n\n>>> cppe5",
  "DatasetDict({\ntrain: Dataset({\nfeatures: ['image_id', 'image', 'width', 'height', 'objects'],\nnum_rows: 850\n})\ntest: Dataset({\nfeatures: ['image_id', 'image', 'width', 'height', 'objects'],\nnum_rows: 29\n})\nvalidation: Dataset({\nfeatures: ['image_id', 'image', 'width', 'height', 'objects'],\nnum_rows: 150\n})\n})\n```\n\nYou'll see that this dataset has 1000 images for train and validation sets and a test set with 29 images.\n\nTo get familiar with the data, explore what the examples look like.\n\n```py\n>>> cppe5[\"train\"][0]\n{\n'image_id': 366,\n'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=500x290>,\n'width': 500,\n'height': 500,\n'objects': {\n'id': [1932, 1933, 1934],\n'area': [27063, 34200, 32431],\n'bbox': [[29.0, 11.0, 97.0, 279.0],\n[201.0, 1.0, 120.0, 285.0],\n[382.0, 0.0, 113.0, 287.0]],\n'category': [0, 0, 0]\n}\n}\n```\n\nThe examples in the dataset have the following fields:\n- `image_id`: the example image id\n- `image`: a `PIL.Image.Image` object containing the image\n- `width`: width of the image\n- `height`: height of the image\n- `objects`: a dictionary containing bounding box metadata for the objects in the image:\n- `id`: the annotation id",
  "- `area`: the area of the bounding box\n- `bbox`: the object's bounding box (in the [COCO format](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco) )\n- `category`: the object's category, with possible values including `Coverall (0)`, `Face_Shield (1)`, `Gloves (2)`, `Goggles (3)` and `Mask (4)`\n\nYou may notice that the `bbox` field follows the COCO format, which is the format that the DETR model expects.\nHowever, the grouping of the fields inside `objects` differs from the annotation format DETR requires. You will\nneed to apply some preprocessing transformations before using this data for training.\n\nTo get an even better understanding of the data, visualize an example in the dataset.\n\n```py\n>>> import numpy as np\n>>> import os\n>>> from PIL import Image, ImageDraw\n\n>>> image = cppe5[\"train\"][2][\"image\"]\n>>> annotations = cppe5[\"train\"][2][\"objects\"]\n>>> draw = ImageDraw.Draw(image)\n\n>>> categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n\n>>> id2label = {index: x for index, x in enumerate(categories, start=0)}\n>>> label2id = {v: k for k, v in id2label.items()}\n\n>>> for i in range(len(annotations[\"id\"])):",
  "...     box = annotations[\"bbox\"][i]\n...     class_idx = annotations[\"category\"][i]\n...     x, y, w, h = tuple(box)\n...     # Check if coordinates are normalized or not\n...     if max(box) > 1.0:\n...         # Coordinates are un-normalized, no need to re-scale them\n...         x1, y1 = int(x), int(y)\n...         x2, y2 = int(x + w), int(y + h)\n...     else:\n...         # Coordinates are normalized, re-scale them\n...         x1 = int(x * width)\n...         y1 = int(y * height)\n...         x2 = int((x + w) * width)\n...         y2 = int((y + h) * height)\n...     draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n...     draw.text((x, y), id2label[class_idx], fill=\"white\")\n\n>>> image\n```\n<div class=\"flex justify-center\">\n<img src=\"https://i.imgur.com/oVQb9SF.png\" alt=\"CPPE-5 Image Example\"/>\n</div>\n\n\nTo visualize the bounding boxes with associated labels, you can get the labels from the dataset's metadata, specifically\nthe `category` field.\nYou'll also want to create dictionaries that map a label id to a label class (`id2label`) and the other way around (`label2id`).",
  "You can use them later when setting up the model. Including these maps will make your model reusable by others if you share\nit on the Hugging Face Hub. Please note that, the part of above code that draws the bounding boxes assume that it is in `COCO` format `(x_min, y_min, width, height)`. It has to be adjusted to work for other formats like `(x_min, y_min, x_max, y_max)`.\n\nAs a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for\nobject detection is bounding boxes that \"stretch\" beyond the edge of the image. Such \"runaway\" bounding boxes can raise\nerrors during training and should be addressed. There are a few examples with this issue in this dataset.\nTo keep things simple in this guide, we will set `clip=True` for `BboxParams` in transformations below.\n\n## Preprocess the data\n\nTo finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model.\n[`AutoImageProcessor`] takes care of processing image data to create `pixel_values`, `pixel_mask`, and",
  "`labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:\n\n- `image_mean = [0.485, 0.456, 0.406 ]`\n- `image_std = [0.229, 0.224, 0.225]`\n\nThese are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial\nto replicate when doing inference or finetuning a pre-trained image model.\n\nInstantiate the image processor from the same checkpoint as the model you want to finetune.\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> MAX_SIZE = IMAGE_SIZE\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\n...     MODEL_NAME,\n...     do_resize=True,\n...     size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n...     do_pad=True,\n...     pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n... )\n```\n\nBefore passing the images to the `image_processor`, apply two preprocessing transformations to the dataset:\n- Augmenting images\n- Reformatting annotations to meet DETR expectations",
  "First, to make sure the model does not overfit on the training data, you can apply image augmentation with any data augmentation library. Here we use [Albumentations](https://albumentations.ai/docs/).\nThis library ensures that transformations affect the image and update the bounding boxes accordingly.\nThe 🤗 Datasets library documentation has a detailed [guide on how to augment images for object detection](https://huggingface.co/docs/datasets/object_detection),\nand it uses the exact same dataset as an example. Apply some geometric and color transformations to the image. For additional augmentation options, explore the [Albumentations Demo Space](https://huggingface.co/spaces/qubvel-hf/albumentations-demo).\n\n```py\n>>> import albumentations as A\n\n>>> train_augment_and_transform = A.Compose(\n...     [\n...         A.Perspective(p=0.1),\n...         A.HorizontalFlip(p=0.5),\n...         A.RandomBrightnessContrast(p=0.5),\n...         A.HueSaturationValue(p=0.1),\n...     ],\n...     bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n... )\n\n>>> validation_transform = A.Compose(\n...     [A.NoOp()],",
  "...     bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n... )\n```\n\nThe `image_processor` expects the annotations to be in the following format: `{'image_id': int, 'annotations': List[Dict]}`,\nwhere each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:\n\n```py\n>>> def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n...     \"\"\"Format one set of image annotations to the COCO format\n\n...     Args:\n...         image_id (str): image id. e.g. \"0001\"\n...         categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n...         areas (List[float]): list of corresponding areas to provided bounding boxes\n...         bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n...             ([center_x, center_y, width, height] in absolute coordinates)\n\n...     Returns:\n...         dict: {\n...             \"image_id\": image id,\n...             \"annotations\": list of formatted annotations\n...         }\n...     \"\"\"\n...     annotations = []\n...     for category, area, bbox in zip(categories, areas, bboxes):",
  "...         formatted_annotation = {\n...             \"image_id\": image_id,\n...             \"category_id\": category,\n...             \"iscrowd\": 0,\n...             \"area\": area,\n...             \"bbox\": list(bbox),\n...         }\n...         annotations.append(formatted_annotation)\n\n...     return {\n...         \"image_id\": image_id,\n...         \"annotations\": annotations,\n...     }\n\n```\n\nNow you can combine the image and annotation transformations to use on a batch of examples:\n\n```py\n>>> def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n...     \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n\n...     images = []\n...     annotations = []\n...     for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n...         image = np.array(image.convert(\"RGB\"))\n\n...         # apply augmentations\n...         output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n...         images.append(output[\"image\"])\n\n...         # format annotations in COCO format\n...         formatted_annotations = format_image_annotations_as_coco(",
  "...             image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n...         )\n...         annotations.append(formatted_annotations)\n\n...     # Apply the image processor transformations: resizing, rescaling, normalization\n...     result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n\n...     if not return_pixel_mask:\n...         result.pop(\"pixel_mask\", None)\n\n...     return result\n```\n\nApply this preprocessing function to the entire dataset using 🤗 Datasets [`~datasets.Dataset.with_transform`] method. This method applies\ntransformations on the fly when you load an element of the dataset.\n\nAt this point, you can check what an example from the dataset looks like after the transformations. You should see a tensor\nwith `pixel_values`, a tensor with `pixel_mask`, and `labels`.\n\n```py\n>>> from functools import partial\n\n>>> # Make transform functions for batch and apply for dataset splits\n>>> train_transform_batch = partial(\n...     augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n... )\n>>> validation_transform_batch = partial(",
  "...     augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n... )\n\n>>> cppe5[\"train\"] = cppe5[\"train\"].with_transform(train_transform_batch)\n>>> cppe5[\"validation\"] = cppe5[\"validation\"].with_transform(validation_transform_batch)\n>>> cppe5[\"test\"] = cppe5[\"test\"].with_transform(validation_transform_batch)\n\n>>> cppe5[\"train\"][15]\n{'pixel_values': tensor([[[ 1.9235,  1.9407,  1.9749,  ..., -0.7822, -0.7479, -0.6965],\n[ 1.9578,  1.9749,  1.9920,  ..., -0.7993, -0.7650, -0.7308],\n[ 2.0092,  2.0092,  2.0263,  ..., -0.8507, -0.8164, -0.7822],\n...,\n[ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741],\n[ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741],\n[ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741]],\n\n[[ 1.6232,  1.6408,  1.6583,  ...,  0.8704,  1.0105,  1.1331],\n[ 1.6408,  1.6583,  1.6758,  ...,  0.8529,  0.9930,  1.0980],\n[ 1.6933,  1.6933,  1.7108,  ...,  0.8179,  0.9580,  1.0630],\n...,\n[ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052],\n[ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052],\n[ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052]],",
  "[[ 1.8905,  1.9080,  1.9428,  ..., -0.1487, -0.0964, -0.0615],\n[ 1.9254,  1.9428,  1.9603,  ..., -0.1661, -0.1138, -0.0790],\n[ 1.9777,  1.9777,  1.9951,  ..., -0.2010, -0.1138, -0.0790],\n...,\n[ 0.4265,  0.4265,  0.4265,  ...,  0.4265,  0.4265,  0.4265],\n[ 0.4265,  0.4265,  0.4265,  ...,  0.4265,  0.4265,  0.4265],\n[ 0.4265,  0.4265,  0.4265,  ...,  0.4265,  0.4265,  0.4265]]]),\n'labels': {'image_id': tensor([688]), 'class_labels': tensor([3, 4, 2, 0, 0]), 'boxes': tensor([[0.4700, 0.1933, 0.1467, 0.0767],\n[0.4858, 0.2600, 0.1150, 0.1000],\n[0.4042, 0.4517, 0.1217, 0.1300],\n[0.4242, 0.3217, 0.3617, 0.5567],\n[0.6617, 0.4033, 0.5400, 0.4533]]), 'area': tensor([ 4048.,  4140.,  5694., 72478., 88128.]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([480, 480])}}\n```\n\nYou have successfully augmented the individual images and prepared their annotations. However, preprocessing isn't\ncomplete yet. In the final step, create a custom `collate_fn` to batch images together.\nPad images (which are now `pixel_values`) to the largest image in a batch, and create a corresponding `pixel_mask`\nto indicate which pixels are real (1) and which are padding (0).\n\n```py\n>>> import torch",
  ">>> def collate_fn(batch):\n...     data = {}\n...     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n...     data[\"labels\"] = [x[\"labels\"] for x in batch]\n...     if \"pixel_mask\" in batch[0]:\n...         data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n...     return data\n\n```\n\n## Preparing function to compute mAP\n\nObject detection models are commonly evaluated with a set of <a href=\"https://cocodataset.org/#detection-eval\">COCO-style metrics</a>. We are going to use `torchmetrics` to compute `mAP` (mean average precision) and `mAR` (mean average recall) metrics and will wrap it to `compute_metrics` function in order to use in [`Trainer`] for evaluation.\n\nIntermediate format of boxes used for training is `YOLO` (normalized) but we will compute metrics for boxes in `Pascal VOC` (absolute) format in order to correctly handle box areas. Let's define a function that converts bounding boxes to `Pascal VOC` format:\n\n```py\n>>> from transformers.image_transforms import center_to_corners_format\n\n>>> def convert_bbox_yolo_to_pascal(boxes, image_size):\n...     \"\"\"",
  "...     Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n...     to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n\n...     Args:\n...         boxes (torch.Tensor): Bounding boxes in YOLO format\n...         image_size (Tuple[int, int]): Image size in format (height, width)\n\n...     Returns:\n...         torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n...     \"\"\"\n...     # convert center to corners format\n...     boxes = center_to_corners_format(boxes)\n\n...     # convert to absolute coordinates\n...     height, width = image_size\n...     boxes = boxes * torch.tensor([[width, height, width, height]])\n\n...     return boxes\n```\n\nThen, in `compute_metrics` function we collect `predicted` and `target` bounding boxes, scores and labels from evaluation loop results and pass it to the scoring function.\n\n```py\n>>> import numpy as np\n>>> from dataclasses import dataclass\n>>> from torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n\n>>> @dataclass\n>>> class ModelOutput:\n...     logits: torch.Tensor\n...     pred_boxes: torch.Tensor\n\n\n>>> @torch.no_grad()",
  ">>> def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n...     \"\"\"\n...     Compute mean average mAP, mAR and their variants for the object detection task.\n\n...     Args:\n...         evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n...         threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n...         id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n\n...     Returns:\n...         Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n...     \"\"\"\n\n...     predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n\n...     # For metric computation we need to provide:\n...     #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n...     #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n\n...     image_sizes = []\n...     post_processed_targets = []\n...     post_processed_predictions = []\n\n...     # Collect targets in the required format for metric computation\n...     for batch in targets:",
  "...         # collect image sizes, we will need them for predictions post processing\n...         batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n...         image_sizes.append(batch_image_sizes)\n...         # collect targets in the required format for metric computation\n...         # boxes were converted to YOLO format needed for model training\n...         # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n...         for image_target in batch:\n...             boxes = torch.tensor(image_target[\"boxes\"])\n...             boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n...             labels = torch.tensor(image_target[\"class_labels\"])\n...             post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n\n...     # Collect predictions in the required format for metric computation,\n...     # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n...     for batch, target_sizes in zip(predictions, image_sizes):\n...         batch_logits, batch_boxes = batch[1], batch[2]",
  "...         output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n...         post_processed_output = image_processor.post_process_object_detection(\n...             output, threshold=threshold, target_sizes=target_sizes\n...         )\n...         post_processed_predictions.extend(post_processed_output)\n\n...     # Compute metrics\n...     metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n...     metric.update(post_processed_predictions, post_processed_targets)\n...     metrics = metric.compute()\n\n...     # Replace list of per class metrics with separate metric for each class\n...     classes = metrics.pop(\"classes\")\n...     map_per_class = metrics.pop(\"map_per_class\")\n...     mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n...     for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n...         class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n...         metrics[f\"map_{class_name}\"] = class_map\n...         metrics[f\"mar_100_{class_name}\"] = class_mar\n\n...     metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n\n...     return metrics",
  ">>> eval_compute_metrics_fn = partial(\n...     compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n... )\n```\n\n## Training the detection model\n\nYou have done most of the heavy lifting in the previous sections, so now you are ready to train your model!\nThe images in this dataset are still quite large, even after resizing. This means that finetuning this model will\nrequire at least one GPU.\n\nTraining involves the following steps:\n1. Load the model with [`AutoModelForObjectDetection`] using the same checkpoint as in the preprocessing.\n2. Define your training hyperparameters in [`TrainingArguments`].\n3. Pass the training arguments to [`Trainer`] along with the model, dataset, image processor, and data collator.\n4. Call [`~Trainer.train`] to finetune your model.\n\nWhen loading the model from the same checkpoint that you used for the preprocessing, remember to pass the `label2id`\nand `id2label` maps that you created earlier from the dataset's metadata. Additionally, we specify `ignore_mismatched_sizes=True` to replace the existing classification head with a new one.\n\n```py\n>>> from transformers import AutoModelForObjectDetection",
  ">>> model = AutoModelForObjectDetection.from_pretrained(\n...     MODEL_NAME,\n...     id2label=id2label,\n...     label2id=label2id,\n...     ignore_mismatched_sizes=True,\n... )\n```\n\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, then configure hyperparameters as you see fit. For `num_train_epochs=30` training will take about 35 minutes in Google Colab T4 GPU, increase the number of epoch to get better results.\n\nImportant notes:\n- Do not remove unused columns because this will drop the image column. Without the image column, you\ncan't create `pixel_values`. For this reason, set `remove_unused_columns` to `False`.\n- Set `eval_do_concat_batches=False` to get proper evaluation results. Images have different number of target boxes, if batches are concatenated we will not be able to determine which boxes belongs to particular image.\n\nIf you wish to share your model by pushing to the Hub, set `push_to_hub` to `True` (you must be signed in to Hugging\nFace to upload your model).\n\n```py\n>>> from transformers import TrainingArguments\n\n>>> training_args = TrainingArguments(\n...     output_dir=\"detr_finetuned_cppe5\",\n...     num_train_epochs=30,",
  "...     fp16=False,\n...     per_device_train_batch_size=8,\n...     dataloader_num_workers=4,\n...     learning_rate=5e-5,\n...     lr_scheduler_type=\"cosine\",\n...     weight_decay=1e-4,\n...     max_grad_norm=0.01,\n...     metric_for_best_model=\"eval_map\",\n...     greater_is_better=True,\n...     load_best_model_at_end=True,\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     eval_do_concat_batches=False,\n...     push_to_hub=True,\n... )\n```\n\nFinally, bring everything together, and call [`~transformers.Trainer.train`]:\n\n```py\n>>> from transformers import Trainer\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=cppe5[\"train\"],\n...     eval_dataset=cppe5[\"validation\"],\n...     processing_class=image_processor,\n...     data_collator=collate_fn,\n...     compute_metrics=eval_compute_metrics_fn,\n... )\n\n>>> trainer.train()\n```\n<div>\n\n<progress value='3210' max='3210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n[3210/3210 26:07, Epoch 30/30]\n</div>\n\n<table border=\"1\" class=\"dataframe\">\n<thead>\n<tr style=\"text-align: left;\">",
  "<th>Epoch</th>\n<th>Training Loss</th>\n<th>Validation Loss</th>\n<th>Map</th>\n<th>Map 50</th>\n<th>Map 75</th>\n<th>Map Small</th>\n<th>Map Medium</th>\n<th>Map Large</th>\n<th>Mar 1</th>\n<th>Mar 10</th>\n<th>Mar 100</th>\n<th>Mar Small</th>\n<th>Mar Medium</th>\n<th>Mar Large</th>\n<th>Map Coverall</th>\n<th>Mar 100 Coverall</th>\n<th>Map Face Shield</th>\n<th>Mar 100 Face Shield</th>\n<th>Map Gloves</th>\n<th>Mar 100 Gloves</th>\n<th>Map Goggles</th>\n<th>Mar 100 Goggles</th>\n<th>Map Mask</th>\n<th>Mar 100 Mask</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>No log</td>\n<td>2.629903</td>\n<td>0.008900</td>\n<td>0.023200</td>\n<td>0.006500</td>\n<td>0.001300</td>\n<td>0.002800</td>\n<td>0.020500</td>\n<td>0.021500</td>\n<td>0.070400</td>\n<td>0.101400</td>\n<td>0.007600</td>\n<td>0.106200</td>\n<td>0.096100</td>\n<td>0.036700</td>\n<td>0.232000</td>\n<td>0.000300</td>\n<td>0.019000</td>\n<td>0.003900</td>\n<td>0.125400</td>\n<td>0.000100</td>\n<td>0.003100</td>\n<td>0.003500</td>\n<td>0.127600</td>\n</tr>\n<tr>\n<td>2</td>\n<td>No log</td>\n<td>3.479864</td>\n<td>0.014800</td>\n<td>0.034600</td>\n<td>0.010800</td>\n<td>0.008600</td>\n<td>0.011700</td>\n<td>0.012500</td>\n<td>0.041100</td>\n<td>0.098700</td>\n<td>0.130000</td>",
  "<td>0.056000</td>\n<td>0.062200</td>\n<td>0.111900</td>\n<td>0.053500</td>\n<td>0.447300</td>\n<td>0.010600</td>\n<td>0.100000</td>\n<td>0.000200</td>\n<td>0.022800</td>\n<td>0.000100</td>\n<td>0.015400</td>\n<td>0.009700</td>\n<td>0.064400</td>\n</tr>\n<tr>\n<td>3</td>\n<td>No log</td>\n<td>2.107622</td>\n<td>0.041700</td>\n<td>0.094000</td>\n<td>0.034300</td>\n<td>0.024100</td>\n<td>0.026400</td>\n<td>0.047400</td>\n<td>0.091500</td>\n<td>0.182800</td>\n<td>0.225800</td>\n<td>0.087200</td>\n<td>0.199400</td>\n<td>0.210600</td>\n<td>0.150900</td>\n<td>0.571200</td>\n<td>0.017300</td>\n<td>0.101300</td>\n<td>0.007300</td>\n<td>0.180400</td>\n<td>0.002100</td>\n<td>0.026200</td>\n<td>0.031000</td>\n<td>0.250200</td>\n</tr>\n<tr>\n<td>4</td>\n<td>No log</td>\n<td>2.031242</td>\n<td>0.055900</td>\n<td>0.120600</td>\n<td>0.046900</td>\n<td>0.013800</td>\n<td>0.038100</td>\n<td>0.090300</td>\n<td>0.105900</td>\n<td>0.225600</td>\n<td>0.266100</td>\n<td>0.130200</td>\n<td>0.228100</td>\n<td>0.330000</td>\n<td>0.191000</td>\n<td>0.572100</td>\n<td>0.010600</td>\n<td>0.157000</td>\n<td>0.014600</td>\n<td>0.235300</td>\n<td>0.001700</td>\n<td>0.052300</td>\n<td>0.061800</td>\n<td>0.313800</td>\n</tr>\n<tr>\n<td>5</td>\n<td>3.889400</td>",
  "<td>1.883433</td>\n<td>0.089700</td>\n<td>0.201800</td>\n<td>0.067300</td>\n<td>0.022800</td>\n<td>0.065300</td>\n<td>0.129500</td>\n<td>0.136000</td>\n<td>0.272200</td>\n<td>0.303700</td>\n<td>0.112900</td>\n<td>0.312500</td>\n<td>0.424600</td>\n<td>0.300200</td>\n<td>0.585100</td>\n<td>0.032700</td>\n<td>0.202500</td>\n<td>0.031300</td>\n<td>0.271000</td>\n<td>0.008700</td>\n<td>0.126200</td>\n<td>0.075500</td>\n<td>0.333800</td>\n</tr>\n<tr>\n<td>6</td>\n<td>3.889400</td>\n<td>1.807503</td>\n<td>0.118500</td>\n<td>0.270900</td>\n<td>0.090200</td>\n<td>0.034900</td>\n<td>0.076700</td>\n<td>0.152500</td>\n<td>0.146100</td>\n<td>0.297800</td>\n<td>0.325400</td>\n<td>0.171700</td>\n<td>0.283700</td>\n<td>0.545900</td>\n<td>0.396900</td>\n<td>0.554500</td>\n<td>0.043000</td>\n<td>0.262000</td>\n<td>0.054500</td>\n<td>0.271900</td>\n<td>0.020300</td>\n<td>0.230800</td>\n<td>0.077600</td>\n<td>0.308000</td>\n</tr>\n<tr>\n<td>7</td>\n<td>3.889400</td>\n<td>1.716169</td>\n<td>0.143500</td>\n<td>0.307700</td>\n<td>0.123200</td>\n<td>0.045800</td>\n<td>0.097800</td>\n<td>0.258300</td>\n<td>0.165300</td>\n<td>0.327700</td>\n<td>0.352600</td>\n<td>0.140900</td>\n<td>0.336700</td>\n<td>0.599400</td>\n<td>0.442900</td>\n<td>0.620700</td>",
  "<td>0.069400</td>\n<td>0.301300</td>\n<td>0.081600</td>\n<td>0.292000</td>\n<td>0.011000</td>\n<td>0.230800</td>\n<td>0.112700</td>\n<td>0.318200</td>\n</tr>\n<tr>\n<td>8</td>\n<td>3.889400</td>\n<td>1.679014</td>\n<td>0.153000</td>\n<td>0.355800</td>\n<td>0.127900</td>\n<td>0.038700</td>\n<td>0.115600</td>\n<td>0.291600</td>\n<td>0.176000</td>\n<td>0.322500</td>\n<td>0.349700</td>\n<td>0.135600</td>\n<td>0.326100</td>\n<td>0.643700</td>\n<td>0.431700</td>\n<td>0.582900</td>\n<td>0.069800</td>\n<td>0.265800</td>\n<td>0.088600</td>\n<td>0.274600</td>\n<td>0.028300</td>\n<td>0.280000</td>\n<td>0.146700</td>\n<td>0.345300</td>\n</tr>\n<tr>\n<td>9</td>\n<td>3.889400</td>\n<td>1.618239</td>\n<td>0.172100</td>\n<td>0.375300</td>\n<td>0.137600</td>\n<td>0.046100</td>\n<td>0.141700</td>\n<td>0.308500</td>\n<td>0.194000</td>\n<td>0.356200</td>\n<td>0.386200</td>\n<td>0.162400</td>\n<td>0.359200</td>\n<td>0.677700</td>\n<td>0.469800</td>\n<td>0.623900</td>\n<td>0.102100</td>\n<td>0.317700</td>\n<td>0.099100</td>\n<td>0.290200</td>\n<td>0.029300</td>\n<td>0.335400</td>\n<td>0.160200</td>\n<td>0.364000</td>\n</tr>\n<tr>\n<td>10</td>\n<td>1.599700</td>\n<td>1.572512</td>\n<td>0.179500</td>\n<td>0.400400</td>\n<td>0.147200</td>\n<td>0.056500</td>",
  "<td>0.141700</td>\n<td>0.316700</td>\n<td>0.213100</td>\n<td>0.357600</td>\n<td>0.381300</td>\n<td>0.197900</td>\n<td>0.344300</td>\n<td>0.638500</td>\n<td>0.466900</td>\n<td>0.623900</td>\n<td>0.101300</td>\n<td>0.311400</td>\n<td>0.104700</td>\n<td>0.279500</td>\n<td>0.051600</td>\n<td>0.338500</td>\n<td>0.173000</td>\n<td>0.353300</td>\n</tr>\n<tr>\n<td>11</td>\n<td>1.599700</td>\n<td>1.528889</td>\n<td>0.192200</td>\n<td>0.415000</td>\n<td>0.160800</td>\n<td>0.053700</td>\n<td>0.150500</td>\n<td>0.378000</td>\n<td>0.211500</td>\n<td>0.371700</td>\n<td>0.397800</td>\n<td>0.204900</td>\n<td>0.374600</td>\n<td>0.684800</td>\n<td>0.491900</td>\n<td>0.632400</td>\n<td>0.131200</td>\n<td>0.346800</td>\n<td>0.122000</td>\n<td>0.300900</td>\n<td>0.038400</td>\n<td>0.344600</td>\n<td>0.177500</td>\n<td>0.364400</td>\n</tr>\n<tr>\n<td>12</td>\n<td>1.599700</td>\n<td>1.517532</td>\n<td>0.198300</td>\n<td>0.429800</td>\n<td>0.159800</td>\n<td>0.066400</td>\n<td>0.162900</td>\n<td>0.383300</td>\n<td>0.220700</td>\n<td>0.382100</td>\n<td>0.405400</td>\n<td>0.214800</td>\n<td>0.383200</td>\n<td>0.672900</td>\n<td>0.469000</td>\n<td>0.610400</td>\n<td>0.167800</td>\n<td>0.379700</td>\n<td>0.119700</td>\n<td>0.307100</td>\n<td>0.038100</td>",
  "<td>0.335400</td>\n<td>0.196800</td>\n<td>0.394200</td>\n</tr>\n<tr>\n<td>13</td>\n<td>1.599700</td>\n<td>1.488849</td>\n<td>0.209800</td>\n<td>0.452300</td>\n<td>0.172300</td>\n<td>0.094900</td>\n<td>0.171100</td>\n<td>0.437800</td>\n<td>0.222000</td>\n<td>0.379800</td>\n<td>0.411500</td>\n<td>0.203800</td>\n<td>0.397300</td>\n<td>0.707500</td>\n<td>0.470700</td>\n<td>0.620700</td>\n<td>0.186900</td>\n<td>0.407600</td>\n<td>0.124200</td>\n<td>0.306700</td>\n<td>0.059300</td>\n<td>0.355400</td>\n<td>0.207700</td>\n<td>0.367100</td>\n</tr>\n<tr>\n<td>14</td>\n<td>1.599700</td>\n<td>1.482210</td>\n<td>0.228900</td>\n<td>0.482600</td>\n<td>0.187800</td>\n<td>0.083600</td>\n<td>0.191800</td>\n<td>0.444100</td>\n<td>0.225900</td>\n<td>0.376900</td>\n<td>0.407400</td>\n<td>0.182500</td>\n<td>0.384800</td>\n<td>0.700600</td>\n<td>0.512100</td>\n<td>0.640100</td>\n<td>0.175000</td>\n<td>0.363300</td>\n<td>0.144300</td>\n<td>0.300000</td>\n<td>0.083100</td>\n<td>0.363100</td>\n<td>0.229900</td>\n<td>0.370700</td>\n</tr>\n<tr>\n<td>15</td>\n<td>1.326800</td>\n<td>1.475198</td>\n<td>0.216300</td>\n<td>0.455600</td>\n<td>0.174900</td>\n<td>0.088500</td>\n<td>0.183500</td>\n<td>0.424400</td>\n<td>0.226900</td>\n<td>0.373400</td>\n<td>0.404300</td>",
  "<td>0.199200</td>\n<td>0.396400</td>\n<td>0.677800</td>\n<td>0.496300</td>\n<td>0.633800</td>\n<td>0.166300</td>\n<td>0.392400</td>\n<td>0.128900</td>\n<td>0.312900</td>\n<td>0.085200</td>\n<td>0.312300</td>\n<td>0.205000</td>\n<td>0.370200</td>\n</tr>\n<tr>\n<td>16</td>\n<td>1.326800</td>\n<td>1.459697</td>\n<td>0.233200</td>\n<td>0.504200</td>\n<td>0.192200</td>\n<td>0.096000</td>\n<td>0.202000</td>\n<td>0.430800</td>\n<td>0.239100</td>\n<td>0.382400</td>\n<td>0.412600</td>\n<td>0.219500</td>\n<td>0.403100</td>\n<td>0.670400</td>\n<td>0.485200</td>\n<td>0.625200</td>\n<td>0.196500</td>\n<td>0.410100</td>\n<td>0.135700</td>\n<td>0.299600</td>\n<td>0.123100</td>\n<td>0.356900</td>\n<td>0.225300</td>\n<td>0.371100</td>\n</tr>\n<tr>\n<td>17</td>\n<td>1.326800</td>\n<td>1.407340</td>\n<td>0.243400</td>\n<td>0.511900</td>\n<td>0.204500</td>\n<td>0.121000</td>\n<td>0.215700</td>\n<td>0.468000</td>\n<td>0.246200</td>\n<td>0.394600</td>\n<td>0.424200</td>\n<td>0.225900</td>\n<td>0.416100</td>\n<td>0.705200</td>\n<td>0.494900</td>\n<td>0.638300</td>\n<td>0.224900</td>\n<td>0.430400</td>\n<td>0.157200</td>\n<td>0.317900</td>\n<td>0.115700</td>\n<td>0.369200</td>\n<td>0.224200</td>\n<td>0.365300</td>\n</tr>\n<tr>\n<td>18</td>\n<td>1.326800</td>",
  "<td>1.419522</td>\n<td>0.245100</td>\n<td>0.521500</td>\n<td>0.210000</td>\n<td>0.116100</td>\n<td>0.211500</td>\n<td>0.489900</td>\n<td>0.255400</td>\n<td>0.391600</td>\n<td>0.419700</td>\n<td>0.198800</td>\n<td>0.421200</td>\n<td>0.701400</td>\n<td>0.501800</td>\n<td>0.634200</td>\n<td>0.226700</td>\n<td>0.410100</td>\n<td>0.154400</td>\n<td>0.321400</td>\n<td>0.105900</td>\n<td>0.352300</td>\n<td>0.236700</td>\n<td>0.380400</td>\n</tr>\n<tr>\n<td>19</td>\n<td>1.158600</td>\n<td>1.398764</td>\n<td>0.253600</td>\n<td>0.519200</td>\n<td>0.213600</td>\n<td>0.135200</td>\n<td>0.207700</td>\n<td>0.491900</td>\n<td>0.257300</td>\n<td>0.397300</td>\n<td>0.428000</td>\n<td>0.241400</td>\n<td>0.401800</td>\n<td>0.703500</td>\n<td>0.509700</td>\n<td>0.631100</td>\n<td>0.236700</td>\n<td>0.441800</td>\n<td>0.155900</td>\n<td>0.330800</td>\n<td>0.128100</td>\n<td>0.352300</td>\n<td>0.237500</td>\n<td>0.384000</td>\n</tr>\n<tr>\n<td>20</td>\n<td>1.158600</td>\n<td>1.390591</td>\n<td>0.248800</td>\n<td>0.520200</td>\n<td>0.216600</td>\n<td>0.127500</td>\n<td>0.211400</td>\n<td>0.471900</td>\n<td>0.258300</td>\n<td>0.407000</td>\n<td>0.429100</td>\n<td>0.240300</td>\n<td>0.407600</td>\n<td>0.708500</td>\n<td>0.505800</td>\n<td>0.623400</td>",
  "<td>0.235500</td>\n<td>0.431600</td>\n<td>0.150000</td>\n<td>0.325000</td>\n<td>0.125700</td>\n<td>0.375400</td>\n<td>0.227200</td>\n<td>0.390200</td>\n</tr>\n<tr>\n<td>21</td>\n<td>1.158600</td>\n<td>1.360608</td>\n<td>0.262700</td>\n<td>0.544800</td>\n<td>0.222100</td>\n<td>0.134700</td>\n<td>0.230000</td>\n<td>0.487500</td>\n<td>0.269500</td>\n<td>0.413300</td>\n<td>0.436300</td>\n<td>0.236200</td>\n<td>0.419100</td>\n<td>0.709300</td>\n<td>0.514100</td>\n<td>0.637400</td>\n<td>0.257200</td>\n<td>0.450600</td>\n<td>0.165100</td>\n<td>0.338400</td>\n<td>0.139400</td>\n<td>0.372300</td>\n<td>0.237700</td>\n<td>0.382700</td>\n</tr>\n<tr>\n<td>22</td>\n<td>1.158600</td>\n<td>1.368296</td>\n<td>0.262800</td>\n<td>0.542400</td>\n<td>0.236400</td>\n<td>0.137400</td>\n<td>0.228100</td>\n<td>0.498500</td>\n<td>0.266500</td>\n<td>0.409000</td>\n<td>0.433000</td>\n<td>0.239900</td>\n<td>0.418500</td>\n<td>0.697500</td>\n<td>0.520500</td>\n<td>0.641000</td>\n<td>0.257500</td>\n<td>0.455700</td>\n<td>0.162600</td>\n<td>0.334800</td>\n<td>0.140200</td>\n<td>0.353800</td>\n<td>0.233200</td>\n<td>0.379600</td>\n</tr>\n<tr>\n<td>23</td>\n<td>1.158600</td>\n<td>1.368176</td>\n<td>0.264800</td>\n<td>0.541100</td>\n<td>0.233100</td>\n<td>0.138200</td>",
  "<td>0.223900</td>\n<td>0.498700</td>\n<td>0.272300</td>\n<td>0.407400</td>\n<td>0.434400</td>\n<td>0.233100</td>\n<td>0.418300</td>\n<td>0.702000</td>\n<td>0.524400</td>\n<td>0.642300</td>\n<td>0.262300</td>\n<td>0.444300</td>\n<td>0.159700</td>\n<td>0.335300</td>\n<td>0.140500</td>\n<td>0.366200</td>\n<td>0.236900</td>\n<td>0.384000</td>\n</tr>\n<tr>\n<td>24</td>\n<td>1.049700</td>\n<td>1.355271</td>\n<td>0.269700</td>\n<td>0.549200</td>\n<td>0.239100</td>\n<td>0.134700</td>\n<td>0.229900</td>\n<td>0.519200</td>\n<td>0.274800</td>\n<td>0.412700</td>\n<td>0.437600</td>\n<td>0.245400</td>\n<td>0.417200</td>\n<td>0.711200</td>\n<td>0.523200</td>\n<td>0.644100</td>\n<td>0.272100</td>\n<td>0.440500</td>\n<td>0.166700</td>\n<td>0.341500</td>\n<td>0.137700</td>\n<td>0.373800</td>\n<td>0.249000</td>\n<td>0.388000</td>\n</tr>\n<tr>\n<td>25</td>\n<td>1.049700</td>\n<td>1.355180</td>\n<td>0.272500</td>\n<td>0.547900</td>\n<td>0.243800</td>\n<td>0.149700</td>\n<td>0.229900</td>\n<td>0.523100</td>\n<td>0.272500</td>\n<td>0.415700</td>\n<td>0.442200</td>\n<td>0.256200</td>\n<td>0.420200</td>\n<td>0.705800</td>\n<td>0.523900</td>\n<td>0.639600</td>\n<td>0.271700</td>\n<td>0.451900</td>\n<td>0.166300</td>\n<td>0.346900</td>\n<td>0.153700</td>",
  "<td>0.383100</td>\n<td>0.247000</td>\n<td>0.389300</td>\n</tr>\n<tr>\n<td>26</td>\n<td>1.049700</td>\n<td>1.349337</td>\n<td>0.275600</td>\n<td>0.556300</td>\n<td>0.246400</td>\n<td>0.146700</td>\n<td>0.234800</td>\n<td>0.516300</td>\n<td>0.274200</td>\n<td>0.418300</td>\n<td>0.440900</td>\n<td>0.248700</td>\n<td>0.418900</td>\n<td>0.705800</td>\n<td>0.523200</td>\n<td>0.636500</td>\n<td>0.274700</td>\n<td>0.440500</td>\n<td>0.172400</td>\n<td>0.349100</td>\n<td>0.155600</td>\n<td>0.384600</td>\n<td>0.252300</td>\n<td>0.393800</td>\n</tr>\n<tr>\n<td>27</td>\n<td>1.049700</td>\n<td>1.350782</td>\n<td>0.275200</td>\n<td>0.548700</td>\n<td>0.246800</td>\n<td>0.147300</td>\n<td>0.236400</td>\n<td>0.527200</td>\n<td>0.280100</td>\n<td>0.416200</td>\n<td>0.442600</td>\n<td>0.253400</td>\n<td>0.424000</td>\n<td>0.710300</td>\n<td>0.526600</td>\n<td>0.640100</td>\n<td>0.273200</td>\n<td>0.445600</td>\n<td>0.167000</td>\n<td>0.346900</td>\n<td>0.160100</td>\n<td>0.387700</td>\n<td>0.249200</td>\n<td>0.392900</td>\n</tr>\n<tr>\n<td>28</td>\n<td>1.049700</td>\n<td>1.346533</td>\n<td>0.277000</td>\n<td>0.552800</td>\n<td>0.252900</td>\n<td>0.147400</td>\n<td>0.240000</td>\n<td>0.527600</td>\n<td>0.280900</td>\n<td>0.420900</td>\n<td>0.444100</td>",
  "<td>0.255500</td>\n<td>0.424500</td>\n<td>0.711200</td>\n<td>0.530200</td>\n<td>0.646800</td>\n<td>0.277400</td>\n<td>0.441800</td>\n<td>0.170900</td>\n<td>0.346900</td>\n<td>0.156600</td>\n<td>0.389200</td>\n<td>0.249600</td>\n<td>0.396000</td>\n</tr>\n<tr>\n<td>29</td>\n<td>0.993700</td>\n<td>1.346575</td>\n<td>0.277100</td>\n<td>0.554800</td>\n<td>0.252900</td>\n<td>0.148400</td>\n<td>0.239700</td>\n<td>0.523600</td>\n<td>0.278400</td>\n<td>0.420000</td>\n<td>0.443300</td>\n<td>0.256300</td>\n<td>0.424000</td>\n<td>0.705600</td>\n<td>0.529600</td>\n<td>0.647300</td>\n<td>0.273900</td>\n<td>0.439200</td>\n<td>0.174300</td>\n<td>0.348700</td>\n<td>0.157600</td>\n<td>0.386200</td>\n<td>0.250100</td>\n<td>0.395100</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.993700</td>\n<td>1.346446</td>\n<td>0.277400</td>\n<td>0.554700</td>\n<td>0.252700</td>\n<td>0.147900</td>\n<td>0.240800</td>\n<td>0.523600</td>\n<td>0.278800</td>\n<td>0.420400</td>\n<td>0.443300</td>\n<td>0.256100</td>\n<td>0.424200</td>\n<td>0.705500</td>\n<td>0.530100</td>\n<td>0.646800</td>\n<td>0.275600</td>\n<td>0.440500</td>\n<td>0.174500</td>\n<td>0.348700</td>\n<td>0.157300</td>\n<td>0.386200</td>\n<td>0.249200</td>\n<td>0.394200</td>\n</tr>\n</tbody>\n</table><p>",
  "If you have set `push_to_hub` to `True` in the `training_args`, the training checkpoints are pushed to the\nHugging Face Hub. Upon training completion, push the final model to the Hub as well by calling the [`~transformers.Trainer.push_to_hub`] method.\n\n```py\n>>> trainer.push_to_hub()\n```\n\n## Evaluate\n\n```py\n>>> from pprint import pprint\n\n>>> metrics = trainer.evaluate(eval_dataset=cppe5[\"test\"], metric_key_prefix=\"test\")\n>>> pprint(metrics)\n{'epoch': 30.0,\n'test_loss': 1.0877351760864258,\n'test_map': 0.4116,\n'test_map_50': 0.741,\n'test_map_75': 0.3663,\n'test_map_Coverall': 0.5937,\n'test_map_Face_Shield': 0.5863,\n'test_map_Gloves': 0.3416,\n'test_map_Goggles': 0.1468,\n'test_map_Mask': 0.3894,\n'test_map_large': 0.5637,\n'test_map_medium': 0.3257,\n'test_map_small': 0.3589,\n'test_mar_1': 0.323,\n'test_mar_10': 0.5237,\n'test_mar_100': 0.5587,\n'test_mar_100_Coverall': 0.6756,\n'test_mar_100_Face_Shield': 0.7294,\n'test_mar_100_Gloves': 0.4721,\n'test_mar_100_Goggles': 0.4125,\n'test_mar_100_Mask': 0.5038,\n'test_mar_large': 0.7283,\n'test_mar_medium': 0.4901,\n'test_mar_small': 0.4469,\n'test_runtime': 1.6526,\n'test_samples_per_second': 17.548,\n'test_steps_per_second': 2.42}\n```",
  "These results can be further improved by adjusting the hyperparameters in [`TrainingArguments`]. Give it a go!\n\n## Inference\n\nNow that you have finetuned a model, evaluated it, and uploaded it to the Hugging Face Hub, you can use it for inference.\n\n```py\n>>> import torch\n>>> import requests\n\n>>> from PIL import Image, ImageDraw\n>>> from transformers import AutoImageProcessor, AutoModelForObjectDetection\n\n>>> url = \"https://images.pexels.com/photos/8413299/pexels-photo-8413299.jpeg?auto=compress&cs=tinysrgb&w=630&h=375&dpr=2\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n```\n\nLoad model and image processor from the Hugging Face Hub (skip to use already trained in this session):\n```py\n>>> from accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n>>> device, _, _ = get_backend()\n>>> model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n\n>>> image_processor = AutoImageProcessor.from_pretrained(model_repo)\n>>> model = AutoModelForObjectDetection.from_pretrained(model_repo)\n>>> model = model.to(device)\n```\n\nAnd detect bounding boxes:\n\n```py\n\n>>> with torch.no_grad():",
  "...     inputs = image_processor(images=[image], return_tensors=\"pt\")\n...     outputs = model(**inputs.to(device))\n...     target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n...     results = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected Gloves with confidence 0.683 at location [244.58, 124.33, 300.35, 185.13]\nDetected Mask with confidence 0.517 at location [143.73, 64.58, 219.57, 125.89]\nDetected Gloves with confidence 0.425 at location [179.15, 155.57, 262.4, 226.35]\nDetected Coverall with confidence 0.407 at location [307.13, -1.18, 477.82, 318.06]\nDetected Coverall with confidence 0.391 at location [68.61, 126.66, 309.03, 318.89]\n```\n\nLet's plot the result:\n\n```py\n>>> draw = ImageDraw.Draw(image)\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):",
  "...     box = [round(i, 2) for i in box.tolist()]\n...     x, y, x2, y2 = tuple(box)\n...     draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n...     draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://i.imgur.com/oDUqD0K.png\" alt=\"Object detection result on a new image\"/>\n</div>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Token classification\n\n[[open-in-colab]]\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:",
  "1. Finetune [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/token-classification).\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate seqeval\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load WNUT 17 dataset\n\nStart by loading the WNUT 17 dataset from the 🤗 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> wnut = load_dataset(\"wnut_17\")\n```\n\nThen take a look at an example:\n\n```py\n>>> wnut[\"train\"][0]\n{'id': '0',\n'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],",
  "'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n}\n```\n\nEach number in `ner_tags` represents an entity. Convert the numbers to their label names to find out what the entities are:\n\n```py\n>>> label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n>>> label_list\n[\n\"O\",\n\"B-corporation\",\n\"I-corporation\",\n\"B-creative-work\",\n\"I-creative-work\",\n\"B-group\",\n\"I-group\",\n\"B-location\",\n\"I-location\",\n\"B-person\",\n\"I-person\",\n\"B-product\",\n\"I-product\",\n]\n```\n\nThe letter that prefixes each `ner_tag` indicates the token position of the entity:\n\n- `B-` indicates the beginning of an entity.\n- `I-` indicates a token is contained inside the same entity (for example, the `State` token is a part of an entity like\n`Empire State Building`).\n- `0` indicates the token doesn't correspond to any entity.\n\n## Preprocess\n\n<Youtube id=\"iY2AZYdZAr0\"/>\n\nThe next step is to load a DistilBERT tokenizer to preprocess the `tokens` field:\n\n```py\n>>> from transformers import AutoTokenizer",
  ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\nAs you saw in the example `tokens` field above, it looks like the input has already been tokenized. But the input actually hasn't been tokenized yet and you'll need to set `is_split_into_words=True` to tokenize the words into subwords. For example:\n\n```py\n>>> example = wnut[\"train\"][0]\n>>> tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n>>> tokens\n['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n```\n\nHowever, this adds some special tokens `[CLS]` and `[SEP]` and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords. You'll need to realign the tokens and labels by:",
  "1. Mapping all tokens to their corresponding word with the [`word_ids`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_ids) method.\n2. Assigning the label `-100` to the special tokens `[CLS]` and `[SEP]` so they're ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)).\n3. Only labeling the first token of a given word. Assign `-100` to other subtokens from the same word.\n\nHere is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT's maximum input length:\n\n```py\n>>> def tokenize_and_align_labels(examples):\n...     tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n\n...     labels = []\n...     for i, label in enumerate(examples[f\"ner_tags\"]):\n...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n...         previous_word_idx = None\n...         label_ids = []\n...         for word_idx in word_ids:  # Set the special tokens to -100.\n...             if word_idx is None:",
  "...                 label_ids.append(-100)\n...             elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n...                 label_ids.append(label[word_idx])\n...             else:\n...                 label_ids.append(-100)\n...             previous_word_idx = word_idx\n...         labels.append(label_ids)\n\n...     tokenized_inputs[\"labels\"] = labels\n...     return tokenized_inputs\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\n\n```py\n>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n```\n\nNow create a batch of examples using [`DataCollatorWithPadding`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\n<frameworkcontent>\n<pt>\n```py\n>>> from transformers import DataCollatorForTokenClassification\n\n>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n```\n</pt>\n<tf>\n```py",
  ">>> from transformers import DataCollatorForTokenClassification\n\n>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) framework (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy.\n\n```py\n>>> import evaluate\n\n>>> seqeval = evaluate.load(\"seqeval\")\n```\n\nGet the NER labels first, and then create a function that passes your true predictions and true labels to [`~evaluate.EvaluationModule.compute`] to calculate the scores:\n\n```py\n>>> import numpy as np\n\n>>> labels = [label_list[i] for i in example[f\"ner_tags\"]]\n\n\n>>> def compute_metrics(p):\n...     predictions, labels = p\n...     predictions = np.argmax(predictions, axis=2)",
  "...     true_predictions = [\n...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n...         for prediction, label in zip(predictions, labels)\n...     ]\n...     true_labels = [\n...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n...         for prediction, label in zip(predictions, labels)\n...     ]\n\n...     results = seqeval.compute(predictions=true_predictions, references=true_labels)\n...     return {\n...         \"precision\": results[\"overall_precision\"],\n...         \"recall\": results[\"overall_recall\"],\n...         \"f1\": results[\"overall_f1\"],\n...         \"accuracy\": results[\"overall_accuracy\"],\n...     }\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\nBefore you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:\n\n```py\n>>> id2label = {\n...     0: \"O\",\n...     1: \"B-corporation\",\n...     2: \"I-corporation\",\n...     3: \"B-creative-work\",\n...     4: \"I-creative-work\",\n...     5: \"B-group\",\n...     6: \"I-group\",\n...     7: \"B-location\",\n...     8: \"I-location\",\n...     9: \"B-person\",",
  "...     10: \"I-person\",\n...     11: \"B-product\",\n...     12: \"I-product\",\n... }\n>>> label2id = {\n...     \"O\": 0,\n...     \"B-corporation\": 1,\n...     \"I-corporation\": 2,\n...     \"B-creative-work\": 3,\n...     \"I-creative-work\": 4,\n...     \"B-group\": 5,\n...     \"I-group\": 6,\n...     \"B-location\": 7,\n...     \"I-location\": 8,\n...     \"B-person\": 9,\n...     \"I-person\": 10,\n...     \"B-product\": 11,\n...     \"I-product\": 12,\n... }\n```\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load DistilBERT with [`AutoModelForTokenClassification`] along with the number of expected labels, and the label mappings:\n\n```py\n>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\n>>> model = AutoModelForTokenClassification.from_pretrained(\n...     \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n... )\n```\n\nAt this point, only three steps remain:",
  "1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the seqeval scores and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_wnut_model\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=2,\n...     weight_decay=0.01,\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_wnut[\"train\"],\n...     eval_dataset=tokenized_wnut[\"test\"],\n...     processing_class=tokenizer,\n...     data_collator=data_collator,",
  "...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_train_epochs = 3\n>>> num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=2e-5,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=0.01,\n...     num_warmup_steps=0,\n... )\n```\n\nThen you can load DistilBERT with [`TFAutoModelForTokenClassification`] along with the number of expected labels, and the label mappings:\n\n```py\n>>> from transformers import TFAutoModelForTokenClassification",
  ">>> model = TFAutoModelForTokenClassification.from_pretrained(\n...     \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n... )\n```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_wnut[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_validation_set = model.prepare_tf_dataset(\n...     tokenized_wnut[\"validation\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```",
  "The last two things to setup before you start training is to compute the seqeval scores from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n\nPass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_wnut_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py",
  ">>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for token classification, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nGrab some text you'd like to run inference on:\n\n```py\n>>> text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for NER with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"ner\", model=\"stevhliu/my_awesome_wnut_model\")\n>>> classifier(text)",
  "[{'entity': 'B-location',\n'score': 0.42658573,\n'index': 2,\n'word': 'golden',\n'start': 4,\n'end': 10},\n{'entity': 'I-location',\n'score': 0.35856336,\n'index': 3,\n'word': 'state',\n'start': 11,\n'end': 16},\n{'entity': 'B-group',\n'score': 0.3064001,\n'index': 4,\n'word': 'warriors',\n'start': 17,\n'end': 25},\n{'entity': 'B-location',\n'score': 0.65523505,\n'index': 13,\n'word': 'san',\n'start': 80,\n'end': 83},\n{'entity': 'B-location',\n'score': 0.4668663,\n'index': 14,\n'word': 'francisco',\n'start': 84,\n'end': 93}]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nTokenize the text and return PyTorch tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n>>> inputs = tokenizer(text, return_tensors=\"pt\")\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import AutoModelForTokenClassification\n\n>>> model = AutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n```",
  "Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:\n\n```py\n>>> predictions = torch.argmax(logits, dim=2)\n>>> predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n>>> predicted_token_class\n['O',\n'O',\n'B-location',\n'I-location',\n'B-group',\n'O',\n'O',\n'O',\n'O',\n'O',\n'O',\n'O',\n'O',\n'B-location',\n'B-location',\n'O',\n'O']\n```\n</pt>\n<tf>\nTokenize the text and return TensorFlow tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n>>> inputs = tokenizer(text, return_tensors=\"tf\")\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import TFAutoModelForTokenClassification\n\n>>> model = TFAutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n>>> logits = model(**inputs).logits\n```\n\nGet the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:\n\n```py\n>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)",
  ">>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n>>> predicted_token_class\n['O',\n'O',\n'B-location',\n'I-location',\n'B-group',\n'O',\n'O',\n'O',\n'O',\n'O',\n'O',\n'O',\n'O',\n'B-location',\n'B-location',\n'O',\n'O']\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Text classification\n\n[[open-in-colab]]\n\n<Youtube id=\"leNG9fN9FQU\"/>\n\nText classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like 🙂 positive, 🙁 negative, or 😐 neutral to a sequence of text.",
  "This guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on the [IMDb](https://huggingface.co/datasets/imdb) dataset to determine whether a movie review is positive or negative.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/text-classification).\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate accelerate\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load IMDb dataset\n\nStart by loading the IMDb dataset from the 🤗 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> imdb = load_dataset(\"imdb\")\n```\n\nThen take a look at an example:\n\n```py\n>>> imdb[\"test\"][0]\n{\n\"label\": 0,",
  "\"text\": \"I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \\\"Gene Roddenberry's Earth...\\\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\",",
  "}\n```\n\nThere are two fields in this dataset:\n\n- `text`: the movie review text.\n- `label`: a value that is either `0` for a negative review or `1` for a positive review.\n\n## Preprocess\n\nThe next step is to load a DistilBERT tokenizer to preprocess the `text` field:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\nCreate a preprocessing function to tokenize `text` and truncate sequences to be no longer than DistilBERT's maximum input length:\n\n```py\n>>> def preprocess_function(examples):\n...     return tokenizer(examples[\"text\"], truncation=True)\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:\n\n```py\ntokenized_imdb = imdb.map(preprocess_function, batched=True)\n```\n\nNow create a batch of examples using [`DataCollatorWithPadding`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\n<frameworkcontent>\n<pt>\n```py",
  ">>> from transformers import DataCollatorWithPadding\n\n>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n```\n</pt>\n<tf>\n```py\n>>> from transformers import DataCollatorWithPadding\n\n>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py\n>>> import evaluate\n\n>>> accuracy = evaluate.load(\"accuracy\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)",
  "...     return accuracy.compute(predictions=predictions, references=labels)\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\nBefore you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:\n\n```py\n>>> id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n>>> label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n```\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load DistilBERT with [`AutoModelForSequenceClassification`] along with the number of expected labels, and the label mappings:\n\n```py\n>>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\n...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n... )\n```\n\nAt this point, only three steps remain:",
  "1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_model\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=2,\n...     weight_decay=0.01,\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_imdb[\"train\"],\n...     eval_dataset=tokenized_imdb[\"test\"],\n...     processing_class=tokenizer,\n...     data_collator=data_collator,",
  "...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\n<Tip>\n\n[`Trainer`] applies dynamic padding by default when you pass `tokenizer` to it. In this case, you don't need to specify a data collator explicitly.\n\n</Tip>\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer\n>>> import tensorflow as tf\n\n>>> batch_size = 16\n>>> num_epochs = 5\n>>> batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n>>> total_train_steps = int(batches_per_epoch * num_epochs)\n>>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n```",
  "Then you can load DistilBERT with [`TFAutoModelForSequenceClassification`] along with the number of expected labels, and the label mappings:\n\n```py\n>>> from transformers import TFAutoModelForSequenceClassification\n\n>>> model = TFAutoModelForSequenceClassification.from_pretrained(\n...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n... )\n```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_imdb[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_validation_set = model.prepare_tf_dataset(\n...     tokenized_imdb[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```",
  "The last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n\nPass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py",
  ">>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for text classification, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nGrab some text you'd like to run inference on:\n\n```py\n>>> text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline",
  ">>> classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n>>> classifier(text)\n[{'label': 'POSITIVE', 'score': 0.9994940757751465}]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nTokenize the text and return PyTorch tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n>>> inputs = tokenizer(text, return_tensors=\"pt\")\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n```\n\nGet the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:\n\n```py\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'POSITIVE'\n```\n</pt>\n<tf>\nTokenize the text and return TensorFlow tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")",
  ">>> inputs = tokenizer(text, return_tensors=\"tf\")\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import TFAutoModelForSequenceClassification\n\n>>> model = TFAutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n>>> logits = model(**inputs).logits\n```\n\nGet the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:\n\n```py\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n>>> model.config.id2label[predicted_class_id]\n'POSITIVE'\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n# Knowledge Distillation for Computer Vision\n\n[[open-in-colab]]",
  "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between its outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in [Distilling the Knowledge in a Neural Network by Hinton et al](https://arxiv.org/abs/1503.02531). In this guide, we will do task-specific knowledge distillation. We will use the [beans dataset](https://huggingface.co/datasets/beans) for this.\n\nThis guide demonstrates how you can distill a [fine-tuned ViT model](https://huggingface.co/merve/vit-mobilenet-beans-224) (teacher model) to a [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224) (student model) using the [Trainer API](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) of 🤗 Transformers.",
  "Let's install the libraries needed for distillation and evaluating the process.\n\n```bash\npip install transformers datasets accelerate tensorboard evaluate --upgrade\n```\n\nIn this example, we are using the `merve/beans-vit-224` model as teacher model. It's an image classification model, based on `google/vit-base-patch16-224-in21k` fine-tuned on beans dataset. We will distill this model to a randomly initialized MobileNetV2.\n\nWe will now load the dataset.\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"beans\")\n```\n\nWe can use an image processor from either of the models, as in this case they return the same output with same resolution. We will use the `map()` method of `dataset` to apply the preprocessing to every split of the dataset.\n\n```python\nfrom transformers import AutoImageProcessor\nteacher_processor = AutoImageProcessor.from_pretrained(\"merve/beans-vit-224\")\n\ndef process(examples):\nprocessed_inputs = teacher_processor(examples[\"image\"])\nreturn processed_inputs\n\nprocessed_datasets = dataset.map(process, batched=True)\n```",
  "Essentially, we want the student model (a randomly initialized MobileNet) to mimic the teacher model (fine-tuned vision transformer). To achieve this, we first get the logits output from the teacher and the student. Then, we divide each of them by the parameter `temperature` which controls the importance of each soft target. A parameter called `lambda` weighs the importance of the distillation loss. In this example, we will use `temperature=5` and `lambda=0.5`. We will use the Kullback-Leibler Divergence loss to compute the divergence between the student and teacher. Given two data P and Q, KL Divergence explains how much extra information we need to represent P using Q. If two are identical, their KL divergence is zero, as there's no other information needed to explain P from Q. Thus, in the context of knowledge distillation, KL divergence is useful.\n\n\n```python\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom accelerate.test_utils.testing import get_backend\n\nclass ImageDistilTrainer(Trainer):",
  "def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\nsuper().__init__(model=student_model, *args, **kwargs)\nself.teacher = teacher_model\nself.student = student_model\nself.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\ndevice, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\nself.teacher.to(device)\nself.teacher.eval()\nself.temperature = temperature\nself.lambda_param = lambda_param\n\ndef compute_loss(self, student, inputs, return_outputs=False):\nstudent_output = self.student(**inputs)\n\nwith torch.no_grad():\nteacher_output = self.teacher(**inputs)\n\n# Compute soft targets for teacher and student\nsoft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\nsoft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n\n# Compute the loss\ndistillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n\n# Compute the true label loss\nstudent_target_loss = student_output.loss\n\n# Calculate final loss\nloss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss",
  "return (loss, student_output) if return_outputs else loss\n```\n\nWe will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through the `Trainer`.\n\n```python\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\nLet's set the `TrainingArguments`, the teacher model and the student model.\n\n```python\nfrom transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n\ntraining_args = TrainingArguments(\noutput_dir=\"my-awesome-model\",\nnum_train_epochs=30,\nfp16=True,\nlogging_dir=f\"{repo_name}/logs\",\nlogging_strategy=\"epoch\",\neval_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nload_best_model_at_end=True,\nmetric_for_best_model=\"accuracy\",\nreport_to=\"tensorboard\",\npush_to_hub=True,\nhub_strategy=\"every_save\",\nhub_model_id=repo_name,\n)\n\nnum_labels = len(processed_datasets[\"train\"].features[\"labels\"].names)\n\n# initialize models\nteacher_model = AutoModelForImageClassification.from_pretrained(\n\"merve/beans-vit-224\",\nnum_labels=num_labels,\nignore_mismatched_sizes=True\n)\n\n# training MobileNetV2 from scratch\nstudent_config = MobileNetV2Config()\nstudent_config.num_labels = num_labels",
  "student_model = MobileNetV2ForImageClassification(student_config)\n```\n\nWe can use `compute_metrics` function to evaluate our model on the test set. This function will be used during the training process to compute the `accuracy` & `f1` of our model.\n\n```python\nimport evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\npredictions, labels = eval_pred\nacc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))\nreturn {\"accuracy\": acc[\"accuracy\"]}\n```\n\nLet's initialize the `Trainer` with the training arguments we defined. We will also initialize our data collator.\n\n```python\nfrom transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()\ntrainer = ImageDistilTrainer(\nstudent_model=student_model,\nteacher_model=teacher_model,\ntraining_args=training_args,\ntrain_dataset=processed_datasets[\"train\"],\neval_dataset=processed_datasets[\"validation\"],\ndata_collator=data_collator,\nprocessing_class=teacher_processor,\ncompute_metrics=compute_metrics,\ntemperature=5,\nlambda_param=0.5\n)\n```\n\nWe can now train our model.\n\n```python\ntrainer.train()\n```\n\nWe can evaluate the model on the test set.\n\n```python",
  "trainer.evaluate(processed_datasets[\"test\"])\n```\n\nOn test set, our model reaches 72 percent accuracy. To have a sanity check over efficiency of distillation, we also trained MobileNet on the beans dataset from scratch with the same hyperparameters and observed 63 percent accuracy on the test set. We invite the readers to try different pre-trained teacher models, student architectures, distillation parameters and report their findings. The training logs and checkpoints for distilled model can be found in [this repository](https://huggingface.co/merve/vit-mobilenet-beans-224), and MobileNetV2 trained from scratch can be found in this [repository](https://huggingface.co/merve/resnet-mobilenet-beans-5).",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Zero-shot image classification\n\n[[open-in-colab]]\n\nZero-shot image classification is a task that involves classifying images into different categories using a model that was\nnot explicitly trained on data containing labeled examples from those specific categories.\n\nTraditionally, image classification requires training a model on a specific set of labeled images, and this model learns to",
  "\"map\" certain image features to labels. When there's a need to use such model for a classification task that introduces a\nnew set of labels, fine-tuning is required to \"recalibrate\" the model.\n\nIn contrast, zero-shot or open vocabulary image classification models are typically multi-modal models that have been trained on a large\ndataset of images and associated descriptions. These models learn aligned vision-language representations that can be used for many downstream tasks including zero-shot image classification.\n\nThis is a more flexible approach to image classification that allows models to generalize to new and unseen categories\nwithout the need for additional training data and enables users to query images with free-form text descriptions of their target objects .\n\nIn this guide you'll learn how to:\n\n* create a zero-shot image classification pipeline\n* run zero-shot image classification inference by hand\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install -q \"transformers[torch]\" pillow\n```\n\n## Zero-shot image classification pipeline",
  "The simplest way to try out inference with a model supporting zero-shot image classification is to use the corresponding [`pipeline`].\nInstantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads):\n\n```python\n>>> from transformers import pipeline\n\n>>> checkpoint = \"openai/clip-vit-large-patch14\"\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-image-classification\")\n```\n\nNext, choose an image you'd like to classify.\n\n```py\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&force=true&w=640\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/owl.jpg\" alt=\"Photo of an owl\"/>\n</div>\n\nPass the image and the candidate object labels to the pipeline. Here we pass the image directly; other suitable options\ninclude a local path to an image or an image url.",
  "The candidate labels can be simple words like in this example, or more descriptive.\n\n```py\n>>> predictions = detector(image, candidate_labels=[\"fox\", \"bear\", \"seagull\", \"owl\"])\n>>> predictions\n[{'score': 0.9996670484542847, 'label': 'owl'},\n{'score': 0.000199399160919711, 'label': 'seagull'},\n{'score': 7.392891711788252e-05, 'label': 'fox'},\n{'score': 5.96074532950297e-05, 'label': 'bear'}]\n```\n\n## Zero-shot image classification by hand\n\nNow that you've seen how to use the zero-shot image classification pipeline, let's take a look how you can run zero-shot\nimage classification manually.\n\nStart by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads).\nHere we'll use the same checkpoint as before:\n\n```py\n>>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n\n>>> model = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\n```\n\nLet's take a different image to switch things up.\n\n```py\n>>> from PIL import Image\n>>> import requests",
  ">>> url = \"https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\" alt=\"Photo of a car\"/>\n</div>\n\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\nimage for the model by resizing and normalizing it, and a tokenizer that takes care of the text inputs.\n\n```py\n>>> candidate_labels = [\"tree\", \"car\", \"bike\", \"cat\"]\n# follows the pipeline prompt template to get same results\n>>> candidate_labels = [f'This is a photo of {label}.' for label in candidate_labels]\n>>> inputs = processor(images=image, text=candidate_labels, return_tensors=\"pt\", padding=True)\n```\n\nPass the inputs through the model, and post-process the results:\n\n```py\n>>> import torch\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits = outputs.logits_per_image[0]\n>>> probs = logits.softmax(dim=-1).numpy()\n>>> scores = probs.tolist()\n\n>>> result = [",
  "...     {\"score\": score, \"label\": candidate_label}\n...     for score, candidate_label in sorted(zip(probs, candidate_labels), key=lambda x: -x[0])\n... ]\n\n>>> result\n[{'score': 0.998572, 'label': 'car'},\n{'score': 0.0010570387, 'label': 'bike'},\n{'score': 0.0003393686, 'label': 'tree'},\n{'score': 3.1572064e-05, 'label': 'cat'}]\n```",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Visual Question Answering\n\n[[open-in-colab]]\n\nVisual Question Answering (VQA) is the task of answering open-ended questions based on an image.\nThe input to models supporting this task is typically a combination of an image and a question, and the output is an\nanswer expressed in natural language.\n\nSome noteworthy use case examples for VQA include:\n* Accessibility applications for visually impaired individuals.",
  "* Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n* Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n* Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask \"Is there a dog?\" to find all images with dogs from a set of images.\n\nIn this guide you'll learn how to:\n\n- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).\n- Use your fine-tuned ViLT for inference.\n- Run zero-shot VQA inference with a generative model, like BLIP-2.\n\n## Fine-tuning ViLT\n\nViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for\nVision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier\nhead is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized.",
  "Visual Question Answering is thus treated as a **classification problem**.\n\nMore recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we\nillustrate how to use them for zero-shot VQA inference.\n\nBefore you begin, make sure you have all the necessary libraries installed.\n\n```bash\npip install -q transformers datasets\n```\n\nWe encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the 🤗 Hub.\nWhen prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\nLet's define the model checkpoint as a global variable.\n\n```py\n>>> model_checkpoint = \"dandelin/vilt-b32-mlm\"\n```\n\n## Load the data\n\nFor illustration purposes, in this guide we use a very small sample of the annotated visual question answering `Graphcore/vqa` dataset.\nYou can find the full dataset on [🤗 Hub](https://huggingface.co/datasets/Graphcore/vqa).\n\nAs an alternative to the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa), you can download the",
  "same data manually from the official [VQA dataset page](https://visualqa.org/download.html). If you prefer to follow the\ntutorial with your custom data, check out how to [Create an image dataset](https://huggingface.co/docs/datasets/image_dataset#loading-script)\nguide in the 🤗 Datasets documentation.\n\nLet's load the first 200 examples from the validation split and explore the dataset's features:\n\n```python\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"Graphcore/vqa\", split=\"validation[:200]\")\n>>> dataset\nDataset({\nfeatures: ['question', 'question_type', 'question_id', 'image_id', 'answer_type', 'label'],\nnum_rows: 200\n})\n```\n\nLet's take a look at an example to understand the dataset's features:\n\n```py\n>>> dataset[0]\n{'question': 'Where is he looking?',\n'question_type': 'none of the above',\n'question_id': 262148000,\n'image_id': '/root/.cache/huggingface/datasets/downloads/extracted/ca733e0e000fb2d7a09fbcc94dbfe7b5a30750681d0e965f8e0a23b1c2f98c75/val2014/COCO_val2014_000000262148.jpg',\n'answer_type': 'other',\n'label': {'ids': ['at table', 'down', 'skateboard', 'table'],\n'weights': [0.30000001192092896,\n1.0,\n0.30000001192092896,\n0.30000001192092896]}}\n```",
  "The features relevant to the task include:\n* `question`: the question to be answered from the image\n* `image_id`: the path to the image the question refers to\n* `label`: the annotations\n\nWe can remove the rest of the features as they won't be necessary:\n\n```py\n>>> dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])\n```\n\nAs you can see, the `label` feature contains several answers to the same question (called `ids` here) collected by different human annotators.\nThis is because the answer to a question can be subjective. In this case, the question is \"where is he looking?\". Some people\nannotated this with \"down\", others with \"at table\", another one with \"skateboard\", etc.\n\nTake a look at the image and consider which answer would you give:\n\n```python\n>>> from PIL import Image\n\n>>> image = Image.open(dataset[0]['image_id'])\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/vqa-example.png\" alt=\"VQA Image Example\"/>\n</div>\n\nDue to the questions' and answers' ambiguity, datasets like this are treated as a multi-label classification problem (as",
  "multiple answers are possibly valid). Moreover, rather than just creating a one-hot encoded vector, one creates a\nsoft encoding, based on the number of times a certain answer appeared in the annotations.\n\nFor instance, in the example above, because the answer \"down\" is selected way more often than other answers, it has a\nscore (called `weight` in the dataset) of 1.0, and the rest of the answers have scores < 1.0.\n\nTo later instantiate the model with an appropriate classification head, let's create two dictionaries: one that maps\nthe label name to an integer and vice versa:\n\n```py\n>>> import itertools\n\n>>> labels = [item['ids'] for item in dataset['label']]\n>>> flattened_labels = list(itertools.chain(*labels))\n>>> unique_labels = list(set(flattened_labels))\n\n>>> label2id = {label: idx for idx, label in enumerate(unique_labels)}\n>>> id2label = {idx: label for label, idx in label2id.items()}\n```\n\nNow that we have the mappings, we can replace the string answers with their ids, and flatten the dataset for a more convenient further preprocessing.\n\n```python\n>>> def replace_ids(inputs):\n...   inputs[\"label\"][\"ids\"] = [label2id[x] for x in inputs[\"label\"][\"ids\"]]\n...   return inputs",
  ">>> dataset = dataset.map(replace_ids)\n>>> flat_dataset = dataset.flatten()\n>>> flat_dataset.features\n{'question': Value(dtype='string', id=None),\n'image_id': Value(dtype='string', id=None),\n'label.ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n'label.weights': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}\n```\n\n## Preprocessing data\n\nThe next step is to load a ViLT processor to prepare the image and text data for the model.\n[`ViltProcessor`] wraps a BERT tokenizer and ViLT image processor into a convenient single processor:\n\n```py\n>>> from transformers import ViltProcessor\n\n>>> processor = ViltProcessor.from_pretrained(model_checkpoint)\n```\n\nTo preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. The processor will use\nthe [`BertTokenizerFast`] to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids` for the text data.\nAs for images, the processor will leverage [`ViltImageProcessor`] to resize and normalize the image, and create `pixel_values` and `pixel_mask`.",
  "All these preprocessing steps are done under the hood, we only need to call the `processor`. However, we still need to\nprepare the target labels. In this representation, each element corresponds to a possible answer (label). For correct answers, the element holds\ntheir respective score (weight), while the remaining elements are set to zero.\n\nThe following function applies the `processor` to the images and questions and formats the labels as described above:\n\n```py\n>>> import torch\n\n>>> def preprocess_data(examples):\n...     image_paths = examples['image_id']\n...     images = [Image.open(image_path) for image_path in image_paths]\n...     texts = examples['question']\n\n...     encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n...     for k, v in encoding.items():\n...           encoding[k] = v.squeeze()\n\n...     targets = []\n\n...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n...         target = torch.zeros(len(id2label))\n\n...         for label, score in zip(labels, scores):\n...             target[label] = score\n\n...         targets.append(target)\n\n...     encoding[\"labels\"] = targets",
  "...     return encoding\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.map`] function. You can speed up `map` by\nsetting `batched=True` to process multiple elements of the dataset at once. At this point, feel free to remove the columns you don't need.\n\n```py\n>>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])\n>>> processed_dataset\nDataset({\nfeatures: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],\nnum_rows: 200\n})\n```\n\nAs a final step, create a batch of examples using [`DefaultDataCollator`]:\n\n```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator()\n```\n\n## Train the model\n\nYou’re ready to start training your model now! Load ViLT with [`ViltForQuestionAnswering`]. Specify the number of labels\nalong with the label mappings:\n\n```py\n>>> from transformers import ViltForQuestionAnswering\n\n>>> model = ViltForQuestionAnswering.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)",
  "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]:\n\n```py\n>>> from transformers import TrainingArguments\n\n>>> repo_id = \"MariaK/vilt_finetuned_200\"\n\n>>> training_args = TrainingArguments(\n...     output_dir=repo_id,\n...     per_device_train_batch_size=4,\n...     num_train_epochs=20,\n...     save_steps=200,\n...     logging_steps=50,\n...     learning_rate=5e-5,\n...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n```\n\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, processor, and data collator.\n\n```py\n>>> from transformers import Trainer\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     data_collator=data_collator,\n...     train_dataset=processed_dataset,\n...     processing_class=processor,\n... )\n```\n\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~Trainer.push_to_hub`] method to share your final model on the 🤗 Hub:\n\n```py\n>>> trainer.push_to_hub()\n```\n\n## Inference",
  "Now that you have fine-tuned a ViLT model, and uploaded it to the 🤗 Hub, you can use it for inference. The simplest\nway to try out your fine-tuned model for inference is to use it in a [`Pipeline`].\n\n```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(\"visual-question-answering\", model=\"MariaK/vilt_finetuned_200\")\n```\n\nThe model in this guide has only been trained on 200 examples, so don't expect a lot from it. Let's see if it at least\nlearned something from the data and take the first example from the dataset to illustrate inference:\n\n```py\n>>> example = dataset[0]\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n>>> print(question)\n>>> pipe(image, question, top_k=1)\n\"Where is he looking?\"\n[{'score': 0.5498199462890625, 'answer': 'down'}]\n```\n\nEven though not very confident, the model indeed has learned something. With more examples and longer training, you'll get far better results!\n\nYou can also manually replicate the results of the pipeline if you'd like:\n1. Take an image and a question, prepare them for the model using the processor from your model.\n2. Forward the result or preprocessing through the model.",
  "3. From the logits, get the most likely answer's id, and find the actual answer in the `id2label`.\n\n```py\n>>> processor = ViltProcessor.from_pretrained(\"MariaK/vilt_finetuned_200\")\n\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n\n>>> # prepare inputs\n>>> inputs = processor(image, question, return_tensors=\"pt\")\n\n>>> model = ViltForQuestionAnswering.from_pretrained(\"MariaK/vilt_finetuned_200\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> idx = logits.argmax(-1).item()\n>>> print(\"Predicted answer:\", model.config.id2label[idx])\nPredicted answer: down\n```\n\n## Zero-shot VQA\n\nThe previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach\nVQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training\nparadigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)).",
  "This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering.\n\nLet's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a\nGPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically:\n\n```py\n>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n>>> import torch\n>>> from accelerate.test_utils.testing import get_backend\n\n>>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n>>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n>>> model.to(device)\n```\n\nThe model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset:\n\n```py\n>>> example = dataset[0]\n>>> image = Image.open(example['image_id'])\n>>> question = example['question']\n```",
  "To use BLIP-2 for visual question answering task, the textual prompt has to follow a specific format: `Question: {} Answer:`.\n\n```py\n>>> prompt = f\"Question: {question} Answer:\"\n```\n\nNow we need to preprocess the image/prompt with the model's processor, pass the processed input through the model, and decode the output:\n\n```py\n>>> inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\n\"He is looking at the crowd\"\n```\n\nAs you can see, the model recognized the crowd, and the direction of the face (looking down), however, it seems to miss\nthe fact the crowd is behind the skater. Still, in cases where acquiring human-annotated datasets is not feasible, this\napproach can quickly produce useful results.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Video-text-to-text\n\n[[open-in-colab]]\n\nVideo-text-to-text models, also known as video language models or vision language models with video input, are language models that take a video input. These models can tackle various tasks, from video question answering to video captioning.",
  "These models have nearly the same architecture as [image-text-to-text](../image_text_to_text.md) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos. Moreover, video-text-to-text models are often trained with all vision modalities. Each example might have videos, multiple videos, images and multiple images. Some of these models can also take interleaved inputs. For example, you can refer to a specific video inside a string of text by adding a video token in text like \"What is happening in this video? `<video>`\".\n\nIn this guide, we provide a brief overview of video LMs and show how to use them with Transformers for inference.\n\nTo begin with, there are multiple types of video LMs:\n- base models used for fine-tuning\n- chat fine-tuned models for conversation\n- instruction fine-tuned models",
  "This guide focuses on inference with an instruction-tuned model, [llava-hf/llava-interleave-qwen-7b-hf](https://huggingface.co/llava-hf/llava-interleave-qwen-7b-hf) which can take in interleaved data. Alternatively, you can try [llava-interleave-qwen-0.5b-hf](https://huggingface.co/llava-hf/llava-interleave-qwen-0.5b-hf) if your hardware doesn't allow running a 7B model.\n\nLet's begin installing the dependencies.\n\n```bash\npip install -q transformers accelerate flash_attn\n```\n\nLet's initialize the model and the processor.\n\n```python\nfrom transformers import LlavaProcessor, LlavaForConditionalGeneration\nimport torch\nmodel_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n\nprocessor = LlavaProcessor.from_pretrained(model_id)\n\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\nmodel.to(\"cuda\") # can also be xpu, mps, npu etc. depending on your hardware accelerator\n```",
  "Some models directly consume the `<video>` token, and others accept `<image>` tokens equal to the number of sampled frames. This model handles videos in the latter fashion. We will write a simple utility to handle image tokens, and another utility to get a video from a url and sample frames from it.\n\n```python\nimport uuid\nimport requests\nimport cv2\nfrom PIL import Image\n\ndef replace_video_with_images(text, frames):\nreturn text.replace(\"<video>\", \"<image>\" * frames)\n\ndef sample_frames(url, num_frames):\n\nresponse = requests.get(url)\npath_id = str(uuid.uuid4())\n\npath = f\"./{path_id}.mp4\"\n\nwith open(path, \"wb\") as f:\nf.write(response.content)\n\nvideo = cv2.VideoCapture(path)\ntotal_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\ninterval = total_frames // num_frames\nframes = []\nfor i in range(total_frames):\nret, frame = video.read()\npil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\nif not ret:\ncontinue\nif i % interval == 0:\nframes.append(pil_img)\nvideo.release()\nreturn frames[:num_frames]\n```\n\nLet's get our inputs. We will sample frames and concatenate them.\n\n```python\nvideo_1 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\"",
  "video_2 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_2.mp4\"\n\nvideo_1 = sample_frames(video_1, 6)\nvideo_2 = sample_frames(video_2, 6)\n\nvideos = video_1 + video_2\n\nvideos\n\n# [<PIL.Image.Image image mode=RGB size=1920x1080>,\n# <PIL.Image.Image image mode=RGB size=1920x1080>,\n# <PIL.Image.Image image mode=RGB size=1920x1080>, ...]\n```\n\nBoth videos have cats.\n\n<div class=\"container\">\n<div class=\"video-container\">\n<video width=\"400\" controls>\n<source src=\"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\" type=\"video/mp4\">\n</video>\n</div>\n\n<div class=\"video-container\">\n<video width=\"400\" controls>\n<source src=\"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_2.mp4\" type=\"video/mp4\">\n</video>\n</div>\n</div>\n\nNow we can preprocess the inputs.\n\nThis model has a prompt template that looks like following. First, we'll put all the sampled frames into one list. Since we have eight frames in each video, we will insert 12 `<image>` tokens to our prompt. Add `assistant` at the end of the prompt to trigger the model to give answers. Then we can preprocess.\n\n```python",
  "user_prompt = \"Are these two cats in these two videos doing the same thing?\"\ntoks = \"<image>\" * 12\nprompt = \"<|im_start|>user\"+ toks + f\"\\n{user_prompt}<|im_end|><|im_start|>assistant\"\ninputs = processor(text=prompt, images=videos, return_tensors=\"pt\").to(model.device, model.dtype)\n```\n\nWe can now call [`~GenerationMixin.generate`] for inference. The model outputs the question in our input and answer, so we only take the text after the prompt and `assistant` part from the model output.\n\n```python\noutput = model.generate(**inputs, max_new_tokens=100, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True)[len(user_prompt)+10:])\n\n# The first cat is shown in a relaxed state, with its eyes closed and a content expression, while the second cat is shown in a more active state, with its mouth open wide, possibly in a yawn or a vocalization.\n\n\n```\n\nAnd voila!\n\nTo learn more about chat templates and token streaming for video-text-to-text models, refer to the [image-text-to-text](../tasks/image_text_to_text) task guide because these models work similarly.",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image classification\n\n[[open-in-colab]]\n\n<Youtube id=\"tjAIM7BOYhw\"/>\n\nImage classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the\npixel values that comprise an image. There are many applications for image classification, such as detecting damage\nafter a natural disaster, monitoring crop health, or helping screen medical images for signs of disease.\n\nThis guide illustrates how to:",
  "1. Fine-tune [ViT](../model_doc/vit) on the [Food-101](https://huggingface.co/datasets/food101) dataset to classify a food item in an image.\n2. Use your fine-tuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/image-classification)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate accelerate pillow torchvision scikit-learn\n```\n\nWe encourage you to log in to your Hugging Face account to upload and share your model with the community. When prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load Food-101 dataset\n\nStart by loading a smaller subset of the Food-101 dataset from the 🤗 Datasets library. This will give you a chance to\nexperiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset\n\n>>> food = load_dataset(\"food101\", split=\"train[:5000]\")\n```",
  "Split the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> food = food.train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:\n\n```py\n>>> food[\"train\"][0]\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F52AFC8AC50>,\n'label': 79}\n```\n\nEach example in the dataset has two fields:\n\n- `image`: a PIL image of the food item\n- `label`: the label class of the food item\n\nTo make it easier for the model to get the label name from the label id, create a dictionary that maps the label name\nto an integer and vice versa:\n\n```py\n>>> labels = food[\"train\"].features[\"label\"].names\n>>> label2id, id2label = dict(), dict()\n>>> for i, label in enumerate(labels):\n...     label2id[label] = str(i)\n...     id2label[str(i)] = label\n```\n\nNow you can convert the label id to a label name:\n\n```py\n>>> id2label[str(79)]\n'prime_rib'\n```\n\n## Preprocess\n\nThe next step is to load a ViT image processor to process the image into a tensor:\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> checkpoint = \"google/vit-base-patch16-224-in21k\"",
  ">>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n```\n\n<frameworkcontent>\n<pt>\nApply some image transformations to the images to make the model more robust against overfitting. Here you'll use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module, but you can also use any image library you like.\n\nCrop a random part of the image, resize it, and normalize it with the image mean and standard deviation:\n\n```py\n>>> from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n\n>>> normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n>>> size = (\n...     image_processor.size[\"shortest_edge\"]\n...     if \"shortest_edge\" in image_processor.size\n...     else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n... )\n>>> _transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n```\n\nThen create a preprocessing function to apply the transforms and return the `pixel_values` - the inputs to the model - of the image:\n\n```py\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]",
  "...     del examples[\"image\"]\n...     return examples\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.with_transform`] method. The transforms are applied on the fly when you load an element of the dataset:\n\n```py\n>>> food = food.with_transform(transforms)\n```\n\nNow create a batch of examples using [`DefaultDataCollator`]. Unlike other data collators in 🤗 Transformers, the `DefaultDataCollator` does not apply additional preprocessing such as padding.\n\n```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator()\n```\n</pt>\n</frameworkcontent>\n\n\n<frameworkcontent>\n<tf>\n\nTo avoid overfitting and to make the model more robust, add some data augmentation to the training part of the dataset.\nHere we use Keras preprocessing layers to define the transformations for the training data (includes data augmentation),\nand transformations for the validation data (only center cropping, resizing and normalizing). You can use `tf.image`or\nany other library you prefer.\n\n```py\n>>> from tensorflow import keras\n>>> from tensorflow.keras import layers",
  ">>> size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n\n>>> train_data_augmentation = keras.Sequential(\n...     [\n...         layers.RandomCrop(size[0], size[1]),\n...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n...         layers.RandomFlip(\"horizontal\"),\n...         layers.RandomRotation(factor=0.02),\n...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n...     ],\n...     name=\"train_data_augmentation\",\n... )\n\n>>> val_data_augmentation = keras.Sequential(\n...     [\n...         layers.CenterCrop(size[0], size[1]),\n...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n...     ],\n...     name=\"val_data_augmentation\",\n... )\n```\n\nNext, create functions to apply appropriate transformations to a batch of images, instead of one image at a time.\n\n```py\n>>> import numpy as np\n>>> import tensorflow as tf\n>>> from PIL import Image\n\n\n>>> def convert_to_tf_tensor(image: Image):\n...     np_image = np.array(image)\n...     tf_image = tf.convert_to_tensor(np_image)\n...     # `expand_dims()` is used to add a batch dimension since\n...     # the TF augmentation layers operates on batched inputs.\n...     return tf.expand_dims(tf_image, 0)",
  ">>> def preprocess_train(example_batch):\n...     \"\"\"Apply train_transforms across a batch.\"\"\"\n...     images = [\n...         train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n...     ]\n...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n...     return example_batch\n\n\n... def preprocess_val(example_batch):\n...     \"\"\"Apply val_transforms across a batch.\"\"\"\n...     images = [\n...         val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n...     ]\n...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n...     return example_batch\n```\n\nUse 🤗 Datasets [`~datasets.Dataset.set_transform`] to apply the transformations on the fly:\n\n```py\nfood[\"train\"].set_transform(preprocess_train)\nfood[\"test\"].set_transform(preprocess_val)\n```\n\nAs a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike other data collators in 🤗 Transformers, the\n`DefaultDataCollator` does not apply additional preprocessing, such as padding.\n\n```py\n>>> from transformers import DefaultDataCollator",
  ">>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load an\nevaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load\nthe [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py\n>>> import evaluate\n\n>>> accuracy = evaluate.load(\"accuracy\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=labels)\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you set up your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>",
  "If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load ViT with [`AutoModelForImageClassification`]. Specify the number of labels along with the number of expected labels, and the label mappings:\n\n```py\n>>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n\n>>> model = AutoModelForImageClassification.from_pretrained(\n...     checkpoint,\n...     num_labels=len(labels),\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n```\n\nAt this point, only three steps remain:",
  "1. Define your training hyperparameters in [`TrainingArguments`]. It is important you don't remove unused columns because that'll drop the `image` column. Without the `image` column, you can't create `pixel_values`. Set `remove_unused_columns=False` to prevent this behavior! The only other required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_food_model\",\n...     remove_unused_columns=False,\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=16,\n...     gradient_accumulation_steps=4,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=3,\n...     warmup_ratio=0.1,",
  "...     logging_steps=10,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"accuracy\",\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     data_collator=data_collator,\n...     train_dataset=food[\"train\"],\n...     eval_dataset=food[\"test\"],\n...     processing_class=image_processor,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n</frameworkcontent>\n\n<frameworkcontent>\n<tf>\n\n<Tip>\n\nIf you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](./training#train-a-tensorflow-model-with-keras) first!\n\n</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pre-trained model.\n3. Convert a 🤗 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks and use the `fit()` method to run the training.",
  "6. Upload your model to 🤗 Hub to share with the community.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_epochs = 5\n>>> num_train_steps = len(food[\"train\"]) * num_epochs\n>>> learning_rate = 3e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```\n\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\n\n```py\n>>> from transformers import TFAutoModelForImageClassification\n\n>>> model = TFAutoModelForImageClassification.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n```\n\nConvert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and your `data_collator`:\n\n```py\n>>> # converting our train dataset to tf.data.Dataset\n>>> tf_train_dataset = food[\"train\"].to_tf_dataset(",
  "...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n... )\n\n>>> # converting our test dataset to tf.data.Dataset\n>>> tf_eval_dataset = food[\"test\"].to_tf_dataset(\n...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n... )\n```\n\nConfigure the model for training with `compile()`:\n\n```py\n>>> from tensorflow.keras.losses import SparseCategoricalCrossentropy\n\n>>> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n>>> model.compile(optimizer=optimizer, loss=loss)\n```\n\nTo compute the accuracy from the predictions and push your model to the 🤗 Hub, use [Keras callbacks](../main_classes/keras_callbacks).\nPass your `compute_metrics` function to [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback),\nand use the [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback) to upload the model:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)",
  ">>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"food_classifier\",\n...     tokenizer=image_processor,\n...     save_strategy=\"no\",\n... )\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you are ready to train your model! Call `fit()` with your training and validation datasets, the number of epochs,\nand your callbacks to fine-tune the model:\n\n```py\n>>> model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)\nEpoch 1/5\n250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290\nEpoch 2/5\n250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690\nEpoch 3/5\n250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820\nEpoch 4/5\n250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900\nEpoch 5/5\n250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890\n```",
  "Congratulations! You have fine-tuned your model and shared it on the 🤗 Hub. You can now use it for inference!\n</tf>\n</frameworkcontent>\n\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for image classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've fine-tuned a model, you can use it for inference!\n\nLoad an image you'd like to run inference on:\n\n```py\n>>> ds = load_dataset(\"food101\", split=\"validation[:10]\")\n>>> image = ds[\"image\"][0]\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\" alt=\"image of beignets\"/>\n</div>\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for image classification with your model, and pass your image to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model\")\n>>> classifier(image)",
  "[{'score': 0.31856709718704224, 'label': 'beignets'},\n{'score': 0.015232225880026817, 'label': 'bruschetta'},\n{'score': 0.01519392803311348, 'label': 'chicken_wings'},\n{'score': 0.013022331520915031, 'label': 'pork_chop'},\n{'score': 0.012728818692266941, 'label': 'prime_rib'}]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nLoad an image processor to preprocess the image and return the `input` as PyTorch tensors:\n\n```py\n>>> from transformers import AutoImageProcessor\n>>> import torch\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"my_awesome_food_model\")\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n```\n\nPass your inputs to the model and return the logits:\n\n```py\n>>> from transformers import AutoModelForImageClassification\n\n>>> model = AutoModelForImageClassification.from_pretrained(\"my_awesome_food_model\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n```\n\nGet the predicted label with the highest probability, and use the model's `id2label` mapping to convert it to a label:\n\n```py\n>>> predicted_label = logits.argmax(-1).item()\n>>> model.config.id2label[predicted_label]\n'beignets'",
  "```\n</pt>\n</frameworkcontent>\n\n<frameworkcontent>\n<tf>\nLoad an image processor to preprocess the image and return the `input` as TensorFlow tensors:\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n>>> inputs = image_processor(image, return_tensors=\"tf\")\n```\n\nPass your inputs to the model and return the logits:\n\n```py\n>>> from transformers import TFAutoModelForImageClassification\n\n>>> model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n>>> logits = model(**inputs).logits\n```\n\nGet the predicted label with the highest probability, and use the model's `id2label` mapping to convert it to a label:\n\n```py\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n>>> model.config.id2label[predicted_class_id]\n'beignets'\n```\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Monocular depth estimation\n\nMonocular depth estimation is a computer vision task that involves predicting the depth information of a scene from a\nsingle image. In other words, it is the process of estimating the distance of objects in a scene from\na single camera viewpoint.\n\nMonocular depth estimation has various applications, including 3D reconstruction, augmented reality, autonomous driving,",
  "and robotics. It is a challenging task as it requires the model to understand the complex relationships between objects\nin the scene and the corresponding depth information, which can be affected by factors such as lighting conditions,\nocclusion, and texture.\n\nThere are two main depth estimation categories:\n\n- **Absolute depth estimation**: This task variant aims to provide exact depth measurements from the camera. The term is used interchangeably with metric depth estimation, where depth is provided in precise measurements in meters or feet. Absolute depth estimation models output depth maps with numerical values that represent real-world distances.\n\n- **Relative depth estimation**: Relative depth estimation aims to predict the depth order of objects or points in a scene without providing the precise measurements. These models output a depth map that indicates which parts of the scene are closer or farther relative to each other without the actual distances to A and B.",
  "In this guide, we will see how to infer with [Depth Anything V2](https://huggingface.co/depth-anything/Depth-Anything-V2-Large), a state-of-the-art zero-shot relative depth estimation model, and [ZoeDepth](https://huggingface.co/docs/transformers/main/en/model_doc/zoedepth), an absolute depth estimation model.\n\n<Tip>\n\nCheck the [Depth Estimation](https://huggingface.co/tasks/depth-estimation) task page to view all compatible architectures and checkpoints.\n\n</Tip>\n\nBefore we begin, we need to install the latest version of Transformers:\n\n```bash\npip install -q -U transformers\n```\n\n## Depth estimation pipeline\n\nThe simplest way to try out inference with a model supporting depth estimation is to use the corresponding [`pipeline`].\nInstantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads):\n\n```py\n>>> from transformers import pipeline\n>>> import torch\n>>> from accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n>>> device, _, _ = get_backend()\n>>> checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"",
  ">>> pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n```\n\nNext, choose an image to analyze:\n\n```py\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> image\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" alt=\"Photo of a bee\"/>\n</div>\n\nPass the image to the pipeline.\n\n```py\n>>> predictions = pipe(image)\n```\n\nThe pipeline returns a dictionary with two entries. The first one, called `predicted_depth`, is a tensor with the values\nbeing the depth expressed in meters for each pixel.\nThe second one, `depth`, is a PIL image that visualizes the depth estimation result.\n\nLet's take a look at the visualized result:\n\n```py\n>>> predictions[\"depth\"]\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization.png\" alt=\"Depth estimation visualization\"/>\n</div>\n\n## Depth estimation inference by hand",
  "Now that you've seen how to use the depth estimation pipeline, let's see how we can replicate the same result by hand.\n\nStart by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads).\nHere we'll use the same checkpoint as before:\n\n```py\n>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n\n>>> checkpoint = \"Intel/zoedepth-nyu-kitti\"\n\n>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n>>> model = AutoModelForDepthEstimation.from_pretrained(checkpoint).to(device)\n```\n\nPrepare the image input for the model using the `image_processor` that will take care of the necessary image transformations\nsuch as resizing and normalization:\n\n```py\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n```\n\nPass the prepared inputs through the model:\n\n```py\n>>> import torch\n\n>>> with torch.no_grad():\n...     outputs = model(pixel_values)\n```",
  "Let's post-process the results to remove any padding and resize the depth map to match the original image size. The `post_process_depth_estimation` outputs a list of dicts containing the `\"predicted_depth\"`.\n\n```py\n>>> # ZoeDepth dynamically pads the input image. Thus we pass the original image size as argument\n>>> # to `post_process_depth_estimation` to remove the padding and resize to original dimensions.\n>>> post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     source_sizes=[(image.height, image.width)],\n... )\n\n>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n>>> depth = depth.detach().cpu().numpy() * 255\n>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n```\n\n<Tip>",
  "<p>In the <a href=\"https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131\">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>\n<pre><code class=\"language-Python\">&gt;&gt;&gt; with torch.no_grad():\n...     outputs = model(pixel_values)\n...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n&gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     source_sizes=[(image.height, image.width)],\n...     outputs_flipped=outputs_flipped,\n... )\n</code></pre>\n</Tip>\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization-zoe.png\" alt=\"Depth estimation visualization\"/>\n</div>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Multiple choice\n\n[[open-in-colab]]\n\nA multiple choice task is similar to question answering, except several candidate answers are provided along with a context and the model is trained to select the correct answer.\n\nThis guide will show you how to:",
  "1. Finetune [BERT](https://huggingface.co/google-bert/bert-base-uncased) on the `regular` configuration of the [SWAG](https://huggingface.co/datasets/swag) dataset to select the best answer given multiple options and some context.\n2. Use your finetuned model for inference.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load SWAG dataset\n\nStart by loading the `regular` configuration of the SWAG dataset from the 🤗 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> swag = load_dataset(\"swag\", \"regular\")\n```\n\nThen take a look at an example:\n\n```py\n>>> swag[\"train\"][0]\n{'ending0': 'passes by walking down the street playing their instruments.',\n'ending1': 'has heard approaching them.',\n'ending2': \"arrives and they're outside dancing and asleep.\",\n'ending3': 'turns the lead singer watches the performance.',\n'fold-ind': '3416',\n'gold-source': 'gold',",
  "'label': 0,\n'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n'sent2': 'A drum line',\n'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n'video-id': 'anetv_jkn6uvmqwh4'}\n```\n\nWhile it looks like there are a lot of fields here, it is actually pretty straightforward:\n\n- `sent1` and `sent2`: these fields show how a sentence starts, and if you put the two together, you get the `startphrase` field.\n- `ending`: suggests a possible ending for how a sentence can end, but only one of them is correct.\n- `label`: identifies the correct sentence ending.\n\n## Preprocess\n\nThe next step is to load a BERT tokenizer to process the sentence starts and the four possible endings:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n```\n\nThe preprocessing function you want to create needs to:\n\n1. Make four copies of the `sent1` field and combine each of them with `sent2` to recreate how a sentence starts.\n2. Combine `sent2` with each of the four possible sentence endings.",
  "3. Flatten these two lists so you can tokenize them, and then unflatten them afterward so each example has a corresponding `input_ids`, `attention_mask`, and `labels` field.\n\n```py\n>>> ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n\n\n>>> def preprocess_function(examples):\n...     first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n...     question_headers = examples[\"sent2\"]\n...     second_sentences = [\n...         [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n...     ]\n\n...     first_sentences = sum(first_sentences, [])\n...     second_sentences = sum(second_sentences, [])\n\n...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\n\n```py\n>>> tokenized_swag = swag.map(preprocess_function, batched=True)\n```",
  "To create a batch of examples, it's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. [`DataCollatorForMultipleChoice`] flattens all the model inputs, applies padding, and then unflattens the results.\n```py\n>>> from transformers import DataCollatorForMultipleChoice\n>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n```\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py\n>>> import evaluate\n\n>>> accuracy = evaluate.load(\"accuracy\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):",
  "...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=labels)\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load BERT with [`AutoModelForMultipleChoice`]:\n\n```py\n>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\n>>> model = AutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.",
  "2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_swag_model\",\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=3,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_swag[\"train\"],\n...     eval_dataset=tokenized_swag[\"validation\"],\n...     processing_class=tokenizer,\n...     data_collator=collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>",
  "If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_train_epochs = 2\n>>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n>>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n```\n\nThen you can load BERT with [`TFAutoModelForMultipleChoice`]:\n\n```py\n>>> from transformers import TFAutoModelForMultipleChoice\n\n>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_swag[\"train\"],\n...     shuffle=True,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )",
  ">>> tf_validation_set = model.prepare_tf_dataset(\n...     tokenized_swag[\"validation\"],\n...     shuffle=False,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```\n\nThe last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using [Keras callbacks](../main_classes/keras_callbacks).\n\nPass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback",
  ">>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for multiple choice, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!",
  "Come up with some text and two candidate answers:\n\n```py\n>>> prompt = \"France has a bread law, Le Décret Pain, with strict rules on what is allowed in a traditional baguette.\"\n>>> candidate1 = \"The law does not apply to croissants and brioche.\"\n>>> candidate2 = \"The law applies to baguettes.\"\n```\n\n<frameworkcontent>\n<pt>\nTokenize each prompt and candidate answer pair and return PyTorch tensors. You should also create some `labels`:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n>>> labels = torch.tensor(0).unsqueeze(0)\n```\n\nPass your inputs and labels to the model and return the `logits`:\n\n```py\n>>> from transformers import AutoModelForMultipleChoice\n\n>>> model = AutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n>>> logits = outputs.logits\n```\n\nGet the class with the highest probability:\n\n```py\n>>> predicted_class = logits.argmax().item()\n>>> predicted_class\n0\n```\n</pt>\n<tf>",
  "Tokenize each prompt and candidate answer pair and return TensorFlow tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import TFAutoModelForMultipleChoice\n\n>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```\n\nGet the class with the highest probability:\n\n```py\n>>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n>>> predicted_class\n0\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Keypoint Detection\n\n[[open-in-colab]]\n\nKeypoint detection identifies and locates specific points of interest within an image. These keypoints, also known as landmarks, represent meaningful features of objects, such as facial features or object parts. These models take an image input and return the following outputs:\n\n- **Keypoints and Scores**: Points of interest and their confidence scores.",
  "- **Descriptors**: A representation of the image region surrounding each keypoint, capturing its texture, gradient, orientation and other properties.\n\nIn this guide, we will show how to extract keypoints from images.\n\nFor this tutorial, we will use [SuperPoint](./model_doc/superpoint.md), a foundation model for keypoint detection.\n\n```python\nfrom transformers import AutoImageProcessor, SuperPointForKeypointDetection\nprocessor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\nmodel = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n```\n\nLet's test the model on the images below.\n\n<div style=\"display: flex; align-items: center;\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nalt=\"Bee\"\nstyle=\"height: 200px; object-fit: contain; margin-right: 10px;\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\nalt=\"Cats\"\nstyle=\"height: 200px; object-fit: contain;\">\n</div>\n\n\n```python\nimport torch\nfrom PIL import Image\nimport requests\nimport cv2",
  "url_image_1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage_1 = Image.open(requests.get(url_image_1, stream=True).raw)\nurl_image_2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\nimage_2 = Image.open(requests.get(url_image_2, stream=True).raw)\n\nimages = [image_1, image_2]\n```\n\nWe can now process our inputs and infer.\n\n```python\ninputs = processor(images,return_tensors=\"pt\").to(model.device, model.dtype)\noutputs = model(**inputs)\n```\n\nThe model output has relative keypoints, descriptors, masks and scores for each item in the batch. The mask highlights areas of the image where keypoints are present.\n\n```python\nSuperPointKeypointDescriptionOutput(loss=None, keypoints=tensor([[[0.0437, 0.0167],\n[0.0688, 0.0167],\n[0.0172, 0.0188],\n...,\n[0.5984, 0.9812],\n[0.6953, 0.9812]]]),\nscores=tensor([[0.0056, 0.0053, 0.0079,  ..., 0.0125, 0.0539, 0.0377],\n[0.0206, 0.0058, 0.0065,  ..., 0.0000, 0.0000, 0.0000]],\ngrad_fn=<CopySlices>), descriptors=tensor([[[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],",
  "[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n...],\ngrad_fn=<CopySlices>), mask=tensor([[1, 1, 1,  ..., 1, 1, 1],\n[1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32), hidden_states=None)\n```\n\nTo plot actual keypoints in the image, we need to postprocess the output. To do so, we have to pass the actual image sizes to `post_process_keypoint_detection` along with outputs.\n\n```python\nimage_sizes = [(image.size[1], image.size[0]) for image in images]\noutputs = processor.post_process_keypoint_detection(outputs, image_sizes)\n```\n\nThe outputs are now a list of dictionaries where each dictionary is a processed output of keypoints, scores and descriptors.\n\n```python\n[{'keypoints': tensor([[ 226,   57],\n[ 356,   57],\n[  89,   64],\n...,\n[3604, 3391]], dtype=torch.int32),\n'scores': tensor([0.0056, 0.0053, ...], grad_fn=<IndexBackward0>),\n'descriptors': tensor([[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357]],\ngrad_fn=<IndexBackward0>)},\n{'keypoints': tensor([[ 46,   6],\n[ 78,   6],\n[422,   6],\n[206, 404]], dtype=torch.int32),",
  "'scores': tensor([0.0206, 0.0058, 0.0065, 0.0053, 0.0070, ...,grad_fn=<IndexBackward0>),\n'descriptors': tensor([[-0.0525,  0.0726,  0.0270,  ...,  0.0389, -0.0189, -0.0211],\n[-0.0525,  0.0726,  0.0270,  ...,  0.0389, -0.0189, -0.0211]}]\n```\n\nWe can use these to plot the keypoints.\n\n```python\nimport matplotlib.pyplot as plt\nimport torch\n\nfor i in range(len(images)):\nkeypoints = outputs[i][\"keypoints\"]\nscores = outputs[i][\"scores\"]\ndescriptors = outputs[i][\"descriptors\"]\nkeypoints = outputs[i][\"keypoints\"].detach().numpy()\nscores = outputs[i][\"scores\"].detach().numpy()\nimage = images[i]\nimage_width, image_height = image.size\n\nplt.axis('off')\nplt.imshow(image)\nplt.scatter(\nkeypoints[:, 0],\nkeypoints[:, 1],\ns=scores * 100,\nc='cyan',\nalpha=0.4\n)\nplt.show()\n```\n\nBelow you can see the outputs.\n\n<div style=\"display: flex; align-items: center;\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee_keypoint.png\"\nalt=\"Bee\"\nstyle=\"height: 200px; object-fit: contain; margin-right: 10px;\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats_keypoint.png\"\nalt=\"Cats\"",
  "style=\"height: 200px; object-fit: contain;\">\n</div>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Question answering\n\n[[open-in-colab]]\n\n<Youtube id=\"ajPx5LwJD-I\"/>\n\nQuestion answering tasks return an answer given a question. If you've ever asked a virtual assistant like Alexa, Siri or Google what the weather is, then you've used a question answering model before. There are two common types of question answering tasks:\n\n- Extractive: extract the answer from the given context.",
  "- Abstractive: generate an answer from the context that correctly answers the question.\n\nThis guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on the [SQuAD](https://huggingface.co/datasets/squad) dataset for extractive question answering.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/question-answering)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load SQuAD dataset\n\nStart by loading a smaller subset of the SQuAD dataset from the 🤗 Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset",
  ">>> squad = load_dataset(\"squad\", split=\"train[:5000]\")\n```\n\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> squad = squad.train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:\n\n```py\n>>> squad[\"train\"][0]\n{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n'id': '5733be284776f41900661182',",
  "'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n'title': 'University_of_Notre_Dame'\n}\n```\n\nThere are several important fields here:\n\n- `answers`: the starting location of the answer token and the answer text.\n- `context`: background information from which the model needs to extract the answer.\n- `question`: the question a model should answer.\n\n## Preprocess\n\n<Youtube id=\"qgaM0weJHpA\"/>\n\nThe next step is to load a DistilBERT tokenizer to process the `question` and `context` fields:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\nThere are a few preprocessing steps particular to question answering tasks you should be aware of:\n\n1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n2. Next, map the start and end positions of the answer to the original `context` by setting\n`return_offset_mapping=True`.",
  "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the [`~tokenizers.Encoding.sequence_ids`] method to\nfind which part of the offset corresponds to the `question` and which corresponds to the `context`.\n\nHere is how you can create a function to truncate and map the start and end tokens of the `answer` to the `context`:\n\n```py\n>>> def preprocess_function(examples):\n...     questions = [q.strip() for q in examples[\"question\"]]\n...     inputs = tokenizer(\n...         questions,\n...         examples[\"context\"],\n...         max_length=384,\n...         truncation=\"only_second\",\n...         return_offsets_mapping=True,\n...         padding=\"max_length\",\n...     )\n\n...     offset_mapping = inputs.pop(\"offset_mapping\")\n...     answers = examples[\"answers\"]\n...     start_positions = []\n...     end_positions = []\n\n...     for i, offset in enumerate(offset_mapping):\n...         answer = answers[i]\n...         start_char = answer[\"answer_start\"][0]\n...         end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n...         sequence_ids = inputs.sequence_ids(i)\n\n...         # Find the start and end of the context\n...         idx = 0",
  "...         while sequence_ids[idx] != 1:\n...             idx += 1\n...         context_start = idx\n...         while sequence_ids[idx] == 1:\n...             idx += 1\n...         context_end = idx - 1\n\n...         # If the answer is not fully inside the context, label it (0, 0)\n...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n...             start_positions.append(0)\n...             end_positions.append(0)\n...         else:\n...             # Otherwise it's the start and end token positions\n...             idx = context_start\n...             while idx <= context_end and offset[idx][0] <= start_char:\n...                 idx += 1\n...             start_positions.append(idx - 1)\n\n...             idx = context_end\n...             while idx >= context_start and offset[idx][1] >= end_char:\n...                 idx -= 1\n...             end_positions.append(idx + 1)\n\n...     inputs[\"start_positions\"] = start_positions\n...     inputs[\"end_positions\"] = end_positions\n...     return inputs\n```",
  "To apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once. Remove any columns you don't need:\n\n```py\n>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n```\n\nNow create a batch of examples using [`DefaultDataCollator`]. Unlike other data collators in 🤗 Transformers, the [`DefaultDataCollator`] does not apply any additional preprocessing such as padding.\n\n<frameworkcontent>\n<pt>\n```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator()\n```\n</pt>\n<tf>\n```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load DistilBERT with [`AutoModelForQuestionAnswering`]:\n\n```py",
  ">>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, and data collator.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_qa_model\",\n...     eval_strategy=\"epoch\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=3,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_squad[\"train\"],\n...     eval_dataset=tokenized_squad[\"test\"],\n...     processing_class=tokenizer,",
  "...     data_collator=data_collator,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_epochs = 2\n>>> total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n>>> optimizer, schedule = create_optimizer(\n...     init_lr=2e-5,\n...     num_warmup_steps=0,\n...     num_train_steps=total_train_steps,\n... )\n```\n\nThen you can load DistilBERT with [`TFAutoModelForQuestionAnswering`]:\n\n```py\n>>> from transformers import TFAutoModelForQuestionAnswering\n\n>>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```",
  "Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_squad[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_validation_set = model.prepare_tf_dataset(\n...     tokenized_squad[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n\n```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)\n```\n\nThe last thing to setup before you start training is to provide a way to push your model to the Hub. This can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> callback = PushToHubCallback(\n...     output_dir=\"my_awesome_qa_model\",\n...     tokenizer=tokenizer,\n... )\n```",
  "Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callback to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])\n```\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for question answering, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n\n</Tip>\n\n## Evaluate\n\nEvaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The [`Trainer`] still calculates the evaluation loss during training so you're not completely in the dark about your model's performance.",
  "If you have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#post-processing) chapter from the 🤗 Hugging Face Course!\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with a question and some context you'd like the model to predict:\n\n```py\n>>> question = \"How many programming languages does BLOOM support?\"\n>>> context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for question answering with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> question_answerer = pipeline(\"question-answering\", model=\"my_awesome_qa_model\")\n>>> question_answerer(question=question, context=context)\n{'score': 0.2058267742395401,\n'start': 10,\n'end': 95,\n'answer': '176 billion parameters and can generate text in 46 languages natural languages and 13'}\n```",
  "You can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nTokenize the text and return PyTorch tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n>>> inputs = tokenizer(question, context, return_tensors=\"pt\")\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> import torch\n>>> from transformers import AutoModelForQuestionAnswering\n\n>>> model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n```\n\nGet the highest probability from the model output for the start and end positions:\n\n```py\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n```\n\nDecode the predicted tokens to get the answer:\n\n```py\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens)\n'176 billion parameters and can generate text in 46 languages natural languages and 13'\n```\n</pt>\n<tf>\nTokenize the text and return TensorFlow tensors:\n\n```py",
  ">>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n>>> inputs = tokenizer(question, context, return_tensors=\"tf\")\n```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import TFAutoModelForQuestionAnswering\n\n>>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n>>> outputs = model(**inputs)\n```\n\nGet the highest probability from the model output for the start and end positions:\n\n```py\n>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n```\n\nDecode the predicted tokens to get the answer:\n\n```py\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens)\n'176 billion parameters and can generate text in 46 languages natural languages and 13'\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Masked language modeling\n\n[[open-in-colab]]\n\n<Youtube id=\"mqElG5QJWUg\"/>\n\nMasked language modeling predicts a masked token in a sequence, and the model can attend to tokens bidirectionally. This\nmeans the model has full access to the tokens on the left and right. Masked language modeling is great for tasks that\nrequire a good contextual understanding of an entire sequence. BERT is an example of a masked language model.",
  "This guide will show you how to:\n\n1. Finetune [DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base) on the [r/askscience](https://www.reddit.com/r/askscience/) subset of the [ELI5](https://huggingface.co/datasets/eli5) dataset.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/fill-mask)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate\n```\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load ELI5 dataset\n\nStart by loading the first 5000 examples from the [ELI5-Category](https://huggingface.co/datasets/eli5_category) dataset with the 🤗 Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset",
  ">>> eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n```\n\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> eli5 = eli5.train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:\n\n```py\n>>> eli5[\"train\"][0]\n{'q_id': '7h191n',\n'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n'selftext': '',\n'category': 'Economics',\n'subreddit': 'explainlikeimfive',\n'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],",
  "'text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n'score': [21, 19, 5, 3],\n'text_urls': [[],\n[],\n[],\n['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},\n'title_urls': ['url'],",
  "'selftext_urls': ['url']}\n```\n\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool about language modeling tasks is you don't need labels (also known as an unsupervised task) because the next word *is* the label.\n\n## Preprocess\n\n<Youtube id=\"8PmhEIXhBvI\"/>\n\nFor masked language modeling, the next step is to load a DistilRoBERTa tokenizer to process the `text` subfield:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n```\n\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. This means you'll need to extract the `text` subfield from its nested structure with the [`flatten`](https://huggingface.co/docs/datasets/process#flatten) method:\n\n```py\n>>> eli5 = eli5.flatten()\n>>> eli5[\"train\"][0]\n{'q_id': '7h191n',\n'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n'selftext': '',\n'category': 'Economics',\n'subreddit': 'explainlikeimfive',\n'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],",
  "'answers.text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n'answers.score': [21, 19, 5, 3],\n'answers.text_urls': [[],\n[],\n[],\n['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],",
  "'title_urls': ['url'],\n'selftext_urls': ['url']}\n```\n\nEach subfield is now a separate column as indicated by the `answers` prefix, and the `text` field is a list now. Instead\nof tokenizing each sentence separately, convert the list to a string so you can jointly tokenize them.\n\nHere is a first preprocessing function to join the list of strings for each example and tokenize the result:\n\n```py\n>>> def preprocess_function(examples):\n...     return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n```\n\nTo apply this preprocessing function over the entire dataset, use the 🤗 Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once, and increasing the number of processes with `num_proc`. Remove any columns you don't need:\n\n```py\n>>> tokenized_eli5 = eli5.map(\n...     preprocess_function,\n...     batched=True,\n...     num_proc=4,\n...     remove_columns=eli5[\"train\"].column_names,\n... )\n```\n\nThis dataset contains the token sequences, but some of these are longer than the maximum input length for the model.\n\nYou can now use a second preprocessing function to",
  "- concatenate all the sequences\n- split the concatenated sequences into shorter chunks defined by `block_size`, which should be both shorter than the maximum input length and short enough for your GPU RAM.\n\n```py\n>>> block_size = 128\n\n\n>>> def group_texts(examples):\n...     # Concatenate all texts.\n...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n...     total_length = len(concatenated_examples[list(examples.keys())[0]])\n...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n...     # customize this part to your needs.\n...     if total_length >= block_size:\n...         total_length = (total_length // block_size) * block_size\n...     # Split by chunks of block_size.\n...     result = {\n...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n...         for k, t in concatenated_examples.items()\n...     }\n...     return result\n```\n\nApply the `group_texts` function over the entire dataset:\n\n```py\n>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)\n```",
  "Now create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\n<frameworkcontent>\n<pt>\n\nUse the end-of-sequence token as the padding token and specify `mlm_probability` to randomly mask tokens each time you iterate over the data:\n\n```py\n>>> from transformers import DataCollatorForLanguageModeling\n\n>>> tokenizer.pad_token = tokenizer.eos_token\n>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n```\n</pt>\n<tf>\n\nUse the end-of-sequence token as the padding token and specify `mlm_probability` to randomly mask tokens each time you iterate over the data:\n\n```py\n>>> from transformers import DataCollatorForLanguageModeling\n\n>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n```\n</tf>\n</frameworkcontent>\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>",
  "You're ready to start training your model now! Load DistilRoBERTa with [`AutoModelForMaskedLM`]:\n\n```py\n>>> from transformers import AutoModelForMaskedLM\n\n>>> model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n2. Pass the training arguments to [`Trainer`] along with the model, datasets, and data collator.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_eli5_mlm_model\",\n...     eval_strategy=\"epoch\",\n...     learning_rate=2e-5,\n...     num_train_epochs=3,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=lm_dataset[\"train\"],\n...     eval_dataset=lm_dataset[\"test\"],\n...     data_collator=data_collator,\n...     tokenizer=tokenizer,\n... )",
  ">>> trainer.train()\n```\n\nOnce training is completed, use the [`~transformers.Trainer.evaluate`] method to evaluate your model and get its perplexity:\n\n```py\n>>> import math\n\n>>> eval_results = trainer.evaluate()\n>>> print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\nPerplexity: 8.76\n```\n\nThen share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer, AdamWeightDecay\n\n>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n```\n\nThen you can load DistilRoBERTa with [`TFAutoModelForMaskedLM`]:\n\n```py\n>>> from transformers import TFAutoModelForMaskedLM\n\n>>> model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n```",
  "Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     lm_dataset[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_test_set = model.prepare_tf_dataset(\n...     lm_dataset[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```\n\nThis can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> callback = PushToHubCallback(\n...     output_dir=\"my_awesome_eli5_mlm_model\",\n...     tokenizer=tokenizer,\n... )\n```",
  "Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callback to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for masked language modeling, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with some text you'd like the model to fill in the blank with, and use the special `<mask>` token to indicate the blank:\n\n```py\n>>> text = \"The Milky Way is a <mask> galaxy.\"\n```",
  "The simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for fill-mask with your model, and pass your text to it. If you like, you can use the `top_k` parameter to specify how many predictions to return:\n\n```py\n>>> from transformers import pipeline\n\n>>> mask_filler = pipeline(\"fill-mask\", \"username/my_awesome_eli5_mlm_model\")\n>>> mask_filler(text, top_k=3)\n[{'score': 0.5150994658470154,\n'token': 21300,\n'token_str': ' spiral',\n'sequence': 'The Milky Way is a spiral galaxy.'},\n{'score': 0.07087188959121704,\n'token': 2232,\n'token_str': ' massive',\n'sequence': 'The Milky Way is a massive galaxy.'},\n{'score': 0.06434620916843414,\n'token': 650,\n'token_str': ' small',\n'sequence': 'The Milky Way is a small galaxy.'}]\n```\n\n<frameworkcontent>\n<pt>\nTokenize the text and return the `input_ids` as PyTorch tensors. You'll also need to specify the position of the `<mask>` token:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n>>> inputs = tokenizer(text, return_tensors=\"pt\")",
  ">>> mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n```\n\nPass your inputs to the model and return the `logits` of the masked token:\n\n```py\n>>> from transformers import AutoModelForMaskedLM\n\n>>> model = AutoModelForMaskedLM.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n>>> logits = model(**inputs).logits\n>>> mask_token_logits = logits[0, mask_token_index, :]\n```\n\nThen return the three masked tokens with the highest probability and print them out:\n\n```py\n>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n\n>>> for token in top_3_tokens:\n...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\nThe Milky Way is a spiral galaxy.\nThe Milky Way is a massive galaxy.\nThe Milky Way is a small galaxy.\n```\n</pt>\n<tf>\nTokenize the text and return the `input_ids` as TensorFlow tensors. You'll also need to specify the position of the `<mask>` token:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n>>> inputs = tokenizer(text, return_tensors=\"tf\")",
  ">>> mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n```\n\nPass your inputs to the model and return the `logits` of the masked token:\n\n```py\n>>> from transformers import TFAutoModelForMaskedLM\n\n>>> model = TFAutoModelForMaskedLM.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n>>> logits = model(**inputs).logits\n>>> mask_token_logits = logits[0, mask_token_index, :]\n```\n\nThen return the three masked tokens with the highest probability and print them out:\n\n```py\n>>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()\n\n>>> for token in top_3_tokens:\n...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\nThe Milky Way is a spiral galaxy.\nThe Milky Way is a massive galaxy.\nThe Milky Way is a small galaxy.\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image-to-Image Task Guide\n\n[[open-in-colab]]\n\nImage-to-Image task is the task where an application receives an image and outputs another image. This has various subtasks, including image enhancement (super resolution, low light enhancement, deraining and so on), image inpainting, and more.\n\nThis guide will show you how to:\n- Use an image-to-image pipeline for super resolution task,",
  "- Run image-to-image models for same task without a pipeline.\n\nNote that as of the time this guide is released, `image-to-image` pipeline only supports super resolution task.\n\nLet's begin by installing the necessary libraries.\n\n```bash\npip install transformers\n```\n\nWe can now initialize the pipeline with a [Swin2SR model](https://huggingface.co/caidas/swin2SR-lightweight-x2-64). We can then infer with the pipeline by calling it with an image. As of now, only [Swin2SR models](https://huggingface.co/models?sort=trending&search=swin2sr) are supported in this pipeline.\n\n```python\nfrom transformers import pipeline\nimport torch\nfrom accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\ndevice, _, _ = get_backend()\npipe = pipeline(task=\"image-to-image\", model=\"caidas/swin2SR-lightweight-x2-64\", device=device)\n```\n\nNow, let's load an image.\n\n```python\nfrom PIL import Image\nimport requests\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprint(image.size)\n```\n```bash\n# (532, 432)\n```",
  "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg\" alt=\"Photo of a cat\"/>\n</div>\n\nWe can now do inference with the pipeline. We will get an upscaled version of the cat image.\n\n```python\nupscaled = pipe(image)\nprint(upscaled.size)\n```\n```bash\n# (1072, 880)\n```\n\nIf you wish to do inference yourself with no pipeline, you can use the `Swin2SRForImageSuperResolution` and `Swin2SRImageProcessor` classes of transformers. We will use the same model checkpoint for this. Let's initialize the model and the processor.\n\n```python\nfrom transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor\n\nmodel = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-lightweight-x2-64\").to(device)\nprocessor = Swin2SRImageProcessor(\"caidas/swin2SR-lightweight-x2-64\")\n```\n\n`pipeline` abstracts away the preprocessing and postprocessing steps that we have to do ourselves, so let's preprocess the image. We will pass the image to the processor and then move the pixel values to GPU.\n\n```python\npixel_values = processor(image, return_tensors=\"pt\").pixel_values\nprint(pixel_values.shape)",
  "pixel_values = pixel_values.to(device)\n```\n\nWe can now infer the image by passing pixel values to the model.\n\n```python\nimport torch\n\nwith torch.no_grad():\noutputs = model(pixel_values)\n```\nOutput is an object of type `ImageSuperResolutionOutput` that looks like below 👇\n\n```\n(loss=None, reconstruction=tensor([[[[0.8270, 0.8269, 0.8275,  ..., 0.7463, 0.7446, 0.7453],\n[0.8287, 0.8278, 0.8283,  ..., 0.7451, 0.7448, 0.7457],\n[0.8280, 0.8273, 0.8269,  ..., 0.7447, 0.7446, 0.7452],\n...,\n[0.5923, 0.5933, 0.5924,  ..., 0.0697, 0.0695, 0.0706],\n[0.5926, 0.5932, 0.5926,  ..., 0.0673, 0.0687, 0.0705],\n[0.5927, 0.5914, 0.5922,  ..., 0.0664, 0.0694, 0.0718]]]],\ndevice='cuda:0'), hidden_states=None, attentions=None)\n```\nWe need to get the `reconstruction` and post-process it for visualization. Let's see how it looks like.\n\n```python\noutputs.reconstruction.data.shape\n# torch.Size([1, 3, 880, 1072])\n```\n\nWe need to squeeze the output and get rid of axis 0, clip the values, then convert it to be numpy float. Then we will arrange axes to have the shape [1072, 880], and finally, bring the output back to range [0, 255].\n\n```python\nimport numpy as np\n\n# squeeze, take to CPU and clip the values",
  "output = outputs.reconstruction.data.squeeze().cpu().clamp_(0, 1).numpy()\n# rearrange the axes\noutput = np.moveaxis(output, source=0, destination=-1)\n# bring values back to pixel values range\noutput = (output * 255.0).round().astype(np.uint8)\nImage.fromarray(output)\n```\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat_upscaled.png\" alt=\"Upscaled photo of a cat\"/>\n</div>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Video classification\n\n[[open-in-colab]]",
  "Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to. These models can be used to categorize what a video is all about. A real-world application of video classification is action / activity recognition, which is useful for fitness applications. It is also helpful for vision-impaired individuals, especially when they are commuting.\n\nThis guide will show you how to:\n\n1. Fine-tune [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) on a subset of the [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) dataset.\n2. Use your fine-tuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/video-classification).\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install -q pytorchvideo transformers evaluate\n```",
  "You will use [PyTorchVideo](https://pytorchvideo.org/) (dubbed `pytorchvideo`) to process and prepare the videos.\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load UCF101 dataset\n\nStart by loading a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). This will give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from huggingface_hub import hf_hub_download\n\n>>> hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n>>> filename = \"UCF101_subset.tar.gz\"\n>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")\n```\n\nAfter the subset has been downloaded, you need to extract the compressed archive:\n\n```py\n>>> import tarfile\n\n>>> with tarfile.open(file_path) as t:\n...      t.extractall(\".\")\n```\n\nAt a high level, the dataset is organized like so:\n\n```bash\nUCF101_subset/\ntrain/\nBandMarching/\nvideo_1.mp4\nvideo_2.mp4\n...\nArchery\nvideo_1.mp4",
  "video_2.mp4\n...\n...\nval/\nBandMarching/\nvideo_1.mp4\nvideo_2.mp4\n...\nArchery\nvideo_1.mp4\nvideo_2.mp4\n...\n...\ntest/\nBandMarching/\nvideo_1.mp4\nvideo_2.mp4\n...\nArchery\nvideo_1.mp4\nvideo_2.mp4\n...\n...\n```\n\nYou can then count the number of total videos.\n\n```py\n>>> import pathlib\n>>> dataset_root_path = \"UCF101_subset\"\n>>> dataset_root_path = pathlib.Path(dataset_root_path)\n```\n\n```py\n>>> video_count_train = len(list(dataset_root_path.glob(\"train/*/*.avi\")))\n>>> video_count_val = len(list(dataset_root_path.glob(\"val/*/*.avi\")))\n>>> video_count_test = len(list(dataset_root_path.glob(\"test/*/*.avi\")))\n>>> video_total = video_count_train + video_count_val + video_count_test\n>>> print(f\"Total videos: {video_total}\")\n```\n\n```py\n>>> all_video_file_paths = (\n...     list(dataset_root_path.glob(\"train/*/*.avi\"))\n...     + list(dataset_root_path.glob(\"val/*/*.avi\"))\n...     + list(dataset_root_path.glob(\"test/*/*.avi\"))\n...  )\n>>> all_video_file_paths[:5]\n```\n\nThe (`sorted`) video paths appear like so:\n\n```bash\n...\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',",
  "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n...\n```\n\nYou will notice that there are video clips belonging to the same group / scene where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi` and `v_ApplyEyeMakeup_g07_c06.avi`, for example.\n\nFor the validation and evaluation splits, you wouldn't want to have video clips from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage). The subset that you are using in this tutorial takes this information into account.\n\nNext up, you will derive the set of labels present in the dataset. Also, create two dictionaries that'll be helpful when initializing the model:\n\n* `label2id`: maps the class names to integers.\n* `id2label`: maps the integers to class names.\n\n```py\n>>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n>>> label2id = {label: i for i, label in enumerate(class_labels)}\n>>> id2label = {i: label for label, i in label2id.items()}",
  ">>> print(f\"Unique classes: {list(label2id.keys())}.\")\n\n# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n```\n\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\n\n## Load a model to fine-tune\n\nInstantiate a video classification model from a pretrained checkpoint and its associated image processor. The model's encoder comes with pre-trained parameters, and the classification head is randomly initialized. The image processor will come in handy when writing the preprocessing pipeline for our dataset.\n\n```py\n>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n\n>>> model_ckpt = \"MCG-NJU/videomae-base\"\n>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n>>> model = VideoMAEForVideoClassification.from_pretrained(\n...     model_ckpt,\n...     label2id=label2id,\n...     id2label=id2label,\n...     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n... )\n```",
  "While the model is loading, you might notice the following warning:\n\n```bash\nSome weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\n- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```",
  "The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some others (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\n\n**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`.\n\n## Prepare the datasets for training\n\nFor preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/). Start by importing the dependencies we need.\n\n```py\n>>> import pytorchvideo.data\n\n>>> from pytorchvideo.transforms import (\n...     ApplyTransformToKey,",
  "...     Normalize,\n...     RandomShortSideScale,\n...     RemoveKey,\n...     ShortSideScale,\n...     UniformTemporalSubsample,\n... )\n\n>>> from torchvision.transforms import (\n...     Compose,\n...     Lambda,\n...     RandomCrop,\n...     RandomHorizontalFlip,\n...     Resize,\n... )\n```\n\nFor the training dataset transformations, use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, keep the same transformation chain except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorchVideo](https://pytorchvideo.org).\n\nUse the `image_processor` associated with the pre-trained model to obtain the following information:\n\n* Image mean and standard deviation with which the video frame pixels will be normalized.\n* Spatial resolution to which the video frames will be resized.\n\nStart by defining some constants.\n\n```py\n>>> mean = image_processor.image_mean\n>>> std = image_processor.image_std\n>>> if \"shortest_edge\" in image_processor.size:",
  "...     height = width = image_processor.size[\"shortest_edge\"]\n>>> else:\n...     height = image_processor.size[\"height\"]\n...     width = image_processor.size[\"width\"]\n>>> resize_to = (height, width)\n\n>>> num_frames_to_sample = model.config.num_frames\n>>> sample_rate = 4\n>>> fps = 30\n>>> clip_duration = num_frames_to_sample * sample_rate / fps\n```\n\nNow, define the dataset-specific transformations and the datasets respectively. Starting with the training set:\n\n```py\n>>> train_transform = Compose(\n...     [\n...         ApplyTransformToKey(\n...             key=\"video\",\n...             transform=Compose(\n...                 [\n...                     UniformTemporalSubsample(num_frames_to_sample),\n...                     Lambda(lambda x: x / 255.0),\n...                     Normalize(mean, std),\n...                     RandomShortSideScale(min_size=256, max_size=320),\n...                     RandomCrop(resize_to),\n...                     RandomHorizontalFlip(p=0.5),\n...                 ]\n...             ),\n...         ),\n...     ]\n... )\n\n>>> train_dataset = pytorchvideo.data.Ucf101(\n...     data_path=os.path.join(dataset_root_path, \"train\"),",
  "...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n...     decode_audio=False,\n...     transform=train_transform,\n... )\n```\n\nThe same sequence of workflow can be applied to the validation and evaluation sets:\n\n```py\n>>> val_transform = Compose(\n...     [\n...         ApplyTransformToKey(\n...             key=\"video\",\n...             transform=Compose(\n...                 [\n...                     UniformTemporalSubsample(num_frames_to_sample),\n...                     Lambda(lambda x: x / 255.0),\n...                     Normalize(mean, std),\n...                     Resize(resize_to),\n...                 ]\n...             ),\n...         ),\n...     ]\n... )\n\n>>> val_dataset = pytorchvideo.data.Ucf101(\n...     data_path=os.path.join(dataset_root_path, \"val\"),\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n...     decode_audio=False,\n...     transform=val_transform,\n... )\n\n>>> test_dataset = pytorchvideo.data.Ucf101(\n...     data_path=os.path.join(dataset_root_path, \"test\"),\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n...     decode_audio=False,\n...     transform=val_transform,",
  "... )\n```\n\n**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorchVideo dataset. So, if you want to use a custom dataset not supported off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine.\n\nYou can access the `num_videos` argument to know the number of videos in the dataset.\n\n```py",
  ">>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\n# (300, 30, 75)\n```\n\n## Visualize the preprocessed video for better debugging\n\n```py\n>>> import imageio\n>>> import numpy as np\n>>> from IPython.display import Image\n\n>>> def unnormalize_img(img):\n...     \"\"\"Un-normalizes the image pixels.\"\"\"\n...     img = (img * std) + mean\n...     img = (img * 255).astype(\"uint8\")\n...     return img.clip(0, 255)\n\n>>> def create_gif(video_tensor, filename=\"sample.gif\"):\n...     \"\"\"Prepares a GIF from a video tensor.\n...\n...     The video tensor is expected to have the following shape:\n...     (num_frames, num_channels, height, width).\n...     \"\"\"\n...     frames = []\n...     for video_frame in video_tensor:\n...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n...         frames.append(frame_unnormalized)\n...     kargs = {\"duration\": 0.25}\n...     imageio.mimsave(filename, frames, \"GIF\", **kargs)\n...     return filename\n\n>>> def display_gif(video_tensor, gif_name=\"sample.gif\"):\n...     \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n...     video_tensor = video_tensor.permute(1, 0, 2, 3)",
  "...     gif_filename = create_gif(video_tensor, gif_name)\n...     return Image(filename=gif_filename)\n\n>>> sample_video = next(iter(train_dataset))\n>>> video_tensor = sample_video[\"video\"]\n>>> display_gif(video_tensor)\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n</div>\n\n## Train the model\n\nLeverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  🤗 Transformers for training the model. To instantiate a `Trainer`, you need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on 🤗 Hub.",
  "Most of the training arguments are self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in this case, you need the unused features ('video' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs).\n\n\n```py\n>>> from transformers import TrainingArguments, Trainer\n\n>>> model_name = model_ckpt.split(\"/\")[-1]\n>>> new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n>>> num_epochs = 4\n\n>>> args = TrainingArguments(\n...     new_model_name,\n...     remove_unused_columns=False,\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=batch_size,\n...     per_device_eval_batch_size=batch_size,\n...     warmup_ratio=0.1,\n...     logging_steps=10,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"accuracy\",\n...     push_to_hub=True,",
  "...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n... )\n```\n\nThe dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As such, we must define `max_steps` when instantiating `TrainingArguments`.\n\nNext, you need to define a function to compute the metrics from the predictions, which will use the `metric` you'll load now. The only preprocessing you have to do is to take the argmax of our predicted logits:\n\n```py\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\npredictions = np.argmax(eval_pred.predictions, axis=1)\nreturn metric.compute(predictions=predictions, references=eval_pred.label_ids)\n```\n\n**A note on evaluation**:\n\nIn the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial.",
  "Also, define a `collate_fn`, which will be used to batch examples together. Each batch consists of 2 keys, namely `pixel_values` and `labels`.\n\n```py\n>>> def collate_fn(examples):\n...     # permute to (num_frames, num_channels, height, width)\n...     pixel_values = torch.stack(\n...         [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n...     )\n...     labels = torch.tensor([example[\"label\"] for example in examples])\n...     return {\"pixel_values\": pixel_values, \"labels\": labels}\n```\n\nThen you just pass all of this along with the datasets to `Trainer`:\n\n```py\n>>> trainer = Trainer(\n...     model,\n...     args,\n...     train_dataset=train_dataset,\n...     eval_dataset=val_dataset,\n...     processing_class=image_processor,\n...     compute_metrics=compute_metrics,\n...     data_collator=collate_fn,\n... )\n```\n\nYou might wonder why you passed along the `image_processor` as a tokenizer when you preprocessed the data already. This is only to make sure the image processor configuration file (stored as JSON) will also be uploaded to the repo on the Hub.\n\nNow fine-tune our model by calling the `train` method:\n\n```py\n>>> train_results = trainer.train()\n```",
  "Once training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n\n## Inference\n\nGreat, now that you have fine-tuned a model, you can use it for inference!\n\nLoad a video for inference:\n\n```py\n>>> sample_test_video = next(iter(test_dataset))\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif\" alt=\"Teams playing basketball\"/>\n</div>\n\nThe simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). Instantiate a `pipeline` for video classification with your model, and pass your video to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n>>> video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")\n[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},",
  "{'score': 0.017777055501937866, 'label': 'BabyCrawling'},\n{'score': 0.01663011871278286, 'label': 'BalanceBeam'},\n{'score': 0.009560945443809032, 'label': 'BandMarching'},\n{'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like.\n\n\n```py\n>>> def run_inference(model, video):\n...     # (num_frames, num_channels, height, width)\n...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n...     inputs = {\n...         \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n...         \"labels\": torch.tensor(\n...             [sample_test_video[\"label\"]]\n...         ),  # this can be skipped if you don't have labels available.\n...     }\n\n...     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n...     inputs = {k: v.to(device) for k, v in inputs.items()}\n...     model = model.to(device)\n\n...     # forward pass\n...     with torch.no_grad():\n...         outputs = model(**inputs)\n...         logits = outputs.logits\n\n...     return logits\n```\n\nNow, pass your input to the model and return the `logits`:\n\n```py\n>>> logits = run_inference(trained_model, sample_test_video[\"video\"])\n```",
  "Decoding the `logits`, we get:\n\n```py\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n# Predicted class: BasketballDunk\n```",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Text to speech\n\n[[open-in-colab]]\n\nText-to-speech (TTS) is the task of creating natural-sounding speech from text, where the speech can be generated in multiple\nlanguages and for multiple speakers. Several text-to-speech models are currently available in 🤗 Transformers, such as\n[Bark](../model_doc/bark), [MMS](../model_doc/mms), [VITS](../model_doc/vits) and [SpeechT5](../model_doc/speecht5).",
  "You can easily generate audio using the `\"text-to-audio\"` pipeline (or its alias - `\"text-to-speech\"`). Some models, like Bark,\ncan also be conditioned to generate non-verbal communications such as laughing, sighing and crying, or even add music.\nHere's an example of how you would use the `\"text-to-speech\"` pipeline with Bark:\n\n```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\")\n>>> text = \"[clears throat] This is a test ... and I just took a long pause.\"\n>>> output = pipe(text)\n```\n\nHere's a code snippet you can use to listen to the resulting audio in a notebook:\n\n```python\n>>> from IPython.display import Audio\n>>> Audio(output[\"audio\"], rate=output[\"sampling_rate\"])\n```\n\nFor more examples on what Bark and other pretrained TTS models can do, refer to our\n[Audio course](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models).\n\nIf you are looking to fine-tune a TTS model, the only text-to-speech models currently available in 🤗 Transformers",
  "are [SpeechT5](model_doc/speecht5) and [FastSpeech2Conformer](model_doc/fastspeech2_conformer), though more will be added in the future. SpeechT5 is pre-trained on a combination of speech-to-text and text-to-speech data, allowing it to learn a unified space of hidden representations shared by both text and speech. This means that the same pre-trained model can be fine-tuned for different tasks. Furthermore, SpeechT5 supports multiple speakers through x-vector speaker embeddings.\n\nThe remainder of this guide illustrates how to:\n\n1. Fine-tune [SpeechT5](../model_doc/speecht5) that was originally trained on English speech on the Dutch (`nl`) language subset of the [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) dataset.\n2. Use your refined model for inference in one of two ways: using a pipeline or directly.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install datasets soundfile speechbrain accelerate\n```\n\nInstall 🤗Transformers from source as not all the SpeechT5 features have been merged into an official release yet:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n<Tip>",
  "To follow this guide you will need a GPU. If you're working in a notebook, run the following line to check if a GPU is available:\n\n```bash\n!nvidia-smi\n```\n\nor alternatively for AMD GPUs:\n\n```bash\n!rocm-smi\n```\n\n</Tip>\n\nWe encourage you to log in to your Hugging Face account to upload and share your model with the community. When prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load the dataset\n\n[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) is a large-scale multilingual speech corpus consisting of\ndata sourced from 2009-2020 European Parliament event recordings. It contains labelled audio-transcription data for 15\nEuropean languages. In this guide, we are using the Dutch language subset, feel free to pick another subset.\n\nNote that VoxPopuli or any other automated speech recognition (ASR) dataset may not be the most suitable\noption for training TTS models. The features that make it beneficial for ASR, such as excessive background noise, are\ntypically undesirable in TTS. However, finding top-quality, multilingual, and multi-speaker TTS datasets can be quite\nchallenging.\n\nLet's load the data:",
  "```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\")\n>>> len(dataset)\n20968\n```\n\n20968 examples should be sufficient for fine-tuning. SpeechT5 expects audio data to have a sampling rate of 16 kHz, so\nmake sure the examples in the dataset meet this requirement:\n\n```py\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n```\n\n## Preprocess the data\n\nLet's begin by defining the model checkpoint to use and loading the appropriate processor:\n\n```py\n>>> from transformers import SpeechT5Processor\n\n>>> checkpoint = \"microsoft/speecht5_tts\"\n>>> processor = SpeechT5Processor.from_pretrained(checkpoint)\n```\n\n### Text cleanup for SpeechT5 tokenization\n\nStart by cleaning up the text data. You'll need the tokenizer part of the processor to process the text:\n\n```py\n>>> tokenizer = processor.tokenizer\n```\n\nThe dataset examples contain `raw_text` and `normalized_text` features. When deciding which feature to use as the text input,\nconsider that the SpeechT5 tokenizer doesn't have any tokens for numbers. In `normalized_text` the numbers are written",
  "out as text. Thus, it is a better fit, and we recommend using    `normalized_text` as input text.\n\nBecause SpeechT5 was trained on the English language, it may not recognize certain characters in the Dutch dataset. If\nleft as is, these characters will be converted to `<unk>` tokens. However, in Dutch, certain characters like `à` are\nused to stress syllables. In order to preserve the meaning of the text, we can replace this character with a regular `a`.\n\nTo identify unsupported tokens, extract all unique characters in the dataset using the `SpeechT5Tokenizer` which\nworks with characters as tokens. To do this, write the `extract_all_chars` mapping function that concatenates\nthe transcriptions from all examples into one string and converts it to a set of characters.\nMake sure to set `batched=True` and `batch_size=-1` in `dataset.map()` so that all transcriptions are available at once for\nthe mapping function.\n\n```py\n>>> def extract_all_chars(batch):\n...     all_text = \" \".join(batch[\"normalized_text\"])\n...     vocab = list(set(all_text))\n...     return {\"vocab\": [vocab], \"all_text\": [all_text]}\n\n\n>>> vocabs = dataset.map(\n...     extract_all_chars,\n...     batched=True,",
  "...     batch_size=-1,\n...     keep_in_memory=True,\n...     remove_columns=dataset.column_names,\n... )\n\n>>> dataset_vocab = set(vocabs[\"vocab\"][0])\n>>> tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}\n```\n\nNow you have two sets of characters: one with the vocabulary from the dataset and one with the vocabulary from the tokenizer.\nTo identify any unsupported characters in the dataset, you can take the difference between these two sets. The resulting\nset will contain the characters that are in the dataset but not in the tokenizer.\n\n```py\n>>> dataset_vocab - tokenizer_vocab\n{' ', 'à', 'ç', 'è', 'ë', 'í', 'ï', 'ö', 'ü'}\n```\n\nTo handle the unsupported characters identified in the previous step, define a function that maps these characters to\nvalid tokens. Note that spaces are already replaced by `▁` in the tokenizer and don't need to be handled separately.\n\n```py\n>>> replacements = [\n...     (\"à\", \"a\"),\n...     (\"ç\", \"c\"),\n...     (\"è\", \"e\"),\n...     (\"ë\", \"e\"),\n...     (\"í\", \"i\"),\n...     (\"ï\", \"i\"),\n...     (\"ö\", \"o\"),\n...     (\"ü\", \"u\"),\n... ]\n\n\n>>> def cleanup_text(inputs):\n...     for src, dst in replacements:",
  "...         inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n...     return inputs\n\n\n>>> dataset = dataset.map(cleanup_text)\n```\n\nNow that you have dealt with special characters in the text, it's time to shift focus to the audio data.\n\n### Speakers\n\nThe VoxPopuli dataset includes speech from multiple speakers, but how many speakers are represented in the dataset? To\ndetermine this, we can count the number of unique speakers and the number of examples each speaker contributes to the dataset.\nWith a total of 20,968 examples in the dataset, this information will give us a better understanding of the distribution of\nspeakers and examples in the data.\n\n```py\n>>> from collections import defaultdict\n\n>>> speaker_counts = defaultdict(int)\n\n>>> for speaker_id in dataset[\"speaker_id\"]:\n...     speaker_counts[speaker_id] += 1\n```\n\nBy plotting a histogram you can get a sense of how much data there is for each speaker.\n\n```py\n>>> import matplotlib.pyplot as plt\n\n>>> plt.figure()\n>>> plt.hist(speaker_counts.values(), bins=20)\n>>> plt.ylabel(\"Speakers\")\n>>> plt.xlabel(\"Examples\")\n>>> plt.show()\n```\n\n<div class=\"flex justify-center\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_speakers_histogram.png\" alt=\"Speakers histogram\"/>\n</div>\n\nThe histogram reveals that approximately one-third of the speakers in the dataset have fewer than 100 examples, while\naround ten speakers have more than 500 examples. To improve training efficiency and balance the dataset, we can limit\nthe data to speakers with between 100 and 400 examples.\n\n```py\n>>> def select_speaker(speaker_id):\n...     return 100 <= speaker_counts[speaker_id] <= 400\n\n\n>>> dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])\n```\n\nLet's check how many speakers remain:\n\n```py\n>>> len(set(dataset[\"speaker_id\"]))\n42\n```\n\nLet's see how many examples are left:\n\n```py\n>>> len(dataset)\n9973\n```\n\nYou are left with just under 10,000 examples from approximately 40 unique speakers, which should be sufficient.\n\nNote that some speakers with few examples may actually have more audio available if the examples are long. However,\ndetermining the total amount of audio for each speaker requires scanning through the entire dataset, which is a",
  "time-consuming process that involves loading and decoding each audio file. As such, we have chosen to skip this step here.\n\n### Speaker embeddings\n\nTo enable the TTS model to differentiate between multiple speakers, you'll need to create a speaker embedding for each example.\nThe speaker embedding is an additional input into the model that captures a particular speaker's voice characteristics.\nTo generate these speaker embeddings, use the pre-trained [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)\nmodel from SpeechBrain.\n\nCreate a function `create_speaker_embedding()` that takes an input audio waveform and outputs a 512-element vector\ncontaining the corresponding speaker embedding.\n\n```py\n>>> import os\n>>> import torch\n>>> from speechbrain.inference.classifiers import EncoderClassifier\n>>> from accelerate.test_utils.testing import get_backend\n\n>>> spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n>>> speaker_model = EncoderClassifier.from_hparams(\n...     source=spk_model_name,\n...     run_opts={\"device\": device},",
  "...     savedir=os.path.join(\"/tmp\", spk_model_name),\n... )\n\n\n>>> def create_speaker_embedding(waveform):\n...     with torch.no_grad():\n...         speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n...         speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n...         speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n...     return speaker_embeddings\n```\n\nIt's important to note that the `speechbrain/spkrec-xvect-voxceleb` model was trained on English speech from the VoxCeleb\ndataset, whereas the training examples in this guide are in Dutch. While we believe that this model will still generate\nreasonable speaker embeddings for our Dutch dataset, this assumption may not hold true in all cases.\n\nFor optimal results, we recommend training an X-vector model on the target speech first. This will ensure that the model\nis better able to capture the unique voice characteristics present in the Dutch language.\n\n### Processing the dataset\n\nFinally, let's process the data into the format the model expects. Create a `prepare_dataset` function that takes in a",
  "single example and uses the `SpeechT5Processor` object to tokenize the input text and load the target audio into a log-mel spectrogram.\nIt should also add the speaker embeddings as an additional input.\n\n```py\n>>> def prepare_dataset(example):\n...     audio = example[\"audio\"]\n\n...     example = processor(\n...         text=example[\"normalized_text\"],\n...         audio_target=audio[\"array\"],\n...         sampling_rate=audio[\"sampling_rate\"],\n...         return_attention_mask=False,\n...     )\n\n...     # strip off the batch dimension\n...     example[\"labels\"] = example[\"labels\"][0]\n\n...     # use SpeechBrain to obtain x-vector\n...     example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n\n...     return example\n```\n\nVerify the processing is correct by looking at a single example:\n\n```py\n>>> processed_example = prepare_dataset(dataset[0])\n>>> list(processed_example.keys())\n['input_ids', 'labels', 'stop_labels', 'speaker_embeddings']\n```\n\nSpeaker embeddings should be a 512-element vector:\n\n```py\n>>> processed_example[\"speaker_embeddings\"].shape\n(512,)\n```\n\nThe labels should be a log-mel spectrogram with 80 mel bins.\n\n```py\n>>> import matplotlib.pyplot as plt",
  ">>> plt.figure()\n>>> plt.imshow(processed_example[\"labels\"].T)\n>>> plt.show()\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_logmelspectrogram_1.png\" alt=\"Log-mel spectrogram with 80 mel bins\"/>\n</div>\n\nSide note: If you find this spectrogram confusing, it may be due to your familiarity with the convention of placing low frequencies\nat the bottom and high frequencies at the top of a plot. However, when plotting spectrograms as an image using the matplotlib library,\nthe y-axis is flipped and the spectrograms appear upside down.\n\nNow apply the processing function to the entire dataset. This will take between 5 and 10 minutes.\n\n```py\n>>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n```\n\nYou'll see a warning saying that some examples in the dataset are longer than the maximum input length the model can handle (600 tokens).\nRemove those examples from the dataset. Here we go even further and to allow for larger batch sizes we remove anything over 200 tokens.\n\n```py\n>>> def is_not_too_long(input_ids):\n...     input_length = len(input_ids)",
  "...     return input_length < 200\n\n\n>>> dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n>>> len(dataset)\n8259\n```\n\nNext, create a basic train/test split:\n\n```py\n>>> dataset = dataset.train_test_split(test_size=0.1)\n```\n\n### Data collator\n\nIn order to combine multiple examples into a batch, you need to define a custom data collator. This collator will pad shorter sequences with padding\ntokens, ensuring that all examples have the same length. For the spectrogram labels, the padded portions are replaced with the special value `-100`. This special value\ninstructs the model to ignore that part of the spectrogram when calculating the spectrogram loss.\n\n```py\n>>> from dataclasses import dataclass\n>>> from typing import Any, Dict, List, Union\n\n\n>>> @dataclass\n... class TTSDataCollatorWithPadding:\n...     processor: Any\n\n...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n...         input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n...         label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]",
  "...         speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n\n...         # collate the inputs and targets into a batch\n...         batch = processor.pad(input_ids=input_ids, labels=label_features, return_tensors=\"pt\")\n\n...         # replace padding with -100 to ignore loss correctly\n...         batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n\n...         # not used during fine-tuning\n...         del batch[\"decoder_attention_mask\"]\n\n...         # round down target lengths to multiple of reduction factor\n...         if model.config.reduction_factor > 1:\n...             target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n...             target_lengths = target_lengths.new(\n...                 [length - length % model.config.reduction_factor for length in target_lengths]\n...             )\n...             max_length = max(target_lengths)\n...             batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n\n...         # also add in the speaker embeddings\n...         batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n\n...         return batch\n```",
  "In SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other words, it throws away every\nother timestep from the target sequence. The decoder then predicts a sequence that is twice as long. Since the original\ntarget sequence length may be odd, the data collator makes sure to round the maximum length of the batch down to be a\nmultiple of 2.\n\n```py\n>>> data_collator = TTSDataCollatorWithPadding(processor=processor)\n```\n\n## Train the model\n\nLoad the pre-trained model from the same checkpoint as you used for loading the processor:\n\n```py\n>>> from transformers import SpeechT5ForTextToSpeech\n\n>>> model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n```\n\nThe `use_cache=True` option is incompatible with gradient checkpointing. Disable it for training.\n\n```py\n>>> model.config.use_cache = False\n```\n\nDefine the training arguments. Here we are not computing any evaluation metrics during the training process. Instead, we'll\nonly look at the loss:\n\n```python\n>>> from transformers import Seq2SeqTrainingArguments\n\n>>> training_args = Seq2SeqTrainingArguments(\n...     output_dir=\"speecht5_finetuned_voxpopuli_nl\",  # change to a repo name of your choice",
  "...     per_device_train_batch_size=4,\n...     gradient_accumulation_steps=8,\n...     learning_rate=1e-5,\n...     warmup_steps=500,\n...     max_steps=4000,\n...     gradient_checkpointing=True,\n...     fp16=True,\n...     eval_strategy=\"steps\",\n...     per_device_eval_batch_size=2,\n...     save_steps=1000,\n...     eval_steps=1000,\n...     logging_steps=25,\n...     report_to=[\"tensorboard\"],\n...     load_best_model_at_end=True,\n...     greater_is_better=False,\n...     label_names=[\"labels\"],\n...     push_to_hub=True,\n... )\n```\n\nInstantiate the `Trainer` object  and pass the model, dataset, and data collator to it.\n\n```py\n>>> from transformers import Seq2SeqTrainer\n\n>>> trainer = Seq2SeqTrainer(\n...     args=training_args,\n...     model=model,\n...     train_dataset=dataset[\"train\"],\n...     eval_dataset=dataset[\"test\"],\n...     data_collator=data_collator,\n...     processing_class=processor,\n... )\n```\n\nAnd with that, you're ready to start training! Training will take several hours. Depending on your GPU,\nit is possible that you will encounter a CUDA \"out-of-memory\" error when you start training. In this case, you can reduce",
  "the `per_device_train_batch_size` incrementally by factors of 2 and increase `gradient_accumulation_steps` by 2x to compensate.\n\n```py\n>>> trainer.train()\n```\n\nTo be able to use your checkpoint with a pipeline, make sure to save the processor with the checkpoint:\n\n```py\n>>> processor.save_pretrained(\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")\n```\n\nPush the final model to the 🤗 Hub:\n\n```py\n>>> trainer.push_to_hub()\n```\n\n## Inference\n\n### Inference with a pipeline\n\nGreat, now that you've fine-tuned a model, you can use it for inference!\nFirst, let's see how you can use it with a corresponding pipeline. Let's create a `\"text-to-speech\"` pipeline with your\ncheckpoint:\n\n```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(\"text-to-speech\", model=\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")\n```\n\nPick a piece of text in Dutch you'd like narrated, e.g.:\n\n```py\n>>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n```\n\nTo use SpeechT5 with the pipeline, you'll need a speaker embedding. Let's get it from an example in the test dataset:\n\n```py\n>>> example = dataset[\"test\"][304]",
  ">>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n```\n\nNow you can pass the text and speaker embeddings to the pipeline, and it will take care of the rest:\n\n```py\n>>> forward_params = {\"speaker_embeddings\": speaker_embeddings}\n>>> output = pipe(text, forward_params=forward_params)\n>>> output\n{'audio': array([-6.82714235e-05, -4.26525949e-04,  1.06134125e-04, ...,\n-1.22392643e-03, -7.76011671e-04,  3.29112721e-04], dtype=float32),\n'sampling_rate': 16000}\n```\n\nYou can then listen to the result:\n\n```py\n>>> from IPython.display import Audio\n>>> Audio(output['audio'], rate=output['sampling_rate'])\n```\n\n### Run inference manually\n\nYou can achieve the same inference results without using the pipeline, however, more steps will be required.\n\nLoad the model from the 🤗 Hub:\n\n```py\n>>> model = SpeechT5ForTextToSpeech.from_pretrained(\"YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl\")\n```\n\nPick an example from the test dataset obtain a speaker embedding.\n\n```py\n>>> example = dataset[\"test\"][304]\n>>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n```\n\nDefine the input text and tokenize it.\n\n```py",
  ">>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n>>> inputs = processor(text=text, return_tensors=\"pt\")\n```\n\nCreate a spectrogram with your model:\n\n```py\n>>> spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n```\n\nVisualize the spectrogram, if you'd like to:\n\n```py\n>>> plt.figure()\n>>> plt.imshow(spectrogram.T)\n>>> plt.show()\n```\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_logmelspectrogram_2.png\" alt=\"Generated log-mel spectrogram\"/>\n</div>\n\nFinally, use the vocoder to turn the spectrogram into sound.\n\n```py\n>>> with torch.no_grad():\n...     speech = vocoder(spectrogram)\n\n>>> from IPython.display import Audio\n\n>>> Audio(speech.numpy(), rate=16000)\n```\n\nIn our experience, obtaining satisfactory results from this model can be challenging. The quality of the speaker\nembeddings appears to be a significant factor. Since SpeechT5 was pre-trained with English x-vectors, it performs best\nwhen using English speaker embeddings. If the synthesized speech sounds poor, try using a different speaker embedding.",
  "Increasing the training duration is also likely to enhance the quality of the results. Even so, the speech clearly is Dutch instead of English, and it does\ncapture the voice characteristics of the speaker (compare to the original audio in the example).\nAnother thing to experiment with is the model's configuration. For example, try using `config.reduction_factor = 1` to\nsee if this improves the results.\n\nFinally, it is essential to consider ethical considerations. Although TTS technology has numerous useful applications, it\nmay also be used for malicious purposes, such as impersonating someone's voice without their knowledge or consent. Please\nuse TTS judiciously and responsibly.",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Audio classification\n\n[[open-in-colab]]\n\n<Youtube id=\"KWwzcmG98Ds\"/>\n\nAudio classification - just like with text - assigns a class label as output from the input data. The only difference is instead of text inputs, you have raw audio waveforms. Some practical applications of audio classification include identifying speaker intent, language classification, and even animal species by their sounds.\n\nThis guide will show you how to:",
  "1. Fine-tune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to classify speaker intent.\n2. Use your fine-tuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/audio-classification)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load MInDS-14 dataset\n\nStart by loading the MInDS-14 dataset from the 🤗 Datasets library:\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n```",
  "Split the dataset's `train` split into a smaller train and test set with the [`~datasets.Dataset.train_test_split`] method. This will give you a chance to experiment and make sure everything works before spending more time on the full dataset.\n\n```py\n>>> minds = minds.train_test_split(test_size=0.2)\n```\n\nThen take a look at the dataset:\n\n```py\n>>> minds\nDatasetDict({\ntrain: Dataset({\nfeatures: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\nnum_rows: 450\n})\ntest: Dataset({\nfeatures: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\nnum_rows: 113\n})\n})\n```\n\nWhile the dataset contains a lot of useful information, like `lang_id` and `english_transcription`, you will focus on the `audio` and `intent_class` in this guide. Remove the other columns with the [`~datasets.Dataset.remove_columns`] method:\n\n```py\n>>> minds = minds.remove_columns([\"path\", \"transcription\", \"english_transcription\", \"lang_id\"])\n```\n\nHere's an example:\n\n```py\n>>> minds[\"train\"][0]\n{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,\n-0.00024414, -0.00024414], dtype=float32),",
  "'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',\n'sampling_rate': 8000},\n'intent_class': 2}\n```\n\nThere are two fields:\n\n- `audio`: a 1-dimensional `array` of the speech signal that must be called to load and resample the audio file.\n- `intent_class`: represents the class id of the speaker's intent.\n\nTo make it easier for the model to get the label name from the label id, create a dictionary that maps the label name to an integer and vice versa:\n\n```py\n>>> labels = minds[\"train\"].features[\"intent_class\"].names\n>>> label2id, id2label = dict(), dict()\n>>> for i, label in enumerate(labels):\n...     label2id[label] = str(i)\n...     id2label[str(i)] = label\n```\n\nNow you can convert the label id to a label name:\n\n```py\n>>> id2label[str(2)]\n'app_error'\n```\n\n## Preprocess\n\nThe next step is to load a Wav2Vec2 feature extractor to process the audio signal:\n\n```py\n>>> from transformers import AutoFeatureExtractor\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n```",
  "The MInDS-14 dataset has a sampling rate of 8kHz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16kHz to use the pretrained Wav2Vec2 model:\n\n```py\n>>> minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> minds[\"train\"][0]\n{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,\n-2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',\n'sampling_rate': 16000},\n'intent_class': 2}\n```\n\nNow create a preprocessing function that:\n\n1. Calls the `audio` column to load, and if necessary, resample the audio file.\n2. Checks if the sampling rate of the audio file matches the sampling rate of the audio data a model was pretrained with. You can find this information in the Wav2Vec2 [model card](https://huggingface.co/facebook/wav2vec2-base).\n3. Set a maximum input length to batch longer inputs without truncating them.\n\n```py\n>>> def preprocess_function(examples):",
  "...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n...     inputs = feature_extractor(\n...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n...     )\n...     return inputs\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once. Remove unnecessary columns and rename `intent_class` to `label`, as required by the model:\n\n```py\n>>> encoded_minds = minds.map(preprocess_function, remove_columns=\"audio\", batched=True)\n>>> encoded_minds = encoded_minds.rename_column(\"intent_class\", \"label\")\n```\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load an evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):\n\n```py",
  ">>> import evaluate\n\n>>> accuracy = evaluate.load(\"accuracy\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions = np.argmax(eval_pred.predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load Wav2Vec2 with [`AutoModelForAudioClassification`] along with the number of expected labels, and the label mappings:\n\n```py\n>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n\n>>> num_labels = len(id2label)\n>>> model = AutoModelForAudioClassification.from_pretrained(\n...     \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n... )\n```",
  "At this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir`, which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to fine-tune your model.\n\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_mind_model\",\n...     eval_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     learning_rate=3e-5,\n...     per_device_train_batch_size=32,\n...     gradient_accumulation_steps=4,\n...     per_device_eval_batch_size=32,\n...     num_train_epochs=10,\n...     warmup_ratio=0.1,\n...     logging_steps=10,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"accuracy\",\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,",
  "...     train_dataset=encoded_minds[\"train\"],\n...     eval_dataset=encoded_minds[\"test\"],\n...     processing_class=feature_extractor,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to fine-tune a model for audio classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've fine-tuned a model, you can use it for inference!\n\nLoad an audio file for inference. Remember to resample the sampling rate of the audio file to match the model's sampling rate, if necessary.\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate",
  ">>> audio_file = dataset[0][\"audio\"][\"path\"]\n```\n\nThe simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for audio classification with your model, and pass your audio file to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"audio-classification\", model=\"stevhliu/my_awesome_minds_model\")\n>>> classifier(audio_file)\n[\n{'score': 0.09766869246959686, 'label': 'cash_deposit'},\n{'score': 0.07998877018690109, 'label': 'app_error'},\n{'score': 0.0781070664525032, 'label': 'joint_account'},\n{'score': 0.07667109370231628, 'label': 'pay_bill'},\n{'score': 0.0755252093076706, 'label': 'balance'}\n]\n```\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nLoad a feature extractor to preprocess the audio file and return the `input` as PyTorch tensors:\n\n```py\n>>> from transformers import AutoFeatureExtractor\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"stevhliu/my_awesome_minds_model\")\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n```",
  "Pass your inputs to the model and return the logits:\n\n```py\n>>> from transformers import AutoModelForAudioClassification\n\n>>> model = AutoModelForAudioClassification.from_pretrained(\"stevhliu/my_awesome_minds_model\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n```\n\nGet the class with the highest probability, and use the model's `id2label` mapping to convert it to a label:\n\n```py\n>>> import torch\n\n>>> predicted_class_ids = torch.argmax(logits).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'cash_deposit'\n```\n</pt>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Automatic speech recognition\n\n[[open-in-colab]]\n\n<Youtube id=\"TksaY_FDgnk\"/>\n\nAutomatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users every day, and there are many other useful user-facing applications like live captioning and note-taking during meetings.\n\nThis guide will show you how to:",
  "1. Fine-tune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to transcribe audio to text.\n2. Use your fine-tuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/automatic-speech-recognition)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate jiwer\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load MInDS-14 dataset\n\nStart by loading a smaller subset of the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset from the 🤗 Datasets library. This will give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:100]\")",
  "```\n\nSplit the dataset's `train` split into a train and test set with the [`~Dataset.train_test_split`] method:\n\n```py\n>>> minds = minds.train_test_split(test_size=0.2)\n```\n\nThen take a look at the dataset:\n\n```py\n>>> minds\nDatasetDict({\ntrain: Dataset({\nfeatures: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\nnum_rows: 16\n})\ntest: Dataset({\nfeatures: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\nnum_rows: 4\n})\n})\n```\n\nWhile the dataset contains a lot of useful information, like `lang_id` and `english_transcription`, this guide focuses on the `audio` and `transcription`. Remove the other columns with the [`~datasets.Dataset.remove_columns`] method:\n\n```py\n>>> minds = minds.remove_columns([\"english_transcription\", \"intent_class\", \"lang_id\"])\n```\n\nReview the example again:\n\n```py\n>>> minds[\"train\"][0]\n{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,\n0.00024414,  0.00024414], dtype=float32),\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',",
  "'sampling_rate': 8000},\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n'transcription': \"hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing\"}\n```\n\nThere are two fields:\n\n- `audio`: a 1-dimensional `array` of the speech signal that must be called to load and resample the audio file.\n- `transcription`: the target text.\n\n## Preprocess\n\nThe next step is to load a Wav2Vec2 processor to process the audio signal:\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")\n```\n\nThe MInDS-14 dataset has a sampling rate of 8000Hz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16000Hz to use the pretrained Wav2Vec2 model:\n\n```py\n>>> minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> minds[\"train\"][0]\n{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,",
  "2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n'sampling_rate': 16000},\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n'transcription': \"hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing\"}\n```\n\nAs you can see in the `transcription` above, the text contains a mix of uppercase and lowercase characters. The Wav2Vec2 tokenizer is only trained on uppercase characters so you'll need to make sure the text matches the tokenizer's vocabulary:\n\n```py\n>>> def uppercase(example):\n...     return {\"transcription\": example[\"transcription\"].upper()}\n\n\n>>> minds = minds.map(uppercase)\n```\n\nNow create a preprocessing function that:\n\n1. Calls the `audio` column to load and resample the audio file.\n2. Extracts the `input_values` from the audio file and tokenize the `transcription` column with the processor.\n\n```py",
  ">>> def prepare_dataset(batch):\n...     audio = batch[\"audio\"]\n...     batch = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"transcription\"])\n...     batch[\"input_length\"] = len(batch[\"input_values\"][0])\n...     return batch\n```\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by increasing the number of processes with the `num_proc` parameter. Remove the columns you don't need with the [`~datasets.Dataset.remove_columns`] method:\n\n```py\n>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[\"train\"], num_proc=4)\n```\n\n🤗 Transformers doesn't have a data collator for ASR, so you'll need to adapt the [`DataCollatorWithPadding`] to create a batch of examples. It'll also dynamically pad your text and labels to the length of the longest element in its batch (instead of the entire dataset) so they are a uniform length. While it is possible to pad your text in the `tokenizer` function by setting `padding=True`, dynamic padding is more efficient.",
  "Unlike other data collators, this specific data collator needs to apply a different padding method to `input_values` and `labels`:\n\n```py\n>>> import torch\n\n>>> from dataclasses import dataclass, field\n>>> from typing import Any, Dict, List, Optional, Union\n\n\n>>> @dataclass\n... class DataCollatorCTCWithPadding:\n...     processor: AutoProcessor\n...     padding: Union[bool, str] = \"longest\"\n\n...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n...         # split inputs and labels since they have to be of different lengths and need\n...         # different padding methods\n...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n...         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n\n...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n\n...         # replace padding with -100 to ignore loss correctly",
  "...         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n...         batch[\"labels\"] = labels\n\n...         return batch\n```\n\nNow instantiate your `DataCollatorForCTCWithPadding`:\n\n```py\n>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")\n```\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's performance. You can quickly load an evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [word error rate](https://huggingface.co/spaces/evaluate-metric/wer) (WER) metric (refer to the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about loading and computing metrics):\n\n```py\n>>> import evaluate\n\n>>> wer = evaluate.load(\"wer\")\n```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the WER:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(pred):\n...     pred_logits = pred.predictions\n...     pred_ids = np.argmax(pred_logits, axis=-1)",
  "...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n...     pred_str = processor.batch_decode(pred_ids)\n...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n...     wer = wer.compute(predictions=pred_str, references=label_str)\n\n...     return {\"wer\": wer}\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou are now ready to start training your model! Load Wav2Vec2 with [`AutoModelForCTC`]. Specify the reduction to apply with the `ctc_loss_reduction` parameter. It is often better to use the average instead of the default summation:\n\n```py\n>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer\n\n>>> model = AutoModelForCTC.from_pretrained(\n...     \"facebook/wav2vec2-base\",\n...     ctc_loss_reduction=\"mean\",\n...     pad_token_id=processor.tokenizer.pad_token_id,\n... )\n```\n\nAt this point, only three steps remain:",
  "1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the WER and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to fine-tune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_asr_mind_model\",\n...     per_device_train_batch_size=8,\n...     gradient_accumulation_steps=2,\n...     learning_rate=1e-5,\n...     warmup_steps=500,\n...     max_steps=2000,\n...     gradient_checkpointing=True,\n...     fp16=True,\n...     group_by_length=True,\n...     eval_strategy=\"steps\",\n...     per_device_eval_batch_size=8,\n...     save_steps=1000,\n...     eval_steps=1000,\n...     logging_steps=25,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"wer\",\n...     greater_is_better=False,\n...     push_to_hub=True,\n... )",
  ">>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=encoded_minds[\"train\"],\n...     eval_dataset=encoded_minds[\"test\"],\n...     processing_class=processor,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so it can be accessible to everyone:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog [post](https://huggingface.co/blog/fine-tune-wav2vec2-english) for English ASR and this [post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) for multilingual ASR.\n\n</Tip>\n\n## Inference\n\nGreat, now that you've fine-tuned a model, you can use it for inference!\n\nLoad an audio file you'd like to run inference on. Remember to resample the sampling rate of the audio file to match the sampling rate of the model if you need to!\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")",
  ">>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n>>> audio_file = dataset[0][\"audio\"][\"path\"]\n```\n\nThe simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for automatic speech recognition with your model, and pass your audio file to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(\"automatic-speech-recognition\", model=\"stevhliu/my_awesome_asr_minds_model\")\n>>> transcriber(audio_file)\n{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}\n```\n\n<Tip>\n\nThe transcription is decent, but it could be better! Try finetuning your model on more examples to get even better results!\n\n</Tip>\n\nYou can also manually replicate the results of the `pipeline` if you'd like:\n\n<frameworkcontent>\n<pt>\nLoad a processor to preprocess the audio file and transcription and return the `input` as PyTorch tensors:\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"stevhliu/my_awesome_asr_mind_model\")",
  ">>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n```\n\nPass your inputs to the model and return the logits:\n\n```py\n>>> from transformers import AutoModelForCTC\n\n>>> model = AutoModelForCTC.from_pretrained(\"stevhliu/my_awesome_asr_mind_model\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n```\n\nGet the predicted `input_ids` with the highest probability, and use the processor to decode the predicted `input_ids` back into text:\n\n```py\n>>> import torch\n\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription\n['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']\n```\n</pt>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Causal language modeling\n\n[[open-in-colab]]\n\nThere are two types of language modeling, causal and masked. This guide illustrates causal language modeling.\nCausal language models are frequently used for text generation. You can use these models for creative applications like\nchoosing your own text adventure or an intelligent coding assistant like Copilot or CodeParrot.\n\n<Youtube id=\"Vpjb1lu0MDk\"/>",
  "Causal language modeling predicts the next token in a sequence of tokens, and the model can only attend to tokens on\nthe left. This means the model cannot see future tokens. GPT-2 is an example of a causal language model.\n\nThis guide will show you how to:\n\n1. Finetune [DistilGPT2](https://huggingface.co/distilbert/distilgpt2) on the [r/askscience](https://www.reddit.com/r/askscience/) subset of the [ELI5](https://huggingface.co/datasets/eli5) dataset.\n2. Use your finetuned model for inference.\n\n<Tip>\n\nTo see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/text-generation)\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate\n```\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load ELI5 dataset",
  "Start by loading the first 5000 examples from the [ELI5-Category](https://huggingface.co/datasets/eli5_category) dataset with the 🤗 Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\n```py\n>>> from datasets import load_dataset\n\n>>> eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n```\n\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n\n```py\n>>> eli5 = eli5.train_test_split(test_size=0.2)\n```\n\nThen take a look at an example:\n\n```py\n>>> eli5[\"train\"][0]\n{'q_id': '7h191n',\n'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n'selftext': '',\n'category': 'Economics',\n'subreddit': 'explainlikeimfive',\n'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],",
  "'text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n'score': [21, 19, 5, 3],\n'text_urls': [[],\n[],\n[],\n['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},\n'title_urls': ['url'],",
  "'selftext_urls': ['url']}\n```\n\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool about language modeling\ntasks is you don't need labels (also known as an unsupervised task) because the next word *is* the label.\n\n## Preprocess\n\n<Youtube id=\"ma1TrR7gE7I\"/>\n\nThe next step is to load a DistilGPT2 tokenizer to process the `text` subfield:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n```\n\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. This means you'll need to\nextract the `text` subfield from its nested structure with the [`flatten`](https://huggingface.co/docs/datasets/process#flatten) method:\n\n```py\n>>> eli5 = eli5.flatten()\n>>> eli5[\"train\"][0]\n{'q_id': '7h191n',\n'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n'selftext': '',\n'category': 'Economics',\n'subreddit': 'explainlikeimfive',\n'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],",
  "'answers.text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n'answers.score': [21, 19, 5, 3],\n'answers.text_urls': [[],\n[],\n[],\n['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],",
  "'title_urls': ['url'],\n'selftext_urls': ['url']}\n```\n\nEach subfield is now a separate column as indicated by the `answers` prefix, and the `text` field is a list now. Instead\nof tokenizing each sentence separately, convert the list to a string so you can jointly tokenize them.\n\nHere is a first preprocessing function to join the list of strings for each example and tokenize the result:\n\n```py\n>>> def preprocess_function(examples):\n...     return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n```\n\nTo apply this preprocessing function over the entire dataset, use the 🤗 Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once, and increasing the number of processes with `num_proc`. Remove any columns you don't need:\n\n```py\n>>> tokenized_eli5 = eli5.map(\n...     preprocess_function,\n...     batched=True,\n...     num_proc=4,\n...     remove_columns=eli5[\"train\"].column_names,\n... )\n```\n\nThis dataset contains the token sequences, but some of these are longer than the maximum input length for the model.\n\nYou can now use a second preprocessing function to",
  "- concatenate all the sequences\n- split the concatenated sequences into shorter chunks defined by `block_size`, which should be both shorter than the maximum input length and short enough for your GPU RAM.\n\n```py\n>>> block_size = 128\n\n\n>>> def group_texts(examples):\n...     # Concatenate all texts.\n...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n...     total_length = len(concatenated_examples[list(examples.keys())[0]])\n...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n...     # customize this part to your needs.\n...     if total_length >= block_size:\n...         total_length = (total_length // block_size) * block_size\n...     # Split by chunks of block_size.\n...     result = {\n...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n...         for k, t in concatenated_examples.items()\n...     }\n...     result[\"labels\"] = result[\"input_ids\"].copy()\n...     return result\n```\n\nApply the `group_texts` function over the entire dataset:\n\n```py\n>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)\n```",
  "Now create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient to *dynamically pad* the\nsentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\n<frameworkcontent>\n<pt>\nUse the end-of-sequence token as the padding token and set `mlm=False`. This will use the inputs as labels shifted to the right by one element:\n\n```py\n>>> from transformers import DataCollatorForLanguageModeling\n\n>>> tokenizer.pad_token = tokenizer.eos_token\n>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n```\n\n</pt>\n<tf>\nUse the end-of-sequence token as the padding token and set `mlm=False`. This will use the inputs as labels shifted to the right by one element:\n\n```py\n>>> from transformers import DataCollatorForLanguageModeling\n\n>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n```\n\n</tf>\n</frameworkcontent>\n\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the [basic tutorial](../training#train-with-pytorch-trainer)!\n\n</Tip>",
  "You're ready to start training your model now! Load DistilGPT2 with [`AutoModelForCausalLM`]:\n\n```py\n>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n2. Pass the training arguments to [`Trainer`] along with the model, datasets, and data collator.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_eli5_clm-model\",\n...     eval_strategy=\"epoch\",\n...     learning_rate=2e-5,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=lm_dataset[\"train\"],\n...     eval_dataset=lm_dataset[\"test\"],\n...     data_collator=data_collator,\n...     tokenizer=tokenizer,\n... )\n\n>>> trainer.train()\n```",
  "Once training is completed, use the [`~transformers.Trainer.evaluate`] method to evaluate your model and get its perplexity:\n\n```py\n>>> import math\n\n>>> eval_results = trainer.evaluate()\n>>> print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\nPerplexity: 49.61\n```\n\nThen share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the [basic tutorial](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\n```py\n>>> from transformers import create_optimizer, AdamWeightDecay\n\n>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n```\n\nThen you can load DistilGPT2 with [`TFAutoModelForCausalLM`]:\n\n```py\n>>> from transformers import TFAutoModelForCausalLM\n\n>>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n```",
  "Convert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n\n```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     lm_dataset[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_test_set = model.prepare_tf_dataset(\n...     lm_dataset[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:\n\n```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```\n\nThis can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> callback = PushToHubCallback(\n...     output_dir=\"my_awesome_eli5_clm-model\",\n...     tokenizer=tokenizer,\n... )\n```",
  "Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callback to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for causal language modeling, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with a prompt you'd like to generate text from:\n\n```py\n>>> prompt = \"Somatic hypermutation allows the immune system to\"\n```",
  "The simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for text generation with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline\n\n>>> generator = pipeline(\"text-generation\", model=\"username/my_awesome_eli5_clm-model\")\n>>> generator(prompt)\n[{'generated_text': \"Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\\n\\n\\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks.\"}]\n```\n\n<frameworkcontent>\n<pt>\nTokenize the text and return the `input_ids` as PyTorch tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n```\n\nUse the [`~generation.GenerationMixin.generate`] method to generate text.\nFor more details about the different text generation strategies and parameters for controlling generation, check out the [Text generation strategies](../generation_strategies) page.\n\n```py",
  ">>> from transformers import AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n[\"Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system\"]\n```\n</pt>\n<tf>\nTokenize the text and return the `input_ids` as TensorFlow tensors:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n>>> inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids\n```",
  "Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text generation strategies](../generation_strategies) page.\n\n```py\n>>> from transformers import TFAutoModelForCausalLM\n\n>>> model = TFAutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)",
  "['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']\n```\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Prompt engineering\n\n[[open-in-colab]]\n\nPrompt engineering or prompting, uses natural language to improve large language model (LLM) performance on a variety of tasks. A prompt can steer the model towards generating a desired output. In many cases, you don't even need a [fine-tuned](#finetuning) model for a task. You just need a good prompt.",
  "Try prompting a LLM to classify some text. When you create a prompt, it's important to provide very specific instructions about the task and what the result should look like.\n\n```py\nfrom transformers import pipeline\nimport torch\n\npipeline = pipeline(task=\"text-generation\", model=\"mistralai/Mistal-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Classify the text into neutral, negative or positive.\nText: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\nSentiment:\n\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=10)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")\nResult: Classify the text into neutral, negative or positive.\nText: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\nSentiment:\nPositive\n```",
  "The challenge lies in designing prompts that produces the results you're expecting because language is so incredibly nuanced and expressive.\n\nThis guide covers prompt engineering best practices, techniques, and examples for how to solve language and reasoning tasks.\n\n## Best practices\n\n1. Try to pick the latest models for the best performance. Keep in mind that LLMs can come in two variants, [base](https://hf.co/mistralai/Mistral-7B-v0.1) and [instruction-tuned](https://hf.co/mistralai/Mistral-7B-Instruct-v0.1) (or chat).\n\nBase models are excellent at completing text given an initial prompt, but they're not as good at following instructions. Instruction-tuned models are specifically trained versions of the base models on instructional or conversational data. This makes instruction-tuned models a better fit for prompting.\n\n> [!WARNING]\n> Modern LLMs are typically decoder-only models, but there are some encoder-decoder LLMs like [Flan-T5](../model_doc/flan-t5) or [BART](../model_doc/bart) that may be used for prompting. For encoder-decoder models, make sure you set the pipeline task identifier to `text2text-generation` instead of `text-generation`.",
  "2. Start with a short and simple prompt, and iterate on it to get better results.\n\n3. Put instructions at the beginning or end of a prompt. For longer prompts, models may apply optimizations to prevent attention from scaling quadratically, which places more emphasis at the beginning and end of a prompt.\n\n4. Clearly separate instructions from the text of interest.\n\n5. Be specific and descriptive about the task and the desired output, including for example, its format, length, style, and language. Avoid ambiguous descriptions and instructions.\n\n6. Instructions should focus on \"what to do\" rather than \"what not to do\".\n\n7. Lead the model to generate the correct output by writing the first word or even the first sentence.\n\n8. Try other techniques like [few-shot](#few-shot) and [chain-of-thought](#chain-of-thought) to improve results.\n\n9. Test your prompts with different models to assess their robustness.\n\n10. Version and track your prompt performance.\n\n## Techniques\n\nCrafting a good prompt alone, also known as zero-shot prompting, may not be enough to get the results you want. You may need to try a few prompting techniques to get the best performance.",
  "This section covers a few prompting techniques.\n\n### Few-shot\n\nFew-shot prompting improves accuracy and performance by including specific examples of what a model should generate given an input. The explicit examples give the model a better understanding of the task and the output format you're looking for. Try experimenting with different numbers of examples (2, 4, 8, etc.) to see how it affects performance.\n\nThe example below provides the model with 1 example (1-shot) of the output format (a date in MM/DD/YYYY format) it should return.\n\n```py\nfrom transformers import pipeline\nimport torch\n\npipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Text: The first human went into space and orbited the Earth on April 12, 1961.\nDate: 04/12/1961\nText: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\nDate:\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=12, do_sample=True, top_k=10)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")",
  "Result: Text: The first human went into space and orbited the Earth on April 12, 1961.\nDate: 04/12/1961\nText: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\nDate: 09/28/1960\n```\n\nThe downside of few-shot prompting is that you need to create lengthier prompts which increases computation and latency. There is also a limit to prompt lengths. Finally, a model can learn unintended patterns from your examples and it doesn't work well on complex reasoning tasks.\n\n### Chain-of-thought\n\nChain-of-thought (CoT) is effective at generating more coherent and well-reasoned outputs by providing a series of prompts that help a model \"think\" more thoroughly about a topic.\n\nThe example below provides the model with several prompts to work through intermediate reasoning steps.\n\n```py\nfrom transformers import pipeline\nimport torch\n\npipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Let's go through this step-by-step:\n1. You start with 15 muffins.\n2. You eat 2 muffins, leaving you with 13 muffins.",
  "3. You give 5 muffins to your neighbor, leaving you with 8 muffins.\n4. Your partner buys 6 more muffins, bringing the total number of muffins to 14.\n5. Your partner eats 2 muffins, leaving you with 12 muffins.\nIf you eat 6 muffins, how many are left?\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=20, do_sample=True, top_k=10)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")\nResult: Let's go through this step-by-step:\n1. You start with 15 muffins.\n2. You eat 2 muffins, leaving you with 13 muffins.\n3. You give 5 muffins to your neighbor, leaving you with 8 muffins.\n4. Your partner buys 6 more muffins, bringing the total number of muffins to 14.\n5. Your partner eats 2 muffins, leaving you with 12 muffins.\nIf you eat 6 muffins, how many are left?\nAnswer: 6\n```\n\nLike [few-shot](#few-shot) prompting, the downside of CoT is that it requires more effort to design a series of prompts that help the model reason through a complex task and prompt length increases latency.\n\n## Fine-tuning\n\nWhile prompting is a powerful way to work with LLMs, there are scenarios where a fine-tuned model or even fine-tuning a model works better.",
  "Here are some examples scenarios where a fine-tuned model makes sense.\n\n- Your domain is extremely different from what a LLM was pretrained on, and extensive prompting didn't produce the results you want.\n- Your model needs to work well in a low-resource language.\n- Your model needs to be trained on sensitive data that have strict regulatory requirements.\n- You're using a small model due to cost, privacy, infrastructure, or other constraints.\n\nIn all of these scenarios, ensure that you have a large enough domain-specific dataset to train your model with, have enough time and resources, and the cost of fine-tuning is worth it. Otherwise, you may be better off trying to optimize your prompt.\n\n## Examples\n\nThe examples below demonstrate prompting a LLM for different tasks.\n\n<hfoptions id=\"tasks\">\n<hfoption id=\"named entity recognition\">\n\n```py\nfrom transformers import pipeline\nimport torch\n\npipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Return a list of named entities in the text.",
  "Text: The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.\nNamed entities:\n\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=50, return_full_text=False)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")\nResult:  [Clément Delangue, Julien Chaumond, Thomas Wolf, company, New York City, chatbot app, teenagers]\n```\n\n</hfoption>\n<hfoption id=\"translation\">\n\n```py\nfrom transformers import pipeline\nimport torch\n\npipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Translate the English text to French.\nText: Sometimes, I've believed as many as six impossible things before breakfast.\nTranslation:\n\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=20, do_sample=True, top_k=10, return_full_text=False)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")\nResult: À l'occasion, j'ai croyu plus de six choses impossibles\n```\n\n</hfoption>\n<hfoption id=\"summarization\">\n\n```py\nfrom transformers import pipeline\nimport torch",
  "pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.\nWrite a summary of the above text.\nSummary:\n\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=30, do_sample=True, top_k=10, return_full_text=False)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")\nResult: Permaculture is the design process that involves mimicking natural ecosystems to provide sustainable solutions to basic needs. It is a holistic approach that comb\n```\n\n</hfoption>\n<hfoption id=\"question answering\">\n\n```py\nfrom transformers import pipeline\nimport torch",
  "pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nprompt = \"\"\"Answer the question using the context below.\nContext: Gazpacho is a cold soup and drink made of raw, blended vegetables. Most gazpacho includes stale bread, tomato, cucumbers, onion, bell peppers, garlic, olive oil, wine vinegar, water, and salt. Northern recipes often include cumin and/or pimentón (smoked sweet paprika). Traditionally, gazpacho was made by pounding the vegetables in a mortar with a pestle; this more laborious method is still sometimes used as it helps keep the gazpacho cool and avoids the foam and silky consistency of smoothie versions made in blenders or food processors.\nQuestion: What modern tool is used to make gazpacho?\nAnswer:\n\"\"\"\n\noutputs = pipeline(prompt, max_new_tokens=10, do_sample=True, top_k=10, return_full_text=False)\nfor output in outputs:\nprint(f\"Result: {output['generated_text']}\")\nResult: A blender or food processor is the modern tool\n```\n\n</hfoption>\n</hfoptions>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image Feature Extraction\n\n[[open-in-colab]]",
  "Image feature extraction is the task of extracting semantically meaningful features given an image. This has many use cases, including image similarity and image retrieval. Moreover, most computer vision models can be used for image feature extraction, where one can remove the task-specific head (image classification, object detection etc) and get the features. These features are very useful on a higher level: edge detection, corner detection and so on. They may also contain information about the real world (e.g. what a cat looks like) depending on how deep the model is. Therefore, these outputs can be used to train new classifiers on a specific dataset.\n\nIn this guide, you will:\n\n- Learn to build a simple image similarity system on top of the `image-feature-extraction` pipeline.\n- Accomplish the same task with bare model inference.\n\n## Image Similarity using `image-feature-extraction` Pipeline\n\nWe have two images of cats sitting on top of fish nets, one of them is generated.\n\n```python\nfrom PIL import Image\nimport requests",
  "img_urls = [\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\", \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.jpeg\"]\nimage_real = Image.open(requests.get(img_urls[0], stream=True).raw).convert(\"RGB\")\nimage_gen = Image.open(requests.get(img_urls[1], stream=True).raw).convert(\"RGB\")\n```\n\nLet's see the pipeline in action. First, initialize the pipeline. If you don't pass any model to it, the pipeline will be automatically initialized with [google/vit-base-patch16-224](google/vit-base-patch16-224). If you'd like to calculate similarity, set `pool` to True.\n\n```python\nimport torch\nfrom transformers import pipeline\nfrom accelerate.test_utils.testing import get_backend\n# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\nDEVICE, _, _ = get_backend()\npipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n```\n\nTo infer with `pipe` pass both images to it.\n\n```python\noutputs = pipe([image_real, image_gen])\n```\n\nThe output contains pooled embeddings of those two images.\n\n```python\n# get the length of a single output",
  "print(len(outputs[0][0]))\n# show outputs\nprint(outputs)\n\n# 768\n# [[[-0.03909236937761307, 0.43381670117378235, -0.06913255900144577,\n```\n\nTo get the similarity score, we need to pass them to a similarity function.\n\n```python\nfrom torch.nn.functional import cosine_similarity\n\nsimilarity_score = cosine_similarity(torch.Tensor(outputs[0]),\ntorch.Tensor(outputs[1]), dim=1)\n\nprint(similarity_score)\n\n# tensor([0.6043])\n```\n\nIf you want to get the last hidden states before pooling, avoid passing any value for the `pool` parameter, as it is set to `False` by default. These hidden states are useful for training new classifiers or models based on the features from the model.\n\n```python\npipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-224\", device=DEVICE)\noutputs = pipe(image_real)\n```\n\nSince the outputs are unpooled, we get the last hidden states where the first dimension is the batch size, and the last two are the embedding shape.\n\n```python\nimport numpy as np\nprint(np.array(outputs).shape)\n# (1, 197, 768)\n```\n\n## Getting Features and Similarities using `AutoModel`",
  "We can also use `AutoModel` class of transformers to get the features. `AutoModel` loads any transformers model with no task-specific head, and we can use this to get the features.\n\n```python\nfrom transformers import AutoImageProcessor, AutoModel\n\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nmodel = AutoModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE)\n```\n\nLet's write a simple function for inference. We will pass the inputs to the `processor` first and pass its outputs to the `model`.\n\n```python\ndef infer(image):\ninputs = processor(image, return_tensors=\"pt\").to(DEVICE)\noutputs = model(**inputs)\nreturn outputs.pooler_output\n```\n\nWe can pass the images directly to this function and get the embeddings.\n\n```python\nembed_real = infer(image_real)\nembed_gen = infer(image_gen)\n```\n\nWe can get the similarity again over the embeddings.\n\n```python\nfrom torch.nn.functional import cosine_similarity\n\nsimilarity_score = cosine_similarity(embed_real, embed_gen, dim=1)\nprint(similarity_score)\n\n# tensor([0.6061], device='cuda:0', grad_fn=<SumBackward1>)\n```",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TimeSformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Facebook Research.",
  "This work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\n\nThe abstract from the paper is the following:",
  "*We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: [this https URL](https://github.com/facebookresearch/TimeSformer).*\n\nThis model was contributed by [fcakyon](https://huggingface.co/fcakyon).",
  "The original code can be found [here](https://github.com/facebookresearch/TimeSformer).\n\n## Usage tips\n\nThere are many pretrained variants. Select your pretrained model based on the dataset it is trained on. Moreover,\nthe number of input frames per clip changes based on the model size so you should consider this parameter while selecting your pretrained model.\n\n## Resources\n\n- [Video classification task guide](../tasks/video_classification)\n\n## TimesformerConfig\n\n[[autodoc]] TimesformerConfig\n\n## TimesformerModel\n\n[[autodoc]] TimesformerModel\n- forward\n\n## TimesformerForVideoClassification\n\n[[autodoc]] TimesformerForVideoClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DeBERTa-v2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n\nIt builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa.\n\nThe abstract from the paper is the following:\n\n*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to",
  "predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*\n\n\nThe following information is visible directly on the [original implementation\nrepository](https://github.com/microsoft/DeBERTa). DeBERTa v2 is the second version of the DeBERTa model. It includes\nthe 1.5B model used for the SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. You can\nfind more details about this submission in the authors'\n[blog](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/)\n\nNew in v2:\n\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K built from the training data.",
  "Instead of a GPT2-based tokenizer, the tokenizer is now\n[sentencepiece-based](https://github.com/google/sentencepiece) tokenizer.\n- **nGiE(nGram Induced Input Encoding)** The DeBERTa-v2 model uses an additional convolution layer aside with the first\ntransformer layer to better learn the local dependency of input tokens.\n- **Sharing position projection matrix with content projection matrix in attention layer** Based on previous\nexperiments, this can save parameters without affecting the performance.\n- **Apply bucket to encode relative positions** The DeBERTa-v2 model uses log bucket to encode relative positions\nsimilar to T5.\n- **900M model & 1.5B model** Two additional model sizes are available: 900M and 1.5B, which significantly improves the\nperformance of downstream tasks.\n\nThis model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/DeBERTa).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)",
  "- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## DebertaV2Config\n\n[[autodoc]] DebertaV2Config\n\n## DebertaV2Tokenizer\n\n[[autodoc]] DebertaV2Tokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## DebertaV2TokenizerFast\n\n[[autodoc]] DebertaV2TokenizerFast\n- build_inputs_with_special_tokens\n- create_token_type_ids_from_sequences\n\n<frameworkcontent>\n<pt>\n\n## DebertaV2Model\n\n[[autodoc]] DebertaV2Model\n- forward\n\n## DebertaV2PreTrainedModel\n\n[[autodoc]] DebertaV2PreTrainedModel\n- forward\n\n## DebertaV2ForMaskedLM\n\n[[autodoc]] DebertaV2ForMaskedLM\n- forward\n\n## DebertaV2ForSequenceClassification\n\n[[autodoc]] DebertaV2ForSequenceClassification\n- forward\n\n## DebertaV2ForTokenClassification\n\n[[autodoc]] DebertaV2ForTokenClassification\n- forward\n\n## DebertaV2ForQuestionAnswering\n\n[[autodoc]] DebertaV2ForQuestionAnswering\n- forward\n\n## DebertaV2ForMultipleChoice\n\n[[autodoc]] DebertaV2ForMultipleChoice\n- forward\n\n</pt>\n<tf>\n\n## TFDebertaV2Model",
  "[[autodoc]] TFDebertaV2Model\n- call\n\n## TFDebertaV2PreTrainedModel\n\n[[autodoc]] TFDebertaV2PreTrainedModel\n- call\n\n## TFDebertaV2ForMaskedLM\n\n[[autodoc]] TFDebertaV2ForMaskedLM\n- call\n\n## TFDebertaV2ForSequenceClassification\n\n[[autodoc]] TFDebertaV2ForSequenceClassification\n- call\n\n## TFDebertaV2ForTokenClassification\n\n[[autodoc]] TFDebertaV2ForTokenClassification\n- call\n\n## TFDebertaV2ForQuestionAnswering\n\n[[autodoc]] TFDebertaV2ForQuestionAnswering\n- call\n\n## TFDebertaV2ForMultipleChoice\n\n[[autodoc]] TFDebertaV2ForMultipleChoice\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TimmWrapper\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nHelper class to enable loading timm models to be used with the transformers library and its autoclasses.\n\n```python\n>>> import torch\n>>> from PIL import Image\n>>> from urllib.request import urlopen",
  ">>> from transformers import AutoModelForImageClassification, AutoImageProcessor\n\n>>> # Load image\n>>> image = Image.open(urlopen(\n...     'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n... ))\n\n>>> # Load model and image processor\n>>> checkpoint = \"timm/resnet50.a1_in1k\"\n>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n>>> model = AutoModelForImageClassification.from_pretrained(checkpoint).eval()\n\n>>> # Preprocess image\n>>> inputs = image_processor(image)\n\n>>> # Forward pass\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # Get top 5 predictions\n>>> top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)\n```\n\n## Resources:\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with TimmWrapper.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [Collection of Example Notebook](https://github.com/ariG23498/timm-wrapper-examples) 🌎\n\n> [!TIP]\n> For a more detailed overview please read the [official blog post](https://huggingface.co/blog/timm-transformers) on the timm integration.\n\n## TimmWrapperConfig",
  "[[autodoc]] TimmWrapperConfig\n\n## TimmWrapperImageProcessor\n\n[[autodoc]] TimmWrapperImageProcessor\n- preprocess\n\n## TimmWrapperModel\n\n[[autodoc]] TimmWrapperModel\n- forward\n\n## TimmWrapperForImageClassification\n\n[[autodoc]] TimmWrapperForImageClassification\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Gemma3\n\n## Overview",
  "The Gemma 3 model was proposed in the [Gemma 3 Techncial Report](https://goo.gle/Gemma3Report) by Google. It is a vision-language model composed by a [SigLIP](siglip) vision encoder and a [Gemma 2](gemma_2) language decoder, linked by a multimodal linear projection. It cuts an image into a fixed number of tokens, in the same way as SigLIP, as long as the image does not exceed certain aspect ratio. For images that exceed the given aspect ratio, it crops the image into multiple smaller patches and concatenates them with the base image embedding. One particularity is that the model uses bidirectional attention on all the image tokens. In addition, the model interleaves sliding window local attention with full causal attention in the language backbone, where each sixth layer is a full causal attention layer.\n\nThis model was contributed by [Ryan Mullins](https://huggingface.co/RyanMullins), [Raushan Turganbay](https://huggingface.co/RaushanTurganbay) [Arthur Zucker](https://huggingface.co/ArthurZ), and [Pedro Cuenca](https://huggingface.co/pcuenq).\n\n\n## Usage tips\n\n\n- For image+text and image-only inputs use `Gemma3ForConditionalGeneration`.",
  "- For text-only inputs use `Gemma3ForCausalLM` for generation to avoid loading the vision tower.\n- Each sample can contain multiple images, and the number of images can vary between samples. However, make sure to pass correctly batched images to the processor, where each batch is a list of one or more images.\n- The text passed to the processor should have a `<start_of_image>` token wherever an image should be inserted.\n- The processor has its own `apply_chat_template` method to convert chat messages to model inputs. See the examples below for more details on how to use it.\n\n\n### Image cropping for high resolution images\n\nThe model supports cropping images into smaller patches when the image aspect ratio exceeds a certain value. By default the images are not cropped and only the base image is forwarded to the model. Users can set `do_pan_and_scan=True` to obtain several crops per image along with the base image to improve the quality in DocVQA or similar tasks requiring higher resolution images.",
  "Pan and scan is an inference time optimization to handle images with skewed aspect ratios. When enabled, it improves performance on tasks related to document understanding, infographics, OCR, etc.\n\n```python\n\nprocessor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\", padding_side=\"left\")\n\nurl = \"https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4=\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n]\n},\n{\n\"role\": \"user\", \"content\": [\n{\"type\": \"image\", \"url\": url},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n]\n},\n]\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\nadd_generation_prompt=True,\ndo_pan_and_scan=True,\n).to(model.device)\n\n```\n\n\n## Usage Example\n\n### Single-image Inference\n\n```python\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\n\nmodel_id = \"google/gemma-3-4b-it\"\nmodel = Gemma3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_id, padding_side=\"left\")",
  "url = \"https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4=\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n]\n},\n{\n\"role\": \"user\", \"content\": [\n{\"type\": \"image\", \"url\": url},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n]\n},\n]\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\nadd_generation_prompt=True,\n).to(model.device)\n\noutput = model.generate(**inputs, max_new_tokens=50)\nprint(processor.decode(output[0], skip_special_tokens=True)[inputs.input_ids.shape[1]: ])\n```\n\n### Multi-image Inference\n\n```python\nmodel_id = \"google/gemma-3-4b-it\"\nmodel = Gemma3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_id, padding_side=\"left\")\n\nurl_cow = \"https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4=\"\nurl_stop = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nmessages = [\n{\n\"role\": \"system\",",
  "\"content\": [\n{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n]\n},\n{\n\"role\": \"user\", \"content\": [\n{\"type\": \"image\", \"url\": url_cow},\n{\"type\": \"image\", \"url\": url_stop},\n{\"type\": \"text\", \"text\": \"Are these two images identical?\"},\n]\n},\n]\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\nadd_generation_prompt=True,\n).to(model.device)\n\noutput = model.generate(**inputs, max_new_tokens=50)\nprint(processor.decode(output[0], skip_special_tokens=True)[inputs.input_ids.shape[1]: ])\n\n```\n\n### Text-only inference\n\nYou can use the VLMs for text-only generation by omitting images in your input. However, you can also load the models in text-only mode as shown below. This will skip loading the vision tower and will save resources when you just need the LLM capabilities.\n```python\nfrom transformers import AutoTokenizer, Gemma3ForCausalLM\n\nmodel_id = \"google/gemma-3-1b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = Gemma3ForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\ninput_ids = tokenizer(\"Write me a poem about Machine Learning.\", return_tensors=\"pt\").to(model.device)",
  "outputs = model.generate(**input_ids, max_new_tokens=100)\ntext = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nprint(text)\n\n```\n\n\n## Gemma3ImageProcessor\n\n[[autodoc]] Gemma3ImageProcessor\n\n## Gemma3ImageProcessorFast\n\n[[autodoc]] Gemma3ImageProcessorFast\n\n## Gemma3Processor\n\n[[autodoc]] Gemma3Processor\n\n## Gemma3TextConfig\n\n[[autodoc]] Gemma3TextConfig\n\n## Gemma3Config\n\n[[autodoc]] Gemma3Config\n\n## Gemma3TextModel\n\n[[autodoc]] Gemma3TextModel\n- forward\n\n## Gemma3ForCausalLM\n\n[[autodoc]] Gemma3ForCausalLM\n- forward\n\n## Gemma3ForConditionalGeneration\n\n[[autodoc]] Gemma3ForConditionalGeneration\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Chinese-CLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Chinese-CLIP model was proposed in [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.",
  "Chinese-CLIP is an implementation of CLIP (Radford et al., 2021) on a large-scale dataset of Chinese image-text pairs. It is capable of performing cross-modal retrieval and also playing as a vision backbone for vision tasks like zero-shot image classification, open-domain object detection, etc. The original Chinese-CLIP code is released [at this link](https://github.com/OFA-Sys/Chinese-CLIP).\n\nThe abstract from the paper is the following:",
  "*The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. Our comprehensive experiments demonstrate that Chinese CLIP can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero-shot learning and finetuning, and it is able to achieve competitive performance in zero-shot image classification based on the evaluation on the ELEVATER benchmark (Li et al., 2022). Our codes, pretrained models, and demos have been released.*\n\nThe Chinese-CLIP model was contributed by [OFA-Sys](https://huggingface.co/OFA-Sys).\n\n## Usage example",
  "The code snippet below shows how to compute image & text features and similarities:\n\n```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n\n>>> model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n>>> processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n>>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> # Squirtle, Bulbasaur, Charmander, Pikachu in English\n>>> texts = [\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]\n\n>>> # compute image feature\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_features = model.get_image_features(**inputs)\n>>> image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n\n>>> # compute text features\n>>> inputs = processor(text=texts, padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n>>> text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n\n>>> # compute image-text similarity scores",
  ">>> inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # probs: [[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]]\n```\n\nCurrently, following scales of pretrained Chinese-CLIP models are available on 🤗 Hub:\n\n- [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)\n- [OFA-Sys/chinese-clip-vit-large-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14)\n- [OFA-Sys/chinese-clip-vit-large-patch14-336px](https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14-336px)\n- [OFA-Sys/chinese-clip-vit-huge-patch14](https://huggingface.co/OFA-Sys/chinese-clip-vit-huge-patch14)\n\n## ChineseCLIPConfig\n\n[[autodoc]] ChineseCLIPConfig\n- from_text_vision_configs\n\n## ChineseCLIPTextConfig\n\n[[autodoc]] ChineseCLIPTextConfig\n\n## ChineseCLIPVisionConfig\n\n[[autodoc]] ChineseCLIPVisionConfig\n\n## ChineseCLIPImageProcessor\n\n[[autodoc]] ChineseCLIPImageProcessor\n- preprocess\n\n## ChineseCLIPFeatureExtractor\n\n[[autodoc]] ChineseCLIPFeatureExtractor",
  "## ChineseCLIPProcessor\n\n[[autodoc]] ChineseCLIPProcessor\n\n## ChineseCLIPModel\n\n[[autodoc]] ChineseCLIPModel\n- forward\n- get_text_features\n- get_image_features\n\n## ChineseCLIPTextModel\n\n[[autodoc]] ChineseCLIPTextModel\n- forward\n\n## ChineseCLIPVisionModel\n\n[[autodoc]] ChineseCLIPVisionModel\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PhoBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe PhoBERT model was proposed in [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf) by Dat Quoc Nguyen, Anh Tuan Nguyen.\n\nThe abstract from the paper is the following:\n\n*We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual\nlanguage models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent\nbest pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple\nVietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and\nNatural language inference.*\n\nThis model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/PhoBERT).\n\n## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n>>> # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!",
  ">>> line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n\n>>> input_ids = torch.tensor([tokenizer.encode(line)])\n\n>>> with torch.no_grad():\n...     features = phobert(input_ids)  # Models outputs are now tuples\n\n>>> # With TensorFlow 2.0+:\n>>> # from transformers import TFAutoModel\n>>> # phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n```\n\n<Tip>\n\nPhoBERT implementation is the same as BERT, except for tokenization. Refer to [BERT documentation](bert) for information on\nconfiguration classes and their parameters. PhoBERT-specific tokenizer is documented below.\n\n</Tip>\n\n## PhobertTokenizer\n\n[[autodoc]] PhobertTokenizer",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fuyu\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar.",
  "The authors introduced Fuyu-8B, a decoder-only multimodal model based on the classic transformers architecture, with query and key normalization. A linear encoder is added to create multimodal embeddings from image inputs.\n\nBy treating image tokens like text tokens and using a special image-newline character, the model knows when an image line ends. Image positional embeddings are removed. This avoids the need for different training phases for various image resolutions. With 8 billion parameters and licensed under CC-BY-NC, Fuyu-8B is notable for its ability to handle both text and images, its impressive context size of 16K, and its overall performance.\n\n<Tip warning={true}>\n\nThe `Fuyu` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.",
  "The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n\nFinetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n\n</Tip>\n\n\nTips:\n\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\n\n```bash\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget path/to/fuyu-8b-model-weights.tar\ntar -xvf fuyu-8b-model-weights.tar\npython src/transformers/models/fuyu/convert_fuyu_weights_to_hf.py  --input_dir /path/to/downloaded/fuyu/weights/ --output_dir /output/path \\\n--pt_model_path /path/to/fuyu_8b_release/iter_0001251/mp_rank_00/model_optim_rng.pt",
  "--ada_lib_path /path/to/adept-inference\n```\n\nFor the chat model:\n```bash\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```\nThen, model can be loaded via:\n\n```py\nfrom transformers import FuyuConfig, FuyuForCausalLM\nmodel_config = FuyuConfig()\nmodel = FuyuForCausalLM(model_config).from_pretrained('/output/path')\n```\n\nInputs need to be passed through a specific Processor to have the correct formats.\nA processor requires an image_processor and a tokenizer. Hence, inputs can be loaded via:\n\n```py\nfrom PIL import Image\nfrom transformers import AutoTokenizer\nfrom transformers.models.fuyu.processing_fuyu import FuyuProcessor\nfrom transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n\n\ntokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\nimage_processor = FuyuImageProcessor()\n\n\nprocessor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\ntext_prompt = \"Generate a coco-style caption.\\\\n\"\n\nbus_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"",
  "bus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))\ninputs_to_model = processor(images=bus_image_pil, text=text_prompt)\n\n\n```\n\nThis model was contributed by [Molbap](https://huggingface.co/Molbap).\nThe original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).\n\n- Fuyu uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\nThe `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n\n- The authors suggest to use the following prompt for image captioning: `f\"Generate a coco-style caption.\\\\n\"`\n\n\n## FuyuConfig\n\n[[autodoc]] FuyuConfig\n\n## FuyuForCausalLM\n\n[[autodoc]] FuyuForCausalLM\n- forward\n\n## FuyuImageProcessor\n\n[[autodoc]] FuyuImageProcessor\n- __call__\n\n## FuyuProcessor\n\n[[autodoc]] FuyuProcessor\n- __call__",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# myt5\n\n## Overview\n\nThe myt5 model was proposed in [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://arxiv.org/pdf/2403.10691.pdf) by Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer.\nMyT5 (**My**te **T5**) is a multilingual language model based on T5 architecture.",
  "The model uses a **m**orphologically-driven **byte** (**MYTE**) representation described in our paper.\n**MYTE** uses codepoints corresponding to morphemes in contrast to characters used in UTF-8 encoding.\nAs a pre-requisite, we used unsupervised morphological segmentation ([Morfessor](https://aclanthology.org/E14-2006.pdf)) to obtain morpheme inventories for 99 languages.\nHowever, the morphological segmentation step is not needed when using the pre-defined morpheme inventory from the hub (see: [Tomli/myt5-base](https://huggingface.co/Tomlim/myt5-base)).\n\nThe abstract from the paper is the following:",
  "*A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world’s writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages.*\n\nThis model was contributed by [Tomasz Limisiewicz](https://huggingface.co/Tomlim).\nThe original code can be found [here](https://github.com/tomlimi/MYTE).\n\n## MyT5Tokenizer",
  "[[autodoc]] MyT5Tokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## MyT5Tokenizer\n\n[[autodoc]] MyT5Tokenizer",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Vision Encoder Decoder Models\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe [`VisionEncoderDecoderModel`] can be used to initialize an image-to-text model with any\npretrained Transformer-based vision model as the encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit), [Swin](swin))\nand any pretrained language model as the decoder (*e.g.* [RoBERTa](roberta), [GPT2](gpt2), [BERT](bert), [DistilBERT](distilbert)).\n\nThe effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for\nexample) [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\nZhoujun Li, Furu Wei.\n\nAfter such a [`VisionEncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like any other models (see the examples below\nfor more information).",
  "An example application is image captioning, in which the encoder is used to encode the image, after which an autoregressive language model generates\nthe caption. Another example is optical character recognition. Refer to [TrOCR](trocr), which is an instance of [`VisionEncoderDecoderModel`].\n\n## Randomly initializing `VisionEncoderDecoderModel` from model configurations.\n\n[`VisionEncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`ViTModel`] configuration for the encoder\nand the default [`BertForCausalLM`] configuration for the decoder.\n\n```python\n>>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel\n\n>>> config_encoder = ViTConfig()\n>>> config_decoder = BertConfig()\n\n>>> config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n>>> model = VisionEncoderDecoderModel(config=config)\n```\n\n## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder.",
  "[`VisionEncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained Transformer-based vision model, *e.g.* [Swin](swin), can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the decoder.\nDepending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.\nInitializing [`VisionEncoderDecoderModel`] from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder).\nTo do so, the `VisionEncoderDecoderModel` class provides a [`VisionEncoderDecoderModel.from_encoder_decoder_pretrained`] method.\n\n```python\n>>> from transformers import VisionEncoderDecoderModel\n\n>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(",
  "...     \"microsoft/swin-base-patch4-window7-224-in22k\", \"google-bert/bert-base-uncased\"\n... )\n```\n\n## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference.\n\nTo load fine-tuned checkpoints of the `VisionEncoderDecoderModel` class, [`VisionEncoderDecoderModel`] provides the `from_pretrained(...)` method just like any other model architecture in Transformers.\n\nTo perform inference, one uses the [`generate`] method, which allows to autoregressively generate text. This method supports various forms of decoding, such as greedy, beam search and multinomial sampling.\n\n```python\n>>> import requests\n>>> from PIL import Image\n\n>>> from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n\n>>> # load a fine-tuned image captioning model and corresponding tokenizer and image processor\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n>>> tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n>>> image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n>>> # let's perform inference on an image",
  ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> # autoregressively generate caption (uses greedy decoding by default)\n>>> generated_ids = model.generate(pixel_values)\n>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\na cat laying on a blanket next to a cat laying on a bed\n```\n\n## Loading a PyTorch checkpoint into `TFVisionEncoderDecoderModel`.\n\n[`TFVisionEncoderDecoderModel.from_pretrained`] currently doesn't support initializing the model from a\nPyTorch checkpoint. Passing `from_pt=True` to this method will throw an exception. If there are only PyTorch\ncheckpoints for a particular vision encoder-decoder model, a workaround is:\n\n```python\n>>> from transformers import VisionEncoderDecoderModel, TFVisionEncoderDecoderModel\n\n>>> _model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n>>> _model.encoder.save_pretrained(\"./encoder\")\n>>> _model.decoder.save_pretrained(\"./decoder\")",
  ">>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n... )\n>>> # This is only for copying some specific attributes of this particular model.\n>>> model.config = _model.config\n```\n\n## Training\n\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other encoder-decoder model on a dataset of (image, text) pairs.\nAs you can see, only 2 inputs are required for the model in order to compute a loss: `pixel_values` (which are the\nimages) and `labels` (which are the `input_ids` of the encoded target sequence).\n\n```python\n>>> from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n\n>>> image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"google/vit-base-patch16-224-in21k\", \"google-bert/bert-base-uncased\"\n... )\n\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id",
  ">>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n>>> pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> labels = tokenizer(\n...     \"an image of two cats chilling on a couch\",\n...     return_tensors=\"pt\",\n... ).input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(pixel_values=pixel_values, labels=labels).loss\n```\n\nThis model was contributed by [nielsr](https://github.com/nielsrogge). This model's TensorFlow and Flax versions\nwere contributed by [ydshieh](https://github.com/ydshieh).\n\n## VisionEncoderDecoderConfig\n\n[[autodoc]] VisionEncoderDecoderConfig\n\n<frameworkcontent>\n<pt>\n\n## VisionEncoderDecoderModel\n\n[[autodoc]] VisionEncoderDecoderModel\n- forward\n- from_encoder_decoder_pretrained\n\n</pt>\n<tf>\n\n## TFVisionEncoderDecoderModel\n\n[[autodoc]] TFVisionEncoderDecoderModel\n- call\n- from_encoder_decoder_pretrained\n\n</tf>\n<jax>\n\n## FlaxVisionEncoderDecoderModel\n\n[[autodoc]] FlaxVisionEncoderDecoderModel\n- __call__\n- from_encoder_decoder_pretrained\n\n</jax>\n</frameworkcontent>",
  "# Cohere\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n[C4AI Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model developed by Cohere and Cohere For AI. It has advanced capabilities optimized for various use cases, including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities that can use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise-relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.",
  "The model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n\nThe model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n## Usage tips\nThe model and tokenizer can be loaded via:\n\n```python\n# pip install transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\n# Format message with the command-r chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n\ngen_tokens = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)",
  "gen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n```\n\n## Cohere2Config\n\n[[autodoc]] Cohere2Config\n\n## Cohere2Model\n\n[[autodoc]] Cohere2Model\n- forward\n\n\n## Cohere2ForCausalLM\n\n[[autodoc]] Cohere2ForCausalLM\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MatCha\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662), from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.\n\nThe abstract of the paper states the following:",
  "*Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.*\n\n## Model description",
  "MatCha is a model that is trained using `Pix2Struct` architecture. You can find more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).\nMatCha is a Visual Question Answering subset of `Pix2Struct` architecture. It renders the input question on the image and predicts the answer.\n\n## Usage\n\nCurrently 6 checkpoints are available for MatCha:\n\n- `google/matcha`: the base MatCha model, used to fine-tune MatCha on downstream tasks\n- `google/matcha-chartqa`: MatCha model fine-tuned on ChartQA dataset. It can be used to answer questions about charts.\n- `google/matcha-plotqa-v1`: MatCha model fine-tuned on PlotQA dataset. It can be used to answer questions about plots.\n- `google/matcha-plotqa-v2`: MatCha model fine-tuned on PlotQA dataset. It can be used to answer questions about plots.\n- `google/matcha-chart2text-statista`: MatCha model fine-tuned on Statista dataset.\n- `google/matcha-chart2text-pew`: MatCha model fine-tuned on Pew dataset.",
  "The models finetuned on `chart2text-pew` and `chart2text-statista` are more suited for summarization, whereas the models finetuned on `plotqa` and `chartqa` are more suited for question answering.\n\nYou can use these models as follows (example on a ChatQA dataset):\n\n```python\nfrom transformers import AutoProcessor, Pix2StructForConditionalGeneration\nimport requests\nfrom PIL import Image\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/matcha-chartqa\").to(0)\nprocessor = AutoProcessor.from_pretrained(\"google/matcha-chartqa\")\nurl = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/20294671002019.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(images=image, text=\"Is the sum of all 4 places greater than Laos?\", return_tensors=\"pt\").to(0)\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n```\n\n## Fine-tuning",
  "To fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb). For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor and cosine learning rate scheduler leads to faster convergence:\n```python\nfrom transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\n\noptimizer = Adafactor(self.parameters(), scale_parameter=False, relative_step=False, lr=0.01, weight_decay=1e-05)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=40000)\n```\n\n<Tip>\n\nMatCha is a model that is trained using `Pix2Struct` architecture. You can find more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).\n\n</Tip>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SwitchTransformers\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.",
  "The Switch Transformer model uses a sparse T5 encoder-decoder architecture, where the MLP are replaced by a Mixture of Experts (MoE). A routing mechanism (top 1 in this case) associates each token to one of the expert, where each expert is a dense MLP. While switch transformers have a lot more weights than their equivalent dense models, the sparsity allows better scaling and better finetuning performance at scale.\nDuring a forward pass, only a fraction of the weights are used. The routing mechanism allows the model to select relevant weights on the fly which increases the model capacity without increasing the number of operations.\n\nThe abstract from the paper is the following:",
  "*In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.*",
  "This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe).\n\n## Usage tips\n\n- SwitchTransformers uses the [`T5Tokenizer`], which can be loaded directly from each model's repository.\n- The released weights are pretrained on English [Masked Language Modeling](https://moon-ci-docs.huggingface.co/docs/transformers/pr_19323/en/glossary#general-terms) task, and should be finetuned.\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## SwitchTransformersConfig\n\n[[autodoc]] SwitchTransformersConfig\n\n## SwitchTransformersTop1Router\n\n[[autodoc]] SwitchTransformersTop1Router\n- _compute_router_probabilities\n- forward\n\n## SwitchTransformersSparseMLP\n\n[[autodoc]] SwitchTransformersSparseMLP\n- forward\n\n## SwitchTransformersModel\n\n[[autodoc]] SwitchTransformersModel\n- forward\n\n## SwitchTransformersForConditionalGeneration\n\n[[autodoc]] SwitchTransformersForConditionalGeneration\n- forward\n\n## SwitchTransformersEncoderModel",
  "[[autodoc]] SwitchTransformersEncoderModel\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# VITS\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe VITS model was proposed in [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.\n\nVITS (**V**ariational **I**nference with adversarial learning for end-to-end **T**ext-to-**S**peech) is an end-to-end",
  "speech synthesis model that predicts a speech waveform conditional on an input text sequence. It is a conditional variational\nautoencoder (VAE) comprised of a posterior encoder, decoder, and conditional prior.\n\nA set of spectrogram-based acoustic features are predicted by the flow-based module, which is formed of a Transformer-based\ntext encoder and multiple coupling layers. The spectrogram is decoded using a stack of transposed convolutional layers,\nmuch in the same style as the HiFi-GAN vocoder. Motivated by the one-to-many nature of the TTS problem, where the same text\ninput can be spoken in multiple ways, the model also includes a stochastic duration predictor, which allows the model to\nsynthesise speech with different rhythms from the same input text.\n\nThe model is trained end-to-end with a combination of losses derived from variational lower bound and adversarial training.\nTo improve the expressiveness of the model, normalizing flows are applied to the conditional prior distribution. During\ninference, the text encodings are up-sampled based on the duration prediction module, and then mapped into the",
  "waveform using a cascade of the flow module and HiFi-GAN decoder. Due to the stochastic nature of the duration predictor,\nthe model is non-deterministic, and thus requires a fixed seed to generate the same speech waveform.\n\nThe abstract from the paper is the following:",
  "*Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.*\n\nThis model can also be used with TTS checkpoints from [Massively Multilingual Speech (MMS)](https://arxiv.org/abs/2305.13516)",
  "as these checkpoints use the same architecture and a slightly modified tokenizer.\n\nThis model was contributed by [Matthijs](https://huggingface.co/Matthijs) and [sanchit-gandhi](https://huggingface.co/sanchit-gandhi). The original code can be found [here](https://github.com/jaywalnut310/vits).\n\n## Usage examples\n\nBoth the VITS and MMS-TTS checkpoints can be used with the same API. Since the flow-based model is non-deterministic, it\nis good practice to set a seed to ensure reproducibility of the outputs. For languages with a Roman alphabet,\nsuch as English or French, the tokenizer can be used directly to pre-process the text inputs. The following code example\nruns a forward pass using the MMS-TTS English checkpoint:\n\n```python\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n\ninputs = tokenizer(text=\"Hello - my dog is cute\", return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\n\nwith torch.no_grad():\noutputs = model(**inputs)\n\nwaveform = outputs.waveform[0]\n```\n\nThe resulting waveform can be saved as a `.wav` file:\n\n```python",
  "import scipy\n\nscipy.io.wavfile.write(\"techno.wav\", rate=model.config.sampling_rate, data=waveform)\n```\n\nOr displayed in a Jupyter Notebook / Google Colab:\n\n```python\nfrom IPython.display import Audio\n\nAudio(waveform, rate=model.config.sampling_rate)\n```\n\nFor certain languages with a non-Roman alphabet, such as Arabic, Mandarin or Hindi, the [`uroman`](https://github.com/isi-nlp/uroman)\nperl package is required to pre-process the text inputs to the Roman alphabet.\n\nYou can check whether you require the `uroman` package for your language by inspecting the `is_uroman` attribute of\nthe pre-trained `tokenizer`:\n\n```python\nfrom transformers import VitsTokenizer\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nprint(tokenizer.is_uroman)\n```\nIf the is_uroman attribute is `True`, the tokenizer will automatically apply the `uroman` package to your text inputs, but you need to install uroman if not already installed using:\n```\npip install --upgrade uroman\n```\nNote: Python version required to use `uroman` as python package should be >= `3.10`.\nYou can use the tokenizer as usual without any additional preprocessing steps:\n```python\nimport torch",
  "from transformers import VitsTokenizer, VitsModel, set_seed\nimport os\nimport subprocess\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-kor\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-kor\")\ntext = \"이봐 무슨 일이야\"\ninputs = tokenizer(text=text, return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\nwith torch.no_grad():\noutputs = model(inputs[\"input_ids\"])\n\nwaveform = outputs.waveform[0]\n```\nIf you don't want to upgrade to python >= `3.10`, then you can use the `uroman` perl package to pre-process the text inputs to the Roman alphabet.\nTo do this, first clone the uroman repository to your local machine and set the bash variable `UROMAN` to the local path:\n\n\n```bash\ngit clone https://github.com/isi-nlp/uroman.git\ncd uroman\nexport UROMAN=$(pwd)\n```\n\nYou can then pre-process the text input using the following code snippet. You can either rely on using the bash variable\n`UROMAN` to point to the uroman repository, or you can pass the uroman directory as an argument to the `uromanize` function:\n\n```python\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\nimport os\nimport subprocess",
  "tokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-kor\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-kor\")\n\ndef uromanize(input_string, uroman_path):\n\"\"\"Convert non-Roman strings to Roman using the `uroman` perl package.\"\"\"\nscript_path = os.path.join(uroman_path, \"bin\", \"uroman.pl\")\n\ncommand = [\"perl\", script_path]\n\nprocess = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n# Execute the perl command\nstdout, stderr = process.communicate(input=input_string.encode())\n\nif process.returncode != 0:\nraise ValueError(f\"Error {process.returncode}: {stderr.decode()}\")\n\n# Return the output as a string and skip the new-line character at the end\nreturn stdout.decode()[:-1]\n\ntext = \"이봐 무슨 일이야\"\nuromanized_text = uromanize(text, uroman_path=os.environ[\"UROMAN\"])\n\ninputs = tokenizer(text=uromanized_text, return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\nwith torch.no_grad():\noutputs = model(inputs[\"input_ids\"])\n\nwaveform = outputs.waveform[0]\n```\n\n## VitsConfig\n\n[[autodoc]] VitsConfig\n\n## VitsTokenizer\n\n[[autodoc]] VitsTokenizer\n- __call__\n- save_vocabulary\n\n## VitsModel\n\n[[autodoc]] VitsModel\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RAG\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n</div>\n\n## Overview",
  "Retrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.\n\nIt is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\n\nThe abstract from the paper is the following:\n\n*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind",
  "task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation",
  "tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\nparametric-only seq2seq baseline.*\n\nThis model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models.\nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq\nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt\nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n- forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n- forward\n- generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n- forward\n- generate\n\n</pt>\n<tf>\n\n## TFRagModel\n\n[[autodoc]] TFRagModel\n- call",
  "## TFRagSequenceForGeneration\n\n[[autodoc]] TFRagSequenceForGeneration\n- call\n- generate\n\n## TFRagTokenForGeneration\n\n[[autodoc]] TFRagTokenForGeneration\n- call\n- generate\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MobileBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The MobileBERT model was proposed in [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny\nZhou. It's a bidirectional transformer based on the BERT model, which is compressed and accelerated using several\napproaches.\n\nThe abstract from the paper is the following:\n\n*Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds\nof millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot\nbe deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating\nthe popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to\nvarious downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while\nequipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.",
  "To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE\nmodel. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is\n4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the\nnatural language inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6 lower than BERT_BASE), and 62 ms\nlatency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of\n90.0/79.2 (1.5/2.1 higher than BERT_BASE).*\n\nThis model was contributed by [vshampor](https://huggingface.co/vshampor). The original code can be found [here](https://github.com/google-research/google-research/tree/master/mobilebert).\n\n## Usage tips\n\n- MobileBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left.\n- MobileBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore",
  "efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\nwith a causal language modeling (CLM) objective are better in that regard.\n\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## MobileBertConfig\n\n[[autodoc]] MobileBertConfig\n\n## MobileBertTokenizer\n\n[[autodoc]] MobileBertTokenizer\n\n## MobileBertTokenizerFast\n\n[[autodoc]] MobileBertTokenizerFast\n\n## MobileBert specific outputs\n\n[[autodoc]] models.mobilebert.modeling_mobilebert.MobileBertForPreTrainingOutput\n\n[[autodoc]] models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## MobileBertModel\n\n[[autodoc]] MobileBertModel\n- forward\n\n## MobileBertForPreTraining\n\n[[autodoc]] MobileBertForPreTraining\n- forward\n\n## MobileBertForMaskedLM\n\n[[autodoc]] MobileBertForMaskedLM\n- forward\n\n## MobileBertForNextSentencePrediction",
  "[[autodoc]] MobileBertForNextSentencePrediction\n- forward\n\n## MobileBertForSequenceClassification\n\n[[autodoc]] MobileBertForSequenceClassification\n- forward\n\n## MobileBertForMultipleChoice\n\n[[autodoc]] MobileBertForMultipleChoice\n- forward\n\n## MobileBertForTokenClassification\n\n[[autodoc]] MobileBertForTokenClassification\n- forward\n\n## MobileBertForQuestionAnswering\n\n[[autodoc]] MobileBertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFMobileBertModel\n\n[[autodoc]] TFMobileBertModel\n- call\n\n## TFMobileBertForPreTraining\n\n[[autodoc]] TFMobileBertForPreTraining\n- call\n\n## TFMobileBertForMaskedLM\n\n[[autodoc]] TFMobileBertForMaskedLM\n- call\n\n## TFMobileBertForNextSentencePrediction\n\n[[autodoc]] TFMobileBertForNextSentencePrediction\n- call\n\n## TFMobileBertForSequenceClassification\n\n[[autodoc]] TFMobileBertForSequenceClassification\n- call\n\n## TFMobileBertForMultipleChoice\n\n[[autodoc]] TFMobileBertForMultipleChoice\n- call\n\n## TFMobileBertForTokenClassification\n\n[[autodoc]] TFMobileBertForTokenClassification\n- call\n\n## TFMobileBertForQuestionAnswering\n\n[[autodoc]] TFMobileBertForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SmolVLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "SmolVLM2 is an adaptation of the Idefics3 model with two main differences:\n\n- It uses SmolLM2 for the text model.\n- It supports multi-image and video inputs\n\n## Usage tips\n\nInput images are processed either by upsampling (if resizing is enabled) or at their original resolution. The resizing behavior depends on two parameters: do_resize and size.\n\nVideos should not be upsampled.\n\nIf `do_resize` is set to `True`, the model resizes images so that the longest edge is 4*512 pixels by default.\nThe default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 512}` is the default, but you can change it to a different value if needed.\n\nHere’s how to control resizing and set a custom size:\n```python\nimage_processor = SmolVLMImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 512}, max_image_size=512)\n```\n\nAdditionally, the `max_image_size` parameter, which controls the size of each square patch the image is decomposed into, is set to 512 by default but can be adjusted as needed. After resizing (if applicable), the image processor decomposes the images into square patches based on the `max_image_size` parameter.",
  "This model was contributed by [orrzohar](https://huggingface.co/orrzohar).\n\n\n\n## Usage example\n\n### Single Media inference\n\nThe model can accept both images and videos as input, but you should use only one of the modalities at a time. Here's an example code for that.\n\n```python\nimport torch\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\n\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\")\nmodel = AutoModelForImageTextToText.from_pretrained(\n\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"cuda\"\n)\n\nconversation = [\n{\n\"role\": \"user\",\n\"content\":[\n{\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"}\n]\n}\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)\nprint(generated_texts)\n\n\n# Video\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [",
  "{\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n{\"type\": \"text\", \"text\": \"Describe this video in detail\"}\n]\n},\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=100)\ngenerated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\nprint(generated_texts[0])\n```\n\n### Batch Mixed Media Inference\n\nThe model can batch inputs composed of several images/videos and text. Here is an example.\n\n```python\nimport torch\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\n\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\")\nmodel = AutoModelForImageTextToText.from_pretrained(\n\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"cuda\"\n)\n\n# Conversation for the first image\nconversation1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"}\n]\n}\n]\n\n# Conversation with two images\nconversation2 = [\n{",
  "\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image.jpg\"},\n{\"type\": \"image\", \"path\": \"/path/to/image.jpg\"},\n{\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n]\n}\n]\n\n# Conversation with pure text\nconversation3 = [\n{\"role\": \"user\",\"content\": \"who are you?\"}\n]\n\n\nconversations = [conversation1, conversation2, conversation3]\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=100)\ngenerated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\nprint(generated_texts[0])\n```\n\n## SmolVLMConfig\n\n[[autodoc]] SmolVLMConfig\n\n## SmolVLMVisionConfig\n\n[[autodoc]] SmolVLMVisionConfig\n\n## Idefics3VisionTransformer\n\n[[autodoc]] SmolVLMVisionTransformer\n\n## SmolVLMModel\n\n[[autodoc]] SmolVLMModel\n- forward\n\n## SmolVLMForConditionalGeneration\n\n[[autodoc]] SmolVLMForConditionalGeneration\n- forward\n\n\n## SmolVLMImageProcessor\n[[autodoc]] SmolVLMImageProcessor\n- preprocess\n\n\n## SmolVLMProcessor\n[[autodoc]] SmolVLMProcessor\n- __call__",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Pyramid Vision Transformer V2 (PVTv2)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe PVTv2 model was proposed in",
  "[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/abs/2106.13797) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. As an improved variant of PVT, it eschews position embeddings, relying instead on positional information encoded through zero-padding and overlapping patch embeddings. This lack of reliance on position embeddings simplifies the architecture, and enables running inference at any resolution without needing to interpolate them.\n\nThe PVTv2 encoder structure has been successfully deployed to achieve state-of-the-art scores in [Segformer](https://arxiv.org/abs/2105.15203) for semantic segmentation, [GLPN](https://arxiv.org/abs/2201.07436) for monocular depth, and [Panoptic Segformer](https://arxiv.org/abs/2109.03814) for panoptic segmentation.",
  "PVTv2 belongs to a family of models called [hierarchical transformers](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f) , which make adaptations to transformer layers in order to generate multi-scale feature maps. Unlike the columnal structure of Vision Transformer ([ViT](https://arxiv.org/abs/2010.11929)) which loses fine-grained detail, multi-scale feature maps are known preserve this detail and aid performance in dense prediction tasks. In the case of PVTv2, this is achieved by generating image patch tokens using 2D convolution with overlapping kernels in each encoder layer.\n\nThe multi-scale features of hierarchical transformers allow them to be easily swapped in for traditional workhorse computer vision backbone models like ResNet in larger architectures. Both Segformer and Panoptic Segformer demonstrated that configurations using PVTv2 for a backbone consistently outperformed those with similarly sized ResNet backbones.",
  "Another powerful feature of the PVTv2 is the complexity reduction in the self-attention layers called Spatial Reduction Attention (SRA), which uses 2D convolution layers to project hidden states to a smaller resolution before attending to them with the queries, improving the $O(n^2)$ complexity of self-attention to $O(n^2/R)$, with $R$ being the spatial reduction ratio (`sr_ratio`, aka kernel size and stride in the 2D convolution).\n\nSRA was introduced in PVT, and is the default attention complexity reduction method used in PVTv2. However, PVTv2 also introduced the option of using a self-attention mechanism with linear complexity related to image size, which they called \"Linear SRA\". This method uses average pooling to reduce the hidden states to a fixed size that is invariant to their original resolution (although this is inherently more lossy than regular SRA). This option can be enabled by setting `linear_attention` to `True` in the PVTv2Config.\n\n### Abstract from the paper:",
  "*Transformer recently has presented encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs, including (1) linear complexity attention layer, (2) overlapping patch embedding, and (3) convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linear and achieves significant improvements on fundamental vision tasks such as classification, detection, and segmentation. Notably, the proposed PVT v2 achieves comparable or better performances than recent works such as Swin Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision. Code is available at https://github.com/whai362/PVT.*\n\nThis model was contributed by [FoamoftheSea](https://huggingface.co/FoamoftheSea). The original code can be found [here](https://github.com/whai362/PVT).\n\n## Usage tips",
  "- [PVTv2](https://arxiv.org/abs/2106.13797) is a hierarchical transformer model which has demonstrated powerful performance in image classification and multiple other tasks, used as a backbone for semantic segmentation in [Segformer](https://arxiv.org/abs/2105.15203), monocular depth estimation in [GLPN](https://arxiv.org/abs/2201.07436), and panoptic segmentation in [Panoptic Segformer](https://arxiv.org/abs/2109.03814), consistently showing higher performance than similar ResNet configurations.\n- Hierarchical transformers like PVTv2 achieve superior data and parameter efficiency on image data compared with pure transformer architectures by incorporating design elements of convolutional neural networks (CNNs) into their encoders. This creates a best-of-both-worlds architecture that infuses the useful inductive biases of CNNs like translation equivariance and locality into the network while still enjoying the benefits of dynamic data response and global relationship modeling provided by the self-attention mechanism of [transformers](https://arxiv.org/abs/1706.03762).",
  "- PVTv2 uses overlapping patch embeddings to create multi-scale feature maps, which are infused with location information using zero-padding and depth-wise convolutions.\n- To reduce the complexity in the attention layers, PVTv2 performs a spatial reduction on the hidden states using either strided 2D convolution (SRA) or fixed-size average pooling (Linear SRA). Although inherently more lossy, Linear SRA provides impressive performance with a linear complexity with respect to image size. To use Linear SRA in the self-attention layers, set `linear_attention=True` in the `PvtV2Config`.\n- [`PvtV2Model`] is the hierarchical transformer encoder (which is also often referred to as Mix Transformer or MiT in the literature). [`PvtV2ForImageClassification`] adds a simple classifier head on top to perform Image Classification. [`PvtV2Backbone`] can be used with the [`AutoBackbone`] system in larger architectures like Deformable DETR.\n- ImageNet pretrained weights for all model sizes can be found on the [hub](https://huggingface.co/models?other=pvt_v2).",
  "The best way to get started with the PVTv2 is to load the pretrained checkpoint with the size of your choosing using `AutoModelForImageClassification`:\n```python\nimport requests\nimport torch\n\nfrom transformers import AutoModelForImageClassification, AutoImageProcessor\nfrom PIL import Image\n\nmodel = AutoModelForImageClassification.from_pretrained(\"OpenGVLab/pvt_v2_b0\")\nimage_processor = AutoImageProcessor.from_pretrained(\"OpenGVLab/pvt_v2_b0\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessed = image_processor(image)\noutputs = model(torch.tensor(processed[\"pixel_values\"]))\n```\n\nTo use the PVTv2 as a backbone for more complex architectures like DeformableDETR, you can use AutoBackbone (this model would need fine-tuning as you're replacing the backbone in the pretrained model):\n\n```python\nimport requests\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForObjectDetection, AutoImageProcessor\nfrom PIL import Image\n\nmodel = AutoModelForObjectDetection.from_config(\nconfig=AutoConfig.from_pretrained(\n\"SenseTime/deformable-detr\",",
  "backbone_config=AutoConfig.from_pretrained(\"OpenGVLab/pvt_v2_b5\"),\nuse_timm_backbone=False\n),\n)\n\nimage_processor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessed = image_processor(image)\noutputs = model(torch.tensor(processed[\"pixel_values\"]))\n```\n\n[PVTv2](https://github.com/whai362/PVT/tree/v2) performance on ImageNet-1K by model size (B0-B5):\n\n| Method           | Size | Acc@1 | #Params (M) |\n|------------------|:----:|:-----:|:-----------:|\n| PVT-V2-B0        |  224 |  70.5 |     3.7     |\n| PVT-V2-B1        |  224 |  78.7 |     14.0    |\n| PVT-V2-B2-Linear |  224 |  82.1 |     22.6    |\n| PVT-V2-B2        |  224 |  82.0 |     25.4    |\n| PVT-V2-B3        |  224 |  83.1 |     45.2    |\n| PVT-V2-B4        |  224 |  83.6 |     62.6    |\n| PVT-V2-B5        |  224 |  83.8 |     82.0    |\n\n\n## PvtV2Config\n\n[[autodoc]] PvtV2Config\n\n## PvtForImageClassification\n\n[[autodoc]] PvtV2ForImageClassification\n- forward\n\n## PvtModel\n\n[[autodoc]] PvtV2Model\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DepthPro\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DepthPro model was proposed in [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/abs/2410.02073) by Aleksei Bochkovskii, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.",
  "DepthPro is a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. It employs a multi-scale Vision Transformer (ViT)-based architecture, where images are downsampled, divided into patches, and processed using a shared Dinov2 encoder. The extracted patch-level features are merged, upsampled, and refined using a DPT-like fusion stage, enabling precise depth estimation.\n\nThe abstract from the paper is the following:",
  "*We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_teaser.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> DepthPro Outputs. Taken from the <a href=\"https://github.com/apple/ml-depth-pro\" target=\"_blank\">official code</a>. </small>\n\nThis model was contributed by [geetu040](https://github.com/geetu040). The original code can be found [here](https://github.com/apple/ml-depth-pro).\n\n## Usage Tips\n\nThe DepthPro model processes an input image by first downsampling it at multiple scales and splitting each scaled version into patches. These patches are then encoded using a shared Vision Transformer (ViT)-based Dinov2 patch encoder, while the full image is processed by a separate image encoder. The extracted patch features are merged into feature maps, upsampled, and fused using a DPT-like decoder to generate the final depth estimation. If enabled, an additional Field of View (FOV) encoder processes the image for estimating the camera's field of view, aiding in depth accuracy.\n\n```py\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n>>> from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\n\n>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'",
  ">>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\n>>> model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").to(device)\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs, target_sizes=[(image.height, image.width)],\n... )\n\n>>> field_of_view = post_processed_output[0][\"field_of_view\"]\n>>> focal_length = post_processed_output[0][\"focal_length\"]\n>>> depth = post_processed_output[0][\"predicted_depth\"]\n>>> depth = (depth - depth.min()) / depth.max()\n>>> depth = depth * 255.\n>>> depth = depth.detach().cpu().numpy()\n>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n```\n\n### Architecture and Configuration\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> DepthPro architecture. Taken from the <a href=\"https://arxiv.org/abs/2410.02073\" target=\"_blank\">original paper</a>. </small>",
  "The `DepthProForDepthEstimation` model uses a `DepthProEncoder`, for encoding the input image and a `FeatureFusionStage` for fusing the output features from encoder.\n\nThe `DepthProEncoder` further uses two encoders:\n- `patch_encoder`\n- Input image is scaled with multiple ratios, as specified in the `scaled_images_ratios` configuration.\n- Each scaled image is split into smaller **patches** of size `patch_size` with overlapping areas determined by `scaled_images_overlap_ratios`.\n- These patches are processed by the **`patch_encoder`**\n- `image_encoder`\n- Input image is also rescaled to `patch_size` and processed by the **`image_encoder`**\n\nBoth these encoders can be configured via `patch_model_config` and `image_model_config` respectively, both of which are separate `Dinov2Model` by default.\n\nOutputs from both encoders (`last_hidden_state`) and selected intermediate states (`hidden_states`) from **`patch_encoder`** are fused by a `DPT`-based `FeatureFusionStage` for depth estimation.\n\n### Field-of-View (FOV) Prediction",
  "The network is supplemented with a focal length estimation head. A small convolutional head ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field-of-view.\n\nThe `use_fov_model` parameter in `DepthProConfig` controls whether **FOV prediction** is enabled. By default, it is set to `False` to conserve memory and computation. When enabled, the **FOV encoder** is instantiated based on the `fov_model_config` parameter, which defaults to a `Dinov2Model`. The `use_fov_model` parameter can also be passed when initializing the `DepthProForDepthEstimation` model.\n\nThe pretrained model at checkpoint `apple/DepthPro-hf` uses the FOV encoder. To use the pretrained-model without FOV encoder, set `use_fov_model=False` when loading the model, which saves computation.\n```py\n>>> from transformers import DepthProForDepthEstimation\n>>> model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", use_fov_model=False)\n```\n\nTo instantiate a new model with FOV encoder, set `use_fov_model=True` in the config.\n```py\n>>> from transformers import DepthProConfig, DepthProForDepthEstimation",
  ">>> config = DepthProConfig(use_fov_model=True)\n>>> model = DepthProForDepthEstimation(config)\n```\n\nOr set `use_fov_model=True` when initializing the model, which overrides the value in config.\n```py\n>>> from transformers import DepthProConfig, DepthProForDepthEstimation\n>>> config = DepthProConfig()\n>>> model = DepthProForDepthEstimation(config, use_fov_model=True)\n```\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```py\nfrom transformers import DepthProForDepthEstimation",
  "model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vit-base-patch16-224` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                         7 |                                         6 |                      1.17 |\n|            2 |                                         8 |                                         6 |                      1.33 |\n|            4 |                                         8 |                                         6 |                      1.33 |",
  "|            8 |                                         8 |                                         6 |                      1.33 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DepthPro:\n\n- Research Paper: [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/pdf/2410.02073)\n- Official Implementation: [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)\n- DepthPro Inference Notebook: [DepthPro Inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DepthPro_inference.ipynb)\n- DepthPro for Super Resolution and Image Segmentation\n- Read blog on Medium: [Depth Pro: Beyond Depth](https://medium.com/@raoarmaghanshakir040/depth-pro-beyond-depth-9d822fc557ba)\n- Code on Github: [geetu040/depthpro-beyond-depth](https://github.com/geetu040/depthpro-beyond-depth)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DepthProConfig\n\n[[autodoc]] DepthProConfig\n\n## DepthProImageProcessor",
  "[[autodoc]] DepthProImageProcessor\n- preprocess\n- post_process_depth_estimation\n\n## DepthProImageProcessorFast\n\n[[autodoc]] DepthProImageProcessorFast\n- preprocess\n- post_process_depth_estimation\n\n## DepthProModel\n\n[[autodoc]] DepthProModel\n- forward\n\n## DepthProForDepthEstimation\n\n[[autodoc]] DepthProForDepthEstimation\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\nLicense. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\nspecific language governing permissions and limitations under the License. -->\n\n# TrOCR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe TrOCR model was proposed in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained\nModels](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,",
  "Zhoujun Li, Furu Wei. TrOCR consists of an image Transformer encoder and an autoregressive text Transformer decoder to\nperform [optical character recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition).\n\nThe abstract from the paper is the following:\n\n*Text recognition is a long-standing research problem for document digitalization. Existing approaches for text recognition\nare usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language\nmodel is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the\nTransformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but\neffective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments\nshow that the TrOCR model outperforms the current state-of-the-art models on both printed and handwritten text recognition\ntasks.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/trocr_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> TrOCR architecture. Taken from the <a href=\"https://arxiv.org/abs/2109.10282\">original paper</a>. </small>\n\nPlease refer to the [`VisionEncoderDecoder`] class on how to use this model.\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n[here](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr).\n\n## Usage tips\n\n- The quickest way to get started with TrOCR is by checking the [tutorial\nnotebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/TrOCR), which show how to use the model\nat inference time as well as fine-tuning on custom data.\n- TrOCR is pre-trained in 2 stages before being fine-tuned on downstream datasets. It achieves state-of-the-art results\non both printed (e.g. the [SROIE dataset](https://paperswithcode.com/dataset/sroie) and handwritten (e.g. the [IAM\nHandwriting dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database>) text recognition tasks. For more",
  "information, see the [official models](https://huggingface.co/models?other=trocr>).\n- TrOCR is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with TrOCR. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on [Accelerating Document AI](https://huggingface.co/blog/document-ai) with TrOCR.\n- A blog post on how to [Document AI](https://github.com/philschmid/document-ai-transformers) with TrOCR.\n- A notebook on how to [finetune TrOCR on IAM Handwriting Database using Seq2SeqTrainer](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb).",
  "- A notebook on [inference with TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb) and Gradio demo.\n- A notebook on [finetune TrOCR on the IAM Handwriting Database](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb) using native PyTorch.\n- A notebook on [evaluating TrOCR on the IAM test set](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb).\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- [Casual language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) task guide.\n\n⚡️ Inference\n\n- An interactive-demo on [TrOCR handwritten character recognition](https://huggingface.co/spaces/nielsr/TrOCR-handwritten).\n\n## Inference\n\nTrOCR's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.",
  "The [`ViTImageProcessor`/`DeiTImageProcessor`] class is responsible for preprocessing the input image and\n[`RobertaTokenizer`/`XLMRobertaTokenizer`] decodes the generated target tokens to the target string. The\n[`TrOCRProcessor`] wraps [`ViTImageProcessor`/`DeiTImageProcessor`] and [`RobertaTokenizer`/`XLMRobertaTokenizer`]\ninto a single instance to both extract the input features and decode the predicted token ids.\n\n- Step-by-step Optical Character Recognition (OCR)\n\n``` py\n>>> from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n>>> # load image from the IAM dataset\n>>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> generated_ids = model.generate(pixel_values)\n\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```",
  "See the [model hub](https://huggingface.co/models?filter=trocr) to look for TrOCR checkpoints.\n\n## TrOCRConfig\n\n[[autodoc]] TrOCRConfig\n\n## TrOCRProcessor\n\n[[autodoc]] TrOCRProcessor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## TrOCRForCausalLM\n\n[[autodoc]] TrOCRForCausalLM\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BARTpho\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n\nThe abstract from the paper is the following:\n\n*We present BARTpho with two versions -- BARTpho_word and BARTpho_syllable -- the first public large-scale monolingual\nsequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the \"large\" architecture and pre-training\nscheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments\non a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho\noutperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future\nresearch and applications of generative Vietnamese NLP tasks.*\n\nThis model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/BARTpho).\n\n## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer",
  ">>> bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n\n>>> line = \"Chúng tôi là những nghiên cứu viên.\"\n\n>>> input_ids = tokenizer(line, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     features = bartpho(**input_ids)  # Models outputs are now tuples\n\n>>> # With TensorFlow 2.0+:\n>>> from transformers import TFAutoModel\n\n>>> bartpho = TFAutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n>>> input_ids = tokenizer(line, return_tensors=\"tf\")\n>>> features = bartpho(**input_ids)\n```\n\n## Usage tips\n\n- Following mBART, BARTpho uses the \"large\" architecture of BART with an additional layer-normalization layer on top of\nboth the encoder and decoder. Thus, usage examples in the [documentation of BART](bart), when adapting to use\nwith BARTpho, should be adjusted by replacing the BART-specialized classes with the mBART-specialized counterparts.\nFor example:\n\n```python\n>>> from transformers import MBartForConditionalGeneration\n\n>>> bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n>>> TXT = \"Chúng tôi là <mask> nghiên cứu viên.\"",
  ">>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n>>> logits = bartpho(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n>>> print(tokenizer.decode(predictions).split())\n```\n\n- This implementation is only for tokenization: \"monolingual_vocab_file\" consists of Vietnamese-specialized types\nextracted from the pre-trained SentencePiece model \"vocab_file\" that is available from the multilingual XLM-RoBERTa.\nOther languages, if employing this pre-trained multilingual SentencePiece model \"vocab_file\" for subword\nsegmentation, can reuse BartphoTokenizer with their own language-specialized \"monolingual_vocab_file\".\n\n## BartphoTokenizer\n\n[[autodoc]] BartphoTokenizer",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BioGPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The BioGPT model was proposed in [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu. BioGPT is a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch.\n\nThe abstract from the paper is the following:",
  "*Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.*",
  "This model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/BioGPT).\n\n## Usage tips\n\n- BioGPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left.\n- BioGPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows BioGPT to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n- The model can take the `past_key_values` (for PyTorch) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the BioGptForCausalLM.forward() method for more information on its usage.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function",
  "encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import BioGptForCausalLM\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n```\n\nOn a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and `microsoft/biogpt` model with a CausalLM head,\nwe saw the following speedups during training.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).",
  "| num_training_steps | batch_size | seq_len | is cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | sdpa peak mem (MB) | Mem saving (%) |\n|--------------------|------------|---------|---------|----------------------------|---------------------------|-------------|---------------------|--------------------|----------------|\n| 100                | 1          | 128     | False   | 0.038                      | 0.031                     | 21.301      | 1601.862            | 1601.497           | 0.023          |\n| 100                | 1          | 256     | False   | 0.039                      | 0.034                     | 15.084      | 1624.944            | 1625.296           | -0.022         |\n| 100                | 2          | 128     | False   | 0.039                      | 0.033                     | 16.820      | 1624.567            | 1625.296           | -0.045         |\n| 100                | 2          | 256     | False   | 0.065                      | 0.059                     | 10.255      | 1672.164            | 1672.164           | 0.000          |",
  "| 100                | 4          | 128     | False   | 0.062                      | 0.058                     | 6.998       | 1671.435            | 1672.164           | -0.044         |\n| 100                | 4          | 256     | False   | 0.113                      | 0.100                     | 13.316      | 2350.179            | 1848.435           | 27.144         |\n| 100                | 8          | 128     | False   | 0.107                      | 0.098                     | 9.883       | 2098.521            | 1848.435           | 13.530         |\n| 100                | 8          | 256     | False   | 0.222                      | 0.196                     | 13.413      | 3989.980            | 2986.492           | 33.601         |\n\nOn a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and `microsoft/biogpt` model with a simple AutoModel head,\nwe saw the following speedups during inference.\n\n| num_batches | batch_size | seq_len | is cuda | is half | use mask | Per token latency eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem eager (MB) | Mem BT (MB) | Mem saved (%) |",
  "|-------------|------------|---------|---------|---------|----------|------------------------------|-----------------------------|-------------|----------------|--------------|---------------|\n| 50          | 1          | 64      | True    | True    | True     | 0.115                        | 0.098                       | 17.392      | 716.998        | 716.998      | 0.000         |\n| 50          | 1          | 128     | True    | True    | True     | 0.115                        | 0.093                       | 24.640      | 730.916        | 730.916      | 0.000         |\n| 50          | 2          | 64      | True    | True    | True     | 0.114                        | 0.096                       | 19.204      | 730.900        | 730.900      | 0.000         |\n| 50          | 2          | 128     | True    | True    | True     | 0.117                        | 0.095                       | 23.529      | 759.262        | 759.262      | 0.000         |\n| 50          | 4          | 64      | True    | True    | True     | 0.113                        | 0.096                       | 18.325      | 759.229        | 759.229      | 0.000         |",
  "| 50          | 4          | 128     | True    | True    | True     | 0.186                        | 0.178                       | 4.289       | 816.478        | 816.478      | 0.000         |\n\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## BioGptConfig\n\n[[autodoc]] BioGptConfig\n\n\n## BioGptTokenizer\n\n[[autodoc]] BioGptTokenizer\n- save_vocabulary\n\n\n## BioGptModel\n\n[[autodoc]] BioGptModel\n- forward\n\n\n## BioGptForCausalLM\n\n[[autodoc]] BioGptForCausalLM\n- forward\n\n\n## BioGptForTokenClassification\n\n[[autodoc]] BioGptForTokenClassification\n- forward\n\n\n## BioGptForSequenceClassification\n\n[[autodoc]] BioGptForSequenceClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PhiMoE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The PhiMoE model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\n\n### Summary\n\nThe abstract from the Phi-3 paper is the following:",
  "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
  "The original code for PhiMoE can be found [here](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct).\n\n## Usage tips\n\n- This model is very similar to `Mixtral` with the main difference of [`Phi3LongRoPEScaledRotaryEmbedding`], where they are used to extend the context of the rotary embeddings. The query, key and values are fused, and the MLP's up and gate projection layers are also fused.\n- The tokenizer used for this model is identical to the [`LlamaTokenizer`], with the exception of additional tokens.\n\n## How to use PhiMoE\n\n<Tip warning={true}>\n\nPhi-3.5-MoE-instruct has been integrated in the development version (4.44.2.dev) of `transformers`. Until the official version is released through `pip`, ensure that you are doing the following:\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.43.0\n```\n\n</Tip>\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline",
  "torch.random.manual_seed(0)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"microsoft/Phi-3.5-MoE-instruct\",\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\n\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\n\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n```\n\n## PhimoeConfig\n\n[[autodoc]] PhimoeConfig\n\n<frameworkcontent>\n<pt>\n\n## PhimoeModel",
  "[[autodoc]] PhimoeModel\n- forward\n\n## PhimoeForCausalLM\n\n[[autodoc]] PhimoeForCausalLM\n- forward\n- generate\n\n## PhimoeForSequenceClassification\n\n[[autodoc]] PhimoeForSequenceClassification\n- forward\n\n</pt>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SpeechT5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nThe abstract from the paper is the following:",
  "*Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.*",
  "This model was contributed by [Matthijs](https://huggingface.co/Matthijs). The original code can be found [here](https://github.com/microsoft/SpeechT5).\n\n## SpeechT5Config\n\n[[autodoc]] SpeechT5Config\n\n## SpeechT5HifiGanConfig\n\n[[autodoc]] SpeechT5HifiGanConfig\n\n## SpeechT5Tokenizer\n\n[[autodoc]] SpeechT5Tokenizer\n- __call__\n- save_vocabulary\n- decode\n- batch_decode\n\n## SpeechT5FeatureExtractor\n\n[[autodoc]] SpeechT5FeatureExtractor\n- __call__\n\n## SpeechT5Processor\n\n[[autodoc]] SpeechT5Processor\n- __call__\n- pad\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## SpeechT5Model\n\n[[autodoc]] SpeechT5Model\n- forward\n\n## SpeechT5ForSpeechToText\n\n[[autodoc]] SpeechT5ForSpeechToText\n- forward\n\n## SpeechT5ForTextToSpeech\n\n[[autodoc]] SpeechT5ForTextToSpeech\n- forward\n- generate\n\n## SpeechT5ForSpeechToSpeech\n\n[[autodoc]] SpeechT5ForSpeechToSpeech\n- forward\n- generate_speech\n\n## SpeechT5HifiGan\n\n[[autodoc]] SpeechT5HifiGan\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MobileNet V1\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The MobileNet model was proposed in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n\nThe abstract from the paper is the following:\n\n*We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.*",
  "This model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and weights can be found [here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md).\n\n## Usage tips\n\n- The checkpoints are named **mobilenet\\_v1\\_*depth*\\_*size***, for example **mobilenet\\_v1\\_1.0\\_224**, where **1.0** is the depth multiplier (sometimes also referred to as \"alpha\" or the width multiplier) and **224** is the resolution of the input images the model was trained on.\n\n- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32.\n\n- One can use [`MobileNetV1ImageProcessor`] to prepare images for the model.\n\n- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0).",
  "- The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV1Config`] with `tf_padding = False`.\n\nUnsupported features:\n\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidden state. In the original model it is possible to use a 7x7 average pooling layer with stride 2 instead of global pooling. For larger inputs, this gives a pooled output that is larger than 1x1 pixel. The HuggingFace implementation does not support this.\n\n- It is currently not possible to specify an `output_stride`. For smaller output strides, the original model invokes dilated convolution to prevent the spatial resolution from being reduced further. The output stride of the HuggingFace model is always 32.\n\n- The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \"FakeQuantization\" operations to unquantize the weights.",
  "- It's common to extract the output from the pointwise layers at indices 5, 11, 12, 13 for downstream purposes. Using `output_hidden_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with MobileNetV1.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`MobileNetV1ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## MobileNetV1Config\n\n[[autodoc]] MobileNetV1Config\n\n## MobileNetV1FeatureExtractor\n\n[[autodoc]] MobileNetV1FeatureExtractor\n- preprocess",
  "## MobileNetV1ImageProcessor\n\n[[autodoc]] MobileNetV1ImageProcessor\n- preprocess\n\n## MobileNetV1Model\n\n[[autodoc]] MobileNetV1Model\n- forward\n\n## MobileNetV1ForImageClassification\n\n[[autodoc]] MobileNetV1ForImageClassification\n- forward",
  "<!--Copyright 2024 JetMoe team and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# JetMoe\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "**JetMoe-8B** is an 8B Mixture-of-Experts (MoE) language model developed by [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ) and [MyShell](https://myshell.ai/).\nJetMoe project aims to provide a LLaMA2-level performance and efficient language model with a limited budget.\nTo achieve this goal, JetMoe uses a sparsely activated architecture inspired by the [ModuleFormer](https://arxiv.org/abs/2306.04640).\nEach JetMoe block consists of two MoE layers: Mixture of Attention Heads and Mixture of MLP Experts.\nGiven the input tokens, it activates a subset of its experts to process them.\nThis sparse activation schema enables JetMoe to achieve much better training throughput than similar size dense models.\nThe training throughput of JetMoe-8B is around 100B tokens per day on a cluster of 96 H100 GPUs with a straightforward 3-way pipeline parallelism strategy.\n\nThis model was contributed by [Yikang Shen](https://huggingface.co/YikangS).\n\n\n## JetMoeConfig\n\n[[autodoc]] JetMoeConfig\n\n## JetMoeModel\n\n[[autodoc]] JetMoeModel\n- forward\n\n## JetMoeForCausalLM\n\n[[autodoc]] JetMoeForCausalLM\n- forward\n\n## JetMoeForSequenceClassification",
  "[[autodoc]] JetMoeForSequenceClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLM-RoBERTa-XL\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n\nThe abstract from the paper is the following:\n\n*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*",
  "This model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).\n\n## Usage tips\n\nXLM-RoBERTa-XL is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does\nnot require `lang` tensors to understand which language is used, and should be able to determine the correct\nlanguage from the input ids.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## XLMRobertaXLConfig\n\n[[autodoc]] XLMRobertaXLConfig\n\n## XLMRobertaXLModel\n\n[[autodoc]] XLMRobertaXLModel\n- forward\n\n## XLMRobertaXLForCausalLM\n\n[[autodoc]] XLMRobertaXLForCausalLM\n- forward\n\n## XLMRobertaXLForMaskedLM\n\n[[autodoc]] XLMRobertaXLForMaskedLM\n- forward",
  "## XLMRobertaXLForSequenceClassification\n\n[[autodoc]] XLMRobertaXLForSequenceClassification\n- forward\n\n## XLMRobertaXLForMultipleChoice\n\n[[autodoc]] XLMRobertaXLForMultipleChoice\n- forward\n\n## XLMRobertaXLForTokenClassification\n\n[[autodoc]] XLMRobertaXLForTokenClassification\n- forward\n\n## XLMRobertaXLForQuestionAnswering\n\n[[autodoc]] XLMRobertaXLForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Big Transfer (BiT)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BiT model was proposed in [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.",
  "BiT is a simple recipe for scaling up pre-training of [ResNet](resnet)-like architectures (specifically, ResNetv2). The method results in significant improvements for transfer learning.\n\nThe abstract from the paper is the following:",
  "*Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/google-research/big_transfer).\n\n## Usage tips",
  "- BiT models are equivalent to ResNetv2 in terms of architecture, except that: 1) all batch normalization layers are replaced by [group normalization](https://arxiv.org/abs/1803.08494),\n2) [weight standardization](https://arxiv.org/abs/1903.10520) is used for convolutional layers. The authors show that the combination of both is useful for training with large batch sizes, and has a significant\nimpact on transfer learning.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BiT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`BitForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
  "## BitConfig\n\n[[autodoc]] BitConfig\n\n## BitImageProcessor\n\n[[autodoc]] BitImageProcessor\n- preprocess\n\n## BitModel\n\n[[autodoc]] BitModel\n- forward\n\n## BitForImageClassification\n\n[[autodoc]] BitForImageClassification\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# IDEFICS\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The IDEFICS model was proposed in [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents\n](https://huggingface.co/papers/2306.16527\n) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh\n\nThe abstract from the paper is the following:",
  "*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*\n\nThis model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).",
  "<Tip warning={true}>\n\nIDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.\n\nTo train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)\n\n</Tip>\n\n\n## IdeficsConfig\n\n[[autodoc]] IdeficsConfig\n\n## IdeficsModel\n\n[[autodoc]] IdeficsModel\n- forward\n\n## IdeficsForVisionText2Text\n\n[[autodoc]] IdeficsForVisionText2Text\n- forward\n\n## TFIdeficsModel\n\n[[autodoc]] TFIdeficsModel\n- call\n\n## TFIdeficsForVisionText2Text\n\n[[autodoc]] TFIdeficsForVisionText2Text\n- call\n\n## IdeficsImageProcessor\n\n[[autodoc]] IdeficsImageProcessor\n- preprocess\n\n## IdeficsProcessor\n\n[[autodoc]] IdeficsProcessor\n- __call__",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ViLT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ViLT model was proposed in [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)",
  "by Wonjae Kim, Bokyung Son, Ildoo Kim. ViLT incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design\nfor Vision-and-Language Pre-training (VLP).\n\nThe abstract from the paper is the following:\n\n*Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks.\nCurrent approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision\n(e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we\nfind it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more\ncomputation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive\npower of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model,\nVision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically\nsimplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of",
  "times faster than previous VLP models, yet with competitive or better downstream task performance.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vilt_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViLT architecture. Taken from the <a href=\"https://arxiv.org/abs/2102.03334\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/dandelin/ViLT).\n\n## Usage tips\n\n- The quickest way to get started with ViLT is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViLT)\n(which showcase both inference and fine-tuning on custom data).\n- ViLT is a model that takes both `pixel_values` and `input_ids` as input. One can use [`ViltProcessor`] to prepare data for the model.\nThis processor wraps a image processor (for the image modality) and a tokenizer (for the language modality) into one.\n- ViLT is trained with images of various sizes: the authors resize the shorter edge of input images to 384 and limit the longer edge to",
  "under 640 while preserving the aspect ratio. To make batching of images possible, the authors use a `pixel_mask` that indicates\nwhich pixel values are real and which are padding. [`ViltProcessor`] automatically creates this for you.\n- The design of ViLT is very similar to that of a standard Vision Transformer (ViT). The only difference is that the model includes\nadditional embedding layers for the language modality.\n- The PyTorch version of this model is only available in torch 1.10 and higher.\n\n## ViltConfig\n\n[[autodoc]] ViltConfig\n\n## ViltFeatureExtractor\n\n[[autodoc]] ViltFeatureExtractor\n- __call__\n\n## ViltImageProcessor\n\n[[autodoc]] ViltImageProcessor\n- preprocess\n\n## ViltProcessor\n\n[[autodoc]] ViltProcessor\n- __call__\n\n## ViltModel\n\n[[autodoc]] ViltModel\n- forward\n\n## ViltForMaskedLM\n\n[[autodoc]] ViltForMaskedLM\n- forward\n\n## ViltForQuestionAnswering\n\n[[autodoc]] ViltForQuestionAnswering\n- forward\n\n## ViltForImagesAndTextClassification\n\n[[autodoc]] ViltForImagesAndTextClassification\n- forward\n\n## ViltForImageAndTextRetrieval\n\n[[autodoc]] ViltForImageAndTextRetrieval\n- forward\n\n## ViltForTokenClassification\n\n[[autodoc]] ViltForTokenClassification\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MPT model was proposed by the [MosaicML](https://www.mosaicml.com/) team and released with multiple sizes and finetuned variants. The MPT models are a series of open source and commercially usable LLMs pre-trained on 1T tokens.",
  "MPT models are GPT-style decoder-only transformers with several improvements: performance-optimized layer implementations, architecture changes that provide greater training stability, and the elimination of context length limits by replacing positional embeddings with ALiBi.\n\n- MPT base: MPT base pre-trained models on next token prediction\n- MPT instruct: MPT base models fine-tuned on instruction based tasks\n- MPT storywriter: MPT base models fine-tuned for 2500 steps on 65k-token excerpts of fiction books contained in the books3 corpus, this enables the model to handle very long sequences\n\nThe original code is available at the  [`llm-foundry`](https://github.com/mosaicml/llm-foundry/tree/main) repository.\n\nRead more about it [in the release blogpost](https://www.mosaicml.com/blog/mpt-7b)\n\n## Usage tips\n\n- Learn more about some techniques behind training of the model [in this section of llm-foundry repository](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md#faqs)",
  "- If you want to use the advanced version of the model (triton kernels, direct flash attention integration), you can still use the original model implementation by adding `trust_remote_code=True` when calling `from_pretrained`.\n\n## Resources\n\n- [Fine-tuning Notebook](https://colab.research.google.com/drive/1HCpQkLL7UXW8xJUJJ29X7QAeNJKO0frZ?usp=sharing) on how to fine-tune MPT-7B on a free Google Colab instance to turn the model into a Chatbot.\n\n## MptConfig\n\n[[autodoc]] MptConfig\n- all\n\n## MptModel\n\n[[autodoc]] MptModel\n- forward\n\n## MptForCausalLM\n\n[[autodoc]] MptForCausalLM\n- forward\n\n## MptForSequenceClassification\n\n[[autodoc]] MptForSequenceClassification\n- forward\n\n## MptForTokenClassification\n\n[[autodoc]] MptForTokenClassification\n- forward\n\n## MptForQuestionAnswering\n\n[[autodoc]] MptForQuestionAnswering\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CodeLlama\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe Code Llama model was proposed in [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.\n\nThe abstract from the paper is the following:",
  "*We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.*",
  "Check out all Code Llama model checkpoints [here](https://huggingface.co/models?search=code_llama) and the officially released ones in the [Meta Llama org](https://huggingface.co/meta-llama).\n\nThis model was contributed by [ArthurZucker](https://huggingface.co/ArthurZ). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n## Usage tips and examples\n\n<Tip warning={true}>\n\nThe `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the original inference uses `float16`. Let's look at the different precisions:\n\n* `float32`: PyTorch convention on model initialization is to load models in `float32`, no matter with which `dtype` the model weights were stored. `transformers` also follows this convention for consistency with PyTorch. This will be picked by default. If you want the `AutoModel` API to load the checkpoints with the storage weights type, you must specify `torch_dtype=\"auto\"`, e.g. `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`.\n* `bfloat16`: Code Llama was trained with this precision, so we recommend using it for further training or fine-tuning.",
  "* `float16`: We recommend running inference using this precision, as it's usually faster than `bfloat16`, and evaluation metrics show no discernible degradation with respect to `bfloat16`. You can also run inference using `bfloat16`, and we recommend you check inference results with both `float16` and `bfloat16` after fine-tuning.\n\nAs mentioned above, the `dtype` of the storage weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using. The reason is that the model will first be downloaded (using the `dtype` of the checkpoints online) and then will be casted to the default `dtype` of `torch` (becomes `torch.float32`). If there is a specified `torch_dtype`, it will be used instead.\n\n</Tip>\n\n\nTips:\n- The infilling task is supported out of the box. You should be using the `tokenizer.fill_token` where you want your input to be filled.\n- The model conversion script is the same as for the `Llama2` family:\n\nHere is a sample usage:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n--input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```",
  "Note that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\ncome in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM).\n\nAfter conversion, the model and tokenizer can be loaded via:\n\n```python\n>>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n\n>>> tokenizer = CodeLlamaTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n>>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n>>> PROMPT = '''def remove_non_ascii(s: str) -> str:\n...     \"\"\" <FILL_ME>\n...     return result\n... '''\n>>> input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n>>> generated_ids = model.generate(input_ids, max_new_tokens=128)\n\n>>> filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n>>> print(PROMPT.replace(\"<FILL_ME>\", filling))\ndef remove_non_ascii(s: str) -> str:\n\"\"\" Remove non-ASCII characters from a string.\n<BLANKLINE>\nArgs:\ns: The string to remove non-ASCII characters from.\n<BLANKLINE>\nReturns:\nThe string with non-ASCII characters removed.\n\"\"\"\nresult = \"\"\nfor c in s:",
  "if ord(c) < 128:\nresult += c\nreturn result\n<BLANKLINE>\n```\n\nIf you only want the infilled part:\n```python\n>>> from transformers import pipeline\n>>> import torch\n\n>>> generator = pipeline(\"text-generation\",model=\"meta-llama/CodeLlama-7b-hf\",torch_dtype=torch.float16, device_map=\"auto\")\n>>> generator('def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return result', max_new_tokens = 128)\n[{'generated_text': 'def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return resultRemove non-ASCII characters from a string. \"\"\"\\n    result = \"\"\\n    for c in s:\\n        if ord(c) < 128:\\n            result += c'}]\n```",
  "Under the hood, the tokenizer [automatically splits by `<FILL_ME>`](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer.fill_token) to create a formatted input string that follows [the original training pattern](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402). This is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug.  To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value.\n\nThe LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string.\n\n<Tip>\n\nCode Llama has the same architecture as the `Llama2` models, refer to [Llama2's documentation page](llama2) for the API reference.\nFind Code Llama tokenizer reference below.\n</Tip>",
  "## CodeLlamaTokenizer\n\n[[autodoc]] CodeLlamaTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## CodeLlamaTokenizerFast\n\n[[autodoc]] CodeLlamaTokenizerFast\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- update_post_processor\n- save_vocabulary",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LLaVA-NeXT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The LLaVA-NeXT model was proposed in [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/) by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon [LLaVa](llava) by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning.\n\nThe introduction from the blog is the following:\n\n*In October 2023, we released LLaVA-1.5 with a simple and efficient design along with great performance on a benchmark suite of 12 datasets. It has since served as the foundation of many comprehensive studies of data, model, and capabilities of large multimodal models (LMM), and has enabled various new applications.\n\nToday, we are thrilled to present LLaVA-NeXT, with improved reasoning, OCR, and world knowledge. LLaVA-NeXT even exceeds Gemini Pro on several benchmarks.\n\nCompared with LLaVA-1.5, LLaVA-NeXT has several improvements:",
  "Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.\nBetter visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\nBetter visual conversation for more scenarios, covering different applications. Better world knowledge and logical reasoning.\nEfficient deployment and inference with SGLang.\nAlong with performance improvements, LLaVA-NeXT maintains the minimalist design and data efficiency of LLaVA-1.5. It re-uses the pretrained connector of LLaVA-1.5, and still uses less than 1M visual instruction tuning samples. The largest 34B variant finishes training in ~1 day with 32 A100s.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_overview.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> LLaVa-NeXT incorporates a higher input resolution by encoding various patches of the input image. Taken from the <a href=\"https://arxiv.org/abs/2310.03744\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).",
  "The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main).\n\n## Usage tips\n\n- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n\n<Tip warning={true}>\n\n- Llava-Next uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n\n</Tip>\n\n\n> [!NOTE]\n> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.",
  "Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n\n\n### Formatting Prompts with Chat Templates\n\nEach **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.\n\n**Important:**\n- You must construct a conversation history — passing a plain string won't work.\n- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.\n- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.",
  "Here’s an example of how to structure your input. We will use [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) and a conversation history of text and image.\n\n```python\nfrom transformers import LlavaNextProcessor\n\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What’s shown in this image?\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n},\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe the image in more details.\"},\n],\n},\n]\n\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\n# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images\nprint(text_prompt)\n>>> \"[INST] <image>\\nWhat's shown in this image? [/INST] This image shows a red stop sign. [INST] Describe the image in more details. [/INST]\"\n```\n\n- If you want to construct a chat prompt yourself, below is a list of possible formats\n.",
  "[llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) requires the following format:\n```bash\n\"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n```\n\n[llava-v1.6-vicuna-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf) and [llava-v1.6-vicuna-13b-hf](https://huggingface.co/llava-hf/llava-v1.6-vicuna-13b-hf) require the following format:\n```bash\n\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n```\n\n[llava-v1.6-34b-hf](https://huggingface.co/llava-hf/llava-v1.6-34b-hf) requires the following format:\n```bash\n\"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\\n\"\n```\n\n[llama3-llava-next-8b-hf](https://huggingface.co/llava-hf/llava-next-8b-hf) requires the following format:\n\n```bash",
  "\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.<|eot_id|><|start_header_id|><|start_header_id|>user<|end_header_id|>\\n\\n<image>\\nWhat is shown in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n```\n\n[llava-next-72b-hf](https://huggingface.co/llava-hf/llava-next-72b-hf) and [llava-next-110b-hf](https://huggingface.co/llava-hf/llava-next-110b-hf) require the following format:\n\n```bash\n\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n```\n\n🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n\n\n\n## Usage example\n\n### Single image inference\n\nHere's how to load the model and perform inference in half-precision (`torch.float16`):\n\n```python\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nimport torch",
  "from PIL import Image\nimport requests\n\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\nmodel.to(\"cuda:0\")\n\n# prepare image and text prompt, using the appropriate prompt template\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\ninputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda:0\")\n\n# autoregressively complete prompt\noutput = model.generate(**inputs, max_new_tokens=100)\n\nprint(processor.decode(output[0], skip_special_tokens=True))\n```\n\n### Multi image inference\n\nLLaVa-Next can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). Here is how you can do it:",
  "```python\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\n\n# Load the model in half-precision\nmodel = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\n# Get three different images\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage_stop = Image.open(requests.get(url, stream=True).raw)\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage_cats = Image.open(requests.get(url, stream=True).raw)\n\nurl = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\nimage_snowman = Image.open(requests.get(url, stream=True).raw)\n\n# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\nconversation_1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"There is a red stop sign in the image.\"},\n],\n},\n{\n\"role\": \"user\",\n\"content\": [",
  "{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n],\n},\n]\n\nconversation_2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\n\nprompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\nprompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\nprompts = [prompt_1, prompt_2]\n\n# We can simply feed images in the order they have to be used in the text prompt\n# Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\ninputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device)\n\n# Generate\ngenerate_ids = model.generate(**inputs, max_new_tokens=30)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n\n## Model optimization\n\n### Quantization using Bitsandbytes",
  "The model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes`, and to have access to a GPU/accelerator that is supported by the library.\n\n<Tip>\n\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit [this link](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend).\n\nWe value your feedback to help identify bugs before the full release! Check out [these docs](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends) for more details and feedback links.\n\n</Tip>\n\nSimply change the snippet above with:\n\n```python\nfrom transformers import AutoModelForImageTextToText, BitsAndBytesConfig\n\n# specify how to quantize the model\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.float16,\n)",
  "model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quantization_config, device_map=\"auto\")\n```\n\n### Use Flash-Attention 2 to further speed-up generation\n\nFirst make sure to install flash-attn. Refer to the [original repository of Flash Attention](https://github.com/Dao-AILab/flash-attention) regarding that package installation. Simply change the snippet above with:\n\n```python\nfrom transformers import AutoModelForImageTextToText\n\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\nuse_flash_attention_2=True\n).to(0)\n```\n\n## LlavaNextConfig\n\n[[autodoc]] LlavaNextConfig\n\n## LlavaNextImageProcessor\n\n[[autodoc]] LlavaNextImageProcessor\n- preprocess\n\n## LlavaNextImageProcessorFast\n\n[[autodoc]] LlavaNextImageProcessorFast\n- preprocess\n\n## LlavaNextProcessor\n\n[[autodoc]] LlavaNextProcessor\n\n## LlavaNextForConditionalGeneration\n\n[[autodoc]] LlavaNextForConditionalGeneration\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XGLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe XGLM model was proposed in [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)\nby Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal,\nShruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo,\nJeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n\nThe abstract from the paper is the following:\n\n*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language\ntasks without fine-tuning. While these models are known to be able to jointly represent many different languages,\ntheir training data is dominated by English, potentially limiting their cross-lingual generalization.\nIn this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages,\nand study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters",
  "sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size\nin multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings)\nand natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark,\nour model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the\nofficial supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails,\nshowing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement\non surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models\nin social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.*\n\n\nThis model was contributed by [Suraj](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/xglm).\n\n## Resources",
  "- [Causal language modeling task guide](../tasks/language_modeling)\n\n## XGLMConfig\n\n[[autodoc]] XGLMConfig\n\n## XGLMTokenizer\n\n[[autodoc]] XGLMTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## XGLMTokenizerFast\n\n[[autodoc]] XGLMTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## XGLMModel\n\n[[autodoc]] XGLMModel\n- forward\n\n## XGLMForCausalLM\n\n[[autodoc]] XGLMForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFXGLMModel\n\n[[autodoc]] TFXGLMModel\n- call\n\n## TFXGLMForCausalLM\n\n[[autodoc]] TFXGLMForCausalLM\n- call\n\n</tf>\n<jax>\n\n## FlaxXGLMModel\n\n[[autodoc]] FlaxXGLMModel\n- __call__\n\n## FlaxXGLMForCausalLM\n\n[[autodoc]] FlaxXGLMForCausalLM\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LayoutLMV2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,",
  "Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. LayoutLMV2 improves [LayoutLM](layoutlm) to obtain\nstate-of-the-art results across several document image understanding benchmarks:\n\n- information extraction from scanned documents: the [FUNSD](https://guillaumejaume.github.io/FUNSD/) dataset (a\ncollection of 199 annotated forms comprising more than 30,000 words), the [CORD](https://github.com/clovaai/cord)\ndataset (a collection of 800 receipts for training, 100 for validation and 100 for testing), the [SROIE](https://rrc.cvc.uab.es/?ch=13) dataset (a collection of 626 receipts for training and 347 receipts for testing)\nand the [Kleister-NDA](https://github.com/applicaai/kleister-nda) dataset (a collection of non-disclosure\nagreements from the EDGAR database, including 254 documents for training, 83 documents for validation, and 203\ndocuments for testing).\n- document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\n400,000 images belonging to one of 16 classes).\n- document visual question answering: the [DocVQA](https://arxiv.org/abs/2007.00398) dataset (a collection of 50,000",
  "questions defined on 12,000+ document images).\n\nThe abstract from the paper is the following:\n\n*Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to\nits effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this\npaper, we present LayoutLMv2 by pre-training text, layout and image in a multi-modal framework, where new model\narchitectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked\nvisual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training\nstage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention\nmechanism into the Transformer architecture, so that the model can fully understand the relative positional\nrelationship among different text blocks. Experiment results show that LayoutLMv2 outperforms strong baselines and\nachieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks,",
  "including FUNSD (0.7895 -> 0.8420), CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852),\nRVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672). The pre-trained LayoutLMv2 model is publicly available at\nthis https URL.*\n\nLayoutLMv2 depends on `detectron2`, `torchvision` and `tesseract`. Run the\nfollowing to install them:\n```bash\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\npython -m pip install torchvision tesseract\n```\n(If you are developing for LayoutLMv2, note that passing the doctests also requires the installation of these packages.)\n\n## Usage tips\n\n- The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates visual embeddings during\npre-training (while LayoutLMv1 only adds visual embeddings during fine-tuning).\n- LayoutLMv2 adds both a relative 1D attention bias as well as a spatial 2D attention bias to the attention scores in\nthe self-attention layers. Details can be found on page 5 of the [paper](https://arxiv.org/abs/2012.14740).",
  "- Demo notebooks on how to use the LayoutLMv2 model on RVL-CDIP, FUNSD, DocVQA, CORD can be found [here](https://github.com/NielsRogge/Transformers-Tutorials).\n- LayoutLMv2 uses Facebook AI's [Detectron2](https://github.com/facebookresearch/detectron2/) package for its visual\nbackbone. See [this link](https://detectron2.readthedocs.io/en/latest/tutorials/install.html) for installation\ninstructions.\n- In addition to `input_ids`, [`~LayoutLMv2Model.forward`] expects 2 additional inputs, namely\n`image` and `bbox`. The `image` input corresponds to the original document image in which the text\ntokens occur. The model expects each document image to be of size 224x224. This means that if you have a batch of\ndocument images, `image` should be a tensor of shape (batch_size, 3, 224, 224). This can be either a\n`torch.Tensor` or a `Detectron2.structures.ImageList`. You don't need to normalize the channels, as this is\ndone by the model. Important to note is that the visual backbone expects BGR channels instead of RGB, as all models\nin Detectron2 are pre-trained using the BGR format. The `bbox` input are the bounding boxes (i.e. 2D-positions)",
  "of the input text tokens. This is identical to [`LayoutLMModel`]. These can be obtained using an\nexternal OCR engine such as Google's [Tesseract](https://github.com/tesseract-ocr/tesseract) (there's a [Python\nwrapper](https://pypi.org/project/pytesseract/) available). Each bounding box should be in (x0, y0, x1, y1)\nformat, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1)\nrepresents the position of the lower right corner. Note that one first needs to normalize the bounding boxes to be on\na 0-1000 scale. To normalize, you can use the following function:\n\n```python\ndef normalize_bbox(bbox, width, height):\nreturn [\nint(1000 * (bbox[0] / width)),\nint(1000 * (bbox[1] / height)),\nint(1000 * (bbox[2] / width)),\nint(1000 * (bbox[3] / height)),\n]\n```\n\nHere, `width` and `height` correspond to the width and height of the original document in which the token\noccurs (before resizing the image). Those can be obtained using the Python Image Library (PIL) library for example, as\nfollows:\n\n```python\nfrom PIL import Image\n\nimage = Image.open(",
  "\"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n)\n\nwidth, height = image.size\n```\n\nHowever, this model includes a brand new [`~transformers.LayoutLMv2Processor`] which can be used to directly\nprepare data for the model (including applying OCR under the hood). More information can be found in the \"Usage\"\nsection below.\n\n- Internally, [`~transformers.LayoutLMv2Model`] will send the `image` input through its visual backbone to\nobtain a lower-resolution feature map, whose shape is equal to the `image_feature_pool_shape` attribute of\n[`~transformers.LayoutLMv2Config`]. This feature map is then flattened to obtain a sequence of image tokens. As\nthe size of the feature map is 7x7 by default, one obtains 49 image tokens. These are then concatenated with the text\ntokens, and send through the Transformer encoder. This means that the last hidden states of the model will have a\nlength of 512 + 49 = 561, if you pad the text tokens up to the max length. More generally, the last hidden states\nwill have a shape of `seq_length` + `image_feature_pool_shape[0]` *\n`config.image_feature_pool_shape[1]`.",
  "- When calling [`~transformers.LayoutLMv2Model.from_pretrained`], a warning will be printed with a long list of\nparameter names that are not initialized. This is not a problem, as these parameters are batch normalization\nstatistics, which are going to have values when fine-tuning on a custom dataset.\n- If you want to train the model in a distributed environment, make sure to call [`synchronize_batch_norm`] on the\nmodel in order to properly synchronize the batch normalization layers of the visual backbone.\n\nIn addition, there's LayoutXLM, which is a multilingual version of LayoutLMv2. More information can be found on\n[LayoutXLM's documentation page](layoutxlm).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LayoutLMv2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>",
  "- A notebook on how to [finetune LayoutLMv2 for text-classification on RVL-CDIP dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb).\n- See also: [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- A notebook on how to [finetune LayoutLMv2 for question-answering on DocVQA dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb).\n- See also: [Question answering task guide](../tasks/question_answering)\n- See also: [Document question answering task guide](../tasks/document_question_answering)\n\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- A notebook on how to [finetune LayoutLMv2 for token-classification on CORD dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb).",
  "- A notebook on how to [finetune LayoutLMv2 for token-classification on FUNSD dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb).\n- See also: [Token classification task guide](../tasks/token_classification)\n\n## Usage: LayoutLMv2Processor\n\nThe easiest way to prepare data for the model is to use [`LayoutLMv2Processor`], which internally\ncombines a image processor ([`LayoutLMv2ImageProcessor`]) and a tokenizer\n([`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`]). The image processor\nhandles the image modality, while the tokenizer handles the text modality. A processor combines both, which is ideal\nfor a multi-modal model like LayoutLMv2. Note that you can still use both separately, if you only want to handle one\nmodality.\n\n```python\nfrom transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor\n\nimage_processor = LayoutLMv2ImageProcessor()  # apply_ocr is set to True by default\ntokenizer = LayoutLMv2TokenizerFast.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")",
  "processor = LayoutLMv2Processor(image_processor, tokenizer)\n```\n\nIn short, one can provide a document image (and possibly additional data) to [`LayoutLMv2Processor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`LayoutLMv2ImageProcessor`] to apply OCR on the image to get a list of words and normalized\nbounding boxes, as well to resize the image to a given size in order to get the `image` input. The words and\nnormalized bounding boxes are then provided to [`LayoutLMv2Tokenizer`] or\n[`LayoutLMv2TokenizerFast`], which converts them to token-level `input_ids`,\n`attention_mask`, `token_type_ids`, `bbox`. Optionally, one can provide word labels to the processor,\nwhich are turned into token-level `labels`.\n\n[`LayoutLMv2Processor`] uses [PyTesseract](https://pypi.org/project/pytesseract/), a Python\nwrapper around Google's Tesseract OCR engine, under the hood. Note that you can still use your own OCR engine of\nchoice, and provide the words and normalized boxes yourself. This requires initializing\n[`LayoutLMv2ImageProcessor`] with `apply_ocr` set to `False`.",
  "In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: document image classification (training, inference) + token classification (inference), apply_ocr =\nTrue**\n\nThis is the simplest case, in which the processor (actually the image processor) will perform OCR on the image to get\nthe words and normalized bounding boxes.\n\n```python\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image.open(\n\"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nencoding = processor(\nimage, return_tensors=\"pt\"\n)  # you can also add all tokenizer parameters here such as padding, truncation\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```\n\n**Use case 2: document image classification (training, inference) + token classification (inference), apply_ocr=False**",
  "In case one wants to do OCR themselves, one can initialize the image processor with `apply_ocr` set to\n`False`. In that case, one should provide the words and corresponding (normalized) bounding boxes themselves to\nthe processor.\n\n```python\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n\"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nencoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```\n\n**Use case 3: token classification (training), apply_ocr=False**\n\nFor token classification tasks (such as FUNSD, CORD, SROIE, Kleister-NDA), one can also provide the corresponding word\nlabels in order to train a model. The processor will then convert these into token-level `labels`. By default, it",
  "will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\ninitialize the tokenizer with `only_label_first_subword` set to `False`.\n\n```python\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(\n\"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nword_labels = [1, 2]\nencoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])\n```\n\n**Use case 4: visual question answering (inference), apply_ocr=True**\n\nFor visual question answering tasks (such as DocVQA), you can provide a question to the processor. By default, the",
  "processor will apply OCR on the image, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image.open(\n\"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nquestion = \"What's his name?\"\nencoding = processor(image, question, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```\n\n**Use case 5: visual question answering (inference), apply_ocr=False**\n\nFor visual question answering tasks (such as DocVQA), you can provide a question to the processor. If you want to\nperform OCR yourself, you can provide your own words and (normalized) bounding boxes to the processor.\n\n```python\nfrom transformers import LayoutLMv2Processor\nfrom PIL import Image\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\nimage = Image.open(",
  "\"name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images).\"\n).convert(\"RGB\")\nquestion = \"What's his name?\"\nwords = [\"hello\", \"world\"]\nboxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\nencoding = processor(image, question, words, boxes=boxes, return_tensors=\"pt\")\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])\n```\n\n## LayoutLMv2Config\n\n[[autodoc]] LayoutLMv2Config\n\n## LayoutLMv2FeatureExtractor\n\n[[autodoc]] LayoutLMv2FeatureExtractor\n- __call__\n\n## LayoutLMv2ImageProcessor\n\n[[autodoc]] LayoutLMv2ImageProcessor\n- preprocess\n\n## LayoutLMv2Tokenizer\n\n[[autodoc]] LayoutLMv2Tokenizer\n- __call__\n- save_vocabulary\n\n## LayoutLMv2TokenizerFast\n\n[[autodoc]] LayoutLMv2TokenizerFast\n- __call__\n\n## LayoutLMv2Processor\n\n[[autodoc]] LayoutLMv2Processor\n- __call__\n\n## LayoutLMv2Model\n\n[[autodoc]] LayoutLMv2Model\n- forward\n\n## LayoutLMv2ForSequenceClassification\n\n[[autodoc]] LayoutLMv2ForSequenceClassification\n\n## LayoutLMv2ForTokenClassification\n\n[[autodoc]] LayoutLMv2ForTokenClassification\n\n## LayoutLMv2ForQuestionAnswering\n\n[[autodoc]] LayoutLMv2ForQuestionAnswering",
  "",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RetriBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, so we won't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.",
  "You can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n</Tip>\n\n## Overview\n\nThe RetriBERT model was proposed in the blog post [Explain Anything Like I'm Five: A Model for Open Domain Long Form\nQuestion Answering](https://yjernite.github.io/lfqa.html). RetriBERT is a small model that uses either a single or\npair of BERT encoders with lower-dimension projection for dense semantic indexing of text.\n\nThis model was contributed by [yjernite](https://huggingface.co/yjernite). Code to train and use the model can be\nfound [here](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation).\n\n\n## RetriBertConfig\n\n[[autodoc]] RetriBertConfig\n\n## RetriBertTokenizer\n\n[[autodoc]] RetriBertTokenizer\n\n## RetriBertTokenizerFast\n\n[[autodoc]] RetriBertTokenizerFast\n\n## RetriBertModel\n\n[[autodoc]] RetriBertModel\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the MIT License; you may not use this file except in compliance with\nthe License.\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n\n-->\n\n# SuperGlue\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe SuperGlue model was proposed in [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://arxiv.org/abs/1911.11763) by Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n\nThis model consists of matching two sets of interest points detected in an image. Paired with the",
  "[SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and\nestimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\n\nThe abstract from the paper is the following:\n\n*This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences\nand rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs\nare predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling\nSuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics,\nour technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image\npairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in\nchallenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and",
  "can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at this [URL](https://github.com/magicleap/SuperGluePretrainedNetwork).*\n\n## How to use\n\nHere is a quick example of using the model. Since this model is an image matching model, it requires pairs of images to be matched.\nThe raw outputs contain the list of keypoints detected by the keypoint detector as well as the list of matches with their corresponding\nmatching scores.\n```python\nfrom transformers import AutoImageProcessor, AutoModel\nimport torch\nfrom PIL import Image\nimport requests\n\nurl_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\nimage1 = Image.open(requests.get(url_image1, stream=True).raw)\nurl_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\nimage_2 = Image.open(requests.get(url_image2, stream=True).raw)\n\nimages = [image1, image2]",
  "processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\nmodel = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n\ninputs = processor(images, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\n```\n\nYou can use the `post_process_keypoint_matching` method from the `SuperGlueImageProcessor` to get the keypoints and matches in a more readable format:\n\n```python\nimage_sizes = [[(image.height, image.width) for image in images]]\noutputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\nfor i, output in enumerate(outputs):\nprint(\"For the image pair\", i)\nfor keypoint0, keypoint1, matching_score in zip(\noutput[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n):\nprint(\nf\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n)\n\n```\n\nFrom the outputs, you can visualize the matches between the two images using the following code:\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create side by side image",
  "merged_image = np.zeros((max(image1.height, image2.height), image1.width + image2.width, 3))\nmerged_image[: image1.height, : image1.width] = np.array(image1) / 255.0\nmerged_image[: image2.height, image1.width :] = np.array(image2) / 255.0\nplt.imshow(merged_image)\nplt.axis(\"off\")\n\n# Retrieve the keypoints and matches\noutput = outputs[0]\nkeypoints0 = output[\"keypoints0\"]\nkeypoints1 = output[\"keypoints1\"]\nmatching_scores = output[\"matching_scores\"]\nkeypoints0_x, keypoints0_y = keypoints0[:, 0].numpy(), keypoints0[:, 1].numpy()\nkeypoints1_x, keypoints1_y = keypoints1[:, 0].numpy(), keypoints1[:, 1].numpy()\n\n# Plot the matches\nfor keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\nkeypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, matching_scores\n):\nplt.plot(\n[keypoint0_x, keypoint1_x + image1.width],\n[keypoint0_y, keypoint1_y],\ncolor=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\nalpha=0.9,\nlinewidth=0.5,\n)\nplt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\nplt.scatter(keypoint1_x + image1.width, keypoint1_y, c=\"black\", s=2)\n\n# Save the plot\nplt.savefig(\"matched_image.png\", dpi=300, bbox_inches='tight')\nplt.close()\n```",
  "![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/01ZYaLB1NL5XdA8u7yCo4.png)\n\nThis model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\nThe original code can be found [here](https://github.com/magicleap/SuperGluePretrainedNetwork).\n\n## SuperGlueConfig\n\n[[autodoc]] SuperGlueConfig\n\n## SuperGlueImageProcessor\n\n[[autodoc]] SuperGlueImageProcessor\n\n- preprocess\n\n## SuperGlueForKeypointMatching\n\n[[autodoc]] SuperGlueForKeypointMatching\n\n- forward\n- post_process_keypoint_matching",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Bark\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n</div>\n\n## Overview\n\nBark is a transformer-based text-to-speech model proposed by Suno AI in [suno-ai/bark](https://github.com/suno-ai/bark).\n\nBark is made of 4 main models:",
  "- [`BarkSemanticModel`] (also referred to as the 'text' model): a causal auto-regressive transformer model that takes as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.\n- [`BarkCoarseModel`] (also referred to as the 'coarse acoustics' model): a causal autoregressive transformer, that takes as input the results of the [`BarkSemanticModel`] model. It aims at predicting the first two audio codebooks necessary for EnCodec.\n- [`BarkFineModel`] (the 'fine acoustics' model), this time a non-causal autoencoder transformer, which iteratively predicts the last codebooks based on the sum of the previous codebooks embeddings.\n- having predicted all the codebook channels from the [`EncodecModel`], Bark uses it to decode the output audio array.\n\nIt should be noted that each of the first three modules can support conditional speaker embeddings to condition the output sound according to specific predefined voice.\n\nThis model was contributed by [Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe) and [Sanchit Gandhi (sanchit-gandhi)](https://github.com/sanchit-gandhi).\nThe original code can be found [here](https://github.com/suno-ai/bark).",
  "### Optimizing Bark\n\nBark can be optimized with just a few extra lines of code, which **significantly reduces its memory footprint** and **accelerates inference**.\n\n#### Using half-precision\n\nYou can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision.\n\n```python\nfrom transformers import BarkModel\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n```\n\n#### Using CPU offload\n\nAs mentioned above, Bark is made up of 4 sub-models, which are called up sequentially during audio generation. In other words, while one sub-model is in use, the other sub-models are idle.\n\nIf you're using a CUDA device, a simple solution to benefit from an 80% reduction in memory footprint is to offload the submodels from GPU to CPU when they're idle. This operation is called *CPU offloading*. You can use it with one line of code as follows:\n\n```python\nmodel.enable_cpu_offload()\n```\n\nNote that 🤗 Accelerate must be installed before using this feature. [Here's how to install it.](https://huggingface.co/docs/accelerate/basic_tutorials/install)",
  "#### Using Better Transformer\n\nBetter Transformer is an 🤗 Optimum feature that performs kernel fusion under the hood. You can gain 20% to 30% in speed with zero performance degradation. It only requires one line of code to export the model to 🤗 Better Transformer:\n\n```python\nmodel =  model.to_bettertransformer()\n```\n\nNote that 🤗 Optimum must be installed before using this feature. [Here's how to install it.](https://huggingface.co/docs/optimum/installation)\n\n#### Using Flash Attention 2\n\nFlash Attention 2 is an even faster, optimized version of the previous optimization.\n\n##### Installation\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).",
  "Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n\n##### Usage\n\nTo load a model using Flash Attention 2, we can pass the `attn_implementation=\"flash_attention_2\"` flag to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\n```python\nmodel = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n```\n\n##### Performance comparison\n\n\nThe following diagram shows the latency for the native attention implementation (no optimisation) against Better Transformer and Flash Attention 2. In all cases, we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1. Flash Attention 2 is also consistently faster than Better Transformer, and its performance improves even more as batch sizes increase:",
  "<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/Bark%20Optimization%20Benchmark.png\">\n</div>\n\nTo put this into perspective, on an NVIDIA A100 and when generating 400 semantic tokens with a batch size of 16, you can get 17 times the [throughput](https://huggingface.co/blog/optimizing-bark#throughput) and still be 2 seconds faster than generating sentences one by one with the native model implementation. In other words, all the samples will be generated 17 times faster.\n\nAt batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than Better Transformer, and at batch size 16, 25%.\n\n\n#### Combining optimization techniques\n\nYou can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 (or 🤗 Better Transformer) all at once.\n\n```python\nfrom transformers import BarkModel\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# load in fp16 and use Flash Attention 2\nmodel = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n\n# enable CPU offload\nmodel.enable_cpu_offload()\n```",
  "Find out more on inference optimization techniques [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one).\n\n### Usage tips\n\nSuno offers a library of voice presets in a number of languages [here](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c).\nThese presets are also uploaded in the hub [here](https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings) or [here](https://huggingface.co/suno/bark/tree/main/speaker_embeddings).\n\n```python\n>>> from transformers import AutoProcessor, BarkModel\n\n>>> processor = AutoProcessor.from_pretrained(\"suno/bark\")\n>>> model = BarkModel.from_pretrained(\"suno/bark\")\n\n>>> voice_preset = \"v2/en_speaker_6\"\n\n>>> inputs = processor(\"Hello, my dog is cute\", voice_preset=voice_preset)\n\n>>> audio_array = model.generate(**inputs)\n>>> audio_array = audio_array.cpu().numpy().squeeze()\n```\n\nBark can generate highly realistic, **multilingual** speech as well as other audio - including music, background noise and simple sound effects.\n\n```python\n>>> # Multilingual speech - simplified Chinese\n>>> inputs = processor(\"惊人的！我会说中文\")",
  ">>> # Multilingual speech - French - let's use a voice_preset as well\n>>> inputs = processor(\"Incroyable! Je peux générer du son.\", voice_preset=\"fr_speaker_5\")\n\n>>> # Bark can also generate music. You can help it out by adding music notes around your lyrics.\n>>> inputs = processor(\"♪ Hello, my dog is cute ♪\")\n\n>>> audio_array = model.generate(**inputs)\n>>> audio_array = audio_array.cpu().numpy().squeeze()\n```\n\nThe model can also produce **nonverbal communications** like laughing, sighing and crying.\n\n\n```python\n>>> # Adding non-speech cues to the input text\n>>> inputs = processor(\"Hello uh ... [clears throat], my dog is cute [laughter]\")\n\n>>> audio_array = model.generate(**inputs)\n>>> audio_array = audio_array.cpu().numpy().squeeze()\n```\n\nTo save the audio, simply take the sample rate from the model config and some scipy utility:\n\n```python\n>>> from scipy.io.wavfile import write as write_wav\n\n>>> # save audio to disk, but first take the sample rate from the model config\n>>> sample_rate = model.generation_config.sample_rate\n>>> write_wav(\"bark_generation.wav\", sample_rate, audio_array)\n```\n\n## BarkConfig\n\n[[autodoc]] BarkConfig\n- all\n\n## BarkProcessor\n\n[[autodoc]] BarkProcessor",
  "- all\n- __call__\n\n## BarkModel\n\n[[autodoc]] BarkModel\n- generate\n- enable_cpu_offload\n\n## BarkSemanticModel\n\n[[autodoc]] BarkSemanticModel\n- forward\n\n## BarkCoarseModel\n\n[[autodoc]] BarkCoarseModel\n- forward\n\n## BarkFineModel\n\n[[autodoc]] BarkFineModel\n- forward\n\n## BarkCausalModel\n\n[[autodoc]] BarkCausalModel\n- forward\n\n## BarkCoarseConfig\n\n[[autodoc]] BarkCoarseConfig\n- all\n\n## BarkFineConfig\n\n[[autodoc]] BarkFineConfig\n- all\n\n## BarkSemanticConfig\n\n[[autodoc]] BarkSemanticConfig\n- all",
  "<!--Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ErnieM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe ErnieM model was proposed in [ERNIE-M: Enhanced Multilingual Representation by Aligning\nCross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674)  by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, Haifeng Wang.\n\nThe abstract from the paper is the following:",
  "*Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for lowresource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks.*",
  "This model was contributed by [Susnato Dhar](https://huggingface.co/susnato). The original code can be found [here](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/ernie_m).\n\n\n## Usage tips\n\n- Ernie-M is a BERT-like model so it is a stacked Transformer Encoder.\n- Instead of using MaskedLM for pretraining (like BERT) the authors used two novel techniques: `Cross-attention Masked Language Modeling` and `Back-translation Masked Language Modeling`. For now these two LMHead objectives are not implemented here.\n- It is a multilingual language model.\n- Next Sentence Prediction was not used in pretraining process.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## ErnieMConfig\n\n[[autodoc]] ErnieMConfig\n\n\n## ErnieMTokenizer\n\n[[autodoc]] ErnieMTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n\n## ErnieMModel\n\n[[autodoc]] ErnieMModel\n- forward",
  "## ErnieMForSequenceClassification\n\n[[autodoc]] ErnieMForSequenceClassification\n- forward\n\n\n## ErnieMForMultipleChoice\n\n[[autodoc]] ErnieMForMultipleChoice\n- forward\n\n\n## ErnieMForTokenClassification\n\n[[autodoc]] ErnieMForTokenClassification\n- forward\n\n\n## ErnieMForQuestionAnswering\n\n[[autodoc]] ErnieMForQuestionAnswering\n- forward\n\n## ErnieMForInformationExtraction\n\n[[autodoc]] ErnieMForInformationExtraction\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DAB-DETR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DAB-DETR model was proposed in [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://arxiv.org/abs/2201.12329) by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.",
  "DAB-DETR is an enhanced variant of Conditional DETR. It utilizes dynamically updated anchor boxes to provide both a reference query point (x, y) and a reference anchor size (w, h), improving cross-attention computation. This new approach achieves 45.7% AP when trained for 50 epochs with a single ResNet-50 model as the backbone.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dab_detr_convergence_plot.png\"\nalt=\"drawing\" width=\"600\"/>\n\nThe abstract from the paper is the following:\n\n*We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as queries\nin Transformer decoders and dynamically updates them layer-by-layer. Using box\ncoordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR,\nbut also allows us to modulate the positional attention map using the box width\nand height information. Such a design makes it clear that queries in DETR can be",
  "implemented as performing soft ROI pooling layer-by-layer in a cascade manner.\nAs a result, it leads to the best performance on MS-COCO benchmark among\nthe DETR-like detection models under the same setting, e.g., AP 45.7% using\nResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive\nexperiments to confirm our analysis and verify the effectiveness of our methods.*\n\nThis model was contributed by [davidhajdu](https://huggingface.co/davidhajdu).\nThe original code can be found [here](https://github.com/IDEA-Research/DAB-DETR).\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n```python\nimport torch\nimport requests\n\nfrom PIL import Image\nfrom transformers import AutoModelForObjectDetection, AutoImageProcessor\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = AutoImageProcessor.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\nmodel = AutoModelForObjectDetection.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\noutputs = model(**inputs)",
  "results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\n\nfor result in results:\nfor score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\nscore, label = score.item(), label_id.item()\nbox = [round(i, 2) for i in box.tolist()]\nprint(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n```\nThis should output\n```\ncat: 0.87 [14.7, 49.39, 320.52, 469.28]\nremote: 0.86 [41.08, 72.37, 173.39, 117.2]\ncat: 0.86 [344.45, 19.43, 639.85, 367.86]\nremote: 0.61 [334.27, 75.93, 367.92, 188.81]\ncouch: 0.59 [-0.04, 1.34, 639.9, 477.09]\n```\n\nThere are three other ways to instantiate a DAB-DETR model (depending on what you prefer):\n\nOption 1: Instantiate DAB-DETR with pre-trained weights for entire model\n```py\n>>> from transformers import DabDetrForObjectDetection\n\n>>> model = DabDetrForObjectDetection.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n```\n\nOption 2: Instantiate DAB-DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone\n```py\n>>> from transformers import DabDetrConfig, DabDetrForObjectDetection\n\n>>> config = DabDetrConfig()",
  ">>> model = DabDetrForObjectDetection(config)\n```\nOption 3: Instantiate DAB-DETR with randomly initialized weights for backbone + Transformer\n```py\n>>> config = DabDetrConfig(use_pretrained_backbone=False)\n>>> model = DabDetrForObjectDetection(config)\n```\n\n\n## DabDetrConfig\n\n[[autodoc]] DabDetrConfig\n\n## DabDetrModel\n\n[[autodoc]] DabDetrModel\n- forward\n\n## DabDetrForObjectDetection\n\n[[autodoc]] DabDetrForObjectDetection\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SegGPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The SegGPT model was proposed in [SegGPT: Segmenting Everything In Context](https://arxiv.org/abs/2304.03284) by Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. SegGPT employs a decoder-only Transformer that can generate a segmentation mask given an input image, a prompt image and its corresponding prompt mask. The model achieves remarkable one-shot results with 56.1 mIoU on COCO-20 and 85.6 mIoU on FSS-1000.\n\nThe abstract from the paper is the following:",
  "*We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of*\n\nTips:\n- One can use [`SegGptImageProcessor`] to prepare image input, prompt and mask to the model.\n- One can either use segmentation maps or RGB images as prompt masks. If using the latter make sure to set `do_convert_rgb=False` in the `preprocess` method.",
  "- It's highly advisable to pass `num_labels` when using `segmentation_maps` (not considering background) during preprocessing and postprocessing with [`SegGptImageProcessor`] for your use case.\n- When doing inference with [`SegGptForImageSegmentation`] if your `batch_size` is greater than 1 you can use feature ensemble across your images by passing `feature_ensemble=True` in the forward method.\n\nHere's how to use the model for one-shot semantic segmentation:\n\n```python\nimport torch\nfrom datasets import load_dataset\nfrom transformers import SegGptImageProcessor, SegGptForImageSegmentation\n\ncheckpoint = \"BAAI/seggpt-vit-large\"\nimage_processor = SegGptImageProcessor.from_pretrained(checkpoint)\nmodel = SegGptForImageSegmentation.from_pretrained(checkpoint)\n\ndataset_id = \"EduardoPacheco/FoodSeg103\"\nds = load_dataset(dataset_id, split=\"train\")\n# Number of labels in FoodSeg103 (not including background)\nnum_labels = 103\n\nimage_input = ds[4][\"image\"]\nground_truth = ds[4][\"label\"]\nimage_prompt = ds[29][\"image\"]\nmask_prompt = ds[29][\"label\"]\n\ninputs = image_processor(\nimages=image_input,\nprompt_images=image_prompt,\nsegmentation_maps=mask_prompt,\nnum_labels=num_labels,",
  "return_tensors=\"pt\"\n)\n\nwith torch.no_grad():\noutputs = model(**inputs)\n\ntarget_sizes = [image_input.size[::-1]]\nmask = image_processor.post_process_semantic_segmentation(outputs, target_sizes, num_labels=num_labels)[0]\n```\n\nThis model was contributed by [EduardoPacheco](https://huggingface.co/EduardoPacheco).\nThe original code can be found [here]([(https://github.com/baaivision/Painter/tree/main)).\n\n\n## SegGptConfig\n\n[[autodoc]] SegGptConfig\n\n## SegGptImageProcessor\n\n[[autodoc]] SegGptImageProcessor\n- preprocess\n- post_process_semantic_segmentation\n\n## SegGptModel\n\n[[autodoc]] SegGptModel\n- forward\n\n## SegGptForImageSegmentation\n\n[[autodoc]] SegGptForImageSegmentation\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# FastSpeech2Conformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe FastSpeech2Conformer model was proposed with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://arxiv.org/abs/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.",
  "The abstract from the original FastSpeech2 paper is the following:",
  "*Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.*",
  "This model was contributed by [Connor Henderson](https://huggingface.co/connor-henderson). The original code can be found [here](https://github.com/espnet/espnet/blob/master/espnet2/tts/fastspeech2/fastspeech2.py).\n\n\n## 🤗 Model Architecture\nFastSpeech2's general structure with a Mel-spectrogram decoder was implemented, and the traditional transformer blocks were replaced with conformer blocks as done in the ESPnet library.\n\n#### FastSpeech2 Model Architecture\n![FastSpeech2 Model Architecture](https://www.microsoft.com/en-us/research/uploads/prod/2021/04/fastspeech2-1.png)\n\n#### Conformer Blocks\n![Conformer Blocks](https://www.researchgate.net/profile/Hirofumi-Inaguma-2/publication/344911155/figure/fig2/AS:951455406108673@1603856054097/An-overview-of-Conformer-block.png)\n\n#### Convolution Module\n![Convolution Module](https://d3i71xaburhd42.cloudfront.net/8809d0732f6147d4ad9218c8f9b20227c837a746/2-Figure1-1.png)\n\n## 🤗 Transformers Usage\n\nYou can run FastSpeech2Conformer locally with the 🤗 Transformers library.\n\n1. First install the 🤗 [Transformers library](https://github.com/huggingface/transformers), g2p-en:\n\n```bash\npip install --upgrade pip",
  "pip install --upgrade transformers g2p-en\n```\n\n2. Run inference via the Transformers modelling code with the model and hifigan separately\n\n```python\n\nfrom transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerModel, FastSpeech2ConformerHifiGan\nimport soundfile as sf\n\ntokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")\ninputs = tokenizer(\"Hello, my dog is cute.\", return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\n\nmodel = FastSpeech2ConformerModel.from_pretrained(\"espnet/fastspeech2_conformer\")\noutput_dict = model(input_ids, return_dict=True)\nspectrogram = output_dict[\"spectrogram\"]\n\nhifigan = FastSpeech2ConformerHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_hifigan\")\nwaveform = hifigan(spectrogram)\n\nsf.write(\"speech.wav\", waveform.squeeze().detach().numpy(), samplerate=22050)\n```\n\n3. Run inference via the Transformers modelling code with the model and hifigan combined\n\n```python\nfrom transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerWithHifiGan\nimport soundfile as sf\n\ntokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")",
  "inputs = tokenizer(\"Hello, my dog is cute.\", return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\n\nmodel = FastSpeech2ConformerWithHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_with_hifigan\")\noutput_dict = model(input_ids, return_dict=True)\nwaveform = output_dict[\"waveform\"]\n\nsf.write(\"speech.wav\", waveform.squeeze().detach().numpy(), samplerate=22050)\n```\n\n4. Run inference with a pipeline and specify which vocoder to use\n```python\nfrom transformers import pipeline, FastSpeech2ConformerHifiGan\nimport soundfile as sf\n\nvocoder = FastSpeech2ConformerHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_hifigan\")\nsynthesiser = pipeline(model=\"espnet/fastspeech2_conformer\", vocoder=vocoder)\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\")\n\nsf.write(\"speech.wav\", speech[\"audio\"].squeeze(), samplerate=speech[\"sampling_rate\"])\n```\n\n\n## FastSpeech2ConformerConfig\n\n[[autodoc]] FastSpeech2ConformerConfig\n\n## FastSpeech2ConformerHifiGanConfig\n\n[[autodoc]] FastSpeech2ConformerHifiGanConfig\n\n## FastSpeech2ConformerWithHifiGanConfig\n\n[[autodoc]] FastSpeech2ConformerWithHifiGanConfig\n\n## FastSpeech2ConformerTokenizer\n\n[[autodoc]] FastSpeech2ConformerTokenizer\n- __call__",
  "- save_vocabulary\n- decode\n- batch_decode\n\n## FastSpeech2ConformerModel\n\n[[autodoc]] FastSpeech2ConformerModel\n- forward\n\n## FastSpeech2ConformerHifiGan\n\n[[autodoc]] FastSpeech2ConformerHifiGan\n- forward\n\n## FastSpeech2ConformerWithHifiGan\n\n[[autodoc]] FastSpeech2ConformerWithHifiGan\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# X-CLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe X-CLIP model was proposed in [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.",
  "X-CLIP is a minimal extension of [CLIP](clip) for video. The model consists of a text encoder, a cross-frame vision encoder, a multi-frame integration Transformer, and a video-specific prompt generator.\n\nThe abstract from the paper is the following:",
  "*Contrastive language-image pretraining has shown great success in learning visual-textual joint representation from web-scale data, demonstrating remarkable \"zero-shot\" generalization ability for various image tasks. However, how to effectively expand such new language-image pretraining methods to video domains is still an open problem. In this work, we present a simple yet effective approach that adapts the pretrained language-image models to video recognition directly, instead of pretraining a new model from scratch. More concretely, to capture the long-range dependencies of frames along the temporal dimension, we propose a cross-frame attention mechanism that explicitly exchanges information across frames. Such module is lightweight and can be plugged into pretrained language-image models seamlessly. Moreover, we propose a video-specific prompting scheme, which leverages video content information for generating discriminative textual prompts. Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios. In particular, under fully-supervised settings, our approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using 12 times fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot experiments, our approach surpasses the current state-of-the-art methods by +7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In few-shot scenarios, our approach outperforms previous best methods by +32.1% and +23.1% when the labeled data is extremely limited.*",
  "Tips:\n\n- Usage of X-CLIP is identical to [CLIP](clip).\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/xclip_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> X-CLIP architecture. Taken from the <a href=\"https://arxiv.org/abs/2208.02816\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/microsoft/VideoX/tree/master/X-CLIP).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with X-CLIP.\n\n- Demo notebooks for X-CLIP can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/X-CLIP).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## XCLIPProcessor\n\n[[autodoc]] XCLIPProcessor\n\n## XCLIPConfig\n\n[[autodoc]] XCLIPConfig\n- from_text_vision_configs\n\n## XCLIPTextConfig\n\n[[autodoc]] XCLIPTextConfig\n\n## XCLIPVisionConfig\n\n[[autodoc]] XCLIPVisionConfig",
  "## XCLIPModel\n\n[[autodoc]] XCLIPModel\n- forward\n- get_text_features\n- get_video_features\n\n## XCLIPTextModel\n\n[[autodoc]] XCLIPTextModel\n- forward\n\n## XCLIPVisionModel\n\n[[autodoc]] XCLIPVisionModel\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# VideoMAE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The VideoMAE model was proposed in [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\nVideoMAE extends masked auto encoders ([MAE](vit_mae)) to video, claiming state-of-the-art performance on several video classification benchmarks.\n\nThe abstract from the paper is the following:",
  "*Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking and reconstruction. These simple designs turn out to be effective for overcoming information leakage caused by the temporal correlation during video reconstruction. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. This is partially ascribed to the challenging task of video reconstruction to enforce high-level structure learning. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets are important issues in SSVP. Notably, our VideoMAE with the vanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on Something-Something V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any extra data.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/videomae_architecture.jpeg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> VideoMAE pre-training. Taken from the <a href=\"https://arxiv.org/abs/2203.12602\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/MCG-NJU/VideoMAE).\n\n## Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```",
  "from transformers import VideoMAEForVideoClassification\nmodel = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `MCG-NJU/videomae-base-finetuned-kinetics` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                        37 |                                        10 |                      3.7  |\n|            2 |                                        24 |                                        18 |                      1.33 |\n|            4 |                                        43 |                                        32 |                      1.34 |",
  "|            8 |                                        84 |                                        60 |                      1.4  |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with VideoMAE. If\nyou're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll\nreview it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n**Video classification**\n- [A notebook](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb) that shows how\nto fine-tune a VideoMAE model on a custom dataset.\n- [Video classification task guide](../tasks/video_classification)\n- [A 🤗 Space](https://huggingface.co/spaces/sayakpaul/video-classification-ucf101-subset) showing how to perform inference with a video classification model.\n\n## VideoMAEConfig\n\n[[autodoc]] VideoMAEConfig\n\n## VideoMAEFeatureExtractor\n\n[[autodoc]] VideoMAEFeatureExtractor\n- __call__\n\n## VideoMAEImageProcessor\n\n[[autodoc]] VideoMAEImageProcessor\n- preprocess\n\n## VideoMAEModel\n\n[[autodoc]] VideoMAEModel\n- forward\n\n## VideoMAEForPreTraining",
  "`VideoMAEForPreTraining` includes the decoder on top for self-supervised pre-training.\n\n[[autodoc]] transformers.VideoMAEForPreTraining\n- forward\n\n## VideoMAEForVideoClassification\n\n[[autodoc]] transformers.VideoMAEForVideoClassification\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Vision Transformer (ViT)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures.\n\nThe abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall",
  "structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViT architecture. Taken from the <a href=\"https://arxiv.org/abs/2010.11929\">original paper.</a> </small>\n\nFollowing the original Vision Transformer, some follow-up works have been made:\n\n- [DeiT](deit) (Data-efficient Image Transformers) by Facebook AI. DeiT models are distilled vision transformers.\nThe authors of DeiT also released more efficiently trained ViT models, which you can directly plug into [`ViTModel`] or",
  "[`ViTForImageClassification`]. There are 4 variants available (in 3 different sizes): *facebook/deit-tiny-patch16-224*,\n*facebook/deit-small-patch16-224*, *facebook/deit-base-patch16-224* and *facebook/deit-base-patch16-384*. Note that one should\nuse [`DeiTImageProcessor`] in order to prepare images for the model.\n\n- [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperform supervised pre-trained\nvision transformers using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE.\n\n- DINO (a method for self-supervised training of Vision Transformers) by Facebook AI. Vision Transformers trained using\nthe DINO method show very interesting properties not seen with convolutional models. They are capable of segmenting\nobjects, without having ever been trained to do so. DINO checkpoints can be found on the [hub](https://huggingface.co/models?other=dino).\n\n- [MAE](vit_mae) (Masked Autoencoders) by Facebook AI. By pre-training Vision Transformers to reconstruct pixel values for a high portion",
  "(75%) of masked patches (using an asymmetric encoder-decoder architecture), the authors show that this simple method outperforms\nsupervised pre-training after fine-tuning.\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\nNote that we converted the weights from Ross Wightman's [timm library](https://github.com/rwightman/pytorch-image-models),\nwho already converted the weights from JAX to PyTorch. Credits go to him!\n\n## Usage tips\n\n- To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded. A [CLS] token is added to serve as representation of an entire image, which can be\nused for classification. The authors also add absolute position embeddings, and feed the resulting sequence of\nvectors to a standard Transformer encoder.\n- As the Vision Transformer expects each image to be of the same size (resolution), one can use\n[`ViTImageProcessor`] to resize (or rescale) and normalize images for the model.",
  "- Both the patch resolution and image resolution used during pre-training or fine-tuning are reflected in the name of\neach checkpoint. For example, `google/vit-base-patch16-224` refers to a base-sized architecture with patch\nresolution of 16x16 and fine-tuning resolution of 224x224. All checkpoints can be found on the [hub](https://huggingface.co/models?search=vit).\n- The available checkpoints are either (1) pre-trained on [ImageNet-21k](http://www.image-net.org/) (a collection of\n14 million images and 21k classes) only, or (2) also fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/) (also referred to as ILSVRC 2012, a collection of 1.3 million\nimages and 1,000 classes).\n- The Vision Transformer was pre-trained using a resolution of 224x224. During fine-tuning, it is often beneficial to\nuse a higher resolution than pre-training [(Touvron et al., 2019)](https://arxiv.org/abs/1906.06423), [(Kolesnikov\net al., 2020)](https://arxiv.org/abs/1912.11370). In order to fine-tune at higher resolution, the authors perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original image.",
  "- The best results are obtained with supervised pre-training, which is not the case in NLP. The authors also performed\nan experiment with a self-supervised pre-training objective, namely masked patched prediction (inspired by masked\nlanguage modeling). With this approach, the smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a significant\nimprovement of 2% to training from scratch, but still 4% behind supervised pre-training.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```",
  "from transformers import ViTForImageClassification\nmodel = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vit-base-patch16-224` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                         7 |                                         6 |                      1.17 |\n|            2 |                                         8 |                                         6 |                      1.33 |\n|            4 |                                         8 |                                         6 |                      1.33 |",
  "|            8 |                                         8 |                                         6 |                      1.33 |\n\n## Resources\n\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer).\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n`ViTForImageClassification` is supported by:\n<PipelineTag pipeline=\"image-classification\"/>\n\n- A blog post on how to [Fine-Tune ViT for Image Classification with Hugging Face Transformers](https://huggingface.co/blog/fine-tune-vit)\n- A blog post on [Image Classification with Hugging Face Transformers and `Keras`](https://www.philschmid.de/image-classification-huggingface-transformers-keras)",
  "- A notebook on [Fine-tuning for Image Classification with Hugging Face Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n- A notebook on how to [Fine-tune the Vision Transformer on CIFAR-10 with the Hugging Face Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)\n- A notebook on how to [Fine-tune the Vision Transformer on CIFAR-10 with PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)\n\n⚗️ Optimization\n\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization using Optimum](https://www.philschmid.de/optimizing-vision-transformer)\n\n⚡️ Inference\n\n- A notebook on [Quick demo: Vision Transformer (ViT) by Google Brain](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb)\n\n🚀 Deploy",
  "- A blog post on [Deploying Tensorflow Vision Models in Hugging Face with TF Serving](https://huggingface.co/blog/tf-serving-vision)\n- A blog post on [Deploying Hugging Face ViT on Vertex AI](https://huggingface.co/blog/deploy-vertex-ai)\n- A blog post on [Deploying Hugging Face ViT on Kubernetes with TF Serving](https://huggingface.co/blog/deploy-tfserving-kubernetes)\n\n## ViTConfig\n\n[[autodoc]] ViTConfig\n\n## ViTFeatureExtractor\n\n[[autodoc]] ViTFeatureExtractor\n- __call__\n\n## ViTImageProcessor\n\n[[autodoc]] ViTImageProcessor\n- preprocess\n\n## ViTImageProcessorFast\n\n[[autodoc]] ViTImageProcessorFast\n- preprocess\n\n<frameworkcontent>\n<pt>\n\n## ViTModel\n\n[[autodoc]] ViTModel\n- forward\n\n## ViTForMaskedImageModeling\n\n[[autodoc]] ViTForMaskedImageModeling\n- forward\n\n## ViTForImageClassification\n\n[[autodoc]] ViTForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFViTModel\n\n[[autodoc]] TFViTModel\n- call\n\n## TFViTForImageClassification\n\n[[autodoc]] TFViTForImageClassification\n- call\n\n</tf>\n<jax>\n\n## FlaxVitModel\n\n[[autodoc]] FlaxViTModel\n- __call__\n\n## FlaxViTForImageClassification\n\n[[autodoc]] FlaxViTForImageClassification\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Jamba\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. It is the first production-scale Mamba implementation, which opens up interesting research and application opportunities. While this initial experimentation shows encouraging gains, we expect these to be further enhanced with future optimizations and explorations.\n\nFor full details of this model please read the [release blog post](https://www.ai21.com/blog/announcing-jamba).\n\n### Model Details\n\nJamba is a pretrained, mixture-of-experts (MoE) generative text model, with 12B active parameters and an overall of 52B parameters across all experts. It supports a 256K context length, and can fit up to 140K tokens on a single 80GB GPU.\n\nAs depicted in the diagram below, Jamba's architecture features a blocks-and-layers approach that allows Jamba to successfully integrate Transformer and Mamba architectures altogether. Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers.",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/jamba_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n## Usage\n\n### Prerequisites\n\nJamba requires you use `transformers` version 4.39.0 or higher:\n```bash\npip install transformers>=4.39.0\n```\n\nIn order to run optimized Mamba implementations, you first need to install `mamba-ssm` and `causal-conv1d`:\n```bash\npip install mamba-ssm causal-conv1d>=1.2.0\n```\nYou also have to have the model on a CUDA device.\n\nYou can run the model not using the optimized Mamba kernels, but it is **not** recommended as it will result in significantly lower latencies. In order to do that, you'll need to specify `use_mamba_kernels=False` when loading the model.\n\n### Run the model\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n\ninput_ids = tokenizer(\"In the recent Super Bowl LVIII,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n\noutputs = model.generate(input_ids, max_new_tokens=216)\n\nprint(tokenizer.batch_decode(outputs))",
  "# [\"<|startoftext|>In the recent Super Bowl LVIII, the Kansas City Chiefs emerged victorious, defeating the San Francisco 49ers in a thrilling overtime showdown. The game was a nail-biter, with both teams showcasing their skills and determination.\\n\\nThe Chiefs, led by their star quarterback Patrick Mahomes, displayed their offensive prowess, while the 49ers, led by their strong defense, put up a tough fight. The game went into overtime, with the Chiefs ultimately securing the win with a touchdown.\\n\\nThe victory marked the Chiefs' second Super Bowl win in four years, solidifying their status as one of the top teams in the NFL. The game was a testament to the skill and talent of both teams, and a thrilling end to the NFL season.\\n\\nThe Super Bowl is not just about the game itself, but also about the halftime show and the commercials. This year's halftime show featured a star-studded lineup, including Usher, Alicia Keys, and Lil Jon. The show was a spectacle of music and dance, with the performers delivering an energetic and entertaining performance.\\n\"]\n```\n\n<details>\n<summary><strong>Loading the model in half precision</strong></summary>",
  "The published checkpoint is saved in BF16. In order to load it into RAM in BF16/FP16, you need to specify `torch_dtype`:\n\n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\", torch_dtype=torch.bfloat16)\n# you can also use torch_dtype=torch.float16\n```\n\nWhen using half precision, you can enable the [FlashAttention2](https://github.com/Dao-AILab/flash-attention) implementation of the Attention blocks. In order to use it, you also need the model on a CUDA device. Since in this precision the model is to big to fit on a single 80GB GPU, you'll also need to parallelize it using [accelerate](https://huggingface.co/docs/accelerate/index):\n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"auto\")\n```\n\n</details>\n<details><summary><strong>Load the model in 8-bit</strong></summary>",
  "**Using 8-bit precision, it is possible to fit up to 140K sequence lengths on a single 80GB GPU.** You can easily quantize the model to 8-bit using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index). In order to not degrade model quality, we recommend to exclude the Mamba blocks from the quantization:\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_skip_modules=[\"mamba\"])\nmodel = AutoModelForCausalLM.from_pretrained(\n\"ai21labs/Jamba-v0.1\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", quantization_config=quantization_config\n)\n```\n</details>\n\n## JambaConfig\n\n[[autodoc]] JambaConfig\n\n\n## JambaModel\n\n[[autodoc]] JambaModel\n- forward\n\n\n## JambaForCausalLM\n\n[[autodoc]] JambaForCausalLM\n- forward\n\n\n## JambaForSequenceClassification\n\n[[autodoc]] transformers.JambaForSequenceClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LLaMA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.\n\nThe abstract from the paper is the following:",
  "*We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community. *\n\nThis model was contributed by [zphang](https://huggingface.co/zphang) with contributions from [BlackSamorez](https://huggingface.co/BlackSamorez). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n## Usage tips\n\n- Weights for the LLaMA models can be obtained from by filling out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)",
  "- After downloading the weights, they will need to be converted to the Hugging Face Transformers format using the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n--input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```\n\n- After conversion, the model and tokenizer can be loaded via:\n\n```python\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\nmodel = LlamaForCausalLM.from_pretrained(\"/output/path\")\n```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\ncome in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 65B model, it's thus 130GB of RAM needed.",
  "- The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string.\n\nThis model was contributed by [zphang](https://huggingface.co/zphang) with contributions from [BlackSamorez](https://huggingface.co/BlackSamorez). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox). The original code of the authors can be found [here](https://github.com/facebookresearch/llama). The Flax version of the implementation was contributed by [afmck](https://huggingface.co/afmck) with the code in the implementation based on Hugging Face's Flax GPT-Neo.\n\n\nBased on the original LLaMA model, Meta AI has released some follow-up works:\n\n- **Llama2**: Llama2 is an improved version of Llama with some architectural tweaks (Grouped Query Attention), and is pre-trained on 2Trillion tokens. Refer to the documentation of Llama2 which can be found [here](llama2).\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LLaMA. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A [notebook](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb#scrollTo=f04ba4d2) on how to use prompt tuning to adapt the LLaMA model for text classification task. 🌎\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf), a blog post about how to train LLaMA to answer questions on [Stack Exchange](https://stackexchange.com/) with RLHF.\n\n⚗️ Optimization\n- A [notebook](https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing) on how to fine-tune LLaMA model using xturing library on GPU which has limited memory. 🌎\n\n⚡️ Inference",
  "- A [notebook](https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb) on how to run the LLaMA Model using PeftModel from the 🤗 PEFT library. 🌎\n- A [notebook](https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing) on how to load a PEFT adapter LLaMA model with LangChain. 🌎\n\n🚀 Deploy\n- A [notebook](https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb#scrollTo=3PM_DilAZD8T) on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎\n- A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎\n\n## LlamaConfig\n\n[[autodoc]] LlamaConfig\n\n## LlamaTokenizer\n\n[[autodoc]] LlamaTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## LlamaTokenizerFast\n\n[[autodoc]] LlamaTokenizerFast\n- build_inputs_with_special_tokens\n- get_special_tokens_mask",
  "- create_token_type_ids_from_sequences\n- update_post_processor\n- save_vocabulary\n\n## LlamaModel\n\n[[autodoc]] LlamaModel\n- forward\n\n## LlamaForCausalLM\n\n[[autodoc]] LlamaForCausalLM\n- forward\n\n## LlamaForSequenceClassification\n\n[[autodoc]] LlamaForSequenceClassification\n- forward\n\n## LlamaForQuestionAnswering\n\n[[autodoc]] LlamaForQuestionAnswering\n- forward\n\n## LlamaForTokenClassification\n\n[[autodoc]] LlamaForTokenClassification\n- forward\n\n## FlaxLlamaModel\n\n[[autodoc]] FlaxLlamaModel\n- __call__\n\n## FlaxLlamaForCausalLM\n\n[[autodoc]] FlaxLlamaForCausalLM\n- __call__",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LLaVa-NeXT-Video\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video Understanding Model\n](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) by Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, Chunyuan Li. LLaVa-NeXT-Video improves upon [LLaVa-NeXT](llava_next) by fine-tuning on a mix if video and image dataset thus increasing the model's performance on videos.\n\n[LLaVA-NeXT](llava_next) surprisingly has strong performance in understanding video content in zero-shot fashion with the AnyRes technique that it uses. The AnyRes technique naturally represents a high-resolution image into multiple images. This technique is naturally generalizable to represent videos because videos can be considered as a set of frames (similar to a set of images in LLaVa-NeXT). The current version of LLaVA-NeXT makes use of AnyRes and trains with supervised fine-tuning (SFT) on top of LLaVA-Next on video data to achieves better video understanding capabilities.The model is a current SOTA among open-source models on [VideoMME bench](https://arxiv.org/abs/2405.21075).\n\n\nThe introduction from the blog is the following:",
  "On January 30, 2024, we released LLaVA-NeXT, an open-source Large Multimodal Model (LMM) that has been trained exclusively on text-image data. With the proposed AnyRes technique, it boosts capabilities in reasoning, OCR, and world knowledge, demonstrating remarkable performance across a spectrum of image-based multimodal understanding tasks, and even exceeding Gemini-Pro on several image benchmarks, e.g. MMMU and MathVista.\n\n**In today’s exploration, we delve into the performance of LLaVA-NeXT within the realm of video understanding tasks. We reveal that LLaVA-NeXT surprisingly has strong performance in understanding video content. The current version of LLaVA-NeXT for videos has several improvements:",
  "- Zero-shot video representation capabilities with AnyRes: The AnyRes technique naturally represents a high-resolution image into multiple images that a pre-trained VIT is able to digest, and forms them into a concatenated sequence. This technique is naturally generalizable to represent videos (consisting of multiple frames), allowing the image-only-trained LLaVA-Next model to perform surprisingly well on video tasks. Notably, this is the first time that LMMs show strong zero-shot modality transfer ability.\n- Inference with length generalization improves on longer videos. The linear scaling technique enables length generalization, allowing LLaVA-NeXT to effectively handle long-video beyond the limitation of the \"max_token_length\" of the LLM.",
  "- Strong video understanding ability. (1) LLaVA-Next-Image, which combines the above two techniques, yields superior zero-shot performance than open-source LMMs tuned on videos. (2) LLaVA-Next-Video, further supervised fine-tuning (SFT) LLaVA-Next-Image on video data, achieves better video understanding capabilities compared to LLaVA-Next-Image. (3) LLaVA-Next-Video-DPO, which aligns the model response with AI feedback using direct preference optimization (DPO), showing significant performance boost.\n- Efficient deployment and inference with SGLang. It allows 5x faster inference on video tasks, allowing more scalable serving such as million-level video re-captioning. See instructions in our repo.**\n\n\nThis model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\nThe original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/inference).\n\n## Usage tips\n\n- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n\n<Tip warning={true}>",
  "- Llava-Next uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n\n</Tip>\n\n\n> [!NOTE]\n> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.\nAdding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.",
  "The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n\n\n### Formatting Prompts with Chat Templates\n\nEach **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.\n\n**Important:**\n- You must construct a conversation history — passing a plain string won't work.\n- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.\n- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.\n\n\nHere’s an example of how to structure your input. We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images.\n\n```python\nfrom transformers import LlavaNextVideoProcessor\n\nprocessor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n\nconversation = [\n{\n\"role\": \"system\",",
  "\"content\": [\n{\"type\": \"text\", \"text\": \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"},\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What’s shown in this image?\"},\n{\"type\": \"image\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n},\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n{\"type\": \"video\"},\n],\n},\n]\n\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\n# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your visuals\nprint(text_prompt)\n```\n\n🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n\n\n\n## Usage example\n\n### Single Media Mode\n\nThe model can accept both images and videos as input. Here's an example code for inference in half-precision (`torch.float16`):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch",
  "from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n\n# Load the model in half-precision\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n\n# Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos)\nvideo_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n\nconversation = [\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n{\"type\": \"video\", \"path\": video_path},\n],\n},\n]\n\ninputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n\nout = model.generate(**inputs, max_new_tokens=60)\nprocessor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n```\n\n\n### Mixed Media Mode",
  "The model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet:\n\n```python\n\n# Generate from image and video mixed inputs\nconversation = [\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"How many cats are there in the image?\"},\n{\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n],\n},\n{\n\n\"role\": \"assistant\",\n\"content\": [{\"type\": \"text\", \"text\": \"There are two cats\"}],\n},\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n{\"type\": \"video\", \"path\": video_path},\n],\n},\n]\ninputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True, return_tensors=\"pt\")\n\n# Generate\ngenerate_ids = model.generate(**inputs, max_length=50)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n```\n\n## Model optimization\n\n### Quantization using Bitsandbytes for memory efficiency",
  "The model can be loaded in lower bits, significantly reducing memory burden while maintaining the performance of the original model. This allows for efficient deployment on resource-constrained cases.\n\nFirst, make sure to install bitsandbytes by running `pip install bitsandbytes` and to have access to a GPU/accelerator that is supported by the library.\n\n<Tip>\n\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit [this link](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend).\n\nWe value your feedback to help identify bugs before the full release! Check out [these docs](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends) for more details and feedback links.\n\n</Tip>\n\nThen simply load the quantized model by adding [`BitsAndBytesConfig`](../main_classes/quantization#transformers.BitsAndBytesConfig) as shown below:\n\n\n```python\nfrom transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor",
  "# specify how to quantize the model\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", quantization_config=quantization_config, device_map=\"auto\")\n```\n\n\n### Flash-Attention 2 to speed-up generation\n\nAdditionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using Flash Attention-2, simply add `attn_implementation=\"flash_attention_2\"` when loading the model as follows:\n\n```python",
  "from transformers import LlavaNextVideoForConditionalGeneration\n\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\n\"llava-hf/LLaVA-NeXT-Video-7B-hf\",\ntorch_dtype=torch.float16,\nattn_implementation=\"flash_attention_2\",\n).to(0)\n```\n\n\n\n## LlavaNextVideoConfig\n\n[[autodoc]] LlavaNextVideoConfig\n\n## LlavaNextVideoProcessor\n\n[[autodoc]] LlavaNextVideoProcessor\n\n## LlavaNextVideoImageProcessor\n\n[[autodoc]] LlavaNextVideoImageProcessor\n\n## LlavaNextVideoForConditionalGeneration\n\n[[autodoc]] LlavaNextVideoForConditionalGeneration\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLSR-Wav2Vec2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael\nAuli.\n\nThe abstract from the paper is the following:\n\n*This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw\nwaveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over\nmasked latent speech representations and jointly learns a quantization of the latents shared across languages. The\nresulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly\noutperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction\nof 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to\na comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong",
  "individual models. Analysis shows that the latent discrete speech representations are shared across languages with\nincreased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing\nXLSR-53, a large model pretrained in 53 languages.*\n\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).\n\nNote: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert) - it's pretrained on 4.5M hours of audio. We especially recommend using it for fine-tuning tasks, e.g. as per [this guide](https://huggingface.co/blog/fine-tune-w2v2-bert).\n\n## Usage tips\n\n- XLSR-Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- XLSR-Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be\ndecoded using [`Wav2Vec2CTCTokenizer`].\n\n<Tip>\n\nXLSR-Wav2Vec2's architecture is based on the Wav2Vec2 model, so one can refer to [Wav2Vec2's documentation page](wav2vec2).\n\n</Tip>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MVP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n\n\nAccording to the abstract,",
  "- MVP follows a standard Transformer encoder-decoder architecture.\n- MVP is supervised pre-trained using labeled datasets.\n- MVP also has task-specific soft prompts to stimulate the model's capacity in performing a certain task.\n- MVP is specially designed for natural language generation and can be adapted to a wide range of generation tasks, including but not limited to summarization, data-to-text generation, open-ended dialogue system, story generation, question answering, question generation, task-oriented dialogue system, commonsense generation, paraphrase generation, text style transfer, and text simplification. Our model can also be adapted to natural language understanding tasks such as sequence classification and (extractive) question answering.\n\nThis model was contributed by [Tianyi Tang](https://huggingface.co/StevenTang). The detailed information and instructions can be found [here](https://github.com/RUCAIBox/MVP).\n\n## Usage tips\n\n- We have released a series of models [here](https://huggingface.co/models?filter=mvp), including MVP, MVP with task-specific prompts, and multi-task pre-trained variants.",
  "- If you want to use a model without prompts (standard Transformer), you can load it through `MvpForConditionalGeneration.from_pretrained('RUCAIBox/mvp')`.\n- If you want to use a model with task-specific prompts, such as summarization, you can load it through `MvpForConditionalGeneration.from_pretrained('RUCAIBox/mvp-summarization')`.\n- Our model supports lightweight prompt tuning following [Prefix-tuning](https://arxiv.org/abs/2101.00190) with method `set_lightweight_tuning()`.\n\n## Usage examples\n\nFor summarization, it is an example to use MVP and MVP with summarization-specific prompts.\n\n```python\n>>> from transformers import MvpTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n>>> model_with_prompt = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp-summarization\")\n\n>>> inputs = tokenizer(\n...     \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)",
  ">>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Why You Shouldn't Quit Your Job\"]\n\n>>> generated_ids = model_with_prompt.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Don't do it if these are your reasons\"]\n```\n\nFor data-to-text generation, it is an example to use MVP and multi-task pre-trained variants.\n```python\n>>> from transformers import MvpTokenizerFast, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizerFast.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n>>> model_with_mtl = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n\n>>> inputs = tokenizer(\n...     \"Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Stan Lee created the character of Iron Man, a fictional superhero appearing in American comic']\n\n>>> generated_ids = model_with_mtl.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)",
  "['Iron Man is a fictional superhero appearing in American comic books published by Marvel Comics.']\n```\n\nFor lightweight tuning, *i.e.*, fixing the model and only tuning prompts, you can load MVP with randomly initialized prompts or with task-specific prompts. Our code also supports Prefix-tuning with BART following the [original paper](https://arxiv.org/abs/2101.00190).\n\n```python\n>>> from transformers import MvpForConditionalGeneration\n\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\", use_prompt=True)\n>>> # the number of trainable parameters (full tuning)\n>>> sum(p.numel() for p in model.parameters() if p.requires_grad)\n468116832\n\n>>> # lightweight tuning with randomly initialized prompts\n>>> model.set_lightweight_tuning()\n>>> # the number of trainable parameters (lightweight tuning)\n>>> sum(p.numel() for p in model.parameters() if p.requires_grad)\n61823328\n\n>>> # lightweight tuning with task-specific prompts\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n>>> model.set_lightweight_tuning()\n>>> # original lightweight Prefix-tuning",
  ">>> model = MvpForConditionalGeneration.from_pretrained(\"facebook/bart-large\", use_prompt=True)\n>>> model.set_lightweight_tuning()\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## MvpConfig\n\n[[autodoc]] MvpConfig\n\n## MvpTokenizer\n\n[[autodoc]] MvpTokenizer\n\n## MvpTokenizerFast\n\n[[autodoc]] MvpTokenizerFast\n\n## MvpModel\n\n[[autodoc]] MvpModel\n- forward\n\n## MvpForConditionalGeneration\n\n[[autodoc]] MvpForConditionalGeneration\n- forward\n\n## MvpForSequenceClassification\n\n[[autodoc]] MvpForSequenceClassification\n- forward\n\n## MvpForQuestionAnswering\n\n[[autodoc]] MvpForQuestionAnswering\n- forward\n\n## MvpForCausalLM\n\n[[autodoc]] MvpForCausalLM\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MBart and MBart-50\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n\n## Overview of MBart\n\nThe MBart model was presented in [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov Marjan\nGhazvininejad, Mike Lewis, Luke Zettlemoyer.\n\nAccording to the abstract, MBART is a sequence-to-sequence denoising auto-encoder pretrained on large-scale monolingual\ncorpora in many languages using the BART objective. mBART is one of the first methods for pretraining a complete\nsequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only\non the encoder, decoder, or reconstructing parts of the text.\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The Authors' code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/mbart)\n\n### Training of MBart",
  "MBart is a multilingual encoder-decoder (sequence-to-sequence) model primarily intended for translation task. As the\nmodel is multilingual it expects the sequences in a different format. A special language id token is added in both the\nsource and target text. The source text format is `X [eos, src_lang_code]` where `X` is the source text. The\ntarget text format is `[tgt_lang_code] X [eos]`. `bos` is never used.\n\nThe regular [`~MBartTokenizer.__call__`] will encode source text format passed as first argument or with the `text`\nkeyword, and target text format passed with the `text_label` keyword argument.\n\n- Supervised training\n\n```python\n>>> from transformers import MBartForConditionalGeneration, MBartTokenizer\n\n>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n>>> example_english_phrase = \"UN Chief Says There Is No Military Solution in Syria\"\n>>> expected_translation_romanian = \"Şeful ONU declară că nu există o soluţie militară în Siria\"\n\n>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_tensors=\"pt\")",
  ">>> model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\n>>> # forward pass\n>>> model(**inputs)\n```\n\n- Generation\n\nWhile generating the target text set the `decoder_start_token_id` to the target language id. The following\nexample shows how to translate English to Romanian using the *facebook/mbart-large-en-ro* model.\n\n```python\n>>> from transformers import MBartForConditionalGeneration, MBartTokenizer\n\n>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\")\n>>> article = \"UN Chief Says There Is No Military Solution in Syria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n>>> translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"ro_RO\"])\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"Şeful ONU declară că nu există o soluţie militară în Siria\"\n```\n\n## Overview of MBart-50\n\nMBart-50 was introduced in the [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav",
  "Chaudhary, Jiatao Gu, Angela Fan. MBart-50 is created using the original *mbart-large-cc25* checkpoint by extending\nits embedding layers with randomly initialized vectors for an extra set of 25 language tokens and then pretrained on 50\nlanguages.\n\nAccording to the abstract\n\n*Multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one\ndirection, a pretrained model is finetuned on many directions at the same time. It demonstrates that pretrained models\ncan be extended to incorporate additional languages without loss of performance. Multilingual finetuning improves on\naverage 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while\nimproving 9.3 BLEU on average over bilingual baselines from scratch.*\n\n\n### Training of MBart-50\n\nThe text format for MBart-50 is slightly different from mBART. For MBart-50 the language id token is used as a prefix\nfor both source and target text i.e the text format is `[lang_code] X [eos]`, where `lang_code` is source\nlanguage id for source text and target language id for target text, with `X` being the source or target text\nrespectively.",
  "MBart-50 has its own tokenizer [`MBart50Tokenizer`].\n\n-  Supervised training\n\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n\nsrc_text = \" UN Chief Says There Is No Military Solution in Syria\"\ntgt_text = \"Şeful ONU declară că nu există o soluţie militară în Siria\"\n\nmodel_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n\nmodel(**model_inputs)  # forward pass\n```\n\n- Generation\n\nTo generate using the mBART-50 multilingual translation models, `eos_token_id` is used as the\n`decoder_start_token_id` and the target language id is forced as the first generated token. To force the\ntarget language id as the first generated token, pass the *forced_bos_token_id* parameter to the *generate* method.\nThe following example shows how to translate between Hindi to French and Arabic to English using the\n*facebook/mbart-50-large-many-to-many* checkpoint.\n\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast",
  "article_hi = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\narticle_ar = \"الأمين العام للأمم المتحدة يقول إنه لا يوجد حل عسكري في سوريا.\"\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n\n# translate Hindi to French\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"])\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire en Syria.\"\n\n# translate Arabic to English\ntokenizer.src_lang = \"ar_AR\"\nencoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"The Secretary-General of the United Nations says there is no military solution in Syria.\"\n```\n\n## Documentation resources",
  "- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## MBartConfig\n\n[[autodoc]] MBartConfig\n\n## MBartTokenizer\n\n[[autodoc]] MBartTokenizer\n- build_inputs_with_special_tokens\n\n## MBartTokenizerFast\n\n[[autodoc]] MBartTokenizerFast\n\n## MBart50Tokenizer\n\n[[autodoc]] MBart50Tokenizer\n\n## MBart50TokenizerFast\n\n[[autodoc]] MBart50TokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## MBartModel\n\n[[autodoc]] MBartModel\n\n## MBartForConditionalGeneration\n\n[[autodoc]] MBartForConditionalGeneration\n\n## MBartForQuestionAnswering\n\n[[autodoc]] MBartForQuestionAnswering\n\n## MBartForSequenceClassification\n\n[[autodoc]] MBartForSequenceClassification\n\n## MBartForCausalLM\n\n[[autodoc]] MBartForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFMBartModel\n\n[[autodoc]] TFMBartModel\n- call\n\n## TFMBartForConditionalGeneration\n\n[[autodoc]] TFMBartForConditionalGeneration\n- call\n\n</tf>\n<jax>\n\n## FlaxMBartModel",
  "[[autodoc]] FlaxMBartModel\n- __call__\n- encode\n- decode\n\n## FlaxMBartForConditionalGeneration\n\n[[autodoc]] FlaxMBartForConditionalGeneration\n- __call__\n- encode\n- decode\n\n## FlaxMBartForSequenceClassification\n\n[[autodoc]] FlaxMBartForSequenceClassification\n- __call__\n- encode\n- decode\n\n## FlaxMBartForQuestionAnswering\n\n[[autodoc]] FlaxMBartForQuestionAnswering\n- __call__\n- encode\n- decode\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Whisper\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Whisper model was proposed in [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n\nThe abstract from the paper is the following:\n\n*We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.*",
  "This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). The Tensorflow version of this model was contributed by [amyeroberts](https://huggingface.co/amyeroberts).\nThe original code can be found [here](https://github.com/openai/whisper).\n\n## Quick usage\n\nYou can run Whisper in less than 4 lines of code and transcribe in less than a minute!\n\n```python\n# pip install transformers torch\n\nimport torch\nfrom transformers import pipeline\n\nwhisper = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v3\", torch_dtype=torch.float16, device=\"cuda:0\")\n\ntranscription = whisper(\"<audio_file.mp3>\")\n\nprint(transcription[\"text\"])\n```\n\nVoila! You can swap the model with any [Whisper checkpoints](https://huggingface.co/models?other=whisper&sort=downloads) on the Hugging Face Hub with the same pipeline based on your needs.\n\nBonus: You can replace `\"cuda\"` with `\"mps\"` to make it seamlessly work on Macs.\n\n## Usage tips\n\n- The model usually performs well without requiring any finetuning.\n- The architecture follows a classic encoder-decoder architecture, which means that it relies on the [`~generation.GenerationMixin.generate`] function for inference.",
  "- One can use [`WhisperProcessor`] to prepare audio for the model, and decode the predicted ID's back into text.\n\n- To convert the model and the processor, we recommend using the following:\n\n```bash\npython src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path \"\" --pytorch_dump_folder_path \"Arthur/whisper-3\" --convert_preprocessor True\n```\nThe script will automatically determine all necessary parameters from the OpenAI checkpoint. A `tiktoken` library needs to be installed\nto perform the conversion of the OpenAI tokenizer to the `tokenizers` version.\n\n## Inference\n\nHere is a step-by-step guide to transcribing an audio sample using a pre-trained Whisper model:\n\n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n\n>>> # Select an audio file and read it:\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> audio_sample = ds[0][\"audio\"]\n\n>>> # Load the Whisper model in Hugging Face format:\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")",
  ">>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n\n>>> # Use the model and processor to transcribe the audio:\n>>> input_features = processor(\n...     audio_sample[\"array\"], sampling_rate=audio_sample[\"sampling_rate\"], return_tensors=\"pt\"\n... ).input_features\n\n>>> # Generate token ids\n>>> predicted_ids = model.generate(input_features)\n\n>>> # Decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\n>>> transcription[0]\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```\n\nWhisper is compatible with the following optimisations for both short and long-form generation:\n- [PyTorch Scaled Dot Product Attention (SDPA)](../perf_infer_gpu_one#pytorch-scaled-dot-product-attention): flash attention and memory-efficient attention kernels. Enabled by default for `torch>=2.1.1`.\n- [Flash Attention 2](../perf_infer_gpu_one#flashattention-2): improved implementation of flash attention through better parallelism and work partitioning.\n- [torch.compile](../llm_optims#static-kv-cache-and-torchcompile): JIT-compile the forward pass to dispatch to efficient fused kernels.",
  "As an example, the following codesnippet enables SDPA and `torch.compile` for up to 5x faster inference:\n\n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n\n>>> # Select an audio file and read it:\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> audio_sample = ds[0][\"audio\"]\n\n>>> # Load the Whisper model with SDPA attention\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\", attn_implementation=\"sdpa\")\n\n>>> # Enable static cache and compile the forward pass\n>>> model.generation_config.cache_implementation = \"static\"\n>>> model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\n>>> # Use the model and processor to transcribe the audio:\n>>> input_features = processor(\n...     audio_sample[\"array\"], sampling_rate=audio_sample[\"sampling_rate\"], return_tensors=\"pt\"\n... ).input_features\n\n>>> # Compile the forward pass\n>>> for _ in range(2):\n>>>     model.generate(input_features)",
  ">>> # Generate token ids using compiled graph (fast!)\n>>> predicted_ids = model.generate(input_features)\n\n>>> # Decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\n>>> transcription[0]\n' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n```\n\nFor more details on each optimisation, refer to the documentation linked above.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Whisper. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- [Fine-tune Whisper](https://huggingface.co/blog/fine-tune-whisper) on your own dataset for better downstream performance.\n- [Distil-Whisper](https://huggingface.co/distil-whisper): Upto 6x faster, 2x smaller distilled Whisper models for English. We release the [model checkpoints](https://huggingface.co/distil-whisper), and [distillation code](https://github.com/huggingface/distil-whisper).",
  "- A fork with a script to [convert a Whisper model in Hugging Face format to OpenAI format](https://github.com/zuazo-forks/transformers/blob/convert_hf_to_openai/src/transformers/models/whisper/convert_hf_to_openai.py). 🌎\nUsage example:\n```bash\npip install -U openai-whisper\npython convert_hf_to_openai.py \\\n--checkpoint openai/whisper-tiny \\\n--whisper_dump_path whisper-tiny-openai.pt\n```\n\n## WhisperConfig\n\n[[autodoc]] WhisperConfig\n\n## WhisperTokenizer\n\n[[autodoc]] WhisperTokenizer\n- set_prefix_tokens\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n- batch_decode\n- decode\n- basic_normalize\n- normalize\n\n## WhisperTokenizerFast\n\n[[autodoc]] WhisperTokenizerFast\n- set_prefix_tokens\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n- batch_decode\n- decode\n- basic_normalize\n- normalize\n\n## WhisperFeatureExtractor\n\n[[autodoc]] WhisperFeatureExtractor\n- __call__\n\n## WhisperProcessor\n\n[[autodoc]] WhisperProcessor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n<frameworkcontent>\n<pt>\n\n## WhisperModel\n\n[[autodoc]] WhisperModel",
  "- forward\n- _mask_input_features\n\n## WhisperForConditionalGeneration\n\n[[autodoc]] WhisperForConditionalGeneration\n- forward\n- generate\n\n## WhisperForCausalLM\n\n[[autodoc]] WhisperForCausalLM\n- forward\n\n## WhisperForAudioClassification\n\n[[autodoc]] WhisperForAudioClassification\n- forward\n\n</pt>\n<tf>\n\n## TFWhisperModel\n\n[[autodoc]] TFWhisperModel\n- call\n\n## TFWhisperForConditionalGeneration\n\n[[autodoc]] TFWhisperForConditionalGeneration\n- call\n\n</tf>\n<jax>\n\n## FlaxWhisperModel\n\n[[autodoc]] FlaxWhisperModel\n- __call__\n\n## FlaxWhisperForConditionalGeneration\n\n[[autodoc]] FlaxWhisperForConditionalGeneration\n- __call__\n\n## FlaxWhisperForAudioClassification\n\n[[autodoc]] FlaxWhisperForAudioClassification\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LayoutLMv3\n\n## Overview\n\nThe LayoutLMv3 model was proposed in [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.",
  "LayoutLMv3 simplifies [LayoutLMv2](layoutlmv2) by using patch embeddings (as in [ViT](vit)) instead of leveraging a CNN backbone, and pre-trains the model on 3 objectives: masked language modeling (MLM), masked image modeling (MIM)\nand word-patch alignment (WPA).\n\nThe abstract from the paper is the following:",
  "*Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/layoutlmv3_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> LayoutLMv3 architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.08387\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version of this model was added by [chriskoo](https://huggingface.co/chriskoo), [tokec](https://huggingface.co/tokec), and [lre](https://huggingface.co/lre). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/layoutlmv3).\n\n## Usage tips\n\n- In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2](layoutlmv2), except that:\n- images need to be resized and normalized with channels in regular RGB format. LayoutLMv2 on the other hand normalizes the images internally and expects the channels in BGR format.\n- text is tokenized using byte-pair encoding (BPE), as opposed to WordPiece.",
  "Due to these differences in data preprocessing, one can use [`LayoutLMv3Processor`] which internally combines a [`LayoutLMv3ImageProcessor`] (for the image modality) and a [`LayoutLMv3Tokenizer`]/[`LayoutLMv3TokenizerFast`] (for the text modality) to prepare all data for the model.\n- Regarding usage of [`LayoutLMv3Processor`], we refer to the [usage guide](layoutlmv2#usage-layoutlmv2processor) of its predecessor.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LayoutLMv3. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<Tip>\n\nLayoutLMv3 is nearly identical to LayoutLMv2, so we've also included LayoutLMv2 resources you can adapt for LayoutLMv3 tasks. For these notebooks, take care to use [`LayoutLMv2Processor`] instead when preparing data for the model!\n\n</Tip>\n\n- Demo notebooks for LayoutLMv3 can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LayoutLMv3).",
  "- Demo scripts can be found [here](https://github.com/huggingface/transformers-research-projects/tree/main/layoutlmv3).\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- [`LayoutLMv2ForSequenceClassification`] is supported by this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- [`LayoutLMv3ForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers-research-projects/tree/main/layoutlmv3) and [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb).",
  "- A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb) for how to perform inference with [`LayoutLMv2ForTokenClassification`] and a [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb) for how to perform inference when no labels are available with [`LayoutLMv2ForTokenClassification`].\n- A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb) for how to finetune [`LayoutLMv2ForTokenClassification`] with the 🤗 Trainer.\n- [Token classification task guide](../tasks/token_classification)\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- [`LayoutLMv2ForQuestionAnswering`] is supported by this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb).",
  "- [Question answering task guide](../tasks/question_answering)\n\n**Document question answering**\n- [Document question answering task guide](../tasks/document_question_answering)\n\n## LayoutLMv3Config\n\n[[autodoc]] LayoutLMv3Config\n\n## LayoutLMv3FeatureExtractor\n\n[[autodoc]] LayoutLMv3FeatureExtractor\n- __call__\n\n## LayoutLMv3ImageProcessor\n\n[[autodoc]] LayoutLMv3ImageProcessor\n- preprocess\n\n## LayoutLMv3Tokenizer\n\n[[autodoc]] LayoutLMv3Tokenizer\n- __call__\n- save_vocabulary\n\n## LayoutLMv3TokenizerFast\n\n[[autodoc]] LayoutLMv3TokenizerFast\n- __call__\n\n## LayoutLMv3Processor\n\n[[autodoc]] LayoutLMv3Processor\n- __call__\n\n<frameworkcontent>\n<pt>\n\n## LayoutLMv3Model\n\n[[autodoc]] LayoutLMv3Model\n- forward\n\n## LayoutLMv3ForSequenceClassification\n\n[[autodoc]] LayoutLMv3ForSequenceClassification\n- forward\n\n## LayoutLMv3ForTokenClassification\n\n[[autodoc]] LayoutLMv3ForTokenClassification\n- forward\n\n## LayoutLMv3ForQuestionAnswering\n\n[[autodoc]] LayoutLMv3ForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFLayoutLMv3Model\n\n[[autodoc]] TFLayoutLMv3Model\n- call\n\n## TFLayoutLMv3ForSequenceClassification\n\n[[autodoc]] TFLayoutLMv3ForSequenceClassification\n- call\n\n## TFLayoutLMv3ForTokenClassification",
  "[[autodoc]] TFLayoutLMv3ForTokenClassification\n- call\n\n## TFLayoutLMv3ForQuestionAnswering\n\n[[autodoc]] TFLayoutLMv3ForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Deformable DETR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Deformable DETR model was proposed in [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.",
  "Deformable DETR mitigates the slow convergence issues and limited feature spatial resolution of the original [DETR](detr) by leveraging a new deformable attention module which only attends to a small set of key sampling points around a reference.\n\nThe abstract from the paper is the following:\n\n*DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_detr_architecture.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Deformable DETR architecture. Taken from the <a href=\"https://arxiv.org/abs/2010.04159\">original paper</a>.</small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/fundamentalvision/Deformable-DETR).\n\n## Usage tips\n\n- Training Deformable DETR is equivalent to training the original [DETR](detr) model. See the [resources](#resources) section below for demo notebooks.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Deformable DETR.\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- Demo notebooks regarding inference + fine-tuning on a custom dataset for [`DeformableDetrForObjectDetection`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Deformable-DETR).\n- Scripts for finetuning [`DeformableDetrForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection).",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DeformableDetrImageProcessor\n\n[[autodoc]] DeformableDetrImageProcessor\n- preprocess\n- post_process_object_detection\n\n## DeformableDetrImageProcessorFast\n\n[[autodoc]] DeformableDetrImageProcessorFast\n- preprocess\n- post_process_object_detection\n\n## DeformableDetrFeatureExtractor\n\n[[autodoc]] DeformableDetrFeatureExtractor\n- __call__\n- post_process_object_detection\n\n## DeformableDetrConfig\n\n[[autodoc]] DeformableDetrConfig\n\n## DeformableDetrModel\n\n[[autodoc]] DeformableDetrModel\n- forward\n\n## DeformableDetrForObjectDetection\n\n[[autodoc]] DeformableDetrForObjectDetection\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mllama\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image.\n\n**Model Architecture:** Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\n\n## Usage Tips\n\n- For image+text and text inputs use `MllamaForConditionalGeneration`.\n- For text-only inputs use `MllamaForCausalLM` for generation to avoid loading vision tower.",
  "- Each sample can contain multiple images, and the number of images can vary between samples. The processor will pad the inputs to the maximum number of images across samples and to a maximum number of tiles within each image.\n- The text passed to the processor should have the `\"<|image|>\"` tokens where the images should be inserted.\n- The processor has its own `apply_chat_template` method to convert chat messages to text that can then be passed as text to the processor. If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n\n\n\n<Tip warning={true}>\n\nMllama has an extra token used as a placeholder for image positions in the text. It means that input ids and an input embedding layer will have an extra token. But since the weights for input and output embeddings are not tied, the `lm_head` layer has one less token and will fail if you want to calculate loss on image tokens or apply some logit processors. In case you are training, make sure to mask out special `\"<|image|>\"` tokens in the `labels` as the model should not be trained on predicting them.",
  "Otherwise if you see CUDA-side index erros when generating, use the below code to expand the `lm_head` by one more token.\n\n\n```python\nold_embeddings = model.get_output_embeddings()\n\nnum_tokens = model.vocab_size + 1\nresized_embeddings = model._get_resized_lm_head(old_embeddings, new_num_tokens=num_tokens, mean_resizing=True)\nresized_embeddings.requires_grad_(old_embeddings.weight.requires_grad)\nmodel.set_output_embeddings(resized_embeddings)\n```\n</Tip>\n\n\n## Usage Example\n\n#### Instruct model\n```python\nimport torch\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\n\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\nmodel = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n{\"type\": \"text\", \"text\": \"What does the image show?\"}\n]\n}\n],\n]\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, max_new_tokens=25)",
  "print(processor.decode(output[0]))\n```\n\n#### Base model\n```python\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\n\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision\"\nmodel = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nprompt = \"<|image|>If I had to write a haiku for this one\"\nurl = \"https://llava-vl.github.io/static/images/view.jpg\"\nraw_image = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, do_sample=False, max_new_tokens=25)\nprint(processor.decode(output[0], skip_special_tokens=True))\n```\n\n\n## MllamaConfig\n\n[[autodoc]] MllamaConfig\n\n## MllamaProcessor\n\n[[autodoc]] MllamaProcessor\n\n\n## MllamaImageProcessor\n\n[[autodoc]] MllamaImageProcessor\n\n## MllamaForConditionalGeneration\n\n[[autodoc]] MllamaForConditionalGeneration\n- forward\n\n## MllamaForCausalLM\n\n[[autodoc]] MllamaForCausalLM\n- forward\n\n## MllamaTextModel\n\n[[autodoc]] MllamaTextModel\n- forward\n\n## MllamaForCausalLM",
  "[[autodoc]] MllamaForCausalLM\n- forward\n\n## MllamaVisionModel\n\n[[autodoc]] MllamaVisionModel\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n\nBLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering\n- Image-Text retrieval (Image-text matching)\n- Image Captioning\n\nThe abstract from the paper is the following:\n\n*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks.",
  "However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n\n![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif)\n\nThis model was contributed by [ybelkada](https://huggingface.co/ybelkada).",
  "The original code can be found [here](https://github.com/salesforce/BLIP).\n\n## Resources\n\n- [Jupyter notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb) on how to fine-tune BLIP for image captioning on a custom dataset\n\n## BlipConfig\n\n[[autodoc]] BlipConfig\n- from_text_vision_configs\n\n## BlipTextConfig\n\n[[autodoc]] BlipTextConfig\n\n## BlipVisionConfig\n\n[[autodoc]] BlipVisionConfig\n\n## BlipProcessor\n\n[[autodoc]] BlipProcessor\n\n## BlipImageProcessor\n\n[[autodoc]] BlipImageProcessor\n- preprocess\n\n## BlipImageProcessorFast\n\n[[autodoc]] BlipImageProcessorFast\n- preprocess\n\n<frameworkcontent>\n<pt>\n\n## BlipModel\n\n`BlipModel` is going to be deprecated in future versions, please use `BlipForConditionalGeneration`, `BlipForImageTextRetrieval` or `BlipForQuestionAnswering` depending on your usecase.\n\n[[autodoc]] BlipModel\n- forward\n- get_text_features\n- get_image_features\n\n## BlipTextModel\n\n[[autodoc]] BlipTextModel\n- forward\n\n## BlipVisionModel\n\n[[autodoc]] BlipVisionModel\n- forward\n\n## BlipForConditionalGeneration\n\n[[autodoc]] BlipForConditionalGeneration\n- forward\n\n## BlipForImageTextRetrieval\n\n[[autodoc]] BlipForImageTextRetrieval",
  "- forward\n\n## BlipForQuestionAnswering\n\n[[autodoc]] BlipForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFBlipModel\n\n[[autodoc]] TFBlipModel\n- call\n- get_text_features\n- get_image_features\n\n## TFBlipTextModel\n\n[[autodoc]] TFBlipTextModel\n- call\n\n## TFBlipVisionModel\n\n[[autodoc]] TFBlipVisionModel\n- call\n\n## TFBlipForConditionalGeneration\n\n[[autodoc]] TFBlipForConditionalGeneration\n- call\n\n## TFBlipForImageTextRetrieval\n\n[[autodoc]] TFBlipForImageTextRetrieval\n- call\n\n## TFBlipForQuestionAnswering\n\n[[autodoc]] TFBlipForQuestionAnswering\n- call\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Persimmon\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Persimmon model was created by [ADEPT](https://www.adept.ai/blog/persimmon-8b), and authored by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.",
  "The authors introduced Persimmon-8B, a decoder model based on the classic transformers architecture, with query and key normalization. Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters, released under the Apache license.  Some of the key attributes of Persimmon-8B are long context size (16K), performance, and capabilities for multimodal extensions.\n\nThe authors showcase their approach to model evaluation, focusing on practical text generation, mirroring how users interact with language models. The work also includes a comparative analysis, pitting Persimmon-8B against other prominent models (MPT 7B Instruct and Llama 2 Base 7B 1-Shot), across various evaluation tasks. The results demonstrate Persimmon-8B's competitive performance, even with limited training data.",
  "In terms of model details, the work outlines the architecture and training methodology of Persimmon-8B, providing insights into its design choices, sequence length, and dataset composition. The authors present a fast inference code that outperforms traditional implementations through operator fusion and CUDA graph utilization while maintaining code coherence. They express their anticipation of how the community will leverage this contribution to drive innovation, hinting at further upcoming releases as part of an ongoing series of developments.\n\nThis model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).\n\n## Usage tips\n\n<Tip warning={true}>\n\nThe `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.",
  "The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n\nFinetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n\n</Tip>\n\n\nTips:\n\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\n\n```bash\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar\ntar -xvf 8b_base_model_release.tar",
  "python src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \\\n--pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n--ada_lib_path /path/to/adept-inference\n```\n\nFor the chat model:\n```bash\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```\n\nThereafter, models can be loaded via:\n\n```py\nfrom transformers import PersimmonForCausalLM, PersimmonTokenizer\n\nmodel = PersimmonForCausalLM.from_pretrained(\"/output/path\")\ntokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\n```\n\n\n- Perismmon uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\nThe `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece. The `chat` template will be updated with the templating functions in a follow up PR!\n\n- The authors suggest to use the following prompt format for the chat mode: `f\"human: {prompt}\\n\\nadept:\"`",
  "## PersimmonConfig\n\n[[autodoc]] PersimmonConfig\n\n## PersimmonModel\n\n[[autodoc]] PersimmonModel\n- forward\n\n## PersimmonForCausalLM\n\n[[autodoc]] PersimmonForCausalLM\n- forward\n\n## PersimmonForSequenceClassification\n\n[[autodoc]] PersimmonForSequenceClassification\n- forward\n\n## PersimmonForTokenClassification\n\n[[autodoc]] PersimmonForTokenClassification\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DePlot\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\n\nThe abstract of the paper states the following:",
  "*Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.*",
  "DePlot is a model that is trained using `Pix2Struct` architecture. You can find more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).\nDePlot is a Visual Question Answering subset of `Pix2Struct` architecture. It renders the input question on the image and predicts the answer.\n\n## Usage example\n\nCurrently one checkpoint is available for DePlot:\n\n- `google/deplot`: DePlot fine-tuned on ChartQA dataset\n\n\n```python\nfrom transformers import AutoProcessor, Pix2StructForConditionalGeneration\nimport requests\nfrom PIL import Image\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\")\nprocessor = AutoProcessor.from_pretrained(\"google/deplot\")\nurl = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n```\n\n## Fine-tuning",
  "To fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb). For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor and cosine learning rate scheduler leads to faster convergence:\n```python\nfrom transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\n\noptimizer = Adafactor(self.parameters(), scale_parameter=False, relative_step=False, lr=0.01, weight_decay=1e-05)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=40000)\n```\n\n<Tip>\n\nDePlot is a model trained using `Pix2Struct` architecture. For API reference, see [`Pix2Struct` documentation](pix2struct).\n\n</Tip>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Gemma2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Gemma2 model was proposed in [Gemma2: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/google-gemma-2/) by Gemma2 Team, Google.\nTwo Gemma2 models are released, with parameters sizes of 9 billion (9B) and 27 billion (27B).\n\nThe abstract from the blog post is the following:\n\n*Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December.*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script `src/transformers/models/Gemma2/convert_Gemma2_weights_to_hf.py`\n\n<Tip warning={true}>",
  "- Gemma2 uses sliding window attention every second layer, which makes it unsuitable for typical kv caching with [`~DynamicCache`] or tuples of tensors. To enable caching in Gemma2 forward call, you must initialize a [`~HybridCache`] instance and pass it as `past_key_values` to the forward call. Note, that you also have to prepare `cache_position` if the `past_key_values` already contains previous keys and values.\n\n</Tip>\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Pedro Cuenca](https://huggingface.co/pcuenq) and [Tom Arsen]().\n\n\n## Gemma2Config\n\n[[autodoc]] Gemma2Config\n\n## Gemma2Model\n\n[[autodoc]] Gemma2Model\n- forward\n\n## Gemma2ForCausalLM\n\n[[autodoc]] Gemma2ForCausalLM\n- forward\n\n## Gemma2ForSequenceClassification\n\n[[autodoc]] Gemma2ForSequenceClassification\n- forward\n\n## Gemma2ForTokenClassification\n\n[[autodoc]] Gemma2ForTokenClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Moshi\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Moshi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://kyutai.org/Moshi.pdf) by Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave and Neil Zeghidour.\n\nMoshi is a speech-text foundation model that casts spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. Moshi also predicts time-aligned text tokens as a prefix to audio tokens. This “Inner Monologue” method significantly improves the linguistic quality of generated speech and provides streaming speech recognition and text-to-speech. As a result, Moshi is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/moshi_architecture.png\">",
  "</div>\n\nThe abstract from the paper is the following:",
  "*We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.*",
  "Moshi deals with 3 streams of information:\n1. The user's audio\n2. Moshi's audio\n3. Moshi's textual output\n\nSimilarly to [`~MusicgenModel`], audio is represented with audio codebooks, which can be interpreted like tokens. The main difference between text tokens and audio codebooks is that audio codebooks introduce an additional dimension of information.\nText tokens are typically of dim `(batch_size, sequence_length)` but audio tokens are of dim `(batch_size, num_codebooks, sequence_length)`.\n\nMoshi's made of 3 components:\n\n**1. The main decoder (Helium in the paper)**\n\nIt corresponds to [`MoshiForCausalLM`]. It is strictly a classic text LLM, that uses an architecture similar to [` ~GemmaForCausalLM`]. In other words, it takes text tokens, embeds them, pass them through the decoder and a language head, to get text logits.\n\n**2. The depth decoder**\n\nOn its own, it's also a classic LLM, but this time, instead of generating over the time dimension, it generates over the codebook dimension.\n\nIt also means that its context length is `num_codebooks`, thus it can't generate more than `num_codebooks`.",
  "Note that each timestamp - i.e each codebook - gets its own set of Linear Layers and Embeddings.\n\n**3. [`MimiModel`]**\n\nIt's the audio encoder from Kyutai, that has recently been integrated to transformers, which is used to \"tokenize\" audio. It has the same use that [`~EncodecModel`] has in [`~MusicgenModel`].\n\n\n## Tips:\n\nThe original checkpoints can be converted using the conversion script `src/transformers/models/moshi/convert_moshi_transformers.py`\n\n\n### How to use the model:\n\nThis implementation has two main aims:\n1. quickly test model generation by simplifying the original API\n2. simplify training. A training guide will come soon, but user contributions are welcomed!\n\n<Tip>\n\nIt is designed for intermediate use. We strongly recommend using the original [implementation](https://github.com/kyutai-labs/moshi) to infer the model in real-time streaming.\n\n</Tip>\n\n**1. Model generation**\n\nMoshi is a streaming auto-regressive model with two streams of audio. To put it differently, one audio stream corresponds to what the model said/will say and the other audio stream corresponds to what the user said/will say.\n\n[`MoshiForConditionalGeneration.generate`] thus needs 3 inputs:",
  "1. `input_ids` - corresponding to the text token history\n2. `moshi_input_values` or `moshi_audio_codes`- corresponding to the model audio history\n3. `user_input_values` or `user_audio_codes` - corresponding to the user audio history\n\nThese three inputs must be synchronized. Meaning that their lengths must correspond to the same number of tokens.\n\nYou can dynamically use the 3 inputs depending on what you want to test:\n1. Simply check the model response to an user prompt - in that case, `input_ids` can be filled with pad tokens and `user_input_values` can be a zero tensor of the same shape than the user prompt.\n2. Test more complex behaviour - in that case, you must be careful about how the input tokens are synchronized with the audios.\n\n<Tip>\n\nThe original model is synchronized text with audio by padding the text in between each token enunciation.\n\nTo follow the example of the following image, `\"Hello, I'm Moshi\"` could be transformed to `\"Hello,<pad><unk>I'm Moshi\"`.\n\n</Tip>\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/moshi_text_sync.png\">\n</div>",
  "[`MoshiForConditionalGeneration.generate`] then auto-regressively feeds to itself its own audio stream, but since it doesn't have access to the user input stream while using `transformers`, it will thus **assume that the user is producing blank audio**.\n\n\n\n```python\n>>> from datasets import load_dataset, Audio\n>>> import torch, math\n>>> from transformers import MoshiForConditionalGeneration, AutoFeatureExtractor, AutoTokenizer\n\n\n>>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/moshiko-pytorch-bf16\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"kyutai/moshiko-pytorch-bf16\")\n>>> device = \"cuda\"\n>>> dtype = torch.bfloat16\n\n>>> # prepare user input audio\n>>> librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n>>> audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n>>> user_input_values = feature_extractor(raw_audio=audio_sample, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\").to(device=device, dtype=dtype)",
  ">>> # prepare moshi input values - we suppose moshi didn't say anything while the user spoke\n>>> moshi_input_values = torch.zeros_like(user_input_values.input_values)\n\n>>> # prepare moshi input ids - we suppose moshi didn't say anything while the user spoke\n>>> num_tokens = math.ceil(moshi_input_values.shape[-1] * waveform_to_token_ratio)\n>>> input_ids = torch.ones((1, num_tokens), device=device, dtype=torch.int64) * tokenizer.encode(\"<pad>\")[0]\n\n>>> # generate 25 new tokens (around 2s of audio)\n>>> output = model.generate(input_ids=input_ids, user_input_values=user_input_values.input_values, moshi_input_values=moshi_input_values, max_new_tokens=25)\n\n>>> text_tokens = output.sequences\n>>> audio_waveforms = output.audio_sequences\n```\n\n**2. Model training**\n\nMost of the work has to be done during data creation/pre-processing, because of the need to align/synchronize streams.\n\nOnce it's done, you can simply forward `text_labels` and `audio_labels` to [`MoshiForConditionalGeneration.forward`], alongside the usual inputs, to get the model loss.\n\nA training guide will come soon, but user contributions are welcomed!\n\n### How does the model forward the inputs / generate:",
  "1. The input streams are embedded and combined into `inputs_embeds`.\n\n2. `inputs_embeds` is passed through the main decoder, which processes it like a normal LLM would.\n\n3. The main decoder outputs `text logits` but also its `last hidden state` which is called `temporal context` in the paper.\n\n3. The depth decoder switches the dimension on which we forward / generate (codebooks instead of time). It uses the token generated from `text logits`  and the `temporal context` to auto-regressively generate audio codebooks.\n\n\nThis model was contributed by [Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe).\n\nThe original code can be found [here](https://github.com/kyutai-labs/moshi).\n\n\n\n## MoshiConfig\n\n[[autodoc]] MoshiConfig\n\n## MoshiDepthConfig\n\n[[autodoc]] MoshiDepthConfig\n\n## MoshiModel\n\n[[autodoc]] MoshiModel\n- forward\n\n## MoshiForCausalLM\n\n[[autodoc]] MoshiForCausalLM\n- forward\n\n## MoshiForConditionalGeneration\n\n[[autodoc]] MoshiForConditionalGeneration\n- forward\n- generate\n- get_unconditional_inputs",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MaskFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip>\n\nThis is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight",
  "breaking changes to fix it in the future. If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title).\n\n</Tip>\n\n## Overview\n\nThe MaskFormer model was proposed in [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov. MaskFormer addresses semantic segmentation with a mask classification paradigm instead of performing classic pixel-level classification.\n\nThe abstract from the paper is the following:",
  "*Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.*\n\nThe figure below illustrates the architecture of MaskFormer. Taken from the [original paper](https://arxiv.org/abs/2107.06278).",
  "<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/maskformer_architecture.png\"/>\n\nThis model was contributed by [francesco](https://huggingface.co/francesco). The original code can be found [here](https://github.com/facebookresearch/MaskFormer).\n\n## Usage tips\n\n-  MaskFormer's Transformer decoder is identical to the decoder of [DETR](detr). During training, the authors of DETR did find it helpful to use auxiliary losses in the decoder, especially to help the model output the correct number of objects of each class. If you set the parameter `use_auxiliary_loss` of [`MaskFormerConfig`] to `True`, then prediction feedforward neural networks and Hungarian losses are added after each decoder layer (with the FFNs sharing parameters).\n- If you want to train the model in a distributed environment across multiple nodes, then one should update the\n`get_num_masks` function inside in the `MaskFormerLoss` class of `modeling_maskformer.py`. When training on multiple nodes, this should be",
  "set to the average number of target masks across all nodes, as can be seen in the original implementation [here](https://github.com/facebookresearch/MaskFormer/blob/da3e60d85fdeedcb31476b5edd7d328826ce56cc/mask_former/modeling/criterion.py#L169).\n- One can use [`MaskFormerImageProcessor`] to prepare images for the model and optional targets for the model.\n- To get the final segmentation, depending on the task, you can call [`~MaskFormerImageProcessor.post_process_semantic_segmentation`] or [`~MaskFormerImageProcessor.post_process_panoptic_segmentation`]. Both tasks can be solved using [`MaskFormerForInstanceSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.\n\n## Resources\n\n<PipelineTag pipeline=\"image-segmentation\"/>\n\n- All notebooks that illustrate inference as well as fine-tuning on custom data with MaskFormer can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MaskFormer).",
  "- Scripts for finetuning [`MaskFormer`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/instance-segmentation).\n\n## MaskFormer specific outputs\n\n[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerModelOutput\n\n[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput\n\n## MaskFormerConfig\n\n[[autodoc]] MaskFormerConfig\n\n## MaskFormerImageProcessor\n\n[[autodoc]] MaskFormerImageProcessor\n- preprocess\n- encode_inputs\n- post_process_semantic_segmentation\n- post_process_instance_segmentation\n- post_process_panoptic_segmentation\n\n## MaskFormerFeatureExtractor\n\n[[autodoc]] MaskFormerFeatureExtractor\n- __call__\n- encode_inputs\n- post_process_semantic_segmentation\n- post_process_instance_segmentation\n- post_process_panoptic_segmentation\n\n## MaskFormerModel\n\n[[autodoc]] MaskFormerModel\n- forward\n\n## MaskFormerForInstanceSegmentation\n\n[[autodoc]] MaskFormerForInstanceSegmentation\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# KOSMOS-2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe KOSMOS-2 model was proposed in [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.",
  "KOSMOS-2 is a Transformer-based causal language model and is trained using the next-word prediction task on a web-scale\ndataset of grounded image-text pairs [GRIT](https://huggingface.co/datasets/zzliang/GRIT). The spatial coordinates of\nthe bounding boxes in the dataset are converted to a sequence of location tokens, which are appended to their respective\nentity text spans (for example, `a snowman` followed by `<patch_index_0044><patch_index_0863>`). The data format is\nsimilar to “hyperlinks” that connect the object regions in an image to their text span in the corresponding caption.\n\nThe abstract from the paper is the following:",
  "*We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/kosmos_2_overview.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Overview of tasks that KOSMOS-2 can handle. Taken from the <a href=\"https://arxiv.org/abs/2306.14824\">original paper</a>. </small>\n\n## Example\n\n```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, Kosmos2ForConditionalGeneration\n\n>>> model = Kosmos2ForConditionalGeneration.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n\n>>> url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> prompt = \"<grounding> An image of\"\n\n>>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n\n>>> generated_ids = model.generate(\n...     pixel_values=inputs[\"pixel_values\"],\n...     input_ids=inputs[\"input_ids\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     image_embeds=None,\n...     image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n...     use_cache=True,",
  "...     max_new_tokens=64,\n... )\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n>>> processed_text\n'<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.'\n\n>>> caption, entities = processor.post_process_generation(generated_text)\n>>> caption\n'An image of a snowman warming himself by a fire.'\n\n>>> entities\n[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]\n```\n\nThis model was contributed by [Yih-Dar SHIEH](https://huggingface.co/ydshieh). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/kosmos-2).\n\n## Kosmos2Config\n\n[[autodoc]] Kosmos2Config\n\n## Kosmos2ImageProcessor\n\n## Kosmos2Processor\n\n[[autodoc]] Kosmos2Processor\n- __call__\n\n## Kosmos2Model\n\n[[autodoc]] Kosmos2Model\n- forward\n\n## Kosmos2ForConditionalGeneration\n\n[[autodoc]] Kosmos2ForConditionalGeneration\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Wav2Vec2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state",
  "of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\nNote: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert) - it's pretrained on 4.5M hours of audio. We especially recommend using it for fine-tuning tasks, e.g. as per [this guide](https://huggingface.co/blog/fine-tune-w2v2-bert).\n\n## Usage tips\n\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`].\n\n## Using Flash Attention 2\n\nFlash Attention 2 is an faster, optimized version of the model.\n\n### Installation",
  "First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n\nNext, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n### Usage\n\nTo load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\n```python\n>>> from transformers import Wav2Vec2Model",
  "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n...\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of the `facebook/wav2vec2-large-960h-lv60-self` model and the flash-attention-2 and sdpa (scale-dot-product-attention) versions. . We show the average speedup obtained on the `librispeech_asr` `clean` validation split:\n\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/kamilakesbi/transformers_image_doc/resolve/main/data/Wav2Vec2_speedup.png\">\n</div>\n\n\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"audio-classification\"/>",
  "- A notebook on how to [leverage a pretrained Wav2Vec2 model for emotion classification](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb). 🌎\n- [`Wav2Vec2ForCTC`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n- [Audio classification task guide](../tasks/audio_classification)\n\n<PipelineTag pipeline=\"automatic-speech-recognition\"/>\n\n- A blog post on [boosting Wav2Vec2 with n-grams in 🤗 Transformers](https://huggingface.co/blog/wav2vec2-with-ngram).\n- A blog post on how to [finetune Wav2Vec2 for English ASR with 🤗 Transformers](https://huggingface.co/blog/fine-tune-wav2vec2-english).\n- A blog post on [finetuning XLS-R for Multi-Lingual ASR with 🤗 Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).",
  "- A notebook on how to [create YouTube captions from any video by transcribing audio with Wav2Vec2](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb). 🌎\n- [`Wav2Vec2ForCTC`] is supported by a notebook on [how to finetune a speech recognition model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb), and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).\n- [Automatic speech recognition task guide](../tasks/asr)\n\n🚀 Deploy\n\n- A blog post on how to deploy Wav2Vec2 for [Automatic Speech Recognition with Hugging Face's Transformers & Amazon SageMaker](https://www.philschmid.de/automatic-speech-recognition-sagemaker).\n\n## Wav2Vec2Config\n\n[[autodoc]] Wav2Vec2Config\n\n## Wav2Vec2CTCTokenizer\n\n[[autodoc]] Wav2Vec2CTCTokenizer\n- __call__\n- save_vocabulary\n- decode\n- batch_decode\n- set_target_lang\n\n## Wav2Vec2FeatureExtractor\n\n[[autodoc]] Wav2Vec2FeatureExtractor\n- __call__\n\n## Wav2Vec2Processor\n\n[[autodoc]] Wav2Vec2Processor\n- __call__",
  "- pad\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## Wav2Vec2ProcessorWithLM\n\n[[autodoc]] Wav2Vec2ProcessorWithLM\n- __call__\n- pad\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n### Decoding multiple audios\n\nIf you are planning to decode multiple batches of audios, you should consider using [`~Wav2Vec2ProcessorWithLM.batch_decode`] and passing an instantiated `multiprocessing.Pool`.\nOtherwise, [`~Wav2Vec2ProcessorWithLM.batch_decode`] performance will be slower than calling [`~Wav2Vec2ProcessorWithLM.decode`] for each audio individually, as it internally instantiates a new `Pool` for every call. See the example below:\n\n```python\n>>> # Let's see how to use a user-managed pool for batch decoding multiple audios\n>>> from multiprocessing import get_context\n>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC\n>>> from datasets import load_dataset\n>>> import datasets\n>>> import torch\n\n>>> # import model, feature extractor, tokenizer\n>>> model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\").to(\"cuda\")\n>>> processor = AutoProcessor.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")",
  ">>> # load example dataset\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n\n\n>>> def map_to_array(batch):\n...     batch[\"speech\"] = batch[\"audio\"][\"array\"]\n...     return batch\n\n\n>>> # prepare speech data for batch inference\n>>> dataset = dataset.map(map_to_array, remove_columns=[\"audio\"])\n\n\n>>> def map_to_pred(batch, pool):\n...     inputs = processor(batch[\"speech\"], sampling_rate=16_000, padding=True, return_tensors=\"pt\")\n...     inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n\n...     with torch.no_grad():\n...         logits = model(**inputs).logits\n\n...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text\n...     batch[\"transcription\"] = transcription\n...     return batch\n\n\n>>> # note: pool should be instantiated *after* `Wav2Vec2ProcessorWithLM`.\n>>> #       otherwise, the LM won't be available to the pool's sub-processes\n>>> # select number of processes and batch_size based on number of CPU cores available and on dataset size\n>>> with get_context(\"fork\").Pool(processes=2) as pool:\n...     result = dataset.map(",
  "...         map_to_pred, batched=True, batch_size=2, fn_kwargs={\"pool\": pool}, remove_columns=[\"speech\"]\n...     )\n\n>>> result[\"transcription\"][:2]\n['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', \"NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER\"]\n```\n\n## Wav2Vec2 specific outputs\n\n[[autodoc]] models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput\n\n[[autodoc]] models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput\n\n[[autodoc]] models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput\n\n[[autodoc]] models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput\n\n[[autodoc]] models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## Wav2Vec2Model\n\n[[autodoc]] Wav2Vec2Model\n- forward\n\n## Wav2Vec2ForCTC\n\n[[autodoc]] Wav2Vec2ForCTC\n- forward\n- load_adapter\n\n## Wav2Vec2ForSequenceClassification\n\n[[autodoc]] Wav2Vec2ForSequenceClassification\n- forward\n\n## Wav2Vec2ForAudioFrameClassification\n\n[[autodoc]] Wav2Vec2ForAudioFrameClassification\n- forward\n\n## Wav2Vec2ForXVector\n\n[[autodoc]] Wav2Vec2ForXVector\n- forward\n\n## Wav2Vec2ForPreTraining",
  "[[autodoc]] Wav2Vec2ForPreTraining\n- forward\n\n</pt>\n<tf>\n\n## TFWav2Vec2Model\n\n[[autodoc]] TFWav2Vec2Model\n- call\n\n## TFWav2Vec2ForSequenceClassification\n\n[[autodoc]] TFWav2Vec2ForSequenceClassification\n- call\n\n## TFWav2Vec2ForCTC\n\n[[autodoc]] TFWav2Vec2ForCTC\n- call\n\n</tf>\n<jax>\n\n## FlaxWav2Vec2Model\n\n[[autodoc]] FlaxWav2Vec2Model\n- __call__\n\n## FlaxWav2Vec2ForCTC\n\n[[autodoc]] FlaxWav2Vec2ForCTC\n- __call__\n\n## FlaxWav2Vec2ForPreTraining\n\n[[autodoc]] FlaxWav2Vec2ForPreTraining\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPT-Sw3\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC",
  "\">\n</div>\n\n## Overview\n\nThe GPT-Sw3 model was first proposed in\n[Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)\nby Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman,\nFredrik Carlsson, Magnus Sahlgren.\n\nSince that first paper the authors have extended their work and trained new models on their new 1.2TB corpora named The Nordic Pile.\n\nGPT-Sw3 is a collection of large decoder-only pretrained transformer language models that were developed by AI Sweden\nin collaboration with RISE and the WASP WARA for Media and Language. GPT-Sw3 has been trained on a dataset containing\n320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code. The model was pretrained using a\ncausal language modeling (CLM) objective utilizing the NeMo Megatron GPT implementation.\n\nThis model was contributed by [AI Sweden Models](https://huggingface.co/AI-Sweden-Models).\n\n## Usage example\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM",
  ">>> tokenizer = AutoTokenizer.from_pretrained(\"AI-Sweden-Models/gpt-sw3-356m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"AI-Sweden-Models/gpt-sw3-356m\")\n\n>>> input_ids = tokenizer(\"Träd är fina för att\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=10, do_sample=True)[0]\n\n>>> print(tokenizer.decode(generated_token_ids))\nTräd är fina för att de är färgstarka. Men ibland är det fint\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n<Tip>\n\nThe implementation uses the `GPT2Model` coupled with our `GPTSw3Tokenizer`. Refer to [GPT2Model documentation](gpt2)\nfor API reference and examples.\n\nNote that sentencepiece is required to use our tokenizer and can be installed with `pip install transformers[sentencepiece]` or `pip install sentencepiece`\n\n</Tip>\n\n## GPTSw3Tokenizer\n\n[[autodoc]] GPTSw3Tokenizer\n- save_vocabulary",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Video Vision Transformer (ViViT)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Vivit model was proposed in [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.\nThe paper proposes one of the first successful pure-transformer based set of models for video understanding.\n\nThe abstract from the paper is the following:",
  "*We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks.*",
  "This model was contributed by [jegormeister](https://huggingface.co/jegormeister). The original code (written in JAX) can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit).\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import VivitModel\nmodel = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```",
  "For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vivit-b-16x2-kinetics400` model, we saw the following speedups during inference.\n\n### Training\n|   num_training_steps |   batch_size |   is cuda |   Speedup (%) |   Eager peak mem (MB) |   sdpa peak mem (MB) |   Mem saving (%) |\n|---------------------:|-------------:|----------:|--------------:|----------------------:|---------------------:|-----------------:|\n|                  100 |            1 |      True |         7.122 |               2575.28 |              5932.54 |           130.364 |\n\n\n\n### Inference\n|   num_batches |   batch_size |   is cuda |   is half |   Speedup (%) |   Mem eager (MB) |   Mem BT (MB) |   Mem saved (%) |\n|---------------|--------------|-----------|-----------|---------------|------------------|---------------|-----------------|\n|            20 |             1 |   True    |   False   |      15.422   |     715.807      |    317.079    |      125.75     |",
  "|            20 |             2 |   True    |   False   |      17.146   |    1234.75       |    447.175    |      176.122    |\n|            20 |             4 |   True    |   False   |      18.093   |    2275.82       |    709.864    |      220.6      |\n|            20 |             8 |   True    |   False   |      19.284   |    4358.19       |   1233.24     |      253.393    |\n\n\n## VivitConfig\n\n[[autodoc]] VivitConfig\n\n## VivitImageProcessor\n\n[[autodoc]] VivitImageProcessor\n- preprocess\n\n## VivitModel\n\n[[autodoc]] VivitModel\n- forward\n\n## VivitForVideoClassification\n\n[[autodoc]] transformers.VivitForVideoClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ResNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`. This is generally known as \"ResNet v1.5\".\n\nResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\n\nThe abstract from the paper is the following:",
  "*Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.",
  "The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.*\n\nThe figure below illustrates the architecture of ResNet. Taken from the [original paper](https://arxiv.org/abs/1512.03385).\n\n<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png\"/>\n\nThis model was contributed by [Francesco](https://huggingface.co/Francesco). The TensorFlow version of this model was added by [amyeroberts](https://huggingface.co/amyeroberts). The original code can be found [here](https://github.com/KaimingHe/deep-residual-networks).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ResNet.\n\n<PipelineTag pipeline=\"image-classification\"/>",
  "- [`ResNetForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ResNetConfig\n\n[[autodoc]] ResNetConfig\n\n<frameworkcontent>\n<pt>\n\n## ResNetModel\n\n[[autodoc]] ResNetModel\n- forward\n\n## ResNetForImageClassification\n\n[[autodoc]] ResNetForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFResNetModel\n\n[[autodoc]] TFResNetModel\n- call\n\n## TFResNetForImageClassification\n\n[[autodoc]] TFResNetForImageClassification\n- call\n\n</tf>\n<jax>\n\n## FlaxResNetModel\n\n[[autodoc]] FlaxResNetModel\n- __call__\n\n## FlaxResNetForImageClassification\n\n[[autodoc]] FlaxResNetForImageClassification\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# VAN\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.",
  "You can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n</Tip>\n\n## Overview\n\nThe VAN model was proposed in [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n\nThis paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations.\n\nThe abstract from the paper is the following:",
  "*While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at [this https URL](https://github.com/Visual-Attention-Network/VAN-Classification).*\n\nTips:",
  "- VAN does not have an embedding layer, thus the `hidden_states` will have a length equal to the number of stages.\n\nThe figure below illustrates the architecture of a Visual Attention Layer. Taken from the [original paper](https://arxiv.org/abs/2202.09741).\n\n<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/van_architecture.png\"/>\n\nThis model was contributed by [Francesco](https://huggingface.co/Francesco). The original code can be found [here](https://github.com/Visual-Attention-Network/VAN-Classification).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with VAN.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`VanForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## VanConfig\n\n[[autodoc]] VanConfig\n\n## VanModel\n\n[[autodoc]] VanModel\n- forward\n\n## VanForImageClassification\n\n[[autodoc]] VanForImageClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FlauBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The FlauBERT model was proposed in the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le et al. It's a transformer model pretrained using a masked language\nmodeling (MLM) objective (like BERT).\n\nThe abstract from the paper is the following:\n\n*Language models have become a key step to achieve state-of-the art results in many different Natural Language\nProcessing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way\nto pre-train continuous word representations that can be fine-tuned for a downstream task, along with their\ncontextualization at the sentence level. This has been widely demonstrated for English using contextualized\nrepresentations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and\nheterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for",
  "Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text\nclassification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the\ntime they outperform other pretraining approaches. Different versions of FlauBERT as well as a unified evaluation\nprotocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research\ncommunity for further reproducible experiments in French NLP.*\n\nThis model was contributed by [formiel](https://huggingface.co/formiel). The original code can be found [here](https://github.com/getalp/Flaubert).\n\nTips:\n- Like RoBERTa, without the sentence ordering prediction (so just trained on the MLM objective).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## FlaubertConfig\n\n[[autodoc]] FlaubertConfig\n\n## FlaubertTokenizer",
  "[[autodoc]] FlaubertTokenizer\n\n<frameworkcontent>\n<pt>\n\n## FlaubertModel\n\n[[autodoc]] FlaubertModel\n- forward\n\n## FlaubertWithLMHeadModel\n\n[[autodoc]] FlaubertWithLMHeadModel\n- forward\n\n## FlaubertForSequenceClassification\n\n[[autodoc]] FlaubertForSequenceClassification\n- forward\n\n## FlaubertForMultipleChoice\n\n[[autodoc]] FlaubertForMultipleChoice\n- forward\n\n## FlaubertForTokenClassification\n\n[[autodoc]] FlaubertForTokenClassification\n- forward\n\n## FlaubertForQuestionAnsweringSimple\n\n[[autodoc]] FlaubertForQuestionAnsweringSimple\n- forward\n\n## FlaubertForQuestionAnswering\n\n[[autodoc]] FlaubertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFFlaubertModel\n\n[[autodoc]] TFFlaubertModel\n- call\n\n## TFFlaubertWithLMHeadModel\n\n[[autodoc]] TFFlaubertWithLMHeadModel\n- call\n\n## TFFlaubertForSequenceClassification\n\n[[autodoc]] TFFlaubertForSequenceClassification\n- call\n\n## TFFlaubertForMultipleChoice\n\n[[autodoc]] TFFlaubertForMultipleChoice\n- call\n\n## TFFlaubertForTokenClassification\n\n[[autodoc]] TFFlaubertForTokenClassification\n- call\n\n## TFFlaubertForQuestionAnsweringSimple\n\n[[autodoc]] TFFlaubertForQuestionAnsweringSimple\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The Intel Team Authors and HuggingFace Inc. team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# TVP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe text-visual prompting (TVP) framework was proposed in the paper [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995) by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.\n\nThe abstract from the paper is the following:",
  "*In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call ‘prompts’) into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of cross-modal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments on two benchmark datasets, Charades-STA and ActivityNet Captions datasets, empirically show that the proposed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on ActivityNet Captions) and achieves 5× inference acceleration over TVG using 3D visual features.*",
  "This research addresses temporal video grounding (TVG), which is the process of pinpointing the start and end times of specific events in a long video, as described by a text sentence. Text-visual prompting (TVP), is proposed to enhance TVG. TVP involves integrating specially designed patterns, known as 'prompts', into both the visual (image-based) and textual (word-based) input components of a TVG model. These prompts provide additional spatial-temporal context, improving the model's ability to accurately determine event timings in the video. The approach employs 2D visual inputs in place of 3D ones. Although 3D inputs offer more spatial-temporal detail, they are also more time-consuming to process. The use of 2D inputs with the prompting method aims to provide similar levels of context and accuracy more efficiently.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvp_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> TVP architecture. Taken from the <a href=\"https://arxiv.org/abs/2303.04995\">original paper.</a> </small>",
  "This model was contributed by [Jiqing Feng](https://huggingface.co/Jiqing). The original code can be found [here](https://github.com/intel/TVP).\n\n## Usage tips and examples\n\nPrompts are optimized perturbation patterns, which would be added to input video frames or text features. Universal set refers to using the same exact set of prompts for any input, this means that these prompts are added consistently to all video frames and text features, regardless of the input's content.\n\nTVP consists of a visual encoder and cross-modal encoder. A universal set of visual prompts and text prompts to be integrated into sampled video frames and textual features, respectively. Specially, a set of different visual prompts are applied to uniformly-sampled frames of one untrimmed video in order.\n\nThe goal of this model is to incorporate trainable prompts into both visual inputs and textual features to temporal video grounding(TVG) problems.\nIn principle, one can apply any visual, cross-modal encoder in the proposed architecture.\n\nThe [`TvpProcessor`] wraps [`BertTokenizer`] and [`TvpImageProcessor`] into a single instance to both\nencode the text and prepare the images respectively.",
  "The following example shows how to run temporal video grounding using [`TvpProcessor`] and [`TvpForVideoGrounding`].\n```python\nimport av\nimport cv2\nimport numpy as np\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoProcessor, TvpForVideoGrounding\n\n\ndef pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n'''\nConvert the video from its original fps to the target_fps and decode the video with PyAV decoder.\nArgs:\ncontainer (container): pyav container.\nsampling_rate (int): frame sampling rate (interval between two sampled frames).\nnum_frames (int): number of frames to sample.\nclip_idx (int): if clip_idx is -1, perform random temporal sampling.\nIf clip_idx is larger than -1, uniformly split the video to num_clips\nclips, and select the clip_idx-th video clip.\nnum_clips (int): overall number of clips to uniformly sample from the given video.\ntarget_fps (int): the input video may have different fps, convert it to\nthe target video fps before frame sampling.\nReturns:\nframes (tensor): decoded frames from the video. Return None if the no\nvideo stream was found.",
  "fps (float): the number of frames per second of the video.\n'''\nvideo = container.streams.video[0]\nfps = float(video.average_rate)\nclip_size = sampling_rate * num_frames / target_fps * fps\ndelta = max(num_frames - clip_size, 0)\nstart_idx = delta * clip_idx / num_clips\nend_idx = start_idx + clip_size - 1\ntimebase = video.duration / num_frames\nvideo_start_pts = int(start_idx * timebase)\nvideo_end_pts = int(end_idx * timebase)\nseek_offset = max(video_start_pts - 1024, 0)\ncontainer.seek(seek_offset, any_frame=False, backward=True, stream=video)\nframes = {}\nfor frame in container.decode(video=0):\nif frame.pts < video_start_pts:\ncontinue\nframes[frame.pts] = frame\nif frame.pts > video_end_pts:\nbreak\nframes = [frames[pts] for pts in sorted(frames)]\nreturn frames, fps\n\n\ndef decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n'''\nDecode the video and perform temporal sampling.\nArgs:\ncontainer (container): pyav container.\nsampling_rate (int): frame sampling rate (interval between two sampled frames).\nnum_frames (int): number of frames to sample.\nclip_idx (int): if clip_idx is -1, perform random temporal sampling.",
  "If clip_idx is larger than -1, uniformly split the video to num_clips\nclips, and select the clip_idx-th video clip.\nnum_clips (int): overall number of clips to uniformly sample from the given video.\ntarget_fps (int): the input video may have different fps, convert it to\nthe target video fps before frame sampling.\nReturns:\nframes (tensor): decoded frames from the video.\n'''\nassert clip_idx >= -2, \"Not a valid clip_idx {}\".format(clip_idx)\nframes, fps = pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps)\nclip_size = sampling_rate * num_frames / target_fps * fps\nindex = np.linspace(0, clip_size - 1, num_frames)\nindex = np.clip(index, 0, len(frames) - 1).astype(np.int64)\nframes = np.array([frames[idx].to_rgb().to_ndarray() for idx in index])\nframes = frames.transpose(0, 3, 1, 2)\nreturn frames\n\n\nfile = hf_hub_download(repo_id=\"Intel/tvp_demo\", filename=\"AK2KG.mp4\", repo_type=\"dataset\")\nmodel = TvpForVideoGrounding.from_pretrained(\"Intel/tvp-base\")\n\ndecoder_kwargs = dict(\ncontainer=av.open(file, metadata_errors=\"ignore\"),\nsampling_rate=1,\nnum_frames=model.config.num_frames,\nclip_idx=0,\nnum_clips=1,\ntarget_fps=3,\n)",
  "raw_sampled_frms = decode(**decoder_kwargs)\n\ntext = \"a person is sitting on a bed.\"\nprocessor = AutoProcessor.from_pretrained(\"Intel/tvp-base\")\nmodel_inputs = processor(\ntext=[text], videos=list(raw_sampled_frms), return_tensors=\"pt\", max_text_length=100#, size=size\n)\n\nmodel_inputs[\"pixel_values\"] = model_inputs[\"pixel_values\"].to(model.dtype)\noutput = model(**model_inputs)\n\ndef get_video_duration(filename):\ncap = cv2.VideoCapture(filename)\nif cap.isOpened():\nrate = cap.get(5)\nframe_num = cap.get(7)\nduration = frame_num/rate\nreturn duration\nreturn -1\n\nduration = get_video_duration(file)\nstart, end = processor.post_process_video_grounding(output.logits, duration)\n\nprint(f\"The time slot of the video corresponding to the text \\\"{text}\\\" is from {start}s to {end}s\")\n```\n\nTips:\n\n- This implementation of TVP uses [`BertTokenizer`] to generate text embeddings and Resnet-50 model to compute visual embeddings.\n- Checkpoints for pre-trained [tvp-base](https://huggingface.co/Intel/tvp-base) is released.\n- Please refer to [Table 2](https://arxiv.org/pdf/2303.04995.pdf) for TVP's performance on Temporal Video Grounding task.\n\n\n## TvpConfig\n\n[[autodoc]] TvpConfig\n\n## TvpImageProcessor",
  "[[autodoc]] TvpImageProcessor\n- preprocess\n\n## TvpProcessor\n\n[[autodoc]] TvpProcessor\n- __call__\n\n## TvpModel\n\n[[autodoc]] TvpModel\n- forward\n\n## TvpForVideoGrounding\n\n[[autodoc]] TvpForVideoGrounding\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT",
  "model with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\n*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena",
  "benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n## Usage tips\n\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)",
  "## FNetConfig\n\n[[autodoc]] FNetConfig\n\n## FNetTokenizer\n\n[[autodoc]] FNetTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## FNetTokenizerFast\n\n[[autodoc]] FNetTokenizerFast\n\n## FNetModel\n\n[[autodoc]] FNetModel\n- forward\n\n## FNetForPreTraining\n\n[[autodoc]] FNetForPreTraining\n- forward\n\n## FNetForMaskedLM\n\n[[autodoc]] FNetForMaskedLM\n- forward\n\n## FNetForNextSentencePrediction\n\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n\n## FNetForSequenceClassification\n\n[[autodoc]] FNetForSequenceClassification\n- forward\n\n## FNetForMultipleChoice\n\n[[autodoc]] FNetForMultipleChoice\n- forward\n\n## FNetForTokenClassification\n\n[[autodoc]] FNetForTokenClassification\n- forward\n\n## FNetForQuestionAnswering\n\n[[autodoc]] FNetForQuestionAnswering\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UniSpeech\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The UniSpeech model was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael\nZeng, Xuedong Huang .\n\nThe abstract from the paper is the following:\n\n*In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both\nunlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive\nself-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture\ninformation more correlated with phonetic structures and improve the generalization across languages and domains. We\nevaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The\nresults show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech\nrecognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all",
  "testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,\ni.e., a relative word error rate reduction of 6% against the previous approach.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Authors' code can be\nfound [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech).\n\n## Usage tips\n\n- UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please\nuse [`Wav2Vec2Processor`] for the feature extraction.\n- UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be\ndecoded using [`Wav2Vec2CTCTokenizer`].\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## UniSpeechConfig\n\n[[autodoc]] UniSpeechConfig\n\n## UniSpeech specific outputs\n\n[[autodoc]] models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput\n\n## UniSpeechModel\n\n[[autodoc]] UniSpeechModel\n- forward\n\n## UniSpeechForCTC\n\n[[autodoc]] UniSpeechForCTC\n- forward\n\n## UniSpeechForSequenceClassification",
  "[[autodoc]] UniSpeechForSequenceClassification\n- forward\n\n## UniSpeechForPreTraining\n\n[[autodoc]] UniSpeechForPreTraining\n- forward",
  "<!--Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CPMAnt\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. It is also the first milestone of the live training process of CPM-Live. The training process is cost-effective and environment-friendly. CPM-Ant also achieves promising results with delta tuning on the CUGE benchmark. Besides the full model, we also provide various compressed versions to meet the requirements of different hardware configurations. [See more](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)\n\nThis model was contributed by [OpenBMB](https://huggingface.co/openbmb). The original code can be found [here](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live).\n\n## Resources\n\n- A tutorial on [CPM-Live](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live).\n\n## CpmAntConfig\n\n[[autodoc]] CpmAntConfig\n- all\n\n## CpmAntTokenizer\n\n[[autodoc]] CpmAntTokenizer\n- all\n\n## CpmAntModel\n\n[[autodoc]] CpmAntModel\n- all\n\n## CpmAntForCausalLM\n\n[[autodoc]] CpmAntForCausalLM\n- all",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Speech Encoder Decoder Models\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\nThe [`SpeechEncoderDecoderModel`] can be used to initialize a speech-to-text model\nwith any pretrained speech autoencoding model as the encoder (*e.g.* [Wav2Vec2](wav2vec2), [Hubert](hubert)) and any pretrained autoregressive model as the decoder.\n\nThe effectiveness of initializing speech-sequence-to-text-sequence models with pretrained checkpoints for speech\nrecognition and speech translation has *e.g.* been shown in [Large-Scale Self- and Semi-Supervised Learning for Speech\nTranslation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli,\nAlexis Conneau.\n\nAn example of how to use a [`SpeechEncoderDecoderModel`] for inference can be seen in [Speech2Text2](speech_to_text_2).\n\n## Randomly initializing `SpeechEncoderDecoderModel` from model configurations.",
  "[`SpeechEncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`Wav2Vec2Model`] configuration for the encoder\nand the default [`BertForCausalLM`] configuration for the decoder.\n\n```python\n>>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel\n\n>>> config_encoder = Wav2Vec2Config()\n>>> config_decoder = BertConfig()\n\n>>> config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n>>> model = SpeechEncoderDecoderModel(config=config)\n```\n\n## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder.",
  "[`SpeechEncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained Transformer-based speech model, *e.g.* [Wav2Vec2](wav2vec2), [Hubert](hubert) can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the decoder.\nDepending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.\nInitializing [`SpeechEncoderDecoderModel`] from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder).\nTo do so, the `SpeechEncoderDecoderModel` class provides a [`SpeechEncoderDecoderModel.from_encoder_decoder_pretrained`] method.\n\n```python\n>>> from transformers import SpeechEncoderDecoderModel\n\n>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(",
  "...     \"facebook/hubert-large-ll60k\", \"google-bert/bert-base-uncased\"\n... )\n```\n\n## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference.\n\nTo load fine-tuned checkpoints of the `SpeechEncoderDecoderModel` class, [`SpeechEncoderDecoderModel`] provides the `from_pretrained(...)` method just like any other model architecture in Transformers.\n\nTo perform inference, one uses the [`generate`] method, which allows to autoregressively generate text. This method supports various forms of decoding, such as greedy, beam search and multinomial sampling.\n\n```python\n>>> from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> # load a fine-tuned speech translation model and corresponding processor\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n>>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n\n>>> # let's perform inference on a piece of English speech (which we'll translate to German)\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")",
  ">>> input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n\n>>> # autoregressively generate transcription (uses greedy decoding by default)\n>>> generated_ids = model.generate(input_values)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\nMr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heißen zu können.\n```\n\n## Training\n\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other encoder-decoder model on a dataset of (speech, text) pairs.\nAs you can see, only 2 inputs are required for the model in order to compute a loss: `input_values` (which are the\nspeech inputs) and `labels` (which are the `input_ids` of the encoded target sequence).\n\n```python\n>>> from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n>>> from datasets import load_dataset\n\n>>> encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n>>> decoder_id = \"google-bert/bert-base-uncased\"  # text decoder\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)",
  ">>> tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n>>> # Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model\n>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> # load an audio input and pre-process (normalise mean/std to 0/1)\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n\n>>> # load its corresponding transcription and tokenize to generate labels\n>>> labels = tokenizer(ds[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_values=input_values, labels=labels).loss\n>>> loss.backward()\n```\n\n## SpeechEncoderDecoderConfig\n\n[[autodoc]] SpeechEncoderDecoderConfig\n\n## SpeechEncoderDecoderModel\n\n[[autodoc]] SpeechEncoderDecoderModel\n- forward\n- from_encoder_decoder_pretrained\n\n## FlaxSpeechEncoderDecoderModel\n\n[[autodoc]] FlaxSpeechEncoderDecoderModel",
  "- __call__\n- from_encoder_decoder_pretrained",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mimi\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Mimi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://kyutai.org/Moshi.pdf) by Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave and Neil Zeghidour. Mimi is a high-fidelity audio codec model developed by the Kyutai team, that combines semantic and acoustic information into audio tokens running at 12Hz and a bitrate of 1.1kbps. In other words, it can be used to map audio waveforms into “audio tokens”, known as “codebooks”.\n\nThe abstract from the paper is the following:",
  "*We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.*",
  "Its architecture is based on [Encodec](model_doc/encodec) with several major differences:\n* it uses a much lower frame-rate.\n* it uses additional transformers for encoding and decoding for better latent contextualization\n* it uses a different quantization scheme: one codebook is dedicated to semantic projection.\n\n## Usage example\n\nHere is a quick example of how to encode and decode an audio using this model:\n\n```python\n>>> from datasets import load_dataset, Audio\n>>> from transformers import MimiModel, AutoFeatureExtractor\n>>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> # load model and feature extractor\n>>> model = MimiModel.from_pretrained(\"kyutai/mimi\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")\n\n>>> # load audio sample\n>>> librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n>>> audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n>>> inputs = feature_extractor(raw_audio=audio_sample, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")",
  ">>> encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n>>> audio_values = model.decode(encoder_outputs.audio_codes, inputs[\"padding_mask\"])[0]\n>>> # or the equivalent with a forward pass\n>>> audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n```\n\nThis model was contributed by [Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe).\nThe original code can be found [here](https://github.com/kyutai-labs/moshi).\n\n\n## MimiConfig\n\n[[autodoc]] MimiConfig\n\n## MimiModel\n\n[[autodoc]] MimiModel\n- decode\n- encode\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MMS\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516)\nby Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli\n\nThe abstract from the paper is the following:\n\n*Expanding the language coverage of speech technology has the potential to improve access to information for many more people.\nHowever, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000\nlanguages spoken around the world.\nThe Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task.\nThe main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging\nself-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages,\na single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models",
  "for the same number of languages, as well as a language identification model for 4,017 languages.\nExperiments show that our multilingual speech recognition model more than halves the word error rate of\nWhisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.*\n\nHere are the different models open sourced in the MMS project. The models and code are originally released [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms). We have add them to the `transformers` framework, making them easier to use.\n\n### Automatic Speech Recognition (ASR)\n\nThe ASR model checkpoints  can be found here : [mms-1b-fl102](https://huggingface.co/facebook/mms-1b-fl102), [mms-1b-l1107](https://huggingface.co/facebook/mms-1b-l1107), [mms-1b-all](https://huggingface.co/facebook/mms-1b-all). For best accuracy, use the `mms-1b-all` model.\n\nTips:\n\n- All ASR models accept a float array corresponding to the raw waveform of the speech signal. The raw waveform should be pre-processed with [`Wav2Vec2FeatureExtractor`].\n- The models were trained using connectionist temporal classification (CTC) so the model output has to be decoded using",
  "[`Wav2Vec2CTCTokenizer`].\n- You can load different language adapter weights for different languages via [`~Wav2Vec2PreTrainedModel.load_adapter`]. Language adapters only consists of roughly 2 million parameters\nand can therefore be efficiently loaded on the fly when needed.\n\n#### Loading\n\nBy default MMS loads adapter weights for English. If you want to load adapter weights of another language\nmake sure to specify `target_lang=<your-chosen-target-lang>` as well as `\"ignore_mismatched_sizes=True`.\nThe `ignore_mismatched_sizes=True` keyword has to be passed to allow the language model head to be resized according\nto the vocabulary of the specified language.\nSimilarly, the processor should be loaded with the same target language\n\n```py\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\n\nmodel_id = \"facebook/mms-1b-all\"\ntarget_lang = \"fra\"\n\nprocessor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True)\n```\n\n<Tip>\n\nYou can safely ignore a warning such as:\n\n```text",
  "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([314]) in the model instantiated\n- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([314, 1280]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```\n\n</Tip>\n\nIf you want to use the ASR pipeline, you can load your chosen target language as such:\n\n```py\nfrom transformers import pipeline\n\nmodel_id = \"facebook/mms-1b-all\"\ntarget_lang = \"fra\"\n\npipe = pipeline(model=model_id, model_kwargs={\"target_lang\": \"fra\", \"ignore_mismatched_sizes\": True})\n```\n\n#### Inference\n\nNext, let's look at how we can run MMS in inference and change adapter layers after having called [`~PretrainedModel.from_pretrained`]\nFirst, we load audio data in different languages using the [Datasets](https://github.com/huggingface/datasets).\n\n```py\nfrom datasets import load_dataset, Audio\n\n# English",
  "stream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n\n# French\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"fr\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nfr_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n```\n\nNext, we load the model and processor\n\n```py\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\nimport torch\n\nmodel_id = \"facebook/mms-1b-all\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id)\n```\n\nNow we process the audio data, pass the processed audio data to the model and transcribe the model output,\njust like we usually do for [`Wav2Vec2ForCTC`].\n\n```py\ninputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\noutputs = model(**inputs).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n# 'joe keton disapproved of films and buster also had reservations about the media'\n```",
  "We can now keep the same model in memory and simply switch out the language adapters by\ncalling the convenient [`~Wav2Vec2ForCTC.load_adapter`] function for the model and [`~Wav2Vec2CTCTokenizer.set_target_lang`] for the tokenizer.\nWe pass the target language as an input - `\"fra\"` for French.\n\n```py\nprocessor.tokenizer.set_target_lang(\"fra\")\nmodel.load_adapter(\"fra\")\n\ninputs = processor(fr_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\noutputs = model(**inputs).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n# \"ce dernier est volé tout au long de l'histoire romaine\"\n```\n\nIn the same way the language can be switched out for all other supported languages. Please have a look at:\n\n```py\nprocessor.tokenizer.vocab.keys()\n```\n\nto see all supported languages.\n\nTo further improve performance from ASR models, language model decoding can be used. See the documentation [here](https://huggingface.co/facebook/mms-1b-all) for further details.\n\n### Speech Synthesis (TTS)\n\nMMS-TTS uses the same model architecture as VITS, which was added to 🤗 Transformers in v4.33. MMS trains a separate",
  "model checkpoint for each of the 1100+ languages in the project. All available checkpoints can be found on the Hugging\nFace Hub: [facebook/mms-tts](https://huggingface.co/models?sort=trending&search=facebook%2Fmms-tts), and the inference\ndocumentation under [VITS](https://huggingface.co/docs/transformers/main/en/model_doc/vits).\n\n#### Inference\n\nTo use the MMS model, first update to the latest version of the Transformers library:\n\n```bash\npip install --upgrade transformers accelerate\n```\n\nSince the flow-based model in VITS is non-deterministic, it is good practice to set a seed to ensure reproducibility of\nthe outputs.\n\n- For languages with a Roman alphabet, such as English or French, the tokenizer can be used directly to\npre-process the text inputs. The following code example runs a forward pass using the MMS-TTS English checkpoint:\n\n```python\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n\ninputs = tokenizer(text=\"Hello - my dog is cute\", return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\n\nwith torch.no_grad():",
  "outputs = model(**inputs)\n\nwaveform = outputs.waveform[0]\n```\n\nThe resulting waveform can be saved as a `.wav` file:\n\n```python\nimport scipy\n\nscipy.io.wavfile.write(\"synthesized_speech.wav\", rate=model.config.sampling_rate, data=waveform)\n```\n\nOr displayed in a Jupyter Notebook / Google Colab:\n\n```python\nfrom IPython.display import Audio\n\nAudio(waveform, rate=model.config.sampling_rate)\n```\n\nFor certain languages with non-Roman alphabets, such as Arabic, Mandarin or Hindi, the [`uroman`](https://github.com/isi-nlp/uroman)\nperl package is required to pre-process the text inputs to the Roman alphabet.\n\nYou can check whether you require the `uroman` package for your language by inspecting the `is_uroman` attribute of\nthe pre-trained `tokenizer`:\n\n```python\nfrom transformers import VitsTokenizer\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nprint(tokenizer.is_uroman)\n```\n\nIf required, you should apply the uroman package to your text inputs **prior** to passing them to the `VitsTokenizer`,\nsince currently the tokenizer does not support performing the pre-processing itself.",
  "To do this, first clone the uroman repository to your local machine and set the bash variable `UROMAN` to the local path:\n\n```bash\ngit clone https://github.com/isi-nlp/uroman.git\ncd uroman\nexport UROMAN=$(pwd)\n```\n\nYou can then pre-process the text input using the following code snippet. You can either rely on using the bash variable\n`UROMAN` to point to the uroman repository, or you can pass the uroman directory as an argument to the `uromanize` function:\n\n```python\nimport torch\nfrom transformers import VitsTokenizer, VitsModel, set_seed\nimport os\nimport subprocess\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-kor\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-kor\")\n\ndef uromanize(input_string, uroman_path):\n\"\"\"Convert non-Roman strings to Roman using the `uroman` perl package.\"\"\"\nscript_path = os.path.join(uroman_path, \"bin\", \"uroman.pl\")\n\ncommand = [\"perl\", script_path]\n\nprocess = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n# Execute the perl command\nstdout, stderr = process.communicate(input=input_string.encode())\n\nif process.returncode != 0:",
  "raise ValueError(f\"Error {process.returncode}: {stderr.decode()}\")\n\n# Return the output as a string and skip the new-line character at the end\nreturn stdout.decode()[:-1]\n\ntext = \"이봐 무슨 일이야\"\nuromanized_text = uromanize(text, uroman_path=os.environ[\"UROMAN\"])\n\ninputs = tokenizer(text=uromanized_text, return_tensors=\"pt\")\n\nset_seed(555)  # make deterministic\nwith torch.no_grad():\noutputs = model(inputs[\"input_ids\"])\n\nwaveform = outputs.waveform[0]\n```\n\n**Tips:**\n\n* The MMS-TTS checkpoints are trained on lower-cased, un-punctuated text. By default, the `VitsTokenizer` *normalizes* the inputs by removing any casing and punctuation, to avoid passing out-of-vocabulary characters to the model. Hence, the model is agnostic to casing and punctuation, so these should be avoided in the text prompt. You can disable normalisation by setting `normalize=False` in the call to the tokenizer, but this will lead to un-expected behaviour and is discouraged.\n* The speaking rate can be varied by setting the attribute `model.speaking_rate` to a chosen value. Likewise, the randomness of the noise is controlled by `model.noise_scale`:\n\n```python\nimport torch",
  "from transformers import VitsTokenizer, VitsModel, set_seed\n\ntokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\nmodel = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n\ninputs = tokenizer(text=\"Hello - my dog is cute\", return_tensors=\"pt\")\n\n# make deterministic\nset_seed(555)\n\n# make speech faster and more noisy\nmodel.speaking_rate = 1.5\nmodel.noise_scale = 0.8\n\nwith torch.no_grad():\noutputs = model(**inputs)\n```\n\n### Language Identification (LID)\n\nDifferent LID models are available based on the number of languages they can recognize - [126](https://huggingface.co/facebook/mms-lid-126), [256](https://huggingface.co/facebook/mms-lid-256), [512](https://huggingface.co/facebook/mms-lid-512), [1024](https://huggingface.co/facebook/mms-lid-1024), [2048](https://huggingface.co/facebook/mms-lid-2048), [4017](https://huggingface.co/facebook/mms-lid-4017).\n\n#### Inference\nFirst, we install transformers and some other libraries\n\n```bash\npip install torch accelerate datasets[audio]\npip install --upgrade transformers\n````\n\nNext, we load a couple of audio samples via `datasets`. Make sure that the audio data is sampled to 16000 kHz.\n\n```py",
  "from datasets import load_dataset, Audio\n\n# English\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n\n# Arabic\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ar\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nar_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n```\n\nNext, we load the model and processor\n\n```py\nfrom transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\nimport torch\n\nmodel_id = \"facebook/mms-lid-126\"\n\nprocessor = AutoFeatureExtractor.from_pretrained(model_id)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n```\n\nNow we process the audio data, pass the processed audio data to the model to classify it into a language, just like we usually do for Wav2Vec2 audio classification models such as [ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)\n\n```py\n# English",
  "inputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\noutputs = model(**inputs).logits\n\nlang_id = torch.argmax(outputs, dim=-1)[0].item()\ndetected_lang = model.config.id2label[lang_id]\n# 'eng'\n\n# Arabic\ninputs = processor(ar_sample, sampling_rate=16_000, return_tensors=\"pt\")\n\nwith torch.no_grad():\noutputs = model(**inputs).logits\n\nlang_id = torch.argmax(outputs, dim=-1)[0].item()\ndetected_lang = model.config.id2label[lang_id]\n# 'ara'\n```\n\nTo see all the supported languages of a checkpoint, you can print out the language ids as follows:\n```py\nprocessor.id2label.values()\n```\n\n### Audio Pretrained Models\n\nPretrained models are available for two different sizes - [300M](https://huggingface.co/facebook/mms-300m) ,\n[1Bil](https://huggingface.co/facebook/mms-1b).\n\n<Tip>\n\nThe MMS for ASR architecture is based on the Wav2Vec2 model, refer to [Wav2Vec2's documentation page](wav2vec2) for further\ndetails on how to finetune with models for various downstream tasks.\n\nMMS-TTS uses the same model architecture as VITS, refer to [VITS's documentation page](vits) for API reference.\n</Tip>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BORT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we do not accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n</Tip>\n\n## Overview\n\nThe BORT model was proposed in [Optimal Subarchitecture Extraction for BERT](https://arxiv.org/abs/2010.10499) by\nAdrian de Wynter and Daniel J. Perry. It is an optimal subset of architectural parameters for the BERT, which the\nauthors refer to as \"Bort\".\n\nThe abstract from the paper is the following:\n\n*We extract an optimal subset of architectural parameters for the BERT architecture from Devlin et al. (2018) by\napplying recent breakthroughs in algorithms for neural architecture search. This optimal subset, which we refer to as\n\"Bort\", is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of 5.5% the\noriginal BERT-large architecture, and 16% of the net size. Bort is also able to be pretrained in 288 GPU hours, which",
  "is 1.2% of the time required to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large\n(Liu et al., 2019), and about 33% of that of the world-record, in GPU hours, required to train BERT-large on the same\nhardware. It is also 7.9x faster on a CPU, as well as being better performing than other compressed variants of the\narchitecture, and some of the non-compressed variants: it obtains performance improvements of between 0.3% and 31%,\nabsolute, with respect to BERT-large, on multiple public natural language understanding (NLU) benchmarks.*\n\nThis model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/alexa/bort/).\n\n## Usage tips\n\n- BORT's model architecture is based on BERT, refer to [BERT's documentation page](bert) for the\nmodel's API reference as well as usage examples.\n- BORT uses the RoBERTa tokenizer instead of the BERT tokenizer, refer to [RoBERTa's documentation page](roberta) for the tokenizer's API reference as well as usage examples.",
  "- BORT requires a specific fine-tuning algorithm, called [Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology) ,\nthat is sadly not open-sourced yet. It would be very useful for the community, if someone tries to implement the\nalgorithm to make BORT fine-tuning work.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Emu3\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang.\n\nEmu3 is a multimodal LLM that uses vector quantization to tokenize images into discrete tokens. Discretized image tokens are later fused with text token ids for image and text generation. The model can additionally generate images by predicting image token ids.\n\n\nThe abstract from the paper is the following:",
  "*While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence. We simplify complex multimodal model designs by converging on a singular focus: tokens, unlocking great potential for scaling both during training and inference. Our results demonstrate that next-token prediction is a promising path towards building general multimodal intelligence beyond language. We open-source key techniques and models to support further research in this direction.*",
  "Tips:\n\n- We advise users to set `processor.tokenizer.padding_side = \"left\"` before batched generation as it leads to more accurate results.\n\n- Note that the model has been trained with a specific prompt format for chatting. Use `processor.apply_chat_template(my_conversation_dict)` to correctly format your prompts.\n\n- Emu3 has two different checkpoints for image-generation and text-generation, make sure to use the correct checkpoint when loading the model. To generate an image, it is advised to use `prefix_constraints` so that the generated tokens are sampled only from possible image tokens. See more below for usage examples.\n\n> [!TIP]\n> Emu3 implementation in Transformers uses a special image token to indicate where to merge image embeddings. The special image token isn't new and uses one of the reserved tokens: `<|extra_0|>`. You have to add `<image>` to your prompt in the place where the image should be embedded for correct generation.\n\n\nThis model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\nThe original code can be found [here](https://github.com/baaivision/Emu3).\n\n\n## Usage example\n\n### Text generation inference",
  "Here's how to load the model and perform inference in half-precision (`torch.bfloat16`) to generate textual output from text or text and image inputs:\n\n```python\nfrom transformers import Emu3Processor, Emu3ForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport requests\n\nprocessor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\nmodel = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n\n# prepare image and text prompt\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"What do you see in this image?<image>\"\n\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n# autoregressively complete prompt\noutput = model.generate(**inputs, max_new_tokens=50)\nprint(processor.decode(output[0], skip_special_tokens=True))\n```\n\n### Image generation inference\n\nEmu3 can also generate images from textual input. Here is how you can do it:\n\n```python\nprocessor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Gen-hf\")",
  "model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Gen-hf\", torch_dtype=\"bfloat16\", device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n\n\ninputs = processor(\ntext=[\"a portrait of young girl. masterpiece, film grained, best quality.\", \"a dog running under the rain\"],\npadding=True,\nreturn_tensors=\"pt\",\nreturn_for_image_generation=True,\n)\ninputs = inputs.to(device=\"cuda:0\", dtype=torch.bfloat16)\n\nneg_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry.\"\nneg_inputs = processor(text=[neg_prompt] * 2, return_tensors=\"pt\").to(device=\"cuda:0\")\n\nimage_sizes = inputs.pop(\"image_sizes\")\nHEIGHT, WIDTH = image_sizes[0]\nVISUAL_TOKENS = model.vocabulary_mapping.image_tokens\n\ndef prefix_allowed_tokens_fn(batch_id, input_ids):\nheight, width = HEIGHT, WIDTH\nvisual_tokens = VISUAL_TOKENS\nimage_wrapper_token_id = torch.tensor([processor.tokenizer.image_wrapper_token_id], device=model.device)\neoi_token_id = torch.tensor([processor.tokenizer.eoi_token_id], device=model.device)",
  "eos_token_id = torch.tensor([processor.tokenizer.eos_token_id], device=model.device)\npad_token_id = torch.tensor([processor.tokenizer.pad_token_id], device=model.device)\neof_token_id = torch.tensor([processor.tokenizer.eof_token_id], device=model.device)\neol_token_id = processor.tokenizer.encode(\"<|extra_200|>\", return_tensors=\"pt\")[0]\n\nposition = torch.nonzero(input_ids == image_wrapper_token_id, as_tuple=True)[0][0]\noffset = input_ids.shape[0] - position\nif offset % (width + 1) == 0:\nreturn (eol_token_id, )\nelif offset == (width + 1) * height + 1:\nreturn (eof_token_id, )\nelif offset == (width + 1) * height + 2:\nreturn (eoi_token_id, )\nelif offset == (width + 1) * height + 3:\nreturn (eos_token_id, )\nelif offset > (width + 1) * height + 3:\nreturn (pad_token_id, )\nelse:\nreturn visual_tokens\n\n\nout = model.generate(\n**inputs,\nmax_new_tokens=50_000, # make sure to have enough tokens for one image\nprefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\nreturn_dict_in_generate=True,\nnegative_prompt_ids=neg_inputs.input_ids, # indicate for Classifier-Free Guidance\nnegative_prompt_attention_mask=neg_inputs.attention_mask,\n)",
  "image = model.decode_image_tokens(out.sequences[:, inputs.input_ids.shape[1]: ], height=HEIGHT, width=WIDTH)\nimages = processor.postprocess(list(image.float()), return_tensors=\"PIL.Image.Image\") # internally we convert to np but it's not supported in bf16 precision\nfor i, image in enumerate(images['pixel_values']):\nimage.save(f\"result{i}.png\")\n\n```\n\n\n## Emu3Config\n\n[[autodoc]] Emu3Config\n\n## Emu3VQVAEConfig\n\n[[autodoc]] Emu3VQVAEConfig\n\n## Emu3TextConfig\n\n[[autodoc]] Emu3TextConfig\n\n## Emu3Processor\n\n[[autodoc]] Emu3Processor\n\n## Emu3ImageProcessor\n\n[[autodoc]] Emu3ImageProcessor\n- preprocess\n\n## Emu3VQVAE\n\n[[autodoc]] Emu3VQVAE\n- forward\n\n## Emu3TextModel\n\n[[autodoc]] Emu3TextModel\n- forward\n\n## Emu3ForCausalLM\n\n[[autodoc]] Emu3ForCausalLM\n- forward\n\n## Emu3ForConditionalGeneration\n\n[[autodoc]] Emu3ForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a",
  "bidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:\n\n*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods",
  "such as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*\n\nThis model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\nrather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\nwith a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the",
  "*squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## SqueezeBertConfig\n\n[[autodoc]] SqueezeBertConfig\n\n## SqueezeBertTokenizer\n\n[[autodoc]] SqueezeBertTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## SqueezeBertTokenizerFast\n\n[[autodoc]] SqueezeBertTokenizerFast\n\n## SqueezeBertModel\n\n[[autodoc]] SqueezeBertModel\n\n## SqueezeBertForMaskedLM\n\n[[autodoc]] SqueezeBertForMaskedLM\n\n## SqueezeBertForSequenceClassification\n\n[[autodoc]] SqueezeBertForSequenceClassification\n\n## SqueezeBertForMultipleChoice\n\n[[autodoc]] SqueezeBertForMultipleChoice\n\n## SqueezeBertForTokenClassification\n\n[[autodoc]] SqueezeBertForTokenClassification\n\n## SqueezeBertForQuestionAnswering\n\n[[autodoc]] SqueezeBertForQuestionAnswering",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Wav2Vec2Phoneme\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al.,\n2021](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech\nrecognition systems without any labeled data. However, in many cases there is labeled data available for related\nlanguages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer\nlearning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by\nmapping phonemes of the training languages to the target language using articulatory features. Experiments show that\nthis simple method significantly outperforms prior work which introduced task-specific architectures and used only part\nof a monolingually pretrained model.*\n\nRelevant checkpoints can be found under https://huggingface.co/models?other=phoneme-recognition.",
  "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten)\n\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).\n\n## Usage tips\n\n- Wav2Vec2Phoneme uses the exact same architecture as Wav2Vec2\n- Wav2Vec2Phoneme is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Wav2Vec2Phoneme model was trained using connectionist temporal classification (CTC) so the model output has to be\ndecoded using [`Wav2Vec2PhonemeCTCTokenizer`].\n- Wav2Vec2Phoneme can be fine-tuned on multiple language at once and decode unseen languages in a single forward pass\nto a sequence of phonemes\n- By default, the model outputs a sequence of phonemes. In order to transform the phonemes to a sequence of words one\nshould make use of a dictionary and language model.\n\n\n<Tip>\n\nWav2Vec2Phoneme's architecture is based on the Wav2Vec2 model, for API reference, check out [`Wav2Vec2`](wav2vec2)'s documentation page\nexcept for the tokenizer.\n\n</Tip>\n\n## Wav2Vec2PhonemeCTCTokenizer\n\n[[autodoc]] Wav2Vec2PhonemeCTCTokenizer\n- __call__\n- batch_decode\n- decode\n- phonemize",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BigBird\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\nZaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\nSantiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\nbased transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse\nattention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\nhas been shown that applying sparse, global, and random attention approximates full attention, while being\ncomputationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\nBigBird has shown improved performance on various long document NLP tasks, such as question answering and\nsummarization, compared to BERT or RoBERTa.\n\nThe abstract from the paper is the following:\n\n*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.",
  "Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\nlength due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that\nreduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and\nis Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\ntheoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\nsequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\n8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,\nBigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\npropose novel applications to genomics data.*\n\nThis model was contributed by [vasudevgupta](https://huggingface.co/vasudevgupta). The original code can be found\n[here](https://github.com/google-research/bigbird).\n\n## Usage tips",
  "- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\n- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\n**original_full** is advised as there is no benefit in using **block_sparse** attention.\n- The code currently uses window size of 3 blocks and 2 global blocks.\n- Sequence length must be divisible by block size.\n- Current implementation supports only **ITC**.\n- Current implementation doesn't support **num_random_blocks = 0**\n- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## BigBirdConfig\n\n[[autodoc]] BigBirdConfig\n\n## BigBirdTokenizer",
  "[[autodoc]] BigBirdTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## BigBirdTokenizerFast\n\n[[autodoc]] BigBirdTokenizerFast\n\n## BigBird specific outputs\n\n[[autodoc]] models.big_bird.modeling_big_bird.BigBirdForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## BigBirdModel\n\n[[autodoc]] BigBirdModel\n- forward\n\n## BigBirdForPreTraining\n\n[[autodoc]] BigBirdForPreTraining\n- forward\n\n## BigBirdForCausalLM\n\n[[autodoc]] BigBirdForCausalLM\n- forward\n\n## BigBirdForMaskedLM\n\n[[autodoc]] BigBirdForMaskedLM\n- forward\n\n## BigBirdForSequenceClassification\n\n[[autodoc]] BigBirdForSequenceClassification\n- forward\n\n## BigBirdForMultipleChoice\n\n[[autodoc]] BigBirdForMultipleChoice\n- forward\n\n## BigBirdForTokenClassification\n\n[[autodoc]] BigBirdForTokenClassification\n- forward\n\n## BigBirdForQuestionAnswering\n\n[[autodoc]] BigBirdForQuestionAnswering\n- forward\n\n</pt>\n<jax>\n\n## FlaxBigBirdModel\n\n[[autodoc]] FlaxBigBirdModel\n- __call__\n\n## FlaxBigBirdForPreTraining\n\n[[autodoc]] FlaxBigBirdForPreTraining\n- __call__\n\n## FlaxBigBirdForCausalLM\n\n[[autodoc]] FlaxBigBirdForCausalLM\n- __call__\n\n## FlaxBigBirdForMaskedLM",
  "[[autodoc]] FlaxBigBirdForMaskedLM\n- __call__\n\n## FlaxBigBirdForSequenceClassification\n\n[[autodoc]] FlaxBigBirdForSequenceClassification\n- __call__\n\n## FlaxBigBirdForMultipleChoice\n\n[[autodoc]] FlaxBigBirdForMultipleChoice\n- __call__\n\n## FlaxBigBirdForTokenClassification\n\n[[autodoc]] FlaxBigBirdForTokenClassification\n- __call__\n\n## FlaxBigBirdForQuestionAnswering\n\n[[autodoc]] FlaxBigBirdForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mixtral\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Mixtral-8x7B was introduced in the [Mixtral of Experts blogpost](https://mistral.ai/news/mixtral-of-experts/) by Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.\n\nThe introduction of the blog post says:\n\n*Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts models (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.*\n\nMixtral-8x7B is the second large language model (LLM) released by [mistral.ai](https://mistral.ai/), after [Mistral-7B](mistral).\n\n### Architectural details\n\nMixtral-8x7B is a decoder-only Transformer with the following architectural choices:",
  "- Mixtral is a Mixture of Experts (MoE) model with 8 experts per MLP, with a total of 45 billion parameters. To learn more about mixture-of-experts, refer to the [blog post](https://huggingface.co/blog/moe).\n- Despite the model having 45 billion parameters, the compute required for a single forward pass is the same as that of a 14 billion parameter model. This is because even though each of the experts have to be loaded in RAM (70B like ram requirement) each token from the hidden states are dispatched twice (top 2 routing) and thus the compute (the operation required at each forward computation) is just 2 X sequence_length.\n\nThe following implementation details are shared with Mistral AI's first model [Mistral-7B](mistral):\n- Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens\n- GQA (Grouped Query Attention) - allowing faster inference and lower cache size.\n- Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens.\n\nFor more details refer to the [release blog post](https://mistral.ai/news/mixtral-of-experts/).\n\n### License",
  "`Mixtral-8x7B` is released under the Apache 2.0 license.\n\n## Usage tips\n\nThe Mistral team has released 2 checkpoints:\n- a base model, [Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1), which has been pre-trained to predict the next token on internet-scale data.\n- an instruction tuned model, [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1), which is the base model optimized for chat purposes using supervised fine-tuning (SFT) and direct preference optimization (DPO).\n\nThe base model can be used as follows:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"My favourite condiment is to ...\"\n```\n\nThe instruction tuned model can be used as follows:\n\n```python",
  ">>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n\n>>> messages = [\n...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n... ]\n\n>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\n>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"Mayonnaise can be made as follows: (...)\"\n```\n\nAs can be seen, the instruction-tuned model requires a [chat template](../chat_templating) to be applied to make sure the inputs are prepared in the right format.\n\n## Speeding up Mixtral by using Flash Attention",
  "The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). Make also sure to load your model in half-precision (e.g. `torch.float16`)\n\nTo load and run a model using Flash Attention-2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")",
  ">>> prompt = \"My favourite condiment is\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"The expected output\"\n```\n\n### Expected speedups\n\nBelow is a expected speedup diagram that compares pure inference time between the native implementation in transformers using `mistralai/Mixtral-8x7B-v0.1` checkpoint and the Flash Attention 2 version of the model.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png\">\n</div>\n\n### Sliding window Attention\n\nThe current implementation supports the sliding window attention mechanism and memory efficient cache management.\nTo enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`).",
  "The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding_window`), support batched generation only for `padding_side=\"left\"` and use the absolute position of the current token to compute the positional embedding.\n\n## Shrinking down Mixtral using quantization\n\nAs the Mixtral model has 45 billion parameters, that would require about 90GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization.md). If the model is quantized to 4 bits (or half a byte per parameter), a single A100 with 40GB of RAM is enough to fit the entire model, as in that case only about 27 GB of RAM is required.\n\nQuantizing a model is as simple as passing a `quantization_config` to the model. Below, we'll leverage the bitsandbytes quantization library (but refer to [this page](../quantization.md) for alternative quantization methods):\n\n```python\n>>> import torch",
  ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n>>> # specify how to quantize the model\n>>> quantization_config = BitsAndBytesConfig(\n...         load_in_4bit=True,\n...         bnb_4bit_quant_type=\"nf4\",\n...         bnb_4bit_compute_dtype=\"torch.float16\",\n... )\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", quantization_config=True, device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> messages = [\n...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n... ]\n\n>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\n>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"The expected output\"\n```",
  "This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](https://huggingface.co/ArthurZ) .\nThe original code can be found [here](https://github.com/mistralai/mistral-src).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Mixtral. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- A demo notebook to perform supervised fine-tuning (SFT) of Mixtral-8x7B can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb). 🌎\n- A [blog post](https://medium.com/@prakharsaxena11111/finetuning-mixtral-7bx8-6071b0ebf114) on fine-tuning Mixtral-8x7B using PEFT. 🌎",
  "- The [Alignment Handbook](https://github.com/huggingface/alignment-handbook) by Hugging Face includes scripts and recipes to perform supervised fine-tuning (SFT) and direct preference optimization with Mistral-7B. This includes scripts for full fine-tuning, QLoRa on a single GPU as well as multi-GPU fine-tuning.\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## MixtralConfig\n\n[[autodoc]] MixtralConfig\n\n## MixtralModel\n\n[[autodoc]] MixtralModel\n- forward\n\n## MixtralForCausalLM\n\n[[autodoc]] MixtralForCausalLM\n- forward\n\n## MixtralForSequenceClassification\n\n[[autodoc]] MixtralForSequenceClassification\n- forward\n\n## MixtralForTokenClassification\n\n[[autodoc]] MixtralForTokenClassification\n- forward\n\n## MixtralForQuestionAnswering\n[[autodoc]] MixtralForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\n\nLicensed under the MIT License; you may not use this file except in compliance with\nthe License.\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Graphormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview",
  "The Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234)  by\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention.\n\nThe abstract from the paper is the following:",
  "*The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.*",
  "This model was contributed by [clefourrier](https://huggingface.co/clefourrier). The original code can be found [here](https://github.com/microsoft/Graphormer).\n\n## Usage tips\n\nThis model will not work well on large graphs (more than 100 nodes/edges), as it will make the memory explode.\nYou can reduce the batch size, increase your RAM, or decrease the `UNREACHABLE_NODE_DISTANCE` parameter in algos_graphormer.pyx, but it will be hard to go above 700 nodes/edges.\n\nThis model does not use a tokenizer, but instead a special collator during training.\n\n## GraphormerConfig\n\n[[autodoc]] GraphormerConfig\n\n## GraphormerModel\n\n[[autodoc]] GraphormerModel\n- forward\n\n## GraphormerForGraphClassification\n\n[[autodoc]] GraphormerForGraphClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DeBERTa\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n\nIt builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa.\n\nThe abstract from the paper is the following:\n\n*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to",
  "predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*\n\n\nThis model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj) . The original code can be found [here](https://github.com/microsoft/DeBERTa).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DeBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
  "<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on how to [Accelerate Large Model Training using DeepSpeed](https://huggingface.co/blog/accelerate-deepspeed) with DeBERTa.\n- A blog post on [Supercharged Customer Service with Machine Learning](https://huggingface.co/blog/supercharge-customer-service-with-machine-learning) with DeBERTa.\n- [`DebertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).\n- [`TFDebertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"token-classification\" />",
  "- [`DebertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).\n- [`TFDebertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Byte-Pair Encoding tokenization](https://huggingface.co/course/chapter6/5?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Token classification task guide](../tasks/token_classification)\n\n<PipelineTag pipeline=\"fill-mask\"/>",
  "- [`DebertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFDebertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- [`DebertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).",
  "- [`TFDebertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n## DebertaConfig\n\n[[autodoc]] DebertaConfig\n\n## DebertaTokenizer\n\n[[autodoc]] DebertaTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## DebertaTokenizerFast\n\n[[autodoc]] DebertaTokenizerFast\n- build_inputs_with_special_tokens\n- create_token_type_ids_from_sequences\n\n<frameworkcontent>\n<pt>\n\n## DebertaModel\n\n[[autodoc]] DebertaModel\n- forward\n\n## DebertaPreTrainedModel\n\n[[autodoc]] DebertaPreTrainedModel\n\n## DebertaForMaskedLM\n\n[[autodoc]] DebertaForMaskedLM\n- forward\n\n## DebertaForSequenceClassification\n\n[[autodoc]] DebertaForSequenceClassification\n- forward\n\n## DebertaForTokenClassification\n\n[[autodoc]] DebertaForTokenClassification",
  "- forward\n\n## DebertaForQuestionAnswering\n\n[[autodoc]] DebertaForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFDebertaModel\n\n[[autodoc]] TFDebertaModel\n- call\n\n## TFDebertaPreTrainedModel\n\n[[autodoc]] TFDebertaPreTrainedModel\n- call\n\n## TFDebertaForMaskedLM\n\n[[autodoc]] TFDebertaForMaskedLM\n- call\n\n## TFDebertaForSequenceClassification\n\n[[autodoc]] TFDebertaForSequenceClassification\n- call\n\n## TFDebertaForTokenClassification\n\n[[autodoc]] TFDebertaForTokenClassification\n- call\n\n## TFDebertaForQuestionAnswering\n\n[[autodoc]] TFDebertaForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Hybrid Vision Transformer (ViT Hybrid)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.",
  "If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \"tokens\" for the Transformer.\n\nThe abstract from the paper is the following:\n\n*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its",
  "applications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function",
  "encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import ViTHybridForImageClassification\nmodel = ViTHybridForImageClassification.from_pretrained(\"google/vit-hybrid-base-bit-384\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vit-hybrid-base-bit-384` model, we saw the following speedups during inference.",
  "|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                        29 |                                        18 |                      1.61 |\n|            2 |                                        26 |                                        18 |                      1.44 |\n|            4 |                                        25 |                                        18 |                      1.39 |\n|            8 |                                        34 |                                        24 |                      1.42 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>",
  "- [`ViTHybridForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ViTHybridConfig\n\n[[autodoc]] ViTHybridConfig\n\n## ViTHybridImageProcessor\n\n[[autodoc]] ViTHybridImageProcessor\n- preprocess\n\n## ViTHybridModel\n\n[[autodoc]] ViTHybridModel\n- forward\n\n## ViTHybridForImageClassification\n\n[[autodoc]] ViTHybridForImageClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n# Zamba\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\nZamba is a large language model (LLM) trained by Zyphra, and made available under an Apache 2.0 license. Please see the [Zyphra Hugging Face](https://huggingface.co/collections/zyphra/) repository for model weights.",
  "This model was contributed by [pglo](https://huggingface.co/pglo).\n\n\n## Model details\n\nZamba-7B-v1 is a hybrid between state-space models (Specifically [Mamba](https://github.com/state-spaces/mamba)) and transformer, and was trained using next-token prediction. Zamba uses a shared transformer layer after every 6 mamba blocks. It uses the [Mistral v0.1 tokenizer](https://huggingface.co/mistralai/Mistral-7B-v0.1). We came to this architecture after a series of ablations at small scales. Zamba-7B-v1 was pre-trained on 1T tokens of text and code data.\n\n<img src=https://github.com/user-attachments/assets/c2cff209-b901-483c-87aa-774b82a0769f width=30% height=40% />\n\n## Quick start\n\n\n### Presequities\n\nZamba requires you use `transformers` version 4.46.0 or higher:\n```bash\npip install transformers>=4.45.0\n```\n\nIn order to run optimized Mamba implementations, you first need to install `mamba-ssm` and `causal-conv1d`:\n```bash\npip install mamba-ssm causal-conv1d>=1.2.0\n```\nYou also have to have the model on a CUDA device.",
  "You can run the model not using the optimized Mamba kernels, but it is **not** recommended as it will result in significantly lower latencies. In order to do that, you'll need to specify `use_mamba_kernels=False` when loading the model.\n\n\n## Inference\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Zyphra/Zamba-7B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba-7B-v1\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninput_text = \"A funny prompt would be \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0]))\n```\n\n\n## Model card\n\nThe model cards can be found at:\n* [Zamba-7B](MODEL_CARD_ZAMBA-7B-v1.md)\n\n\n## Issues\nFor issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/zyphra/zamba-7b)\n\n\n## License\n\nThe model weights are open-sourced via an Apache 2.0 license.\n\n\n## ZambaConfig\n\n[[autodoc]] ZambaConfig\n\n\n## ZambaModel\n\n[[autodoc]] ZambaModel\n- forward\n\n\n## ZambaForCausalLM\n\n[[autodoc]] ZambaForCausalLM\n- forward",
  "## ZambaForSequenceClassification\n\n[[autodoc]] transformers.ZambaForSequenceClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Data2Vec\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.\nData2Vec proposes a unified framework for self-supervised learning across different data modalities - text, audio and images.\nImportantly, predicted targets for pre-training are contextualized latent representations of the inputs, rather than modality-specific, context-independent targets.\n\nThe abstract from the paper is the following:\n\n*While the general idea of self-supervised learning is identical across modalities, the actual algorithms and\nobjectives differ widely because they were developed with a single modality in mind. To get us closer to general\nself-supervised learning, we present data2vec, a framework that uses the same learning method for either speech,\nNLP or computer vision. The core idea is to predict latent representations of the full input data based on a\nmasked view of the input in a selfdistillation setup using a standard Transformer architecture.",
  "Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which\nare local in nature, data2vec predicts contextualized latent representations that contain information from\nthe entire input. Experiments on the major benchmarks of speech recognition, image classification, and\nnatural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.\nModels and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec.*\n\nThis model was contributed by [edugp](https://huggingface.co/edugp) and [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n[sayakpaul](https://github.com/sayakpaul) and [Rocketknight1](https://github.com/Rocketknight1) contributed Data2Vec for vision in TensorFlow.\n\nThe original code (for NLP and Speech) can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/data2vec).\nThe original code for vision can be found [here](https://github.com/facebookresearch/data2vec_vision/tree/main/beit).\n\n## Usage tips\n\n- Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using the same self-supervised learning method.",
  "- For Data2VecAudio, preprocessing is identical to [`Wav2Vec2Model`], including feature extraction\n- For Data2VecText, preprocessing is identical to [`RobertaModel`], including tokenization.\n- For Data2VecVision, preprocessing is identical to [`BeitModel`], including feature extraction.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\nThe SDPA implementation is currently available for the Data2VecAudio and Data2VecVision models.\n\n```",
  "from transformers import Data2VecVisionForImageClassification\nmodel = Data2VecVisionForImageClassification.from_pretrained(\"facebook/data2vec-vision-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nFor the Data2VecVision model, on a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.5.1, OS Ubuntu 20.04)\nwith `float16` and `facebook/data2vec-vision-base` model, we saw the following improvements during training and\ninference:\n\n#### Training\n\n| num_training_steps | batch_size | image_size   | is_cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) | Mem saving (%) |\n|--------------------|------------|--------------|---------|----------------------------|---------------------------|-------------|----------------------|--------------------|----------------|\n| 50                 | 2          | (1048, 640)  | True    | 0.996                      | 0.754                     | 32.147      | 6722.198            | 4264.653          | 57.626         |\n\n#### Inference",
  "|   Image batch size |   Eager (s/iter) | Eager CI, %   |   Eager memory (MB) |   SDPA (s/iter) | SDPA CI, %   |   SDPA memory (MB) |   SDPA speedup |   SDPA memory saved |\n|-------------------:|-----------------:|:--------------|--------------------:|----------------:|:-------------|-------------------:|---------------:|--------------------:|\n|                  1 |            0.011 | ±0.3%         |         3.76143e+08 |           0.01  | ±0.3%        |        3.74397e+08 |          1.101 |               0.466 |\n|                  4 |            0.014 | ±0.1%         |         4.02756e+08 |           0.012 | ±0.2%        |        3.91373e+08 |          1.219 |               2.909 |\n|                 16 |            0.046 | ±0.3%         |         4.96482e+08 |           0.035 | ±0.2%        |        4.51017e+08 |          1.314 |              10.081 |\n|                 32 |            0.088 | ±0.1%         |         6.23903e+08 |           0.067 | ±0.1%        |        5.32974e+08 |          1.33  |              17.061 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Data2Vec.",
  "<PipelineTag pipeline=\"image-classification\"/>\n\n- [`Data2VecVisionForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- To fine-tune [`TFData2VecVisionForImageClassification`] on a custom dataset, see [this notebook](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb).\n\n**Data2VecText documentation resources**\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n**Data2VecAudio documentation resources**\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)",
  "**Data2VecVision documentation resources**\n- [Image classification](../tasks/image_classification)\n- [Semantic segmentation](../tasks/semantic_segmentation)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## Data2VecTextConfig\n\n[[autodoc]] Data2VecTextConfig\n\n## Data2VecAudioConfig\n\n[[autodoc]] Data2VecAudioConfig\n\n## Data2VecVisionConfig\n\n[[autodoc]] Data2VecVisionConfig\n\n<frameworkcontent>\n<pt>\n\n## Data2VecAudioModel\n\n[[autodoc]] Data2VecAudioModel\n- forward\n\n## Data2VecAudioForAudioFrameClassification\n\n[[autodoc]] Data2VecAudioForAudioFrameClassification\n- forward\n\n## Data2VecAudioForCTC\n\n[[autodoc]] Data2VecAudioForCTC\n- forward\n\n## Data2VecAudioForSequenceClassification\n\n[[autodoc]] Data2VecAudioForSequenceClassification\n- forward\n\n## Data2VecAudioForXVector\n\n[[autodoc]] Data2VecAudioForXVector\n- forward\n\n## Data2VecTextModel\n\n[[autodoc]] Data2VecTextModel\n- forward\n\n## Data2VecTextForCausalLM\n\n[[autodoc]] Data2VecTextForCausalLM\n- forward\n\n## Data2VecTextForMaskedLM",
  "[[autodoc]] Data2VecTextForMaskedLM\n- forward\n\n## Data2VecTextForSequenceClassification\n\n[[autodoc]] Data2VecTextForSequenceClassification\n- forward\n\n## Data2VecTextForMultipleChoice\n\n[[autodoc]] Data2VecTextForMultipleChoice\n- forward\n\n## Data2VecTextForTokenClassification\n\n[[autodoc]] Data2VecTextForTokenClassification\n- forward\n\n## Data2VecTextForQuestionAnswering\n\n[[autodoc]] Data2VecTextForQuestionAnswering\n- forward\n\n## Data2VecVisionModel\n\n[[autodoc]] Data2VecVisionModel\n- forward\n\n## Data2VecVisionForImageClassification\n\n[[autodoc]] Data2VecVisionForImageClassification\n- forward\n\n## Data2VecVisionForSemanticSegmentation\n\n[[autodoc]] Data2VecVisionForSemanticSegmentation\n- forward\n\n</pt>\n<tf>\n\n## TFData2VecVisionModel\n\n[[autodoc]] TFData2VecVisionModel\n- call\n\n## TFData2VecVisionForImageClassification\n\n[[autodoc]] TFData2VecVisionForImageClassification\n- call\n\n## TFData2VecVisionForSemanticSegmentation\n\n[[autodoc]] TFData2VecVisionForSemanticSegmentation\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UL2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n\nThe abstract from the paper is the following:",
  "*Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization.*",
  "This model was contributed by [DanielHesslow](https://huggingface.co/Seledorn). The original code can be found [here](https://github.com/google-research/google-research/tree/master/ul2).\n\n## Usage tips\n\n- UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions as well as fine-tuned on an array of downstream tasks.\n- UL2 has the same architecture as [T5v1.1](t5v1.1) but uses the Gated-SiLU activation function instead of Gated-GELU.\n- The authors release checkpoints of one architecture which can be seen [here](https://huggingface.co/google/ul2)\n\n<Tip>\n\nAs UL2 has the same architecture as T5v1.1,  refer to [T5's documentation page](t5) for API reference, tips, code examples and notebooks.\n\n</Tip>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OLMo\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The OLMo model was proposed in [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi.\n\nOLMo is a series of **O**pen **L**anguage **Mo**dels designed to enable the science of language models. The OLMo models are trained on the Dolma dataset. We release all code, checkpoints, logs (coming soon), and details involved in training these models.\n\nThe abstract from the paper is the following:",
  "*Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.*\n\nThis model was contributed by [shanearora](https://huggingface.co/shanearora).\nThe original code can be found [here](https://github.com/allenai/OLMo/tree/main/olmo).",
  "## OlmoConfig\n\n[[autodoc]] OlmoConfig\n\n## OlmoModel\n\n[[autodoc]] OlmoModel\n- forward\n\n## OlmoForCausalLM\n\n[[autodoc]] OlmoForCausalLM\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLM-RoBERTa\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume\nWenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's\nRoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl\ndata.\n\nThe abstract from the paper is the following:\n\n*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a\nwide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly\noutperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on\nXNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on",
  "low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We\nalso present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the\ntrade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource\nlanguages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing\nper-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make XLM-R code, data, and models publicly available.*\n\nThis model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).\n\n## Usage tips\n\n- XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does\nnot require `lang` tensors to understand which language is used, and should be able to determine the correct\nlanguage from the input ids.",
  "- Uses RoBERTa tricks on the XLM approach, but does not use the translation language modeling objective. It only uses masked language modeling on sentences coming from one language.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with XLM-RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on how to [finetune XLM RoBERTa for multiclass classification with Habana Gaudi on AWS](https://www.philschmid.de/habana-distributed-training)\n- [`XLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).",
  "- [`TFXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n- [`FlaxXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n- [Text classification](https://huggingface.co/docs/transformers/tasks/sequence_classification) chapter of the 🤗 Hugging Face Task Guides.\n- [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- [`XLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).",
  "- [`TFXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n- [`FlaxXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).\n- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Token classification task guide](../tasks/token_classification)\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- [`XLMRobertaForCausalLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) chapter of the 🤗 Hugging Face Task Guides.\n- [Causal language modeling task guide](../tasks/language_modeling)",
  "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`XLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Masked language modeling](../tasks/masked_language_modeling)",
  "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`XLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n- [`TFXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n**Multiple choice**",
  "- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n- [`TFXLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n🚀 Deploy\n\n- A blog post on how to [Deploy Serverless XLM RoBERTa on AWS Lambda](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface).\n\n<Tip>\n\nThis implementation is the same as RoBERTa. Refer to the [documentation of RoBERTa](roberta) for usage examples as well as the information relative to the inputs and outputs.\n</Tip>\n\n## XLMRobertaConfig\n\n[[autodoc]] XLMRobertaConfig\n\n## XLMRobertaTokenizer\n\n[[autodoc]] XLMRobertaTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences",
  "- save_vocabulary\n\n## XLMRobertaTokenizerFast\n\n[[autodoc]] XLMRobertaTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## XLMRobertaModel\n\n[[autodoc]] XLMRobertaModel\n- forward\n\n## XLMRobertaForCausalLM\n\n[[autodoc]] XLMRobertaForCausalLM\n- forward\n\n## XLMRobertaForMaskedLM\n\n[[autodoc]] XLMRobertaForMaskedLM\n- forward\n\n## XLMRobertaForSequenceClassification\n\n[[autodoc]] XLMRobertaForSequenceClassification\n- forward\n\n## XLMRobertaForMultipleChoice\n\n[[autodoc]] XLMRobertaForMultipleChoice\n- forward\n\n## XLMRobertaForTokenClassification\n\n[[autodoc]] XLMRobertaForTokenClassification\n- forward\n\n## XLMRobertaForQuestionAnswering\n\n[[autodoc]] XLMRobertaForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFXLMRobertaModel\n\n[[autodoc]] TFXLMRobertaModel\n- call\n\n## TFXLMRobertaForCausalLM\n\n[[autodoc]] TFXLMRobertaForCausalLM\n- call\n\n## TFXLMRobertaForMaskedLM\n\n[[autodoc]] TFXLMRobertaForMaskedLM\n- call\n\n## TFXLMRobertaForSequenceClassification\n\n[[autodoc]] TFXLMRobertaForSequenceClassification\n- call\n\n## TFXLMRobertaForMultipleChoice\n\n[[autodoc]] TFXLMRobertaForMultipleChoice\n- call\n\n## TFXLMRobertaForTokenClassification\n\n[[autodoc]] TFXLMRobertaForTokenClassification\n- call",
  "## TFXLMRobertaForQuestionAnswering\n\n[[autodoc]] TFXLMRobertaForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxXLMRobertaModel\n\n[[autodoc]] FlaxXLMRobertaModel\n- __call__\n\n## FlaxXLMRobertaForCausalLM\n\n[[autodoc]] FlaxXLMRobertaForCausalLM\n- __call__\n\n## FlaxXLMRobertaForMaskedLM\n\n[[autodoc]] FlaxXLMRobertaForMaskedLM\n- __call__\n\n## FlaxXLMRobertaForSequenceClassification\n\n[[autodoc]] FlaxXLMRobertaForSequenceClassification\n- __call__\n\n## FlaxXLMRobertaForMultipleChoice\n\n[[autodoc]] FlaxXLMRobertaForMultipleChoice\n- __call__\n\n## FlaxXLMRobertaForTokenClassification\n\n[[autodoc]] FlaxXLMRobertaForTokenClassification\n- __call__\n\n## FlaxXLMRobertaForQuestionAnswering\n\n[[autodoc]] FlaxXLMRobertaForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DiT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nDiT was proposed in [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\nDiT applies the self-supervised objective of [BEiT](beit) (BERT pre-training of Image Transformers) to 42 million document images, allowing for state-of-the-art results on tasks including:\n\n- document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\n400,000 images belonging to one of 16 classes).\n- document layout analysis: the [PubLayNet](https://github.com/ibm-aur-nlp/PubLayNet) dataset (a collection of more\nthan 360,000 document images constructed by automatically parsing PubMed XML files).\n- table detection: the [ICDAR 2019 cTDaR](https://github.com/cndplab-founder/ICDAR2019_cTDaR) dataset (a collection of\n600 training images and 240 testing images).\n\nThe abstract from the paper is the following:",
  "*Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, as well as table detection. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 → 92.69), document layout analysis (91.0 → 94.9) and table detection (94.23 → 96.55). *\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dit_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Summary of the approach. Taken from the [original paper](https://arxiv.org/abs/2203.02378). </small>",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/dit).\n\n## Usage tips\n\nOne can directly use the weights of DiT with the AutoModel API:\n\n```python\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"microsoft/dit-base\")\n```\n\nThis will load the model pre-trained on masked image modeling. Note that this won't include the language modeling head on top, used to predict visual tokens.\n\nTo include the head, you can load the weights into a `BeitForMaskedImageModeling` model, like so:\n\n```python\nfrom transformers import BeitForMaskedImageModeling\n\nmodel = BeitForMaskedImageModeling.from_pretrained(\"microsoft/dit-base\")\n```\n\nYou can also load a fine-tuned model from the [hub](https://huggingface.co/models?other=dit), like so:\n\n```python\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/dit-base-finetuned-rvlcdip\")\n```\n\nThis particular checkpoint was fine-tuned on [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/), an important benchmark for document image classification.",
  "A notebook that illustrates inference for document image classification can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DiT/Inference_with_DiT_(Document_Image_Transformer)_for_document_image_classification.ipynb).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DiT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`BeitForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<Tip>\n\nAs DiT's architecture is equivalent to that of BEiT, one can refer to [BEiT's documentation page](beit) for all tips, code examples and notebooks.\n</Tip>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TVLT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe TVLT model was proposed in [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156)\nby Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal (the first three authors contributed equally). The Textless Vision-Language Transformer (TVLT) is a model that uses raw visual and audio inputs for vision-and-language representation learning, without using text-specific modules such as tokenization or automatic speech recognition (ASR). It can perform various audiovisual and vision-language tasks like retrieval, question answering, etc.\n\nThe abstract from the paper is the following:",
  "*In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text.*\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvlt_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n</p>",
  "<small> TVLT architecture. Taken from the <a href=\"[https://arxiv.org/abs/2102.03334](https://arxiv.org/abs/2209.14156)\">original paper</a>. </small>\n\nThe original code can be found [here](https://github.com/zinengtang/TVLT). This model was contributed by [Zineng Tang](https://huggingface.co/ZinengTang).\n\n## Usage tips\n\n- TVLT is a model that takes both `pixel_values` and `audio_values` as input. One can use [`TvltProcessor`] to prepare data for the model.\nThis processor wraps an image processor (for the image/video modality) and an audio feature extractor (for the audio modality) into one.\n- TVLT is trained with images/videos and audios of various sizes: the authors resize and crop the input images/videos to 224 and limit the length of audio spectrogram to 2048. To make batching of videos and audios possible, the authors use a `pixel_mask` that indicates which pixels are real/padding and `audio_mask` that indicates which audio values are real/padding.\n- The design of TVLT is very similar to that of a standard Vision Transformer (ViT) and masked autoencoder (MAE) as in [ViTMAE](vitmae). The difference is that the model includes embedding layers for the audio modality.",
  "- The PyTorch version of this model is only available in torch 1.10 and higher.\n\n## TvltConfig\n\n[[autodoc]] TvltConfig\n\n## TvltProcessor\n\n[[autodoc]] TvltProcessor\n- __call__\n\n## TvltImageProcessor\n\n[[autodoc]] TvltImageProcessor\n- preprocess\n\n## TvltFeatureExtractor\n\n[[autodoc]] TvltFeatureExtractor\n- __call__\n\n## TvltModel\n\n[[autodoc]] TvltModel\n- forward\n\n## TvltForPreTraining\n\n[[autodoc]] TvltForPreTraining\n- forward\n\n## TvltForAudioVisualClassification\n\n[[autodoc]] TvltForAudioVisualClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Time Series Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Time Series Transformer model is a vanilla encoder-decoder Transformer for time series forecasting.\nThis model was contributed by [kashif](https://huggingface.co/kashif).\n\n## Usage tips",
  "- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer without any head on top, and [`TimeSeriesTransformerForPrediction`]\nadds a distribution head on top of the former, which can be used for time-series forecasting. Note that this is a so-called probabilistic forecasting model, not a\npoint forecasting model. This means that the model learns a distribution, from which one can sample. The model doesn't directly output values.\n- [`TimeSeriesTransformerForPrediction`] consists of 2 blocks: an encoder, which takes a `context_length` of time series values as input (called `past_values`),\nand a decoder, which predicts a `prediction_length` of time series values into the future (called `future_values`). During training, one needs to provide\npairs of (`past_values` and `future_values`) to the model.\n- In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:\n- `past_time_features`: temporal features which the model will add to `past_values`. These serve as \"positional encodings\" for the Transformer encoder.",
  "Examples are \"day of the month\", \"month of the year\", etc. as scalar values (and then stacked together as a vector).\ne.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as time feature vector (11 being \"day of the month\", 8 being \"month of the year\").\n- `future_time_features`: temporal features which the model will add to `future_values`. These serve as \"positional encodings\" for the Transformer decoder.\nExamples are \"day of the month\", \"month of the year\", etc. as scalar values (and then stacked together as a vector).\ne.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as time feature vector (11 being \"day of the month\", 8 being \"month of the year\").\n- `static_categorical_features`: categorical features which are static over time (i.e., have the same value for all `past_values` and `future_values`).\nAn example here is the store ID or region ID that identifies a given time-series.\nNote that these features need to be known for ALL data points (also those in the future).",
  "- `static_real_features`: real-valued features which are static over time (i.e., have the same value for all `past_values` and `future_values`).\nAn example here is the image representation of the product for which you have the time-series values (like the [ResNet](resnet) embedding of a \"shoe\" picture,\nif your time-series is about the sales of shoes).\nNote that these features need to be known for ALL data points (also those in the future).\n- The model is trained using \"teacher-forcing\", similar to how a Transformer is trained for machine translation. This means that, during training, one shifts the\n`future_values` one position to the right as input to the decoder, prepended by the last value of `past_values`. At each time step, the model needs to predict the\nnext target. So the set-up of training is similar to a GPT model for language, except that there's no notion of `decoder_start_token_id` (we just use the last value\nof the context as initial input for the decoder).\n- At inference time, we give the final value of the `past_values` as input to the decoder. Next, we can sample from the model to make a prediction at the next time step,",
  "which is then fed to the decoder in order to make the next prediction (also called autoregressive generation).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- Check out the Time Series Transformer blog-post in HuggingFace blog: [Probabilistic Time Series Forecasting with 🤗 Transformers](https://huggingface.co/blog/time-series-transformers)\n\n\n## TimeSeriesTransformerConfig\n\n[[autodoc]] TimeSeriesTransformerConfig\n\n## TimeSeriesTransformerModel\n\n[[autodoc]] TimeSeriesTransformerModel\n- forward\n\n## TimeSeriesTransformerForPrediction\n\n[[autodoc]] TimeSeriesTransformerForPrediction\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPT Neo\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n</div>\n\n## Overview\n\nThe GPTNeo model was released in the [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) repository by Sid\nBlack, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the\n[Pile](https://pile.eleuther.ai/) dataset.\n\nThe architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of\n256 tokens.\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla).\n\n## Usage example\n\nThe `generate()` method can be used to generate text using GPT Neo model.\n\n```python\n>>> from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n\n>>> model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n>>> tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"",
  "...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```\n\n## Combining GPT-Neo and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature, and make sure your hardware is compatible with Flash-Attention 2. More details are available [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) concerning the installation.\n\nMake sure as well to load your model in half-precision (e.g. `torch.float16`).\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")",
  ">>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n\n>>> prompt = \"def hello_world():\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"def hello_world():\\n    >>> run_script(\"hello.py\")\\n    >>> exit(0)\\n<|endoftext|>\"\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `EleutherAI/gpt-neo-2.7B` checkpoint and the Flash Attention 2 version of the model.\nNote that for GPT-Neo it is not possible to train / run on very long context as the max [position embeddings](https://huggingface.co/EleutherAI/gpt-neo-2.7B/blob/main/config.json#L58 ) is limited to 2048 - but this is applicable to all gpt-neo models and not specific to FA-2\n\n<div style=\"text-align: center\">\n<img src=\"https://user-images.githubusercontent.com/49240599/272241893-b1c66b75-3a48-4265-bc47-688448568b3d.png\">\n</div>\n\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)",
  "- [Causal language modeling task guide](../tasks/language_modeling)\n\n## GPTNeoConfig\n\n[[autodoc]] GPTNeoConfig\n\n\n<frameworkcontent>\n<pt>\n\n## GPTNeoModel\n\n[[autodoc]] GPTNeoModel\n- forward\n\n## GPTNeoForCausalLM\n\n[[autodoc]] GPTNeoForCausalLM\n- forward\n\n## GPTNeoForQuestionAnswering\n\n[[autodoc]] GPTNeoForQuestionAnswering\n- forward\n\n## GPTNeoForSequenceClassification\n\n[[autodoc]] GPTNeoForSequenceClassification\n- forward\n\n## GPTNeoForTokenClassification\n\n[[autodoc]] GPTNeoForTokenClassification\n- forward\n\n</pt>\n<jax>\n\n## FlaxGPTNeoModel\n\n[[autodoc]] FlaxGPTNeoModel\n- __call__\n\n## FlaxGPTNeoForCausalLM\n\n[[autodoc]] FlaxGPTNeoForCausalLM\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Hubert\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">",
  "<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nHubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan\nSalakhutdinov, Abdelrahman Mohamed.\n\nThe abstract from the paper is the following:\n\n*Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are\nmultiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we\npropose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our\napproach is applying the prediction loss over the masked regions only, which forces the model to learn a combined",
  "acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised\nclustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means\nteacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the\nstate-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h,\n10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER\nreduction on the more challenging dev-other and test-other evaluation subsets.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n# Usage tips\n\n- Hubert is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Hubert model was fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`].\n\n\n## Using Flash Attention 2\n\nFlash Attention 2 is an faster, optimized version of the model.\n\n### Installation",
  "First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n\nNext, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n### Usage\n\nBelow is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of `facebook/hubert-large-ls960-ft`, the flash-attention-2 and the sdpa (scale-dot-product-attention) version. We show the average speedup obtained on the `librispeech_asr` `clean` validation split:\n\n```python\n>>> from transformers import HubertModel\n>>> import torch",
  ">>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\")\n...\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of the `facebook/hubert-large-ls960-ft` model and the flash-attention-2 and sdpa (scale-dot-product-attention) versions. . We show the average speedup obtained on the `librispeech_asr` `clean` validation split:\n\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/kamilakesbi/transformers_image_doc/resolve/main/data/Hubert_speedup.png\">\n</div>\n\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## HubertConfig\n\n[[autodoc]] HubertConfig\n\n<frameworkcontent>\n<pt>\n\n## HubertModel\n\n[[autodoc]] HubertModel\n- forward\n\n## HubertForCTC\n\n[[autodoc]] HubertForCTC\n- forward\n\n## HubertForSequenceClassification\n\n[[autodoc]] HubertForSequenceClassification\n- forward\n\n</pt>\n<tf>\n\n## TFHubertModel\n\n[[autodoc]] TFHubertModel\n- call\n\n## TFHubertForCTC\n\n[[autodoc]] TFHubertForCTC",
  "- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The Qwen Team and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Qwen2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Qwen2 is the new model series of large language models from the Qwen team. Previously, we released the Qwen series, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B, Qwen2-Audio, etc.\n\n### Model Details\n\nQwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.\n\n\n## Usage tips\n\n`Qwen2-7B` and `Qwen2-7B-Instruct` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n\nIn the following, we demonstrate how to use `Qwen2-7B-Instruct` for the inference. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto",
  ">>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-7B-Instruct\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n\n>>> prompt = \"Give me a short introduction to large language model.\"\n\n>>> messages = [{\"role\": \"user\", \"content\": prompt}]\n\n>>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n>>> model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n\n>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n\n>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## Qwen2Config\n\n[[autodoc]] Qwen2Config\n\n## Qwen2Tokenizer\n\n[[autodoc]] Qwen2Tokenizer\n- save_vocabulary\n\n## Qwen2TokenizerFast\n\n[[autodoc]] Qwen2TokenizerFast\n\n## Qwen2Model\n\n[[autodoc]] Qwen2Model\n- forward\n\n## Qwen2ForCausalLM\n\n[[autodoc]] Qwen2ForCausalLM\n- forward\n\n## Qwen2ForSequenceClassification\n\n[[autodoc]] Qwen2ForSequenceClassification\n- forward\n\n## Qwen2ForTokenClassification\n\n[[autodoc]] Qwen2ForTokenClassification",
  "- forward\n\n## Qwen2ForQuestionAnswering\n\n[[autodoc]] Qwen2ForQuestionAnswering\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ZoeDepth\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The ZoeDepth model was proposed in [ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth](https://arxiv.org/abs/2302.12288) by Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, Matthias Müller. ZoeDepth extends the [DPT](dpt) framework for metric (also called absolute) depth estimation. ZoeDepth is pre-trained on 12 datasets using relative depth and fine-tuned on two domains (NYU and KITTI) using metric depth. A lightweight head is used with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier.\n\nThe abstract from the paper is the following:",
  "*This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/zoedepth_architecture_bis.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ZoeDepth architecture. Taken from the <a href=\"https://arxiv.org/abs/2302.12288\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/isl-org/ZoeDepth).\n\n## Usage tips\n\n- ZoeDepth is an absolute (also called metric) depth estimation model, unlike DPT which is a relative depth estimation model. This means that ZoeDepth is able to estimate depth in metric units like meters.\n\nThe easiest to perform inference with ZoeDepth is by leveraging the [pipeline API](../main_classes/pipelines.md):\n\n```python\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> pipe = pipeline(task=\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n>>> result = pipe(image)\n>>> depth = result[\"depth\"]\n```\n\nAlternatively, one can also perform inference using the classes:\n\n```python",
  ">>> from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n>>> import torch\n>>> import numpy as np\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n>>> model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(inputs)\n\n>>> # interpolate to original size and visualize the prediction\n>>> ## ZoeDepth dynamically pads the input image. Thus we pass the original image size as argument\n>>> ## to `post_process_depth_estimation` to remove the padding and resize to original dimensions.\n>>> post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     source_sizes=[(image.height, image.width)],\n... )\n\n>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())",
  ">>> depth = depth.detach().cpu().numpy() * 255\n>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n```\n\n<Tip>\n<p>In the <a href=\"https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131\">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>\n<pre><code class=\"language-Python\">&gt;&gt;&gt; with torch.no_grad():\n...     outputs = model(pixel_values)\n...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n&gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     source_sizes=[(image.height, image.width)],\n...     outputs_flipped=outputs_flipped,\n... )\n</code></pre>\n</Tip>\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ZoeDepth.",
  "- A demo notebook regarding inference with ZoeDepth models can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ZoeDepth). 🌎\n\n## ZoeDepthConfig\n\n[[autodoc]] ZoeDepthConfig\n\n## ZoeDepthImageProcessor\n\n[[autodoc]] ZoeDepthImageProcessor\n- preprocess\n\n## ZoeDepthForDepthEstimation\n\n[[autodoc]] ZoeDepthForDepthEstimation\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Idefics3\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Idefics3 model was proposed in [Building and better understanding vision-language models: insights and future directions](https://huggingface.co/papers/2408.12637) by Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon.\n\nIdefics3 is an adaptation of the Idefics2 model with three main differences:\n\n- It uses Llama3 for the text model.\n- It uses an updated processing logic for the images.\n- It removes the perceiver.\n\nThe abstract from the paper is the following:",
  "*The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.*\n\n## Usage tips",
  "Input images are processed either by upsampling (if resizing is enabled) or at their original resolution. The resizing behavior depends on two parameters: do_resize and size.\n\nIf `do_resize` is set to `True`, the model resizes images so that the longest edge is 4*364 pixels by default.\nThe default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 364}` is the default, but you can change it to a different value if needed.\n\nHere’s how to control resizing and set a custom size:\n```python\nimage_processor = Idefics3ImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 364}, max_image_size=364)\n```\n\nAdditionally, the `max_image_size` parameter, which controls the size of each square patch the image is decomposed into, is set to 364 by default but can be adjusted as needed. After resizing (if applicable), the image processor decomposes the images into square patches based on the `max_image_size` parameter.\n\nThis model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [andimarafioti](https://huggingface.co/andito).\n\n\n## Idefics3Config\n\n[[autodoc]] Idefics3Config\n\n## Idefics3VisionConfig",
  "[[autodoc]] Idefics3VisionConfig\n\n## Idefics3VisionTransformer\n\n[[autodoc]] Idefics3VisionTransformer\n\n## Idefics3Model\n\n[[autodoc]] Idefics3Model\n- forward\n\n## Idefics3ForConditionalGeneration\n\n[[autodoc]] Idefics3ForConditionalGeneration\n- forward\n\n\n## Idefics3ImageProcessor\n[[autodoc]] Idefics3ImageProcessor\n- preprocess\n\n\n## Idefics3Processor\n[[autodoc]] Idefics3Processor\n- __call__",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LayoutLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n<a id='Overview'></a>\n\n## Overview\n\nThe LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image",
  "Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and\nMing Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and\ninformation extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results\non several downstream tasks:\n\n- form understanding: the [FUNSD](https://guillaumejaume.github.io/FUNSD/) dataset (a collection of 199 annotated\nforms comprising more than 30,000 words).\n- receipt understanding: the [SROIE](https://rrc.cvc.uab.es/?ch=13) dataset (a collection of 626 receipts for\ntraining and 347 receipts for testing).\n- document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\n400,000 images belonging to one of 16 classes).\n\nThe abstract from the paper is the following:\n\n*Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the\nwidespread use of pretraining models for NLP applications, they almost exclusively focus on text-level manipulation,",
  "while neglecting layout and style information that is vital for document image understanding. In this paper, we propose\nthe LayoutLM to jointly model interactions between text and layout information across scanned document images, which is\nbeneficial for a great number of real-world document image understanding tasks such as information extraction from\nscanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM.\nTo the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for\ndocument-level pretraining. It achieves new state-of-the-art results in several downstream tasks, including form\nunderstanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42).*\n\n## Usage tips\n\n- In addition to *input_ids*, [`~transformers.LayoutLMModel.forward`] also expects the input `bbox`, which are\nthe bounding boxes (i.e. 2D-positions) of the input tokens. These can be obtained using an external OCR engine such",
  "as Google's [Tesseract](https://github.com/tesseract-ocr/tesseract) (there's a [Python wrapper](https://pypi.org/project/pytesseract/) available). Each bounding box should be in (x0, y0, x1, y1) format, where\n(x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1) represents the\nposition of the lower right corner. Note that one first needs to normalize the bounding boxes to be on a 0-1000\nscale. To normalize, you can use the following function:\n\n```python\ndef normalize_bbox(bbox, width, height):\nreturn [\nint(1000 * (bbox[0] / width)),\nint(1000 * (bbox[1] / height)),\nint(1000 * (bbox[2] / width)),\nint(1000 * (bbox[3] / height)),\n]\n```\n\nHere, `width` and `height` correspond to the width and height of the original document in which the token\noccurs. Those can be obtained using the Python Image Library (PIL) library for example, as follows:\n\n```python\nfrom PIL import Image\n\n# Document can be a png, jpg, etc. PDFs must be converted to images.\nimage = Image.open(name_of_your_document).convert(\"RGB\")\n\nwidth, height = image.size\n```\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LayoutLM. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n<PipelineTag pipeline=\"document-question-answering\" />\n\n- A blog post on [fine-tuning\nLayoutLM for document-understanding using Keras & Hugging Face\nTransformers](https://www.philschmid.de/fine-tuning-layoutlm-keras).\n\n- A blog post on how to [fine-tune LayoutLM for document-understanding using only Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm).\n\n- A notebook on how to [fine-tune LayoutLM on the FUNSD dataset with image embeddings](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb).\n\n- See also: [Document question answering task guide](../tasks/document_question_answering)\n\n<PipelineTag pipeline=\"text-classification\" />",
  "- A notebook on how to [fine-tune LayoutLM for sequence classification on the RVL-CDIP dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"token-classification\" />\n\n- A notebook on how to [ fine-tune LayoutLM for token classification on the FUNSD dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb).\n- [Token classification task guide](../tasks/token_classification)\n\n**Other resources**\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n🚀 Deploy\n\n- A blog post on how to [Deploy LayoutLM with Hugging Face Inference Endpoints](https://www.philschmid.de/inference-endpoints-layoutlm).\n\n## LayoutLMConfig\n\n[[autodoc]] LayoutLMConfig\n\n## LayoutLMTokenizer\n\n[[autodoc]] LayoutLMTokenizer\n\n## LayoutLMTokenizerFast\n\n[[autodoc]] LayoutLMTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## LayoutLMModel\n\n[[autodoc]] LayoutLMModel\n\n## LayoutLMForMaskedLM",
  "[[autodoc]] LayoutLMForMaskedLM\n\n## LayoutLMForSequenceClassification\n\n[[autodoc]] LayoutLMForSequenceClassification\n\n## LayoutLMForTokenClassification\n\n[[autodoc]] LayoutLMForTokenClassification\n\n## LayoutLMForQuestionAnswering\n\n[[autodoc]] LayoutLMForQuestionAnswering\n\n</pt>\n<tf>\n\n## TFLayoutLMModel\n\n[[autodoc]] TFLayoutLMModel\n\n## TFLayoutLMForMaskedLM\n\n[[autodoc]] TFLayoutLMForMaskedLM\n\n## TFLayoutLMForSequenceClassification\n\n[[autodoc]] TFLayoutLMForSequenceClassification\n\n## TFLayoutLMForTokenClassification\n\n[[autodoc]] TFLayoutLMForTokenClassification\n\n## TFLayoutLMForQuestionAnswering\n\n[[autodoc]] TFLayoutLMForQuestionAnswering\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Table Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://arxiv.org/abs/2110.00061) by",
  "Brandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\nas well as table structure recognition and functional analysis. The authors train 2 [DETR](detr) models, one for table detection and one for table structure recognition, dubbed Table Transformers.\n\nThe abstract from the paper is the following:\n\n*Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents.\nHowever, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more\ncomprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input\nmodalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant",
  "source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a\nsignificant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based\nobject detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any\nspecial customization for these tasks.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/table_transformer_architecture.jpeg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Table detection and table structure recognition clarified. Taken from the <a href=\"https://arxiv.org/abs/2110.00061\">original paper</a>. </small>\n\nThe authors released 2 models, one for [table detection](https://huggingface.co/microsoft/table-transformer-detection) in\ndocuments, one for [table structure recognition](https://huggingface.co/microsoft/table-transformer-structure-recognition)",
  "(the task of recognizing the individual rows, columns etc. in a table).\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be\nfound [here](https://github.com/microsoft/table-transformer).\n\n## Resources\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- A demo notebook for the Table Transformer can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Table%20Transformer).\n- It turns out padding of images is quite important for detection. An interesting Github thread with replies from the authors can be found [here](https://github.com/microsoft/table-transformer/issues/68).\n\n## TableTransformerConfig\n\n[[autodoc]] TableTransformerConfig\n\n## TableTransformerModel\n\n[[autodoc]] TableTransformerModel\n- forward\n\n## TableTransformerForObjectDetection\n\n[[autodoc]] TableTransformerForObjectDetection\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LiLT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe LiLT model was proposed in [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.",
  "LiLT allows to combine any pre-trained RoBERTa text encoder with a lightweight Layout Transformer, to enable [LayoutLM](layoutlm)-like document understanding for many\nlanguages.\n\nThe abstract from the paper is the following:\n\n*Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/lilt_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> LiLT architecture. Taken from the <a href=\"https://arxiv.org/abs/2202.13669\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/jpwang/lilt).\n\n## Usage tips\n\n- To combine the Language-Independent Layout Transformer with a new RoBERTa checkpoint from the [hub](https://huggingface.co/models?search=roberta), refer to [this guide](https://github.com/jpWang/LiLT#or-generate-your-own-checkpoint-optional).\nThe script will result in `config.json` and `pytorch_model.bin` files being stored locally. After doing this, one can do the following (assuming you're logged in with your HuggingFace account):\n\n```python\nfrom transformers import LiltModel\n\nmodel = LiltModel.from_pretrained(\"path_to_your_files\")\nmodel.push_to_hub(\"name_of_repo_on_the_hub\")\n```\n\n- When preparing data for the model, make sure to use the token vocabulary that corresponds to the RoBERTa checkpoint you combined with the Layout Transformer.",
  "- As [lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-roberta-en-base) uses the same vocabulary as [LayoutLMv3](layoutlmv3), one can use [`LayoutLMv3TokenizerFast`] to prepare data for the model.\nThe same is true for [lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-infoxlm-base): one can use [`LayoutXLMTokenizerFast`] for that model.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LiLT.\n\n- Demo notebooks for LiLT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LiLT).\n\n**Documentation resources**\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## LiltConfig\n\n[[autodoc]] LiltConfig\n\n## LiltModel\n\n[[autodoc]] LiltModel\n- forward\n\n## LiltForSequenceClassification",
  "[[autodoc]] LiltForSequenceClassification\n- forward\n\n## LiltForTokenClassification\n\n[[autodoc]] LiltForTokenClassification\n- forward\n\n## LiltForQuestionAnswering\n\n[[autodoc]] LiltForQuestionAnswering\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# M2M100\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\nSiddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n\nThe abstract from the paper is the following:\n\n*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a\nsingle model able to translate between any pair of languages. However, much of this work is English-Centric by training\nonly on data which was translated from or to English. While this is supported by large sources of training data, it\ndoes not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation\nmodel that can translate directly between any pair of 100 languages. We build and open source a training dataset that\ncovers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how",
  "to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters\nto create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly\ntranslating between non-English directions while performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.*\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla).\n\n\n## Usage tips and examples\n\nM2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks. As the model is\nmultilingual it expects the sequences in a certain format: A special language id token is used as prefix in both the\nsource and target text. The source text format is `[lang_code] X [eos]`, where `lang_code` is source language\nid for source text and target language id for target text, with `X` being the source or target text.\n\nThe [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the\nexamples. To install `sentencepiece` run `pip install sentencepiece`.\n\n**Supervised Training**",
  "```python\nfrom transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer\n\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"fr\")\n\nsrc_text = \"Life is like a box of chocolates.\"\ntgt_text = \"La vie est comme une boîte de chocolat.\"\n\nmodel_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n\nloss = model(**model_inputs).loss  # forward pass\n```\n\n**Generation**\n\nM2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation with the target language id\nbeing forced as the first generated token. To force the target language id as the first generated token, pass the\n*forced_bos_token_id* parameter to the *generate* method. The following example shows how to translate between\nHindi to French and Chinese to English using the *facebook/m2m100_418M* checkpoint.\n\n```python\n>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\n>>> hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n>>> chinese_text = \"生活就像一盒巧克力。\"\n\n>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")",
  ">>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n>>> # translate Hindi to French\n>>> tokenizer.src_lang = \"hi\"\n>>> encoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\n>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\"La vie est comme une boîte de chocolat.\"\n\n>>> # translate Chinese to English\n>>> tokenizer.src_lang = \"zh\"\n>>> encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\"Life is like a box of chocolate.\"\n```\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## M2M100Config\n\n[[autodoc]] M2M100Config\n\n## M2M100Tokenizer\n\n[[autodoc]] M2M100Tokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## M2M100Model\n\n[[autodoc]] M2M100Model\n- forward\n\n## M2M100ForConditionalGeneration",
  "[[autodoc]] M2M100ForConditionalGeneration\n- forward\n\n## Using Flash Attention 2\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on `cuda` kernels.\n\n### Installation\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n\nNext, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n### Usage\n\nTo load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). You can use either `torch.float16` or `torch.bfloat16` precision.\n\n```python\n>>> import torch\n>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer",
  ">>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\").eval()\n>>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n>>> # translate Hindi to French\n>>> hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n>>> tokenizer.src_lang = \"hi\"\n>>> encoded_hi = tokenizer(hi_text, return_tensors=\"pt\").to(\"cuda\")\n>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\"La vie est comme une boîte de chocolat.\"\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation and the Flash Attention 2.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/visheratin/documentation-images/resolve/main/nllb-speedup.webp\">\n</div>\n\n## Using Scaled Dot Product Attention (SDPA)\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function",
  "encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```python\nfrom transformers import M2M100ForConditionalGeneration\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Bamba\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Bamba-9B is a decoder-only language model based on the [Mamba-2](https://github.com/state-spaces/mamba) architecture and is designed to handle a wide range of text generation tasks. It is trained from scratch using a two-stage training approach. In the first stage, the model is trained on 2 trillion tokens from the Dolma v1.7 dataset. In the second stage, it undergoes additional training on 200 billion tokens, leveraging a carefully curated blend of high-quality data to further refine its performance and enhance output quality.\n\nCheckout all Bamba-9B model checkpoints [here](https://github.com/foundation-model-stack/bamba).\n\n## BambaConfig\n\n| Model            | Params       | # Layers | Hidden Dim. | Attention Heads | GQA | KV Heads | Context Length |  Tied Embeddings |\n|-------------------|--------------|----------|-------------|-----------------|-----|----------|----------------|------------------|\n| Bamba  | 9B (9.78B)   | 32       | 4096        | 32              | Yes | 8        | 4096           | True |\n\n[[autodoc]] BambaConfig\n\n<!---\n## Usage Tips\n\nTips:\n\n- The architecture is based on Mamba-2 models.\n\n## BambaModel\n\n[[autodoc]] BambaModel\n- forward\n-->\n\n## BambaForCausalLM",
  "```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ibm-fms/Bamba-9B\")\ntokenizer = AutoTokenizer.from_pretrained(\"ibm-fms/Bamba-9B\")\n\nmessage = [\"Mamba is a snake with following properties  \"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\nresponse = model.generate(**inputs, max_new_tokens=64)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n```\n\n[[autodoc]] BambaForCausalLM\n- forward\n\nThis HF implementation is contributed by [ani300](https://github.com/ani300) and [fabianlim](https://github.com/fabianlim).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# InstructBlipVideo\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe InstructBLIPVideo is an extension of the models proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.",
  "InstructBLIPVideo uses the same architecture as [InstructBLIP](instructblip) and works with the same checkpoints as [InstructBLIP](instructblip). The only difference is the ability to process videos.\n\nThe abstract from the paper is the following:",
  "*General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> InstructBLIPVideo architecture. Taken from the <a href=\"https://arxiv.org/abs/2305.06500\">original paper.</a> </small>\n\nThis model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\nThe original code can be found [here](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip).\n\n## Usage tips\n\n- The model was trained by sampling 4 frames per video, so it's recommended to sample 4 frames\n\n> [!NOTE]",
  "> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n\n## InstructBlipVideoConfig\n\n[[autodoc]] InstructBlipVideoConfig\n- from_vision_qformer_text_configs\n\n## InstructBlipVideoVisionConfig\n\n[[autodoc]] InstructBlipVideoVisionConfig\n\n## InstructBlipVideoQFormerConfig\n\n[[autodoc]] InstructBlipVideoQFormerConfig\n\n## InstructBlipVideoProcessor",
  "[[autodoc]] InstructBlipVideoProcessor\n\n## InstructBlipVideoImageProcessor\n\n[[autodoc]] InstructBlipVideoImageProcessor\n- preprocess\n\n## InstructBlipVideoVisionModel\n\n[[autodoc]] InstructBlipVideoVisionModel\n- forward\n\n## InstructBlipVideoQFormerModel\n\n[[autodoc]] InstructBlipVideoQFormerModel\n- forward\n\n## InstructBlipVideoForConditionalGeneration\n\n[[autodoc]] InstructBlipVideoForConditionalGeneration\n- forward\n- generate",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OWLv2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "OWLv2 was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2 scales up [OWL-ViT](owlvit) using self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. This results in large gains over the previous state-of-the-art for zero-shot object detection.\n\nThe abstract from the paper is the following:",
  "*Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations, from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/owlv2_overview.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> OWLv2 high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2306.09683\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit).\n\n## Usage example",
  "OWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](clip) as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.",
  "[`Owlv2ImageProcessor`] can be used to resize (or rescale) and normalize images for the model and [`CLIPTokenizer`] is used to encode the text. [`Owlv2Processor`] wraps [`Owlv2ImageProcessor`] and [`CLIPTokenizer`] into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using [`Owlv2Processor`] and [`Owlv2ForObjectDetection`].\n\n```python\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n\n>>> from transformers import Owlv2Processor, Owlv2ForObjectDetection\n\n>>> processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n>>> model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n>>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n>>> target_sizes = torch.tensor([(image.height, image.width)])",
  ">>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> results = processor.post_process_grounded_object_detection(\n...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n... )\n>>> # Retrieve predictions for the first image for the corresponding text queries\n>>> result = results[0]\n>>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n>>> for box, score, text_label in zip(boxes, scores, text_labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a photo of a cat with confidence 0.614 at location [341.67, 23.39, 642.32, 371.35]\nDetected a photo of a cat with confidence 0.665 at location [6.75, 51.96, 326.62, 473.13]\n```\n\n## Resources\n\n- A demo notebook on using OWLv2 for zero- and one-shot (image-guided) object detection can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OWLv2).\n- [Zero-shot object detection task guide](../tasks/zero_shot_object_detection)\n\n<Tip>",
  "The architecture of OWLv2 is identical to [OWL-ViT](owlvit), however the object detection head now also includes an objectness classifier, which predicts the (query-agnostic) likelihood that a predicted box contains an object (as opposed to background). The objectness score can be used to rank or filter predictions independently of text queries.\nUsage of OWLv2 is identical to [OWL-ViT](owlvit) with a new, updated image processor ([`Owlv2ImageProcessor`]).\n\n</Tip>\n\n## Owlv2Config\n\n[[autodoc]] Owlv2Config\n- from_text_vision_configs\n\n## Owlv2TextConfig\n\n[[autodoc]] Owlv2TextConfig\n\n## Owlv2VisionConfig\n\n[[autodoc]] Owlv2VisionConfig\n\n## Owlv2ImageProcessor\n\n[[autodoc]] Owlv2ImageProcessor\n- preprocess\n- post_process_object_detection\n- post_process_image_guided_detection\n\n## Owlv2Processor\n\n[[autodoc]] Owlv2Processor\n- __call__\n- post_process_grounded_object_detection\n- post_process_image_guided_detection\n\n## Owlv2Model\n\n[[autodoc]] Owlv2Model\n- forward\n- get_text_features\n- get_image_features\n\n## Owlv2TextModel\n\n[[autodoc]] Owlv2TextModel\n- forward\n\n## Owlv2VisionModel\n\n[[autodoc]] Owlv2VisionModel\n- forward\n\n## Owlv2ForObjectDetection\n\n[[autodoc]] Owlv2ForObjectDetection",
  "- forward\n- image_guided_detection",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Funnel Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview\n\nThe Funnel Transformer model was proposed in the paper [Funnel-Transformer: Filtering out Sequential Redundancy for",
  "Efficient Language Processing](https://arxiv.org/abs/2006.03236). It is a bidirectional transformer model, like\nBERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks\n(CNN) in computer vision.\n\nThe abstract from the paper is the following:\n\n*With the success of language pretraining, it is highly desirable to develop more efficient architectures of good\nscalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the\nmuch-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only\nrequire a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which\ngradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More\nimportantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further\nimprove the model capacity. In addition, to perform token-level predictions as required by common pretraining",
  "objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence\nvia a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on\na wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading\ncomprehension.*\n\nThis model was contributed by [sgugger](https://huggingface.co/sgugger). The original code can be found [here](https://github.com/laiguokun/Funnel-Transformer).\n\n## Usage tips\n\n- Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers. This way, their length is divided by 2, which speeds up the computation of the next hidden states.\nThe base model therefore has a final sequence length that is a quarter of the original one. This model can be used\ndirectly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other\ntasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same\nsequence length as the input.",
  "- For tasks such as classification, this is not a problem, but for tasks like masked language modeling or token classification, we need a hidden state with the same sequence length as the original input. In those cases, the final hidden states are upsampled to the input sequence length and go through two additional layers. That's why there are two versions of each checkpoint. The version suffixed with “-base” contains only the three blocks, while the version without that suffix contains the three blocks and the upsampling head with its additional layers.\n- The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be\nused for [`FunnelModel`], [`FunnelForPreTraining`],\n[`FunnelForMaskedLM`], [`FunnelForTokenClassification`] and\n[`FunnelForQuestionAnswering`]. The second ones should be used for\n[`FunnelBaseModel`], [`FunnelForSequenceClassification`] and\n[`FunnelForMultipleChoice`].\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)",
  "- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n\n## FunnelConfig\n\n[[autodoc]] FunnelConfig\n\n## FunnelTokenizer\n\n[[autodoc]] FunnelTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## FunnelTokenizerFast\n\n[[autodoc]] FunnelTokenizerFast\n\n## Funnel specific outputs\n\n[[autodoc]] models.funnel.modeling_funnel.FunnelForPreTrainingOutput\n\n[[autodoc]] models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## FunnelBaseModel\n\n[[autodoc]] FunnelBaseModel\n- forward\n\n## FunnelModel\n\n[[autodoc]] FunnelModel\n- forward\n\n## FunnelModelForPreTraining\n\n[[autodoc]] FunnelForPreTraining\n- forward\n\n## FunnelForMaskedLM\n\n[[autodoc]] FunnelForMaskedLM\n- forward\n\n## FunnelForSequenceClassification\n\n[[autodoc]] FunnelForSequenceClassification\n- forward\n\n## FunnelForMultipleChoice\n\n[[autodoc]] FunnelForMultipleChoice\n- forward\n\n## FunnelForTokenClassification\n\n[[autodoc]] FunnelForTokenClassification\n- forward\n\n## FunnelForQuestionAnswering\n\n[[autodoc]] FunnelForQuestionAnswering\n- forward\n\n</pt>\n<tf>",
  "## TFFunnelBaseModel\n\n[[autodoc]] TFFunnelBaseModel\n- call\n\n## TFFunnelModel\n\n[[autodoc]] TFFunnelModel\n- call\n\n## TFFunnelModelForPreTraining\n\n[[autodoc]] TFFunnelForPreTraining\n- call\n\n## TFFunnelForMaskedLM\n\n[[autodoc]] TFFunnelForMaskedLM\n- call\n\n## TFFunnelForSequenceClassification\n\n[[autodoc]] TFFunnelForSequenceClassification\n- call\n\n## TFFunnelForMultipleChoice\n\n[[autodoc]] TFFunnelForMultipleChoice\n- call\n\n## TFFunnelForTokenClassification\n\n[[autodoc]] TFFunnelForTokenClassification\n- call\n\n## TFFunnelForQuestionAnswering\n\n[[autodoc]] TFFunnelForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Llama2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview",
  "The Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom. It is a collection of foundation language models ranging from 7B to 70B parameters, with checkpoints finetuned for chat application!",
  "The abstract from the paper is the following:\n\n*In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.*\n\nCheckout all Llama2 model checkpoints [here](https://huggingface.co/models?search=llama2).\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ) with contributions from [Lysandre Debut](https://huggingface.co/lysandre). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox). The original code of the authors can be found [here](https://github.com/facebookresearch/llama).\n\n## Usage tips",
  "<Tip warning={true}>\n\nThe `Llama2` models were trained using `bfloat16`, but the original inference uses `float16`. The checkpoints uploaded on the Hub use `torch_dtype = 'float16'`, which will be\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\n\nThe `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online), then it will be casted to the default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is a `torch_dtype` provided in the config, it will be used.\n\nTraining the model in `float16` is not recommended and is known to produce `nan`; as such, the model should be trained in `bfloat16`.\n\n</Tip>\n\nTips:\n\n- Weights for the Llama2 models can be obtained by filling out [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)",
  "- The architecture is very similar to the first Llama, with the addition of Grouped Query Attention (GQA) following this [paper](https://arxiv.org/pdf/2305.13245.pdf)\n- Setting `config.pretraining_tp` to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits.\n- The original model uses `pad_id = -1` which means that there is no padding token. We can't have the same logic, make sure to add a padding token using `tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})` and resize the token embedding accordingly. You should also set the `model.config.pad_token_id`. The `embed_tokens` layer of the model is initialized with `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`, which makes sure that encoding the padding token will output zeros, so passing it when initializing is recommended.",
  "- After filling out the form and gaining access to the model checkpoints, you should be able to use the already converted checkpoints. Otherwise, if you are converting your own model, feel free to use the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n--input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n```\n\n- After conversion, the model and tokenizer can be loaded via:\n\n```python\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\nmodel = LlamaForCausalLM.from_pretrained(\"/output/path\")\n```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\ncome in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 75B model, it's thus 145GB of RAM needed.",
  "- The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string.\n\n- When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `torch_dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LLaMA2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
  "- [Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2), a blog post about Llama 2 and how to use it with 🤗 Transformers and 🤗 PEFT.\n- [LLaMA 2 - Every Resource you need](https://www.philschmid.de/llama-2), a compilation of relevant resources to learn about LLaMA 2 and how to get started quickly.\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- A [notebook](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing) on how to fine-tune Llama 2 in Google Colab using QLoRA and 4-bit precision. 🌎\n- A [notebook](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing) on how to fine-tune the \"Llama-v2-7b-guanaco\" model with 4-bit QLoRA and generate Q&A datasets from PDFs. 🌎\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A [notebook](https://colab.research.google.com/drive/1ggaa2oRFphdBmqIjSEbnb_HGkcIRC2ZB?usp=sharing) on how to fine-tune the Llama 2 model with QLoRa, TRL, and Korean text classification dataset. 🌎🇰🇷\n\n⚗️ Optimization\n- [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl), a guide to using the TRL library's DPO method to fine tune Llama 2 on a specific dataset.",
  "- [Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2), a guide to training Llama 2 to generate instructions from inputs, transforming the model from instruction-following to instruction-giving.\n- A [notebook](https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing) on how to fine-tune the Llama 2 model on a personal computer using QLoRa and TRL. 🌎\n\n⚡️ Inference\n- A [notebook](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7h?usp=sharing) on how to quantize the Llama 2 model using GPTQ from the AutoGPTQ library. 🌎\n- A [notebook](https://colab.research.google.com/drive/1X1z9Q6domMKl2CnEM0QGHNwidLfR4dW2?usp=sharing) on how to run the Llama 2 Chat Model with 4-bit quantization on a local computer or Google Colab. 🌎\n\n🚀 Deploy\n- [Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora), a complete guide from setup to QLoRA fine-tuning and deployment on Amazon SageMaker.\n- [Deploy Llama 2 7B/13B/70B on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama-llm), a guide on using Hugging Face's LLM DLC container for secure and scalable deployment.",
  "## LlamaConfig\n\n[[autodoc]] LlamaConfig\n\n\n## LlamaTokenizer\n\n[[autodoc]] LlamaTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## LlamaTokenizerFast\n\n[[autodoc]] LlamaTokenizerFast\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- update_post_processor\n- save_vocabulary\n\n## LlamaModel\n\n[[autodoc]] LlamaModel\n- forward\n\n\n## LlamaForCausalLM\n\n[[autodoc]] LlamaForCausalLM\n- forward\n\n## LlamaForSequenceClassification\n\n[[autodoc]] LlamaForSequenceClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# M-CTC-T\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, so we won't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.",
  "You can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n</Tip>\n\n## Overview\n\nThe M-CTC-T model was proposed in [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. The model is a 1B-param transformer encoder, with a CTC head over 8065 character labels and a language identification head over 60 language ID labels. It is trained on Common Voice (version 6.1, December 2020 release) and VoxPopuli. After training on Common Voice and VoxPopuli, the model is trained on Common Voice only. The labels are unnormalized character-level transcripts (punctuation and capitalization are not removed). The model takes as input Mel filterbank features from a 16Khz audio signal.\n\nThe abstract from the paper is the following:\n\n*Semi-supervised learning through pseudo-labeling has become a staple of state-of-the-art monolingual\nspeech recognition systems. In this work, we extend pseudo-labeling to massively multilingual speech\nrecognition with 60 languages. We propose a simple pseudo-labeling recipe that works well even",
  "with low-resource languages: train a supervised multilingual model, fine-tune it with semi-supervised\nlearning on a target language, generate pseudo-labels for that language, and train a final model using\npseudo-labels for all languages, either from scratch or by fine-tuning. Experiments on the labeled\nCommon Voice and unlabeled VoxPopuli datasets show that our recipe can yield a model with better\nperformance for many languages that also transfers well to LibriSpeech.*\n\nThis model was contributed by [cwkeam](https://huggingface.co/cwkeam). The original code can be found [here](https://github.com/flashlight/wav2letter/tree/main/recipes/mling_pl).\n\n## Usage tips\n\nThe PyTorch version of this model is only available in torch 1.9 and higher.\n\n## Resources\n\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## MCTCTConfig\n\n[[autodoc]] MCTCTConfig\n\n## MCTCTFeatureExtractor\n\n[[autodoc]] MCTCTFeatureExtractor\n- __call__\n\n## MCTCTProcessor\n\n[[autodoc]] MCTCTProcessor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## MCTCTModel\n\n[[autodoc]] MCTCTModel\n- forward\n\n## MCTCTForCTC\n\n[[autodoc]] MCTCTForCTC\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Blenderbot Small\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\nNote that [`BlenderbotSmallModel`] and\n[`BlenderbotSmallForConditionalGeneration`] are only used in combination with the checkpoint\n[facebook/blenderbot-90M](https://huggingface.co/facebook/blenderbot-90M). Larger Blenderbot checkpoints should\ninstead be used with [`BlenderbotModel`] and\n[`BlenderbotForConditionalGeneration`]\n\n## Overview\n\nThe Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that\nscaling neural models in the number of parameters and the size of the data they are trained on gives improved results,\nwe show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of\nskills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to",
  "their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent\npersona. We show that large scale models can learn these skills when given appropriate training data and choice of\ngeneration strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models\nand code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn\ndialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing\nfailure cases of our models.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The authors' code can be\nfound [here](https://github.com/facebookresearch/ParlAI).\n\n## Usage tips\n\nBlenderbot Small is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## BlenderbotSmallConfig\n\n[[autodoc]] BlenderbotSmallConfig",
  "## BlenderbotSmallTokenizer\n\n[[autodoc]] BlenderbotSmallTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## BlenderbotSmallTokenizerFast\n\n[[autodoc]] BlenderbotSmallTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## BlenderbotSmallModel\n\n[[autodoc]] BlenderbotSmallModel\n- forward\n\n## BlenderbotSmallForConditionalGeneration\n\n[[autodoc]] BlenderbotSmallForConditionalGeneration\n- forward\n\n## BlenderbotSmallForCausalLM\n\n[[autodoc]] BlenderbotSmallForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFBlenderbotSmallModel\n\n[[autodoc]] TFBlenderbotSmallModel\n- call\n\n## TFBlenderbotSmallForConditionalGeneration\n\n[[autodoc]] TFBlenderbotSmallForConditionalGeneration\n- call\n\n</tf>\n<jax>\n\n## FlaxBlenderbotSmallModel\n\n[[autodoc]] FlaxBlenderbotSmallModel\n- __call__\n- encode\n- decode\n\n## FlaxBlenderbotForConditionalGeneration\n\n[[autodoc]] FlaxBlenderbotSmallForConditionalGeneration\n- __call__\n- encode\n- decode\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ViTMatte\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ViTMatte model was proposed in [Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\nViTMatte leverages plain [Vision Transformers](vit) for the task of image matting, which is the process of accurately estimating the foreground object in images and videos.\n\nThe abstract from the paper is the following:",
  "*Recently, plain vision Transformers (ViTs) have shown impressive performance on various computer vision tasks, thanks to their strong modeling capacity and large-scale pretraining. However, they have not yet conquered the problem of image matting. We hypothesize that image matting could also be boosted by ViTs and present a new efficient and robust ViT-based matting system, named ViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with a convolution neck to help ViTs achieve an excellent performance-computation trade-off in matting tasks. (ii) Additionally, we introduce the detail capture module, which just consists of simple lightweight convolutions to complement the detailed information required by matting. To the best of our knowledge, ViTMatte is the first work to unleash the potential of ViT on image matting with concise adaptation. It inherits many superior properties from ViT to matting, including various pretraining strategies, concise architecture design, and flexible inference strategies. We evaluate ViTMatte on Composition-1k and Distinctions-646, the most commonly used benchmark for image matting, our method achieves state-of-the-art performance and outperforms prior matting works by a large margin.*",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/hustvl/ViTMatte).\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitmatte_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViTMatte high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2305.15272\">original paper.</a> </small>\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViTMatte.\n\n- A demo notebook regarding inference with [`VitMatteForImageMatting`], including background replacement, can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViTMatte).\n\n<Tip>\n\nThe model expects both the image and trimap (concatenated) as input. Use [`ViTMatteImageProcessor`] for this purpose.\n</Tip>\n\n## VitMatteConfig\n\n[[autodoc]] VitMatteConfig\n\n## VitMatteImageProcessor\n\n[[autodoc]] VitMatteImageProcessor\n- preprocess\n\n## VitMatteForImageMatting\n\n[[autodoc]] VitMatteForImageMatting\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BLOOM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe BLOOM model has been proposed with its various versions through the [BigScience Workshop](https://bigscience.huggingface.co/). BigScience is inspired by other open science initiatives where researchers have pooled their time and resources to collectively achieve a higher impact.\nThe architecture of BLOOM is essentially similar to GPT3 (auto-regressive model for next token prediction), but has been trained on 46 different languages and 13 programming languages.\nSeveral smaller versions of the models have been trained on the same dataset. BLOOM is available in the following versions:\n\n- [bloom-560m](https://huggingface.co/bigscience/bloom-560m)\n- [bloom-1b1](https://huggingface.co/bigscience/bloom-1b1)\n- [bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)\n- [bloom-3b](https://huggingface.co/bigscience/bloom-3b)\n- [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1)\n- [bloom](https://huggingface.co/bigscience/bloom) (176B parameters)\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BLOOM. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- [`BloomForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n\nSee also:\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n\n⚡️ Inference\n- A blog on [Optimization story: Bloom inference](https://huggingface.co/blog/bloom-inference-optimization).",
  "- A blog on [Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts).\n\n⚙️ Training\n- A blog on [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed).\n\n## BloomConfig\n\n[[autodoc]] BloomConfig\n- all\n\n## BloomTokenizerFast\n\n[[autodoc]] BloomTokenizerFast\n- all\n\n\n<frameworkcontent>\n<pt>\n\n## BloomModel\n\n[[autodoc]] BloomModel\n- forward\n\n## BloomForCausalLM\n\n[[autodoc]] BloomForCausalLM\n- forward\n\n## BloomForSequenceClassification\n\n[[autodoc]] BloomForSequenceClassification\n- forward\n\n## BloomForTokenClassification\n\n[[autodoc]] BloomForTokenClassification\n- forward\n\n## BloomForQuestionAnswering\n\n[[autodoc]] BloomForQuestionAnswering\n- forward\n\n</pt>\n<jax>\n\n## FlaxBloomModel\n\n[[autodoc]] FlaxBloomModel\n- __call__\n\n## FlaxBloomForCausalLM\n\n[[autodoc]] FlaxBloomForCausalLM\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Speech2Text2\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview",
  "The Speech2Text2 model is used together with [Wav2Vec2](wav2vec2) for Speech Translation models proposed in\n[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by\nChanghan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n\nSpeech2Text2 is a *decoder-only* transformer model that can be used with any speech *encoder-only*, such as\n[Wav2Vec2](wav2vec2) or [HuBERT](hubert) for Speech-to-Text tasks. Please refer to the\n[SpeechEncoderDecoder](speech-encoder-decoder) class on how to combine Speech2Text2 with any speech *encoder-only*\nmodel.\n\nThis model was contributed by [Patrick von Platen](https://huggingface.co/patrickvonplaten).\n\nThe original code can be found [here](https://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/fairseq/models/wav2vec/wav2vec2_asr.py#L266).\n\n## Usage tips\n\n- Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dataset. For more information, see\nthe [official models](https://huggingface.co/models?other=speech2text2) .\n- Speech2Text2 is always used within the [SpeechEncoderDecoder](speech-encoder-decoder) framework.",
  "- Speech2Text2's tokenizer is based on [fastBPE](https://github.com/glample/fastBPE).\n\n## Inference\n\nSpeech2Text2's [`SpeechEncoderDecoderModel`] model accepts raw waveform input values from speech and\nmakes use of [`~generation.GenerationMixin.generate`] to translate the input speech\nautoregressively to the target language.\n\nThe [`Wav2Vec2FeatureExtractor`] class is responsible for preprocessing the input speech and\n[`Speech2Text2Tokenizer`] decodes the generated target tokens to the target string. The\n[`Speech2Text2Processor`] wraps [`Wav2Vec2FeatureExtractor`] and\n[`Speech2Text2Tokenizer`] into a single instance to both extract the input features and decode the\npredicted token ids.\n\n- Step-by-step Speech Translation\n\n```python\n>>> import torch\n>>> from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n>>> processor = Speech2Text2Processor.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech",
  "...     return batch\n\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> inputs = processor(ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"pt\")\n>>> generated_ids = model.generate(inputs=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n\n>>> transcription = processor.batch_decode(generated_ids)\n```\n\n- Speech Translation via Pipelines\n\nThe automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code\n\n```python\n>>> from datasets import load_dataset\n>>> from transformers import pipeline\n\n>>> librispeech_en = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> asr = pipeline(\n...     \"automatic-speech-recognition\",\n...     model=\"facebook/s2t-wav2vec2-large-en-de\",\n...     feature_extractor=\"facebook/s2t-wav2vec2-large-en-de\",\n... )\n\n>>> translation_de = asr(librispeech_en[0][\"file\"])\n```\n\nSee [model hub](https://huggingface.co/models?filter=speech2text2) to look for Speech2Text2 checkpoints.\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## Speech2Text2Config",
  "[[autodoc]] Speech2Text2Config\n\n## Speech2TextTokenizer\n\n[[autodoc]] Speech2Text2Tokenizer\n- batch_decode\n- decode\n- save_vocabulary\n\n## Speech2Text2Processor\n\n[[autodoc]] Speech2Text2Processor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## Speech2Text2ForCausalLM\n\n[[autodoc]] Speech2Text2ForCausalLM\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LayoutXLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nLayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha",
  "Zhang, Furu Wei. It's a multilingual extension of the [LayoutLMv2 model](https://arxiv.org/abs/2012.14740) trained\non 53 languages.\n\nThe abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document\nunderstanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In\nthis paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to\nbridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also\nintroduce a multilingual form understanding benchmark dataset named XFUN, which includes form understanding samples in\n7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled\nfor each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUN dataset.*",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm).\n\n## Usage tips and examples\n\nOne can directly plug in the weights of LayoutXLM into a LayoutLMv2 model, like so:\n\n```python\nfrom transformers import LayoutLMv2Model\n\nmodel = LayoutLMv2Model.from_pretrained(\"microsoft/layoutxlm-base\")\n```\n\nNote that LayoutXLM has its own tokenizer, based on\n[`LayoutXLMTokenizer`]/[`LayoutXLMTokenizerFast`]. You can initialize it as\nfollows:\n\n```python\nfrom transformers import LayoutXLMTokenizer\n\ntokenizer = LayoutXLMTokenizer.from_pretrained(\"microsoft/layoutxlm-base\")\n```\n\nSimilar to LayoutLMv2, you can use [`LayoutXLMProcessor`] (which internally applies\n[`LayoutLMv2ImageProcessor`] and\n[`LayoutXLMTokenizer`]/[`LayoutXLMTokenizerFast`] in sequence) to prepare all\ndata for the model.\n\n<Tip>\n\nAs LayoutXLM's architecture is equivalent to that of LayoutLMv2, one can refer to [LayoutLMv2's documentation page](layoutlmv2) for all tips, code examples and notebooks.\n</Tip>\n\n## LayoutXLMTokenizer\n\n[[autodoc]] LayoutXLMTokenizer\n- __call__\n- build_inputs_with_special_tokens\n- get_special_tokens_mask",
  "- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## LayoutXLMTokenizerFast\n\n[[autodoc]] LayoutXLMTokenizerFast\n- __call__\n\n## LayoutXLMProcessor\n\n[[autodoc]] LayoutXLMProcessor\n- __call__",
  "<!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MegatronBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MegatronBERT model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\nParallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,",
  "Jared Casper and Bryan Catanzaro.\n\nThe abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in\nNatural Language Processing applications. However, very large models can be quite difficult to train due to memory\nconstraints. In this work, we present our techniques for training very large transformer models and implement a simple,\nefficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our\napproach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We\nillustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain\n15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline\nthat sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance",
  "the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9\nbillion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in\nBERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we\nachieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA\naccuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy\nof 89.4%).*\n\nThis model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM).\nThat repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular,\nit contains a hybrid model parallel approach using \"tensor parallel\" and \"pipeline parallel\" techniques.\n\n## Usage tips\n\nWe have provided pretrained [BERT-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m) checkpoints\nfor use to evaluate or finetuning downstream tasks.",
  "To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)\nRegistry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).\n\nAlternatively, you can directly download the checkpoints using:\n\nBERT-345M-uncased:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_uncased/zip\n-O megatron_bert_345m_v0_1_uncased.zip\n```\n\nBERT-345M-cased:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_cased/zip -O\nmegatron_bert_345m_v0_1_cased.zip\n```\n\nOnce you have obtained the checkpoints from NVIDIA GPU Cloud (NGC), you have to convert them to a format that will\neasily be loaded by Hugging Face Transformers and our port of the BERT code.\n\nThe following commands allow you to do the conversion. We assume that the folder `models/megatron_bert` contains\n`megatron_bert_345m_v0_1_{cased, uncased}.zip` and that the commands are run from inside that folder:\n\n```bash",
  "python3 $PATH_TO_TRANSFORMERS/models/megatron_bert/convert_megatron_bert_checkpoint.py megatron_bert_345m_v0_1_uncased.zip\n```\n\n```bash\npython3 $PATH_TO_TRANSFORMERS/models/megatron_bert/convert_megatron_bert_checkpoint.py megatron_bert_345m_v0_1_cased.zip\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## MegatronBertConfig\n\n[[autodoc]] MegatronBertConfig\n\n## MegatronBertModel\n\n[[autodoc]] MegatronBertModel\n- forward\n\n## MegatronBertForMaskedLM\n\n[[autodoc]] MegatronBertForMaskedLM\n- forward\n\n## MegatronBertForCausalLM\n\n[[autodoc]] MegatronBertForCausalLM\n- forward\n\n## MegatronBertForNextSentencePrediction\n\n[[autodoc]] MegatronBertForNextSentencePrediction\n- forward\n\n## MegatronBertForPreTraining\n\n[[autodoc]] MegatronBertForPreTraining\n- forward\n\n## MegatronBertForSequenceClassification",
  "[[autodoc]] MegatronBertForSequenceClassification\n- forward\n\n## MegatronBertForMultipleChoice\n\n[[autodoc]] MegatronBertForMultipleChoice\n- forward\n\n## MegatronBertForTokenClassification\n\n[[autodoc]] MegatronBertForTokenClassification\n- forward\n\n## MegatronBertForQuestionAnswering\n\n[[autodoc]] MegatronBertForQuestionAnswering\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLM-ProphetNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=xprophetnet\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-xprophetnet-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/xprophetnet-large-wiki100-cased-xglue-ntg\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n**DISCLAIMER:** If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title) and assign\n@patrickvonplaten\n\n\n## Overview\n\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\nZhang, Ming Zhou on 13 Jan, 2020.\n\nXLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of",
  "just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual\n\"wiki100\" Wikipedia dump. XLM-ProphetNet's model architecture and pretraining objective is same as ProphetNet, but XLM-ProphetNet was pre-trained on the cross-lingual dataset XGLUE.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\nself-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\nthe optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\nn-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent\noverfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale\ndataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for",
  "abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new\nstate-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.*\n\nThe Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## XLMProphetNetConfig\n\n[[autodoc]] XLMProphetNetConfig\n\n## XLMProphetNetTokenizer\n\n[[autodoc]] XLMProphetNetTokenizer\n\n## XLMProphetNetModel\n\n[[autodoc]] XLMProphetNetModel\n\n## XLMProphetNetEncoder\n\n[[autodoc]] XLMProphetNetEncoder\n\n## XLMProphetNetDecoder\n\n[[autodoc]] XLMProphetNetDecoder\n\n## XLMProphetNetForConditionalGeneration\n\n[[autodoc]] XLMProphetNetForConditionalGeneration\n\n## XLMProphetNetForCausalLM\n\n[[autodoc]] XLMProphetNetForCausalLM",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Open-Llama\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0.",
  "You can do so by running the following command: `pip install -U transformers==4.31.0`.\n\n</Tip>\n\n<Tip warning={true}>\n\nThis model differs from the [OpenLLaMA models](https://huggingface.co/models?search=openllama) on the Hugging Face Hub, which primarily use the [LLaMA](llama) architecture.\n\n</Tip>\n\n## Overview\n\nThe Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL.\n\nThe model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.\nAnd the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.\n\nThis model was contributed by [s-JoL](https://huggingface.co/s-JoL).\nThe original code was released on GitHub by [s-JoL](https://github.com/s-JoL), but is now removed.\n\n## OpenLlamaConfig\n\n[[autodoc]] OpenLlamaConfig\n\n## OpenLlamaModel\n\n[[autodoc]] OpenLlamaModel\n- forward\n\n## OpenLlamaForCausalLM\n\n[[autodoc]] OpenLlamaForCausalLM\n- forward\n\n## OpenLlamaForSequenceClassification\n\n[[autodoc]] OpenLlamaForSequenceClassification\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Phi\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Phi-1 model was proposed in [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li.\n\nThe Phi-1.5 model was proposed in [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\n\n### Summary\n\nIn Phi-1 and Phi-1.5 papers, the authors showed how important the quality of the data is in training relative to the model size.\nThey selected high quality \"textbook\" data alongside with synthetically generated data for training their small sized Transformer\nbased model Phi-1 with 1.3B parameters. Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP.\nThey follow the same strategy for Phi-1.5 and created another 1.3B parameter model with performance on natural language tasks comparable",
  "to models 5x larger, and surpassing most non-frontier LLMs. Phi-1.5 exhibits many of the traits of much larger LLMs such as the ability\nto “think step by step” or perform some rudimentary in-context learning.\nWith these two experiments the authors successfully showed the huge impact of quality of training data when training machine learning models.\n\nThe abstract from the Phi-1 paper is the following:\n\n*We introduce phi-1, a new large language model for code, with significantly smaller size than\ncompeting models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on\n8 A100s, using a selection of “textbook quality” data from the web (6B tokens) and synthetically\ngenerated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent\nproperties compared to phi-1-base, our model before our finetuning stage on a dataset of coding\nexercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as\nphi-1 that still achieves 45% on HumanEval.*\n\nThe abstract from the Phi-1.5 paper is the following:",
  "*We continue the investigation into the power of smaller Transformer-based language models as\ninitiated by TinyStories – a 10 million parameter model that can produce coherent English – and\nthe follow-up work on phi-1, a 1.3 billion parameter model with Python coding performance close\nto the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to\ngenerate “textbook quality” data as a way to enhance the learning process compared to traditional\nweb data. We follow the “Textbooks Are All You Need” approach, focusing this time on common\nsense reasoning in natural language, and create a new 1.3 billion parameter model named phi-1.5,\nwith performance on natural language tasks comparable to models 5x larger, and surpassing most\nnon-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic\ncoding. More generally, phi-1.5 exhibits many of the traits of much larger LLMs, both good –such\nas the ability to “think step by step” or perform some rudimentary in-context learning– and bad,\nincluding hallucinations and the potential for toxic and biased generations –encouragingly though, we",
  "are seeing improvement on that front thanks to the absence of web data. We open-source phi-1.5 to\npromote further research on these urgent topics.*\n\nThis model was contributed by [Susnato Dhar](https://huggingface.co/susnato).\n\nThe original code for Phi-1, Phi-1.5 and Phi-2 can be found [here](https://huggingface.co/microsoft/phi-1), [here](https://huggingface.co/microsoft/phi-1_5) and [here](https://huggingface.co/microsoft/phi-2), respectively.\n\n## Usage tips\n\n- This model is quite similar to `Llama` with the main difference in [`PhiDecoderLayer`], where they used [`PhiAttention`] and [`PhiMLP`] layers in parallel configuration.\n- The tokenizer used for this model is identical to the [`CodeGenTokenizer`].\n\n## How to use Phi-2\n\n<Tip warning={true}>\n\nPhi-2 has been integrated in the development version (4.37.0.dev) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.",
  "* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\n</Tip>\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n\n>>> inputs = tokenizer('Can you help me write a formal email to a potential business partner proposing a joint venture?', return_tensors=\"pt\", return_attention_mask=False)\n\n>>> outputs = model.generate(**inputs, max_length=30)\n>>> text = tokenizer.batch_decode(outputs)[0]\n>>> print(text)\nCan you help me write a formal email to a potential business partner proposing a joint venture?\nInput: Company A: ABC Inc.\nCompany B\n```\n\n### Example :\n\n```python\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define the model and tokenizer.\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\n>>> # feel free to change the prompt to your liking.",
  ">>> prompt = \"If I were an AI that had just achieved\"\n\n>>> # apply the tokenizer.\n>>> tokens = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)\n\n>>> tokenizer.batch_decode(generated_output)[0]\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```\n\n## Combining Phi and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define the model and tokenizer and push the model and tokens to the GPU.",
  ">>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\")  # doctest: +SKIP\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\n>>> # feel free to change the prompt to your liking.\n>>> prompt = \"If I were an AI that had just achieved\"\n\n>>> # apply the tokenizer.\n>>> tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n>>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)  # doctest: +SKIP\n\n>>> tokenizer.batch_decode(generated_output)[0]  # doctest: +SKIP\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `microsoft/phi-1` checkpoint and the Flash Attention 2 version of the model using a sequence length of 2048.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/phi_1_speedup_plot.jpg\">\n</div>\n\n## PhiConfig\n\n[[autodoc]] PhiConfig",
  "<frameworkcontent>\n<pt>\n\n## PhiModel\n\n[[autodoc]] PhiModel\n- forward\n\n## PhiForCausalLM\n\n[[autodoc]] PhiForCausalLM\n- forward\n- generate\n\n## PhiForSequenceClassification\n\n[[autodoc]] PhiForSequenceClassification\n- forward\n\n## PhiForTokenClassification\n\n[[autodoc]] PhiForTokenClassification\n- forward\n\n</pt>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Auto Classes\n\nIn many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you\nare supplying to the `from_pretrained()` method. AutoClasses are here to do this job for you so that you\nautomatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n\nInstantiating one of [`AutoConfig`], [`AutoModel`], and",
  "[`AutoTokenizer`] will directly create a class of the relevant architecture. For instance\n\n\n```python\nmodel = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n```\n\nwill create a model that is an instance of [`BertModel`].\n\nThere is one class of `AutoModel` for each task, and for each backend (PyTorch, TensorFlow, or Flax).\n\n## Extending the Auto Classes\n\nEach of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a\ncustom class of model `NewModel`, make sure you have a `NewModelConfig` then you can add those to the auto\nclasses like this:\n\n```python\nfrom transformers import AutoConfig, AutoModel\n\nAutoConfig.register(\"new-model\", NewModelConfig)\nAutoModel.register(NewModelConfig, NewModel)\n```\n\nYou will then be able to use the auto classes like you would usually do!\n\n<Tip warning={true}>\n\nIf your `NewModelConfig` is a subclass of [`~transformers.PretrainedConfig`], make sure its\n`model_type` attribute is set to the same key you use when registering the config (here `\"new-model\"`).\n\nLikewise, if your `NewModel` is a subclass of [`PreTrainedModel`], make sure its",
  "`config_class` attribute is set to the same class you use when registering the model (here\n`NewModelConfig`).\n\n</Tip>\n\n## AutoConfig\n\n[[autodoc]] AutoConfig\n\n## AutoTokenizer\n\n[[autodoc]] AutoTokenizer\n\n## AutoFeatureExtractor\n\n[[autodoc]] AutoFeatureExtractor\n\n## AutoImageProcessor\n\n[[autodoc]] AutoImageProcessor\n\n## AutoProcessor\n\n[[autodoc]] AutoProcessor\n\n## Generic model classes\n\nThe following auto classes are available for instantiating a base model class without a specific head.\n\n### AutoModel\n\n[[autodoc]] AutoModel\n\n### TFAutoModel\n\n[[autodoc]] TFAutoModel\n\n### FlaxAutoModel\n\n[[autodoc]] FlaxAutoModel\n\n## Generic pretraining classes\n\nThe following auto classes are available for instantiating a model with a pretraining head.\n\n### AutoModelForPreTraining\n\n[[autodoc]] AutoModelForPreTraining\n\n### TFAutoModelForPreTraining\n\n[[autodoc]] TFAutoModelForPreTraining\n\n### FlaxAutoModelForPreTraining\n\n[[autodoc]] FlaxAutoModelForPreTraining\n\n## Natural Language Processing\n\nThe following auto classes are available for the following natural language processing tasks.\n\n### AutoModelForCausalLM\n\n[[autodoc]] AutoModelForCausalLM\n\n### TFAutoModelForCausalLM",
  "[[autodoc]] TFAutoModelForCausalLM\n\n### FlaxAutoModelForCausalLM\n\n[[autodoc]] FlaxAutoModelForCausalLM\n\n### AutoModelForMaskedLM\n\n[[autodoc]] AutoModelForMaskedLM\n\n### TFAutoModelForMaskedLM\n\n[[autodoc]] TFAutoModelForMaskedLM\n\n### FlaxAutoModelForMaskedLM\n\n[[autodoc]] FlaxAutoModelForMaskedLM\n\n### AutoModelForMaskGeneration\n\n[[autodoc]] AutoModelForMaskGeneration\n\n### TFAutoModelForMaskGeneration\n\n[[autodoc]] TFAutoModelForMaskGeneration\n\n### AutoModelForSeq2SeqLM\n\n[[autodoc]] AutoModelForSeq2SeqLM\n\n### TFAutoModelForSeq2SeqLM\n\n[[autodoc]] TFAutoModelForSeq2SeqLM\n\n### FlaxAutoModelForSeq2SeqLM\n\n[[autodoc]] FlaxAutoModelForSeq2SeqLM\n\n### AutoModelForSequenceClassification\n\n[[autodoc]] AutoModelForSequenceClassification\n\n### TFAutoModelForSequenceClassification\n\n[[autodoc]] TFAutoModelForSequenceClassification\n\n### FlaxAutoModelForSequenceClassification\n\n[[autodoc]] FlaxAutoModelForSequenceClassification\n\n### AutoModelForMultipleChoice\n\n[[autodoc]] AutoModelForMultipleChoice\n\n### TFAutoModelForMultipleChoice\n\n[[autodoc]] TFAutoModelForMultipleChoice\n\n### FlaxAutoModelForMultipleChoice\n\n[[autodoc]] FlaxAutoModelForMultipleChoice\n\n### AutoModelForNextSentencePrediction",
  "[[autodoc]] AutoModelForNextSentencePrediction\n\n### TFAutoModelForNextSentencePrediction\n\n[[autodoc]] TFAutoModelForNextSentencePrediction\n\n### FlaxAutoModelForNextSentencePrediction\n\n[[autodoc]] FlaxAutoModelForNextSentencePrediction\n\n### AutoModelForTokenClassification\n\n[[autodoc]] AutoModelForTokenClassification\n\n### TFAutoModelForTokenClassification\n\n[[autodoc]] TFAutoModelForTokenClassification\n\n### FlaxAutoModelForTokenClassification\n\n[[autodoc]] FlaxAutoModelForTokenClassification\n\n### AutoModelForQuestionAnswering\n\n[[autodoc]] AutoModelForQuestionAnswering\n\n### TFAutoModelForQuestionAnswering\n\n[[autodoc]] TFAutoModelForQuestionAnswering\n\n### FlaxAutoModelForQuestionAnswering\n\n[[autodoc]] FlaxAutoModelForQuestionAnswering\n\n### AutoModelForTextEncoding\n\n[[autodoc]] AutoModelForTextEncoding\n\n### TFAutoModelForTextEncoding\n\n[[autodoc]] TFAutoModelForTextEncoding\n\n## Computer vision\n\nThe following auto classes are available for the following computer vision tasks.\n\n### AutoModelForDepthEstimation\n\n[[autodoc]] AutoModelForDepthEstimation\n\n### AutoModelForImageClassification\n\n[[autodoc]] AutoModelForImageClassification\n\n### TFAutoModelForImageClassification",
  "[[autodoc]] TFAutoModelForImageClassification\n\n### FlaxAutoModelForImageClassification\n\n[[autodoc]] FlaxAutoModelForImageClassification\n\n### AutoModelForVideoClassification\n\n[[autodoc]] AutoModelForVideoClassification\n\n### AutoModelForKeypointDetection\n\n[[autodoc]] AutoModelForKeypointDetection\n\n### AutoModelForMaskedImageModeling\n\n[[autodoc]] AutoModelForMaskedImageModeling\n\n### TFAutoModelForMaskedImageModeling\n\n[[autodoc]] TFAutoModelForMaskedImageModeling\n\n### AutoModelForObjectDetection\n\n[[autodoc]] AutoModelForObjectDetection\n\n### AutoModelForImageSegmentation\n\n[[autodoc]] AutoModelForImageSegmentation\n\n### AutoModelForImageToImage\n\n[[autodoc]] AutoModelForImageToImage\n\n### AutoModelForSemanticSegmentation\n\n[[autodoc]] AutoModelForSemanticSegmentation\n\n### TFAutoModelForSemanticSegmentation\n\n[[autodoc]] TFAutoModelForSemanticSegmentation\n\n### AutoModelForInstanceSegmentation\n\n[[autodoc]] AutoModelForInstanceSegmentation\n\n### AutoModelForUniversalSegmentation\n\n[[autodoc]] AutoModelForUniversalSegmentation\n\n### AutoModelForZeroShotImageClassification\n\n[[autodoc]] AutoModelForZeroShotImageClassification\n\n### TFAutoModelForZeroShotImageClassification",
  "[[autodoc]] TFAutoModelForZeroShotImageClassification\n\n### AutoModelForZeroShotObjectDetection\n\n[[autodoc]] AutoModelForZeroShotObjectDetection\n\n## Audio\n\nThe following auto classes are available for the following audio tasks.\n\n### AutoModelForAudioClassification\n\n[[autodoc]] AutoModelForAudioClassification\n\n### AutoModelForAudioFrameClassification\n\n[[autodoc]] TFAutoModelForAudioClassification\n\n### TFAutoModelForAudioFrameClassification\n\n[[autodoc]] AutoModelForAudioFrameClassification\n\n### AutoModelForCTC\n\n[[autodoc]] AutoModelForCTC\n\n### AutoModelForSpeechSeq2Seq\n\n[[autodoc]] AutoModelForSpeechSeq2Seq\n\n### TFAutoModelForSpeechSeq2Seq\n\n[[autodoc]] TFAutoModelForSpeechSeq2Seq\n\n### FlaxAutoModelForSpeechSeq2Seq\n\n[[autodoc]] FlaxAutoModelForSpeechSeq2Seq\n\n### AutoModelForAudioXVector\n\n[[autodoc]] AutoModelForAudioXVector\n\n### AutoModelForTextToSpectrogram\n\n[[autodoc]] AutoModelForTextToSpectrogram\n\n### AutoModelForTextToWaveform\n\n[[autodoc]] AutoModelForTextToWaveform\n\n## Multimodal\n\nThe following auto classes are available for the following multimodal tasks.\n\n### AutoModelForTableQuestionAnswering\n\n[[autodoc]] AutoModelForTableQuestionAnswering",
  "### TFAutoModelForTableQuestionAnswering\n\n[[autodoc]] TFAutoModelForTableQuestionAnswering\n\n### AutoModelForDocumentQuestionAnswering\n\n[[autodoc]] AutoModelForDocumentQuestionAnswering\n\n### TFAutoModelForDocumentQuestionAnswering\n\n[[autodoc]] TFAutoModelForDocumentQuestionAnswering\n\n### AutoModelForVisualQuestionAnswering\n\n[[autodoc]] AutoModelForVisualQuestionAnswering\n\n### AutoModelForVision2Seq\n\n[[autodoc]] AutoModelForVision2Seq\n\n### TFAutoModelForVision2Seq\n\n[[autodoc]] TFAutoModelForVision2Seq\n\n### FlaxAutoModelForVision2Seq\n\n[[autodoc]] FlaxAutoModelForVision2Seq\n\n### AutoModelForImageTextToText\n\n[[autodoc]] AutoModelForImageTextToText",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# InstructBLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe InstructBLIP model was proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\nInstructBLIP leverages the [BLIP-2](blip2) architecture for visual instruction tuning.\n\nThe abstract from the paper is the following:",
  "*General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> InstructBLIP architecture. Taken from the <a href=\"https://arxiv.org/abs/2305.06500\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip).\n\n## Usage tips\n\nInstructBLIP uses the same architecture as [BLIP-2](blip2) with a tiny but important difference: it also feeds the text prompt (instruction) to the Q-Former.\n\n> [!NOTE]",
  "> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n\n## InstructBlipConfig\n\n[[autodoc]] InstructBlipConfig\n- from_vision_qformer_text_configs\n\n## InstructBlipVisionConfig\n\n[[autodoc]] InstructBlipVisionConfig\n\n## InstructBlipQFormerConfig\n\n[[autodoc]] InstructBlipQFormerConfig\n\n## InstructBlipProcessor\n\n[[autodoc]] InstructBlipProcessor\n\n\n## InstructBlipVisionModel",
  "[[autodoc]] InstructBlipVisionModel\n- forward\n\n## InstructBlipQFormerModel\n\n[[autodoc]] InstructBlipQFormerModel\n- forward\n\n## InstructBlipForConditionalGeneration\n\n[[autodoc]] InstructBlipForConditionalGeneration\n- forward\n- generate",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Qwen2Audio\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Qwen2-Audio is the new model series of large audio-language models from the Qwen team. Qwen2-Audio is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. We introduce two distinct audio interaction modes:\n\n* voice chat: users can freely engage in voice interactions with Qwen2-Audio without text input\n* audio analysis: users could provide audio and text instructions for analysis during the interaction\n\nIt was proposed in [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759) by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou.\n\nThe abstract from the paper is the following:",
  "*We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community. *",
  "## Usage tips\n\n`Qwen2-Audio-7B` and `Qwen2-Audio-7B-Instruct` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n\n### Inference\n\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\", trust_remote_code=True, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\", trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\").to(model.device)\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\n# We can also omit the audio_bos and audio_eos tokens",
  "prompt = \"<|AUDIO|>Generate the caption in English:\"\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\").to(model.device)\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n\nIn the following, we demonstrate how to use `Qwen2-Audio-7B-Instruct` for the inference, supporting both voice chat and audio analysis modes. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n\n### Voice Chat Inference\nIn the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input:\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n{\"role\": \"user\", \"content\": [",
  "{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n]},\n{\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\nif isinstance(message[\"content\"], list):\nfor ele in message[\"content\"]:\nif ele[\"type\"] == \"audio\":\naudios.append(librosa.load(\nBytesIO(urlopen(ele['audio_url']).read()),\nsr=processor.feature_extractor.sampling_rate)[0]\n)\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n\n### Audio Analysis Inference",
  "In the audio analysis, users could provide both audio and text instructions for analysis:\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n{'role': 'system', 'content': 'You are a helpful assistant.'},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n{\"type\": \"text\", \"text\": \"What's that sound?\"},\n]},\n{\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n]},\n{\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},",
  "{\"type\": \"text\", \"text\": \"What does the person say?\"},\n]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\nif isinstance(message[\"content\"], list):\nfor ele in message[\"content\"]:\nif ele[\"type\"] == \"audio\":\naudios.append(\nlibrosa.load(\nBytesIO(urlopen(ele['audio_url']).read()),\nsr=processor.feature_extractor.sampling_rate)[0]\n)\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n\n### Batch Inference\nWe also support batch inference:\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [",
  "{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n{\"type\": \"text\", \"text\": \"What's that sound?\"},\n]},\n{\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n{\"type\": \"text\", \"text\": \"What can you hear?\"},\n]}\n]\n\nconversation2 = [\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n{\"type\": \"text\", \"text\": \"What does the person say?\"},\n]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\nfor message in conversation:\nif isinstance(message[\"content\"], list):\nfor ele in message[\"content\"]:\nif ele[\"type\"] == \"audio\":\naudios.append(\nlibrosa.load(\nBytesIO(urlopen(ele['audio_url']).read()),",
  "sr=processor.feature_extractor.sampling_rate)[0]\n)\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n\n## Qwen2AudioConfig\n\n[[autodoc]] Qwen2AudioConfig\n\n## Qwen2AudioEncoderConfig\n\n[[autodoc]] Qwen2AudioEncoderConfig\n\n## Qwen2AudioProcessor\n\n[[autodoc]] Qwen2AudioProcessor\n\n## Qwen2AudioEncoder\n\n[[autodoc]] Qwen2AudioEncoder\n- forward\n\n## Qwen2AudioForConditionalGeneration\n\n[[autodoc]] Qwen2AudioForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MPNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The MPNet model was proposed in [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n\nMPNet adopts a novel pre-training method, named masked and permuted language modeling, to inherit the advantages of\nmasked language modeling and permuted language modeling for natural language understanding.\n\nThe abstract from the paper is the following:\n\n*BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models.\nSince BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and\nthus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the\ndependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position",
  "information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in\nXLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large\nmargin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g.,\nBERT, XLNet, RoBERTa) under the same model setting.*\n\nThe original code can be found [here](https://github.com/microsoft/MPNet).\n\n## Usage tips\n\nMPNet doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. Just\nseparate your segments with the separation token `tokenizer.sep_token` (or `[sep]`).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## MPNetConfig\n\n[[autodoc]] MPNetConfig\n\n## MPNetTokenizer",
  "[[autodoc]] MPNetTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## MPNetTokenizerFast\n\n[[autodoc]] MPNetTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## MPNetModel\n\n[[autodoc]] MPNetModel\n- forward\n\n## MPNetForMaskedLM\n\n[[autodoc]] MPNetForMaskedLM\n- forward\n\n## MPNetForSequenceClassification\n\n[[autodoc]] MPNetForSequenceClassification\n- forward\n\n## MPNetForMultipleChoice\n\n[[autodoc]] MPNetForMultipleChoice\n- forward\n\n## MPNetForTokenClassification\n\n[[autodoc]] MPNetForTokenClassification\n- forward\n\n## MPNetForQuestionAnswering\n\n[[autodoc]] MPNetForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFMPNetModel\n\n[[autodoc]] TFMPNetModel\n- call\n\n## TFMPNetForMaskedLM\n\n[[autodoc]] TFMPNetForMaskedLM\n- call\n\n## TFMPNetForSequenceClassification\n\n[[autodoc]] TFMPNetForSequenceClassification\n- call\n\n## TFMPNetForMultipleChoice\n\n[[autodoc]] TFMPNetForMultipleChoice\n- call\n\n## TFMPNetForTokenClassification\n\n[[autodoc]] TFMPNetForTokenClassification\n- call\n\n## TFMPNetForQuestionAnswering\n\n[[autodoc]] TFMPNetForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ConvNeXT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The ConvNeXT model was proposed in [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\nConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them.\n\nThe abstract from the paper is the following:\n\n*The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model.\nA vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers\n(e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide\nvariety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive",
  "biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design\nof a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models\ndubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy\nand outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ConvNeXT architecture. Taken from the <a href=\"https://arxiv.org/abs/2201.03545\">original paper</a>.</small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow version of the model was contributed by [ariG23498](https://github.com/ariG23498),",
  "[gante](https://github.com/gante), and [sayakpaul](https://github.com/sayakpaul) (equal contribution). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ConvNeXT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`ConvNextForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ConvNextConfig\n\n[[autodoc]] ConvNextConfig\n\n## ConvNextFeatureExtractor\n\n[[autodoc]] ConvNextFeatureExtractor\n\n## ConvNextImageProcessor\n\n[[autodoc]] ConvNextImageProcessor\n- preprocess\n\n## ConvNextImageProcessorFast",
  "[[autodoc]] ConvNextImageProcessorFast\n- preprocess\n\n<frameworkcontent>\n<pt>\n\n## ConvNextModel\n\n[[autodoc]] ConvNextModel\n- forward\n\n## ConvNextForImageClassification\n\n[[autodoc]] ConvNextForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFConvNextModel\n\n[[autodoc]] TFConvNextModel\n- call\n\n## TFConvNextForImageClassification\n\n[[autodoc]] TFConvNextForImageClassification\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SegFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The SegFormer model was proposed in [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping\nLuo. The model consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great\nresults on image segmentation benchmarks such as ADE20K and Cityscapes.\n\nThe abstract from the paper is the following:\n\n*We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with\nlightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel\nhierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding,\nthereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution\ndiffers from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from\ndifferent layers, and thus combining both local attention and global attention to render powerful representations. We",
  "show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our\napproach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance\nand efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters,\nbeing 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on\nCityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.*\n\nThe figure below illustrates the architecture of SegFormer. Taken from the [original paper](https://arxiv.org/abs/2105.15203).\n\n<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/segformer_architecture.png\"/>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version\nof the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/NVlabs/SegFormer).\n\n## Usage tips\n\n- SegFormer consists of a hierarchical Transformer encoder, and a lightweight all-MLP decoder head.",
  "[`SegformerModel`] is the hierarchical Transformer encoder (which in the paper is also referred to\nas Mix Transformer or MiT). [`SegformerForSemanticSegmentation`] adds the all-MLP decoder head on\ntop to perform semantic segmentation of images. In addition, there's\n[`SegformerForImageClassification`] which can be used to - you guessed it - classify images. The\nauthors of SegFormer first pre-trained the Transformer encoder on ImageNet-1k to classify images. Next, they throw\naway the classification head, and replace it by the all-MLP decode head. Next, they fine-tune the model altogether on\nADE20K, Cityscapes and COCO-stuff, which are important benchmarks for semantic segmentation. All checkpoints can be\nfound on the [hub](https://huggingface.co/models?other=segformer).\n- The quickest way to get started with SegFormer is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer) (which showcase both inference and\nfine-tuning on custom data). One can also check out the [blog post](https://huggingface.co/blog/fine-tune-segformer) introducing SegFormer and illustrating how it can be fine-tuned on custom data.",
  "- TensorFlow users should refer to [this repository](https://github.com/deep-diver/segformer-tf-transformers) that shows off-the-shelf inference and fine-tuning.\n- One can also check out [this interactive demo on Hugging Face Spaces](https://huggingface.co/spaces/chansung/segformer-tf-transformers)\nto try out a SegFormer model on custom images.\n- SegFormer works on any input size, as it pads the input to be divisible by `config.patch_sizes`.\n- One can use [`SegformerImageProcessor`] to prepare images and corresponding segmentation maps\nfor the model. Note that this image processor is fairly basic and does not include all data augmentations used in\nthe original paper. The original preprocessing pipelines (for the ADE20k dataset for instance) can be found [here](https://github.com/NVlabs/SegFormer/blob/master/local_configs/_base_/datasets/ade20k_repeat.py). The most\nimportant preprocessing step is that images and segmentation maps are randomly cropped and padded to the same size,\nsuch as 512x512 or 640x640, after which they are normalized.\n- One additional thing to keep in mind is that one can initialize [`SegformerImageProcessor`] with",
  "`do_reduce_labels` set to `True` or `False`. In some datasets (like ADE20k), the 0 index is used in the annotated\nsegmentation maps for background. However, ADE20k doesn't include the \"background\" class in its 150 labels.\nTherefore, `do_reduce_labels` is used to reduce all labels by 1, and to make sure no loss is computed for the\nbackground class (i.e. it replaces 0 in the annotated maps by 255, which is the *ignore_index* of the loss function\nused by [`SegformerForSemanticSegmentation`]). However, other datasets use the 0 index as\nbackground class and include this class as part of all labels. In that case, `do_reduce_labels` should be set to\n`False`, as loss should also be computed for the background class.\n- As most models, SegFormer comes in different sizes, the details of which can be found in the table below\n(taken from Table 7 of the [original paper](https://arxiv.org/abs/2105.15203)).\n\n| **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)** | **ImageNet-1k Top 1** |\n| :---------------: | ------------- | ------------------- | :---------------------: | :------------: | :-------------------: |",
  "| MiT-b0            | [2, 2, 2, 2]  | [32, 64, 160, 256]  | 256                     | 3.7            | 70.5                  |\n| MiT-b1            | [2, 2, 2, 2]  | [64, 128, 320, 512] | 256                     | 14.0           | 78.7                  |\n| MiT-b2            | [3, 4, 6, 3]  | [64, 128, 320, 512] | 768                     | 25.4           | 81.6                  |\n| MiT-b3            | [3, 4, 18, 3] | [64, 128, 320, 512] | 768                     | 45.2           | 83.1                  |\n| MiT-b4            | [3, 8, 27, 3] | [64, 128, 320, 512] | 768                     | 62.6           | 83.6                  |\n| MiT-b5            | [3, 6, 40, 3] | [64, 128, 320, 512] | 768                     | 82.0           | 83.8                  |\n\nNote that MiT in the above table refers to the Mix Transformer encoder backbone introduced in SegFormer. For\nSegFormer's results on the segmentation datasets like ADE20k, refer to the [paper](https://arxiv.org/abs/2105.15203).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SegFormer.\n\n<PipelineTag pipeline=\"image-classification\"/>",
  "- [`SegformerForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- [Image classification task guide](../tasks/image_classification)\n\nSemantic segmentation:\n\n- [`SegformerForSemanticSegmentation`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation).\n- A blog on fine-tuning SegFormer on a custom dataset can be found [here](https://huggingface.co/blog/fine-tune-segformer).\n- More demo notebooks on SegFormer (both inference + fine-tuning on a custom dataset) can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer).\n- [`TFSegformerForSemanticSegmentation`] is supported by this [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb).\n- [Semantic segmentation task guide](../tasks/semantic_segmentation)",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## SegformerConfig\n\n[[autodoc]] SegformerConfig\n\n## SegformerFeatureExtractor\n\n[[autodoc]] SegformerFeatureExtractor\n- __call__\n- post_process_semantic_segmentation\n\n## SegformerImageProcessor\n\n[[autodoc]] SegformerImageProcessor\n- preprocess\n- post_process_semantic_segmentation\n\n<frameworkcontent>\n<pt>\n\n## SegformerModel\n\n[[autodoc]] SegformerModel\n- forward\n\n## SegformerDecodeHead\n\n[[autodoc]] SegformerDecodeHead\n- forward\n\n## SegformerForImageClassification\n\n[[autodoc]] SegformerForImageClassification\n- forward\n\n## SegformerForSemanticSegmentation\n\n[[autodoc]] SegformerForSemanticSegmentation\n- forward\n\n</pt>\n<tf>\n\n## TFSegformerDecodeHead\n\n[[autodoc]] TFSegformerDecodeHead\n- call\n\n## TFSegformerModel\n\n[[autodoc]] TFSegformerModel\n- call\n\n## TFSegformerForImageClassification\n\n[[autodoc]] TFSegformerForImageClassification\n- call\n\n## TFSegformerForSemanticSegmentation\n\n[[autodoc]] TFSegformerForSemanticSegmentation\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLS-R\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe XLS-R model was proposed in [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman\nGoyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.\nWe train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128\nlanguages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range\nof tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation\nbenchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into\nEnglish. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as",
  "VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107\nlanguage identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform\nEnglish-only pretraining when translating English speech into other languages, a setting which favors monolingual\npretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.*\n\nRelevant checkpoints can be found under https://huggingface.co/models?other=xls_r.\n\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).\n\n## Usage tips\n\n- XLS-R is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- XLS-R model was trained using connectionist temporal classification (CTC) so the model output has to be decoded using\n[`Wav2Vec2CTCTokenizer`].\n\n<Tip>\n\nXLS-R's architecture is based on the Wav2Vec2 model, refer to [Wav2Vec2's documentation page](wav2vec2) for API reference.\n\n</Tip>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLM-V\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nXLM-V is multilingual language model with a one million token vocabulary trained on 2.5TB of data from Common Crawl (same as XLM-R).\nIt was introduced in the [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)\npaper by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer and Madian Khabsa.\n\nFrom the abstract of the XLM-V paper:\n\n*Large multilingual language models typically rely on a single vocabulary shared across 100+ languages.\nAs these models have increased in parameter count and depth, vocabulary size has remained largely unchanged.\nThis vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R.\nIn this paper, we introduce a new approach for scaling to very large multilingual vocabularies by\nde-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity\nto achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically",
  "more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V,\na multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we\ntested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), and\nnamed entity recognition (WikiAnn) to low-resource tasks (Americas NLI, MasakhaNER).*\n\nThis model was contributed by [stefan-it](https://huggingface.co/stefan-it), including detailed experiments with XLM-V on downstream tasks.\nThe experiments repository can be found [here](https://github.com/stefan-it/xlm-v-experiments).\n\n## Usage tips\n\n- XLM-V is compatible with the XLM-RoBERTa model architecture, only model weights from [`fairseq`](https://github.com/facebookresearch/fairseq)\nlibrary had to be converted.\n- The `XLMTokenizer` implementation is used to load the vocab and performs tokenization.\n\nA XLM-V (base size) model is available under the [`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base) identifier.\n\n<Tip>\n\nXLM-V architecture is the same as XLM-RoBERTa, refer to [XLM-RoBERTa documentation](xlm-roberta) for API reference, and examples.",
  "</Tip>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RemBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\n\nThe abstract from the paper is the following:\n\n*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on\nstandard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\nfine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger\noutput embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage",
  "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\nfindings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\nnumber of parameters at the fine-tuning stage.*\n\n## Usage tips\n\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBERT-like factorization of the\nembedding layer. The embeddings are not tied in pre-training, in contrast with BERT, which enables smaller input\nembeddings (preserved during fine-tuning) and bigger output embeddings (discarded at fine-tuning). The tokenizer is\nalso similar to the Albert one rather than the BERT one.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RemBertConfig\n\n[[autodoc]] RemBertConfig\n\n## RemBertTokenizer",
  "[[autodoc]] RemBertTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## RemBertTokenizerFast\n\n[[autodoc]] RemBertTokenizerFast\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n<frameworkcontent>\n<pt>\n\n## RemBertModel\n\n[[autodoc]] RemBertModel\n- forward\n\n## RemBertForCausalLM\n\n[[autodoc]] RemBertForCausalLM\n- forward\n\n## RemBertForMaskedLM\n\n[[autodoc]] RemBertForMaskedLM\n- forward\n\n## RemBertForSequenceClassification\n\n[[autodoc]] RemBertForSequenceClassification\n- forward\n\n## RemBertForMultipleChoice\n\n[[autodoc]] RemBertForMultipleChoice\n- forward\n\n## RemBertForTokenClassification\n\n[[autodoc]] RemBertForTokenClassification\n- forward\n\n## RemBertForQuestionAnswering\n\n[[autodoc]] RemBertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFRemBertModel\n\n[[autodoc]] TFRemBertModel\n- call\n\n## TFRemBertForMaskedLM\n\n[[autodoc]] TFRemBertForMaskedLM\n- call\n\n## TFRemBertForCausalLM\n\n[[autodoc]] TFRemBertForCausalLM\n- call\n\n## TFRemBertForSequenceClassification\n\n[[autodoc]] TFRemBertForSequenceClassification\n- call\n\n## TFRemBertForMultipleChoice",
  "[[autodoc]] TFRemBertForMultipleChoice\n- call\n\n## TFRemBertForTokenClassification\n\n[[autodoc]] TFRemBertForTokenClassification\n- call\n\n## TFRemBertForQuestionAnswering\n\n[[autodoc]] TFRemBertForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The Qwen Team and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Qwen2MoE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Qwen2MoE is the new model series of large language models from the Qwen team. Previously, we released the Qwen series, including Qwen-72B, Qwen-1.8B, Qwen-VL, Qwen-Audio, etc.\n\n### Model Details\n\nQwen2MoE is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. Qwen2MoE has the following architectural choices:\n\n- Qwen2MoE is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.\n- Qwen2MoE employs Mixture of Experts (MoE) architecture, where the models are upcycled from dense language models. For instance, `Qwen1.5-MoE-A2.7B` is upcycled from `Qwen-1.8B`. It has 14.3B parameters in total and 2.7B activated parameters during runtime, while it achieves comparable performance with `Qwen1.5-7B`, with only 25% of the training resources.\n\nFor more details refer to the [release blog post](https://qwenlm.github.io/blog/qwen-moe/).\n\n## Usage tips",
  "`Qwen1.5-MoE-A2.7B` and `Qwen1.5-MoE-A2.7B-Chat` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n\nIn the following, we demonstrate how to use `Qwen1.5-MoE-A2.7B-Chat` for the inference. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B-Chat\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B-Chat\")\n\n>>> prompt = \"Give me a short introduction to large language model.\"\n\n>>> messages = [{\"role\": \"user\", \"content\": prompt}]\n\n>>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n>>> model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n\n>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]",
  ">>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## Qwen2MoeConfig\n\n[[autodoc]] Qwen2MoeConfig\n\n## Qwen2MoeModel\n\n[[autodoc]] Qwen2MoeModel\n- forward\n\n## Qwen2MoeForCausalLM\n\n[[autodoc]] Qwen2MoeForCausalLM\n- forward\n\n## Qwen2MoeForSequenceClassification\n\n[[autodoc]] Qwen2MoeForSequenceClassification\n- forward\n\n## Qwen2MoeForTokenClassification\n\n[[autodoc]] Qwen2MoeForTokenClassification\n- forward\n\n## Qwen2MoeForQuestionAnswering\n\n[[autodoc]] Qwen2MoeForQuestionAnswering\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# SeamlessM4T\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe SeamlessM4T model was proposed in [SeamlessM4T — Massively Multilingual & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf) by the Seamless Communication team from Meta AI.\n\nThis is the **version 1** release of the model. For the updated **version 2** release, refer to the [Seamless M4T v2 docs](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t_v2).",
  "SeamlessM4T is a collection of models designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text.\n\nSeamlessM4T enables multiple tasks without relying on separate models:\n\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR)\n\n[`SeamlessM4TModel`] can perform all the above tasks, but each task also has its own dedicated sub-model.\n\nThe abstract from the paper is the following:",
  "*What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all contributions in this work are open-sourced and accessible at https://github.com/facebookresearch/seamless_communication*",
  "## Usage\n\nFirst, load the processor and a checkpoint of the model:\n\n```python\n>>> from transformers import AutoProcessor, SeamlessM4TModel\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n>>> model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n```\n\nYou can seamlessly use this model on text or on audio, to generated either translated text or translated audio.\n\nHere is how to use the processor to process text and audio:\n\n```python\n>>> # let's load an audio sample from an Arabic speech corpus\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"arabic_speech_corpus\", split=\"test\", streaming=True, trust_remote_code=True)\n>>> audio_sample = next(iter(dataset))[\"audio\"]\n\n>>> # now, process it\n>>> audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\")\n\n>>> # now, process some English test as well\n>>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\n```\n\n\n### Speech\n\n[`SeamlessM4TModel`] can *seamlessly* generate text or speech with few or no changes. Let's target Russian voice translation:\n\n```python",
  ">>> audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n>>> audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n```\n\nWith basically the same code, I've translated English text and Arabic speech to Russian speech samples.\n\n### Text\n\nSimilarly, you can generate translated text from audio files or from text with the same model. You only have to pass `generate_speech=False` to [`SeamlessM4TModel.generate`].\nThis time, let's translate to French.\n\n```python\n>>> # from audio\n>>> output_tokens = model.generate(**audio_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n\n>>> # from text\n>>> output_tokens = model.generate(**text_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n```\n\n### Tips\n\n\n#### 1. Use dedicated models",
  "[`SeamlessM4TModel`] is transformers top level model to generate speech and text, but you can also use dedicated models that perform the task without additional components, thus reducing the memory footprint.\nFor example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code:\n\n```python\n>>> from transformers import SeamlessM4TForSpeechToSpeech\n>>> model = SeamlessM4TForSpeechToSpeech.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n```\n\nOr you can replace the text-to-text generation snippet with the model dedicated to the T2TT task, you only have to remove `generate_speech=False`.\n\n```python\n>>> from transformers import SeamlessM4TForTextToText\n>>> model = SeamlessM4TForTextToText.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n```\n\nFeel free to try out [`SeamlessM4TForSpeechToText`] and [`SeamlessM4TForTextToSpeech`] as well.\n\n#### 2. Change the speaker identity\n\nYou have the possibility to change the speaker used for speech synthesis with the `spkr_id` argument. Some `spkr_id` works better than other for some languages!\n\n#### 3. Change the generation strategy",
  "You can use different [generation strategies](./generation_strategies) for speech and text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, speech_do_sample=True)` which will successively perform beam-search decoding on the text model, and multinomial sampling on the speech model.\n\n#### 4. Generate speech and text at the same time\n\nUse `return_intermediate_token_ids=True` with [`SeamlessM4TModel`] to return both speech and text !\n\n## Model architecture\n\n\nSeamlessM4T features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n\nEach modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646) architecture is placed on top of the second seq2seq model.\n\nHere's how the generation process works:\n\n- Input text or speech is processed through its specific encoder.",
  "- A decoder creates text tokens in the desired language.\n- If speech generation is required, the second seq2seq model, following a standard encoder-decoder structure, generates unit tokens.\n- These unit tokens are then passed through the final vocoder to produce the actual speech.\n\n\nThis model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/seamless_communication).\n\n## SeamlessM4TModel\n\n[[autodoc]] SeamlessM4TModel\n- generate\n\n\n## SeamlessM4TForTextToSpeech\n\n[[autodoc]] SeamlessM4TForTextToSpeech\n- generate\n\n\n## SeamlessM4TForSpeechToSpeech\n\n[[autodoc]] SeamlessM4TForSpeechToSpeech\n- generate\n\n\n## SeamlessM4TForTextToText\n\n[[autodoc]] transformers.SeamlessM4TForTextToText\n- forward\n- generate\n\n## SeamlessM4TForSpeechToText\n\n[[autodoc]] transformers.SeamlessM4TForSpeechToText\n- forward\n- generate\n\n## SeamlessM4TConfig\n\n[[autodoc]] SeamlessM4TConfig\n\n\n## SeamlessM4TTokenizer\n\n[[autodoc]] SeamlessM4TTokenizer\n- __call__\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n\n## SeamlessM4TTokenizerFast",
  "[[autodoc]] SeamlessM4TTokenizerFast\n- __call__\n\n## SeamlessM4TFeatureExtractor\n\n[[autodoc]] SeamlessM4TFeatureExtractor\n- __call__\n\n## SeamlessM4TProcessor\n\n[[autodoc]] SeamlessM4TProcessor\n- __call__\n\n## SeamlessM4TCodeHifiGan\n\n[[autodoc]] SeamlessM4TCodeHifiGan\n\n\n## SeamlessM4THifiGan\n\n[[autodoc]] SeamlessM4THifiGan\n\n## SeamlessM4TTextToUnitModel\n\n[[autodoc]] SeamlessM4TTextToUnitModel\n\n## SeamlessM4TTextToUnitForConditionalGeneration\n\n[[autodoc]] SeamlessM4TTextToUnitForConditionalGeneration",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\nLicense. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\nspecific language governing permissions and limitations under the License. -->\n\n# ImageGPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt) by Mark\nChen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever. ImageGPT (iGPT) is a GPT-2-like",
  "model trained to predict the next pixel value, allowing for both unconditional and conditional image generation.\n\nThe abstract from the paper is the following:\n\n*Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models\ncan learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels,\nwithout incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels,\nwe find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and\nlow-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide\nResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also\ncompetitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0%\ntop-1 accuracy on a linear probe of our features.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/imagegpt_architecture.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Summary of the approach. Taken from the [original paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf). </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr), based on [this issue](https://github.com/openai/image-gpt/issues/7). The original code can be found\n[here](https://github.com/openai/image-gpt).\n\n## Usage tips\n\n- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activation\nfunction is used (namely \"quick gelu\"), and the layer normalization layers don't mean center the inputs. ImageGPT\nalso doesn't have tied input- and output embeddings.\n- As the time- and memory requirements of the attention mechanism of Transformers scales quadratically in the sequence\nlength, the authors pre-trained ImageGPT on smaller input resolutions, such as 32x32 and 64x64. However, feeding a\nsequence of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large. Therefore, the authors\napplied k-means clustering to the (R,G,B) pixel values with k=512. This way, we only have a 32*32 = 1024-long",
  "sequence, but now of integers in the range 0..511. So we are shrinking the sequence length at the cost of a bigger\nembedding matrix. In other words, the vocabulary size of ImageGPT is 512, + 1 for a special \"start of sentence\" (SOS)\ntoken, used at the beginning of every sequence. One can use [`ImageGPTImageProcessor`] to prepare\nimages for the model.\n- Despite being pre-trained entirely unsupervised (i.e. without the use of any labels), ImageGPT produces fairly\nperformant image features useful for downstream tasks, such as image classification. The authors showed that the\nfeatures in the middle of the network are the most performant, and can be used as-is to train a linear model (such as\na sklearn logistic regression model for example). This is also referred to as \"linear probing\". Features can be\neasily obtained by first forwarding the image through the model, then specifying `output_hidden_states=True`, and\nthen average-pool the hidden states at whatever layer you like.\n- Alternatively, one can further fine-tune the entire model on a downstream dataset, similar to BERT. For this, you can\nuse [`ImageGPTForImageClassification`].",
  "- ImageGPT comes in different sizes: there's ImageGPT-small, ImageGPT-medium and ImageGPT-large. The authors did also\ntrain an XL variant, which they didn't release. The differences in size are summarized in the following table:\n\n| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **ImageNet-1k Top 1** |\n|---|---|---|---|---|---|\n| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |\n| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |\n| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |\n| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |\n| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |\n| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ImageGPT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- Demo notebooks for ImageGPT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT).",
  "- [`ImageGPTForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ImageGPTConfig\n\n[[autodoc]] ImageGPTConfig\n\n## ImageGPTFeatureExtractor\n\n[[autodoc]] ImageGPTFeatureExtractor\n- __call__\n\n## ImageGPTImageProcessor\n\n[[autodoc]] ImageGPTImageProcessor\n- preprocess\n\n## ImageGPTModel\n\n[[autodoc]] ImageGPTModel\n- forward\n\n## ImageGPTForCausalImageModeling\n\n[[autodoc]] ImageGPTForCausalImageModeling\n- forward\n\n## ImageGPTForImageClassification\n\n[[autodoc]] ImageGPTForImageClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Nezha\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe Nezha model was proposed in [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei et al.\n\nThe abstract from the paper is the following:\n\n*The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks\ndue to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora.\nIn this technical report, we present our practice of pre-training language models named NEZHA (NEural contextualiZed\nrepresentation for CHinese lAnguage understanding) on Chinese corpora and finetuning for the Chinese NLU tasks.\nThe current version of NEZHA is based on BERT with a collection of proven improvements, which include Functional\nRelative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy,\nMixed Precision Training and the LAMB Optimizer in training the models. The experimental results show that NEZHA",
  "achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including\nnamed entity recognition (People's Daily NER), sentence matching (LCQMC), Chinese sentiment classification (ChnSenti)\nand natural language inference (XNLI).*\n\nThis model was contributed by [sijunhe](https://huggingface.co/sijunhe). The original code can be found [here](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## NezhaConfig\n\n[[autodoc]] NezhaConfig\n\n## NezhaModel\n\n[[autodoc]] NezhaModel\n- forward\n\n## NezhaForPreTraining\n\n[[autodoc]] NezhaForPreTraining\n- forward\n\n## NezhaForMaskedLM\n\n[[autodoc]] NezhaForMaskedLM\n- forward\n\n## NezhaForNextSentencePrediction\n\n[[autodoc]] NezhaForNextSentencePrediction\n- forward\n\n## NezhaForSequenceClassification\n\n[[autodoc]] NezhaForSequenceClassification",
  "- forward\n\n## NezhaForMultipleChoice\n\n[[autodoc]] NezhaForMultipleChoice\n- forward\n\n## NezhaForTokenClassification\n\n[[autodoc]] NezhaForTokenClassification\n- forward\n\n## NezhaForQuestionAnswering\n\n[[autodoc]] NezhaForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Audio Spectrogram Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\nThe Audio Spectrogram Transformer applies a [Vision Transformer](vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results\nfor audio classification.\n\nThe abstract from the paper is the following:",
  "*In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/audio_spectogram_transformer_architecture.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Audio Spectrogram Transformer architecture. Taken from the <a href=\"https://arxiv.org/abs/2104.01778\">original paper</a>.</small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/YuanGongND/ast).\n\n## Usage tips\n\n- When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset, it's recommended to take care of the input normalization (to make\nsure the input has mean of 0 and std of 0.5). [`ASTFeatureExtractor`] takes care of this. Note that it uses the AudioSet\nmean and std by default. You can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py) to see how\nthe authors compute the stats for a downstream dataset.\n- Note that the AST needs a low learning rate (the authors use a 10 times smaller learning rate compared to their CNN model proposed in the\n[PSLA paper](https://arxiv.org/abs/2102.01243)) and converges quickly, so please search for a suitable learning rate and learning rate scheduler for your task.\n\n### Using Scaled Dot Product Attention (SDPA)",
  "PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import ASTForAudioClassification\nmodel = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).",
  "On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `MIT/ast-finetuned-audioset-10-10-0.4593` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                        27 |                                         6 |                      4.5 |\n|            2 |                                        12 |                                         6 |                      2   |\n|            4 |                                        21 |                                         8 |                      2.62 |\n|            8 |                                        40 |                                        14 |                      2.86 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with the Audio Spectrogram Transformer.\n\n<PipelineTag pipeline=\"audio-classification\"/>",
  "- A notebook illustrating inference with AST for audio classification can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST).\n- [`ASTForAudioClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n- See also: [Audio classification](../tasks/audio_classification).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ASTConfig\n\n[[autodoc]] ASTConfig\n\n## ASTFeatureExtractor\n\n[[autodoc]] ASTFeatureExtractor\n- __call__\n\n## ASTModel\n\n[[autodoc]] ASTModel\n- forward\n\n## ASTForAudioClassification\n\n[[autodoc]] ASTForAudioClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mask2Former\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Mask2Former model was proposed in [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. Mask2Former is a unified framework for panoptic, instance and semantic segmentation and features significant performance and efficiency improvements over [MaskFormer](maskformer).\n\nThe abstract from the paper is the following:\n\n*Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice",
  "of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/mask2former_architecture.jpg\" alt=\"drawing\" width=\"600\"/>\n\n<small> Mask2Former architecture. Taken from the <a href=\"https://arxiv.org/abs/2112.01527\">original paper.</a> </small>",
  "This model was contributed by [Shivalika Singh](https://huggingface.co/shivi) and [Alara Dirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/facebookresearch/Mask2Former).\n\n## Usage tips\n\n- Mask2Former uses the same preprocessing and postprocessing steps as [MaskFormer](maskformer). Use [`Mask2FormerImageProcessor`] or [`AutoImageProcessor`] to prepare images and optional targets for the model.\n- To get the final segmentation, depending on the task, you can call [`~Mask2FormerImageProcessor.post_process_semantic_segmentation`] or [`~Mask2FormerImageProcessor.post_process_instance_segmentation`] or [`~Mask2FormerImageProcessor.post_process_panoptic_segmentation`]. All three tasks can be solved using [`Mask2FormerForUniversalSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Mask2Former.",
  "- Demo notebooks regarding inference + fine-tuning Mask2Former on custom data can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Mask2Former).\n- Scripts for finetuning [`Mask2Former`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/instance-segmentation).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.\nThe resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## Mask2FormerConfig\n\n[[autodoc]] Mask2FormerConfig\n\n## MaskFormer specific outputs\n\n[[autodoc]] models.mask2former.modeling_mask2former.Mask2FormerModelOutput\n\n[[autodoc]] models.mask2former.modeling_mask2former.Mask2FormerForUniversalSegmentationOutput\n\n## Mask2FormerModel\n\n[[autodoc]] Mask2FormerModel\n- forward\n\n## Mask2FormerForUniversalSegmentation\n\n[[autodoc]] Mask2FormerForUniversalSegmentation\n- forward\n\n## Mask2FormerImageProcessor\n\n[[autodoc]] Mask2FormerImageProcessor\n- preprocess\n- encode_inputs\n- post_process_semantic_segmentation",
  "- post_process_instance_segmentation\n- post_process_panoptic_segmentation",
  "<!--Copyright 2023 IBM and HuggingFace Inc. team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PatchTSMixer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.",
  "PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.\n\n\nThe abstract from the paper is the following:",
  "*TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules designed for multivariate forecasting and representation learning on patched time series. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*",
  "This model was contributed by [ajati](https://huggingface.co/ajati), [vijaye12](https://huggingface.co/vijaye12),\n[gsinthong](https://huggingface.co/gsinthong), [namctin](https://huggingface.co/namctin),\n[wmgifford](https://huggingface.co/wmgifford), [kashif](https://huggingface.co/kashif).\n\n## Usage example\n\nThe code snippet below shows how to randomly initialize a PatchTSMixer model. The model is compatible with the [Trainer API](../trainer.md).\n\n```python\n\nfrom transformers import PatchTSMixerConfig, PatchTSMixerForPrediction\nfrom transformers import Trainer, TrainingArguments,\n\n\nconfig = PatchTSMixerConfig(context_length = 512, prediction_length = 96)\nmodel = PatchTSMixerForPrediction(config)\ntrainer = Trainer(model=model, args=training_args,\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset)\ntrainer.train()\nresults = trainer.evaluate(test_dataset)\n```\n\n## Usage tips\n\nThe model can also be used for time series classification and time series regression. See the respective [`PatchTSMixerForTimeSeriesClassification`] and [`PatchTSMixerForRegression`] classes.\n\n## Resources",
  "- A blog post explaining PatchTSMixer in depth can be found [here](https://huggingface.co/blog/patchtsmixer). The blog can also be opened in Google Colab.\n\n## PatchTSMixerConfig\n\n[[autodoc]] PatchTSMixerConfig\n\n\n## PatchTSMixerModel\n\n[[autodoc]] PatchTSMixerModel\n- forward\n\n\n## PatchTSMixerForPrediction\n\n[[autodoc]] PatchTSMixerForPrediction\n- forward\n\n\n## PatchTSMixerForTimeSeriesClassification\n\n[[autodoc]] PatchTSMixerForTimeSeriesClassification\n- forward\n\n\n## PatchTSMixerForPretraining\n\n[[autodoc]] PatchTSMixerForPretraining\n- forward\n\n\n## PatchTSMixerForRegression\n\n[[autodoc]] PatchTSMixerForRegression\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPTBigCode\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\n\nThe abstract from the paper is the following:",
  "*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at [this https URL.](https://huggingface.co/bigcode)*",
  "The model is an optimized [GPT2 model](https://huggingface.co/docs/transformers/model_doc/gpt2) with support for Multi-Query Attention.\n\n## Implementation details\n\nThe main differences compared to GPT2.\n- Added support for Multi-Query Attention.\n- Use `gelu_pytorch_tanh` instead of classic `gelu`.\n- Avoid unnecessary synchronizations (this has since been added to GPT2 in #20061, but wasn't in the reference codebase).\n- Use Linear layers instead of Conv1D (good speedup but makes the checkpoints incompatible).\n- Merge `_attn` and `_upcast_and_reordered_attn`. Always merge the matmul with scaling. Rename `reorder_and_upcast_attn`->`attention_softmax_in_fp32`\n- Cache the attention mask value to avoid recreating it every time.\n- Use jit to fuse the attention fp32 casting, masking, softmax, and scaling.\n- Combine the attention and causal masks into a single one, pre-computed for the whole model instead of every layer.\n- Merge the key and value caches into one (this changes the format of layer_past/ present, does it risk creating problems?)",
  "- Use the memory layout (self.num_heads, 3, self.head_dim) instead of `(3, self.num_heads, self.head_dim)` for the QKV tensor with MHA. (prevents an overhead with the merged key and values, but makes the checkpoints incompatible with the original openai-community/gpt2 model).\n\nYou can read more about the optimizations in the [original pull request](https://github.com/huggingface/transformers/pull/22575)\n\n## Combining Starcoder and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto",
  ">>> model = AutoModelForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n\n>>> prompt = \"def hello_world():\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n>>> tokenizer.batch_decode(generated_ids)[0]\n'def hello_world():\\n    print(\"hello world\")\\n\\nif __name__ == \"__main__\":\\n    print(\"hello world\")\\n<|endoftext|>'\n```\n\n### Expected speedups\n\nBelow is a expected speedup diagram that compares pure inference time between the native implementation in transformers using `bigcode/starcoder` checkpoint and the Flash Attention 2 version of the model using two different sequence lengths.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/starcoder-speedup.png\">\n</div>\n\n\n## GPTBigCodeConfig\n\n[[autodoc]] GPTBigCodeConfig\n\n## GPTBigCodeModel\n\n[[autodoc]] GPTBigCodeModel\n- forward\n\n## GPTBigCodeForCausalLM",
  "[[autodoc]] GPTBigCodeForCausalLM\n- forward\n\n## GPTBigCodeForSequenceClassification\n\n[[autodoc]] GPTBigCodeForSequenceClassification\n- forward\n\n## GPTBigCodeForTokenClassification\n\n[[autodoc]] GPTBigCodeForTokenClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Nyströmformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Nyströmformer model was proposed in [*Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh.",
  "The abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nyströmformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than",
  "standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## NystromformerConfig\n\n[[autodoc]] NystromformerConfig\n\n## NystromformerModel\n\n[[autodoc]] NystromformerModel\n- forward\n\n## NystromformerForMaskedLM\n\n[[autodoc]] NystromformerForMaskedLM\n- forward\n\n## NystromformerForSequenceClassification\n\n[[autodoc]] NystromformerForSequenceClassification\n- forward\n\n## NystromformerForMultipleChoice\n\n[[autodoc]] NystromformerForMultipleChoice\n- forward\n\n## NystromformerForTokenClassification",
  "[[autodoc]] NystromformerForTokenClassification\n- forward\n\n## NystromformerForQuestionAnswering\n\n[[autodoc]] NystromformerForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GLPN\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip>\n\nThis is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight",
  "breaking changes to fix it in the future. If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title).\n\n</Tip>\n\n## Overview\n\nThe GLPN model was proposed in [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436)  by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\nGLPN combines [SegFormer](segformer)'s hierarchical mix-Transformer with a lightweight decoder for monocular depth estimation. The proposed decoder shows better performance than the previously proposed decoders, with considerably\nless computational complexity.\n\nThe abstract from the paper is the following:",
  "*Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Summary of the approach. Taken from the <a href=\"https://arxiv.org/abs/2201.07436\" target=\"_blank\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/vinvino02/GLPDepth).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GLPN.\n\n- Demo notebooks for [`GLPNForDepthEstimation`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GLPN).\n- [Monocular depth estimation task guide](../tasks/monocular_depth_estimation)\n\n## GLPNConfig\n\n[[autodoc]] GLPNConfig\n\n## GLPNFeatureExtractor\n\n[[autodoc]] GLPNFeatureExtractor\n- __call__\n\n## GLPNImageProcessor\n\n[[autodoc]] GLPNImageProcessor\n- preprocess\n\n## GLPNModel\n\n[[autodoc]] GLPNModel\n- forward\n\n## GLPNForDepthEstimation\n\n[[autodoc]] GLPNForDepthEstimation\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# AyaVision\n\n## Overview\n\nThe Aya Vision 8B and 32B models is a state-of-the-art multilingual multimodal models developed by Cohere For AI. They build on the Aya Expanse recipe to handle both visual and textual information without compromising on the strong multilingual textual performance of the original model.",
  "Aya Vision 8B combines the `Siglip2-so400-384-14` vision encoder with the Cohere CommandR-7B language model further post-trained with the Aya Expanse recipe, creating a powerful vision-language model capable of understanding images and generating text across 23 languages. Whereas, Aya Vision 32B uses Aya Expanse 32B as the language model.\n\nKey features of Aya Vision include:\n- Multimodal capabilities in 23 languages\n- Strong text-only multilingual capabilities inherited from CommandR-7B post-trained with the Aya Expanse recipe and Aya Expanse 32B\n- High-quality visual understanding using the Siglip2-so400-384-14 vision encoder\n- Seamless integration of visual and textual information in 23 languages.\n\n<!-- <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/aya_vision_architecture.webp\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Aya Vision architecture. </small> -->\n\nTips:\n\n- Aya Vision is a multimodal model that takes images and text as input and produces text as output.\n- Images are represented using the `<image>` tag in the templated input.",
  "- For best results, use the `apply_chat_template` method of the processor to format your inputs correctly.\n- The model can process multiple images in a single conversation.\n- Aya Vision can understand and generate text in 23 languages, making it suitable for multilingual multimodal applications.\n\nThis model was contributed by [saurabhdash](https://huggingface.co/saurabhdash) and [yonigozlan](https://huggingface.co/yonigozlan).\n\n\n## Usage\n\nHere's how to use Aya Vision for inference:\n\n```python\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\nmodel_id = \"CohereForAI/aya-vision-8b\"\ntorch_device = \"cuda:0\"\n\n# Use fast image processor\nprocessor = AutoProcessor.from_pretrained(model_id, use_fast=True)\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id, device_map=torch_device, torch_dtype=torch.float16\n)\n\n# Format message with the aya-vision chat template\nmessages = [\n{\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://pbs.twimg.com/media/Fx7YvfQWYAIp6rZ?format=jpg&name=medium\"},\n{\"type\": \"text\", \"text\": \"चित्र में लिखा पाठ क्या कहता है?\"},\n]},\n]\n\n# Process image on CUDA\ninputs = processor.apply_chat_template(",
  "messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\", device=torch_device\n).to(model.device)\n\ngen_tokens = model.generate(\n**inputs,\nmax_new_tokens=300,\ndo_sample=True,\ntemperature=0.3,\n)\n\ngen_text = print(processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))\n```\n### Pipeline\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(model=\"CohereForAI/aya-vision-8b\", task=\"image-text-to-text\", device_map=\"auto\")\n\n# Format message with the aya-vision chat template\nmessages = [\n{\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://media.istockphoto.com/id/458012057/photo/istanbul-turkey.jpg?s=612x612&w=0&k=20&c=qogAOVvkpfUyqLUMr_XJQyq-HkACXyYUSZbKhBlPrxo=\"},\n{\"type\": \"text\", \"text\": \"Bu resimde hangi anıt gösterilmektedir?\"},\n]},\n]\noutputs = pipe(text=messages, max_new_tokens=300, return_full_text=False)\n\nprint(outputs)\n```\n\n### Multiple Images and Batched Inputs\n\nAya Vision can process multiple images in a single conversation. Here's how to use it with multiple images:\n\n```python\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch",
  "model_id = \"CohereForAI/aya-vision-8b\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id, device_map=\"cuda:0\", torch_dtype=torch.float16\n)\n\n# Example with multiple images in a single message\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n},\n{\n\"type\": \"image\",\n\"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n},\n{\n\"type\": \"text\",\n\"text\": \"These images depict two different landmarks. Can you identify them?\",\n},\n],\n},\n]\n\ninputs = processor.apply_chat_template(\nmessages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n).to(model.device)\n\ngen_tokens = model.generate(\n**inputs,\nmax_new_tokens=300,\ndo_sample=True,\ntemperature=0.3,\n)\n\ngen_text = processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(gen_text)\n```\n\nFor processing batched inputs (multiple conversations at once):\n\n```python",
  "from transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\nmodel_id = \"CohereForAI/aya-vision-8b\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id, device_map=\"cuda:0\", torch_dtype=torch.float16\n)\n\n# Prepare two different conversations\nbatch_messages = [\n# First conversation with a single image\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n{\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n],\n},\n],\n# Second conversation with multiple images\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n},\n{\n\"type\": \"image\",\n\"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n},\n{\n\"type\": \"text\",\n\"text\": \"These images depict two different landmarks. Can you identify them?\",\n},\n],\n},\n],\n]\n\n# Process each conversation separately and combine into a batch\nbatch_inputs = processor.apply_chat_template(\nbatch_messages,\npadding=True,",
  "add_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n\n# Generate responses for the batch\nbatch_outputs = model.generate(\n**batch_inputs,\nmax_new_tokens=300,\ndo_sample=True,\ntemperature=0.3,\n)\n\n# Decode the generated responses\nfor i, output in enumerate(batch_outputs):\nresponse = processor.tokenizer.decode(\noutput[batch_inputs.input_ids.shape[1]:],\nskip_special_tokens=True\n)\nprint(f\"Response {i+1}:\\n{response}\\n\")\n```\n\n## AyaVisionProcessor\n\n[[autodoc]] AyaVisionProcessor\n\n## AyaVisionConfig\n\n[[autodoc]] AyaVisionConfig\n\n## AyaVisionForConditionalGeneration\n\n[[autodoc]] AyaVisionForConditionalGeneration\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Gemma\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Gemma model was proposed in [Gemma: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/gemma-open-models/) by Gemma Team, Google.\nGemma models are trained on 6T tokens, and released with 2 versions, 2b and 7b.\n\nThe abstract from the paper is the following:",
  "*This work introduces Gemma, a new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of our model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script `src/transformers/models/gemma/convert_gemma_weights_to_hf.py`\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Younes Belkada](https://huggingface.co/ybelkada), [Sanchit Gandhi](https://huggingface.co/sanchit-gandhi), [Pedro Cuenca](https://huggingface.co/pcuenq).\n\n\n## GemmaConfig\n\n[[autodoc]] GemmaConfig\n\n## GemmaTokenizer\n\n[[autodoc]] GemmaTokenizer\n\n\n## GemmaTokenizerFast",
  "[[autodoc]] GemmaTokenizerFast\n\n## GemmaModel\n\n[[autodoc]] GemmaModel\n- forward\n\n## GemmaForCausalLM\n\n[[autodoc]] GemmaForCausalLM\n- forward\n\n## GemmaForSequenceClassification\n\n[[autodoc]] GemmaForSequenceClassification\n- forward\n\n## GemmaForTokenClassification\n\n[[autodoc]] GemmaForTokenClassification\n- forward\n\n## FlaxGemmaModel\n\n[[autodoc]] FlaxGemmaModel\n- __call__\n\n## FlaxGemmaForCausalLM\n\n[[autodoc]] FlaxGemmaForCausalLM\n- __call__",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# WavLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,",
  "Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\nMichael Zeng, Furu Wei.\n\nThe abstract from the paper is the following:\n\n*Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been\nattempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker\nidentity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is\nchallenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks.\nWavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity\npreservation. We first equip the Transformer structure with gated relative position bias to improve its capability on\nrecognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where\nadditional overlapped utterances are created unsupervisedly and incorporated during model training. Lastly, we scale up",
  "the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB\nbenchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.*\n\nRelevant checkpoints can be found under https://huggingface.co/models?other=wavlm.\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Authors' code can be\nfound [here](https://github.com/microsoft/unilm/tree/master/wavlm).\n\n## Usage tips\n\n- WavLM is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please use\n[`Wav2Vec2Processor`] for the feature extraction.\n- WavLM model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`].\n- WavLM performs especially well on speaker verification, speaker identification, and speaker diarization tasks.\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## WavLMConfig\n\n[[autodoc]] WavLMConfig\n\n## WavLMModel\n\n[[autodoc]] WavLMModel\n- forward\n\n## WavLMForCTC",
  "[[autodoc]] WavLMForCTC\n- forward\n\n## WavLMForSequenceClassification\n\n[[autodoc]] WavLMForSequenceClassification\n- forward\n\n## WavLMForAudioFrameClassification\n\n[[autodoc]] WavLMForAudioFrameClassification\n- forward\n\n## WavLMForXVector\n\n[[autodoc]] WavLMForXVector\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UniSpeech-SAT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The UniSpeech-SAT model was proposed in [UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware\nPre-Training](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen,\nShujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu .\n\nThe abstract from the paper is the following:\n\n*Self-supervised learning (SSL) is a long-standing goal for speech processing, since it utilizes large-scale unlabeled\ndata and avoids extensive human labeling. Recent years witness great successes in applying self-supervised learning in\nspeech recognition, while limited exploration was attempted in applying SSL for modeling speaker characteristics. In\nthis paper, we aim to improve the existing SSL framework for speaker representation learning. Two methods are\nintroduced for enhancing the unsupervised speaker information extraction. First, we apply the multi-task learning to\nthe current SSL framework, where we integrate the utterance-wise contrastive loss with the SSL objective function.\nSecond, for better speaker discrimination, we propose an utterance mixing strategy for data augmentation, where",
  "additional overlapped utterances are created unsupervisedly and incorporate during training. We integrate the proposed\nmethods into the HuBERT framework. Experiment results on SUPERB benchmark show that the proposed system achieves\nstate-of-the-art performance in universal representation learning, especially for speaker identification oriented\ntasks. An ablation study is performed verifying the efficacy of each proposed method. Finally, we scale up training\ndataset to 94 thousand hours public audio data and achieve further performance improvement in all SUPERB tasks.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Authors' code can be\nfound [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech-SAT).\n\n## Usage tips\n\n- UniSpeechSat is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\nPlease use [`Wav2Vec2Processor`] for the feature extraction.\n- UniSpeechSat model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be\ndecoded using [`Wav2Vec2CTCTokenizer`].",
  "- UniSpeechSat performs especially well on speaker verification, speaker identification, and speaker diarization tasks.\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## UniSpeechSatConfig\n\n[[autodoc]] UniSpeechSatConfig\n\n## UniSpeechSat specific outputs\n\n[[autodoc]] models.unispeech_sat.modeling_unispeech_sat.UniSpeechSatForPreTrainingOutput\n\n## UniSpeechSatModel\n\n[[autodoc]] UniSpeechSatModel\n- forward\n\n## UniSpeechSatForCTC\n\n[[autodoc]] UniSpeechSatForCTC\n- forward\n\n## UniSpeechSatForSequenceClassification\n\n[[autodoc]] UniSpeechSatForSequenceClassification\n- forward\n\n## UniSpeechSatForAudioFrameClassification\n\n[[autodoc]] UniSpeechSatForAudioFrameClassification\n- forward\n\n## UniSpeechSatForXVector\n\n[[autodoc]] UniSpeechSatForXVector\n- forward\n\n## UniSpeechSatForPreTraining\n\n[[autodoc]] UniSpeechSatForPreTraining\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DPR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was\nintroduced in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by\nVladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.\n\nThe abstract from the paper is the following:\n\n*Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional\nsparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can\nbe practically implemented using dense representations alone, where embeddings are learned from a small number of\nquestions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,\nour dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage\nretrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.*",
  "This model was contributed by [lhoestq](https://huggingface.co/lhoestq). The original code can be found [here](https://github.com/facebookresearch/DPR).\n\n## Usage tips\n\n- DPR consists in three models:\n\n* Question encoder: encode questions as vectors\n* Context encoder: encode contexts as vectors\n* Reader: extract the answer of the questions inside retrieved contexts, along with a relevance score (high if the inferred span actually answers the question).\n\n## DPRConfig\n\n[[autodoc]] DPRConfig\n\n## DPRContextEncoderTokenizer\n\n[[autodoc]] DPRContextEncoderTokenizer\n\n## DPRContextEncoderTokenizerFast\n\n[[autodoc]] DPRContextEncoderTokenizerFast\n\n## DPRQuestionEncoderTokenizer\n\n[[autodoc]] DPRQuestionEncoderTokenizer\n\n## DPRQuestionEncoderTokenizerFast\n\n[[autodoc]] DPRQuestionEncoderTokenizerFast\n\n## DPRReaderTokenizer\n\n[[autodoc]] DPRReaderTokenizer\n\n## DPRReaderTokenizerFast\n\n[[autodoc]] DPRReaderTokenizerFast\n\n## DPR specific outputs\n\n[[autodoc]] models.dpr.modeling_dpr.DPRContextEncoderOutput\n\n[[autodoc]] models.dpr.modeling_dpr.DPRQuestionEncoderOutput\n\n[[autodoc]] models.dpr.modeling_dpr.DPRReaderOutput\n\n<frameworkcontent>\n<pt>\n\n## DPRContextEncoder\n\n[[autodoc]] DPRContextEncoder",
  "- forward\n\n## DPRQuestionEncoder\n\n[[autodoc]] DPRQuestionEncoder\n- forward\n\n## DPRReader\n\n[[autodoc]] DPRReader\n- forward\n\n</pt>\n<tf>\n\n## TFDPRContextEncoder\n\n[[autodoc]] TFDPRContextEncoder\n- call\n\n## TFDPRQuestionEncoder\n\n[[autodoc]] TFDPRQuestionEncoder\n- call\n\n## TFDPRReader\n\n[[autodoc]] TFDPRReader\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# REALM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe REALM model was proposed in [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It's a\nretrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then\nutilizes retrieved documents to process question answering tasks.\n\nThe abstract from the paper is the following:\n\n*Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks\nsuch as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we\naugment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend\nover documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the",
  "first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language\nmodeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We\ndemonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the\nchallenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both\nexplicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous\nmethods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as\ninterpretability and modularity.*\n\nThis model was contributed by [qqaatw](https://huggingface.co/qqaatw). The original code can be found\n[here](https://github.com/google-research/language/tree/master/language/realm).\n\n## RealmConfig\n\n[[autodoc]] RealmConfig\n\n## RealmTokenizer\n\n[[autodoc]] RealmTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n- batch_encode_candidates\n\n## RealmTokenizerFast\n\n[[autodoc]] RealmTokenizerFast",
  "- batch_encode_candidates\n\n## RealmRetriever\n\n[[autodoc]] RealmRetriever\n\n## RealmEmbedder\n\n[[autodoc]] RealmEmbedder\n- forward\n\n## RealmScorer\n\n[[autodoc]] RealmScorer\n- forward\n\n## RealmKnowledgeAugEncoder\n\n[[autodoc]] RealmKnowledgeAugEncoder\n- forward\n\n## RealmReader\n\n[[autodoc]] RealmReader\n- forward\n\n## RealmForOpenQA\n\n[[autodoc]] RealmForOpenQA\n- block_embedding_to\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Video-LLaVA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Video-LLaVa is an open-source multimodal LLM trained by fine-tuning LlamA/Vicuna on multimodal instruction-following data generated by Llava1.5 and VideChat. It is an auto-regressive language model, based on the transformer architecture. Video-LLaVa unifies visual representations to the language feature space, and enables an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n\n\nThe Video-LLaVA model was proposed in [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122) by Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munang Ning, Peng Jin, Li Yuan.\n\nThe abstract from the paper is the following:\n\n*The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in\nvisual-language understanding. Most existing approaches\nencode images and videos into separate feature spaces,\nwhich are then fed as inputs to large language models.\nHowever, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it\nbecomes challenging for a Large Language Model (LLM)",
  "to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA,\nwhich learns from a mixed dataset of images and videos,\nmutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4\nimage benchmark toolkits. Additionally, our Video-LLaVA\nalso outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%,\nand 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that\nVideo-LLaVA mutually benefits images and videos within\na unified visual representation, outperforming models designed specifically for images or videos. We aim for this\nwork to provide modest insights into the multi-modal inputs\nfor the LLM*\n\n## Usage tips:\n\n- We advise users to use padding_side=\"left\" when computing batched generation as it leads to more accurate results. Simply make sure to call processor.tokenizer.padding_side = \"left\" before generating.",
  "- Note the model has not been explicitly trained to process multiple images/videos in the same prompt, although this is technically possible, you may experience inaccurate results.\n\n- Note that the video inputs should have exactly 8 frames at the input, since the models were trained in that setting.\n\nThis model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\nThe original code can be found [here](https://github.com/PKU-YuanGroup/Video-LLaVA).\n\n\n> [!NOTE]\n> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.",
  "Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n\n\n## Usage example\n\n### Single Media Mode\n\nThe model can accept both images and videos as input. Here's an example code for inference in half-precision (`torch.float16`):\n\n```python\nimport av\nimport torch\nimport numpy as np\nfrom transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor\n\ndef read_video_pyav(container, indices):\n'''\nDecode the video with PyAV decoder.\nArgs:\ncontainer (`av.container.input.InputContainer`): PyAV container.\nindices (`List[int]`): List of frame indices to decode.\nReturns:",
  "result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n'''\nframes = []\ncontainer.seek(0)\nstart_index = indices[0]\nend_index = indices[-1]\nfor i, frame in enumerate(container.decode(video=0)):\nif i > end_index:\nbreak\nif i >= start_index and i in indices:\nframes.append(frame)\nreturn np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n# Load the model in half-precision\nmodel = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n\n# Load the video as an np.arrau, sampling uniformly 8 frames\nvideo_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\ncontainer = av.open(video_path)\ntotal_frames = container.streams.video[0].frames\nindices = np.arange(0, total_frames, total_frames / 8).astype(int)\nvideo = read_video_pyav(container, indices)\n\n# For better results, we recommend to prompt the model in the following format\nprompt = \"USER: <video>\\nWhy is this funny? ASSISTANT:\"",
  "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n\nout = model.generate(**inputs, max_new_tokens=60)\nprocessor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n```\n\nFor multiple turns conversation change the prompt format to:\n\n```bash\n\"USER: <video>\\nWhat do you see in this video? ASSISTANT: A baby reading a book. USER: Why is the it funny? ASSISTANT:\"\n```\n\n### Mixed Media Mode\n\nThe model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet:\n\n```python\nfrom PIL import Image\nimport requests\n\n# Generate from image and video mixed inputs\n# Load and image and write a new prompt\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"USER: <image>\\nHow many cats are there in the image? ASSISTANT: There are two cats. USER: <video>\\nWhy is this video funny? ASSISTANT:\"\n\ninputs = processor(text=prompt, images=image, videos=clip, padding=True, return_tensors=\"pt\")",
  "# Generate\ngenerate_ids = model.generate(**inputs, max_length=50)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n```\n\n## Model optimization\n\n### Quantization using Bitsandbytes for memory efficiency\n\nThe model can be loaded in lower bits, significantly reducing memory burden while maintaining the performance of the original model. his allows for efficient deployment on resource-constrained cases.\n\nFirst make sure to install bitsandbytes by running `pip install bitsandbytes` and to have access to a GPU/accelerator that is supported by the library.\n\n<Tip>\n\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit [this link](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend).\n\nWe value your feedback to help identify bugs before the full release! Check out [these docs](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends) for more details and feedback links.\n\n</Tip>",
  "Load the quantized model by simply adding [`BitsAndBytesConfig`](../main_classes/quantization#transformers.BitsAndBytesConfig) as shown below:\n\n\n```python\nfrom transformers import VideoLlavaForConditionalGeneration, BitsAndBytesConfig\n\n# specify how to quantize the model\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=quantization_config, device_map=\"auto\")\n```\n\n\n### Flash-Attention 2 to speed-up generation\n\nAdditionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```",
  "Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using Flash Attention-2, simply add `attn_implementation=\"flash_attention_2\"` when loading the model as follows:\n\n```python\nfrom transformers import VideoLlavaForConditionalGeneration\n\nmodel = VideoLlavaForConditionalGeneration.from_pretrained(\n\"LanguageBind/Video-LLaVA-7B-hf\",\ntorch_dtype=torch.float16,\nattn_implementation=\"flash_attention_2\",\n).to(0)\n```\n\n\n## VideoLlavaConfig\n\n[[autodoc]] VideoLlavaConfig\n\n## VideoLlavaImageProcessor\n\n[[autodoc]] VideoLlavaImageProcessor\n\n## VideoLlavaProcessor\n\n[[autodoc]] VideoLlavaProcessor\n\n## VideoLlavaForConditionalGeneration\n\n[[autodoc]] VideoLlavaForConditionalGeneration\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MGP-STR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The MGP-STR model was proposed in [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao. MGP-STR is a conceptually **simple** yet **powerful** vision Scene Text Recognition (STR) model, which is built upon the [Vision Transformer (ViT)](vit). To integrate linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed to inject information from the language modality into the model in an implicit way.\n\nThe abstract from the paper is the following:",
  "*Scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this challenging problem, numerous innovative methods have been successively proposed and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet powerful vision STR model, which is built upon ViT and outperforms previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, i.e. , subword representations (BPE and WordPiece) widely-used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push the performance envelop of STR to an even higher level. Specifically, it achieves an average recognition accuracy of 93.35% on standard benchmarks.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/mgp_str_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> MGP-STR architecture. Taken from the <a href=\"https://arxiv.org/abs/2209.03592\">original paper</a>. </small>\n\nMGP-STR is trained on two synthetic datasets [MJSynth]((http://www.robots.ox.ac.uk/~vgg/data/text/)) (MJ) and [SynthText](http://www.robots.ox.ac.uk/~vgg/data/scenetext/) (ST) without fine-tuning on other datasets. It achieves state-of-the-art results on six standard Latin scene text benchmarks, including 3 regular text datasets (IC13, SVT, IIIT) and 3 irregular ones (IC15, SVTP, CUTE).\nThis model was contributed by [yuekun](https://huggingface.co/yuekun). The original code can be found [here](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR).\n\n## Inference example\n\n[`MgpstrModel`] accepts images as input and generates three types of predictions, which represent textual information at different granularities.\nThe three types of predictions are fused to give the final prediction result.",
  "The [`ViTImageProcessor`] class is responsible for preprocessing the input image and\n[`MgpstrTokenizer`] decodes the generated character tokens to the target string. The\n[`MgpstrProcessor`] wraps [`ViTImageProcessor`] and [`MgpstrTokenizer`]\ninto a single instance to both extract the input features and decode the predicted token ids.\n\n- Step-by-step Optical Character Recognition (OCR)\n\n```py\n>>> from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\n>>> model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\n\n>>> # load image from the IIIT-5k dataset\n>>> url = \"https://i.postimg.cc/ZKwLg2Gw/367-14.png\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(pixel_values)\n\n>>> generated_text = processor.batch_decode(outputs.logits)['generated_text']\n```\n\n## MgpstrConfig\n\n[[autodoc]] MgpstrConfig\n\n## MgpstrTokenizer\n\n[[autodoc]] MgpstrTokenizer\n- save_vocabulary\n\n## MgpstrProcessor",
  "[[autodoc]] MgpstrProcessor\n- __call__\n- batch_decode\n\n## MgpstrModel\n\n[[autodoc]] MgpstrModel\n- forward\n\n## MgpstrForSceneTextRecognition\n\n[[autodoc]] MgpstrForSceneTextRecognition\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MEGA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe MEGA model was proposed in [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\nMEGA proposes a new approach to self-attention with each encoder layer having a multi-headed exponential moving average in addition to a single head of standard dot-product attention, giving the attention mechanism\nstronger positional biases. This allows MEGA to perform competitively to Transformers on standard benchmarks including LRA\nwhile also having significantly fewer parameters. MEGA's compute efficiency allows it to scale to very long sequences, making it an\nattractive option for long-document NLP tasks.\n\nThe abstract from the paper is the following:",
  "*The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models. *\n\nThis model was contributed by [mnaylor](https://huggingface.co/mnaylor).\nThe original code can be found [here](https://github.com/facebookresearch/mega).\n\n\n## Usage tips",
  "- MEGA can perform quite well with relatively few parameters. See Appendix D in the MEGA paper for examples of architectural specs which perform well in various settings. If using MEGA as a decoder, be sure to set `bidirectional=False` to avoid errors with default bidirectional.\n- Mega-chunk is a variant of mega that reduces time and spaces complexity from quadratic to linear. Utilize chunking with MegaConfig.use_chunking and control chunk size with MegaConfig.chunk_size\n\n\n## Implementation Notes\n\n- The original implementation of MEGA had an inconsistent expectation of attention masks for padding and causal self-attention between the softmax attention and Laplace/squared ReLU method. This implementation addresses that inconsistency.\n- The original implementation did not include token type embeddings; this implementation adds support for these, with the option controlled by MegaConfig.add_token_type_embeddings\n\n\n## MegaConfig\n\n[[autodoc]] MegaConfig\n\n## MegaModel\n\n[[autodoc]] MegaModel\n- forward\n\n## MegaForCausalLM\n\n[[autodoc]] MegaForCausalLM\n- forward\n\n## MegaForMaskedLM\n\n[[autodoc]] MegaForMaskedLM\n- forward\n\n## MegaForSequenceClassification",
  "[[autodoc]] MegaForSequenceClassification\n- forward\n\n## MegaForMultipleChoice\n\n[[autodoc]] MegaForMultipleChoice\n- forward\n\n## MegaForTokenClassification\n\n[[autodoc]] MegaForTokenClassification\n- forward\n\n## MegaForQuestionAnswering\n\n[[autodoc]] MegaForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CodeGen\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.",
  "CodeGen is an autoregressive language model for program synthesis trained sequentially on [The Pile](https://pile.eleuther.ai/), BigQuery, and BigPython.\n\nThe abstract from the paper is the following:",
  "*Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: [this https URL](https://github.com/salesforce/codegen).*",
  "This model was contributed by [Hiroaki Hayashi](https://huggingface.co/rooa).\nThe original code can be found [here](https://github.com/salesforce/codegen).\n\n## Checkpoint Naming\n\n* CodeGen model [checkpoints](https://huggingface.co/models?other=codegen) are available on different pre-training data with variable sizes.\n* The format is: `Salesforce/codegen-{size}-{data}`, where\n* `size`: `350M`, `2B`, `6B`, `16B`\n* `data`:\n* `nl`: Pre-trained on the Pile\n* `multi`: Initialized with `nl`, then further pre-trained on multiple programming languages data\n* `mono`: Initialized with `multi`, then further pre-trained on Python data\n* For example, `Salesforce/codegen-350M-mono` offers a 350 million-parameter checkpoint pre-trained sequentially on the Pile, multiple programming languages, and Python.\n\n## Usage example\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> checkpoint = \"Salesforce/codegen-350M-mono\"\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n>>> text = \"def hello_world():\"\n\n>>> completion = model.generate(**tokenizer(text, return_tensors=\"pt\"))",
  ">>> print(tokenizer.decode(completion[0]))\ndef hello_world():\nprint(\"Hello World\")\n\nhello_world()\n```\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## CodeGenConfig\n\n[[autodoc]] CodeGenConfig\n- all\n\n## CodeGenTokenizer\n\n[[autodoc]] CodeGenTokenizer\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## CodeGenTokenizerFast\n\n[[autodoc]] CodeGenTokenizerFast\n\n## CodeGenModel\n\n[[autodoc]] CodeGenModel\n- forward\n\n## CodeGenForCausalLM\n\n[[autodoc]] CodeGenForCausalLM\n- forward",
  "# Cohere\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Cohere Command-R model was proposed in the blogpost [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/) by the Cohere Team.\n\nThe abstract from the paper is the following:\n\n*Command-R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise. Today, we are introducing Command-R, a new LLM aimed at large-scale production workloads. Command-R targets the emerging “scalable” category of models that balance high efficiency with strong accuracy, enabling companies to move beyond proof of concept, and into production.*",
  "*Command-R is a generative model optimized for long context tasks such as retrieval augmented generation (RAG) and using external APIs and tools. It is designed to work in concert with our industry-leading Embed and Rerank models to provide best-in-class integration for RAG applications and excel at enterprise use cases. As a model built for companies to implement at scale, Command-R boasts:\n- Strong accuracy on RAG and Tool Use\n- Low latency, and high throughput\n- Longer 128k context and lower pricing\n- Strong capabilities across 10 key languages\n- Model weights available on HuggingFace for research and evaluation\n\nCheckout model checkpoints [here](https://huggingface.co/CohereForAI/c4ai-command-r-v01).\nThis model was contributed by [Saurabh Dash](https://huggingface.co/saurabhdash) and [Ahmet Üstün](https://huggingface.co/ahmetustun). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox).\n\n## Usage tips\n\n<Tip warning={true}>\n\nThe checkpoints uploaded on the Hub use `torch_dtype = 'float16'`, which will be\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.",
  "The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online), then it will be casted to the default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is a `torch_dtype` provided in the config, it will be used.\n\nTraining the model in `float16` is not recommended and is known to produce `nan`; as such, the model should be trained in `bfloat16`.\n\n</Tip>\nThe model and tokenizer can be loaded via:\n\n```python\n# pip install transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\n# Format message with the command-r chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")",
  "## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n\ngen_tokens = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n```\n\n- When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `torch_dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Command-R. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n<PipelineTag pipeline=\"text-generation\"/>\n\nLoading FP16 model\n```python\n# pip install transformers",
  "from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\n# Format message with the command-r chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n\ngen_tokens = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n```\n\nLoading bitsnbytes 4bit quantized model\n```python\n# pip install transformers bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n\ngen_tokens = model.generate(",
  "input_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n```\n\n\n## CohereConfig\n\n[[autodoc]] CohereConfig\n\n## CohereTokenizerFast\n\n[[autodoc]] CohereTokenizerFast\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- update_post_processor\n- save_vocabulary\n\n## CohereModel\n\n[[autodoc]] CohereModel\n- forward\n\n\n## CohereForCausalLM\n\n[[autodoc]] CohereForCausalLM\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Granite\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Granite model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n\nPowerLM-3B is a 3B state-of-the-art small language model trained with the Power learning rate scheduler. It is trained on a wide range of open-source and synthetic datasets with permissive licenses. PowerLM-3B has shown promising results compared to other models in the size categories across various benchmarks, including natural language multi-choices, code generation, and math reasoning.\n\nThe abstract from the paper is the following:\n\n*Finding the optimal learning rate for language model pretraining is a challenging task.",
  "This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored.",
  "In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler, that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (\\mup) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models.\nWe [open source](https://huggingface.co/collections/ibm/power-lm-66be64ae647ddf11b9808000) these pretrained models.*\n\nTips:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"ibm/PowerLM-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# drop device_map if running on CPU",
  "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\nmodel.eval()\n\n# change input text as desired\nprompt = \"Write a code to find the maximum value in a list of numbers.\"\n\n# tokenize the text\ninput_tokens = tokenizer(prompt, return_tensors=\"pt\")\n# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# loop over the batch to print, in this example the batch size is 1\nfor i in output:\nprint(i)\n```\n\nThis model was contributed by [mayank-mishra](https://huggingface.co/mayank-mishra).\n\n\n## GraniteConfig\n\n[[autodoc]] GraniteConfig\n\n## GraniteModel\n\n[[autodoc]] GraniteModel\n- forward\n\n## GraniteForCausalLM\n\n[[autodoc]] GraniteForCausalLM\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ProphetNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\nZhang, Ming Zhou on 13 Jan, 2020.",
  "ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of just\nthe next token.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\nself-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\nthe optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\nn-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent\noverfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale\ndataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for\nabstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new",
  "state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.*\n\nThe Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n\n## Usage tips\n\n- ProphetNet is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n- The model architecture is based on the original Transformer, but replaces the “standard” self-attention mechanism in the decoder by a main self-attention mechanism and a self and n-stream (predict) self-attention mechanism.\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## ProphetNetConfig\n\n[[autodoc]] ProphetNetConfig\n\n## ProphetNetTokenizer\n\n[[autodoc]] ProphetNetTokenizer\n\n## ProphetNet specific outputs\n\n[[autodoc]] models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput\n\n[[autodoc]] models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput\n\n[[autodoc]] models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",
  "[[autodoc]] models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput\n\n## ProphetNetModel\n\n[[autodoc]] ProphetNetModel\n- forward\n\n## ProphetNetEncoder\n\n[[autodoc]] ProphetNetEncoder\n- forward\n\n## ProphetNetDecoder\n\n[[autodoc]] ProphetNetDecoder\n- forward\n\n## ProphetNetForConditionalGeneration\n\n[[autodoc]] ProphetNetForConditionalGeneration\n- forward\n\n## ProphetNetForCausalLM\n\n[[autodoc]] ProphetNetForCausalLM\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPT-NeoX\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nWe introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will",
  "be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge,\nthe largest dense autoregressive model that has publicly available weights at the time of submission. In this work,\nwe describe GPT-NeoX-20B's architecture and training and evaluate its performance on a range of language-understanding,\nmathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and\ngains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source\nthe training and evaluation code, as well as the model weights, at [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\n\nDevelopment of the model was led by Sid Black, Stella Biderman and Eric Hallahan, and the model was trained with\ngenerous the support of [CoreWeave](https://www.coreweave.com/).\n\nGPT-NeoX-20B was trained with fp16, thus it is recommended to initialize the model as follows:\n\n```python\nmodel = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\").half().cuda()\n```",
  "GPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new tokenizer allocates\nadditional tokens to whitespace characters, making the model more suitable for certain tasks like code generation.\n\n## Usage example\n\nThe `generate()` method can be used to generate text using GPT Neo model.\n\n```python\n>>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n\n>>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n>>> tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n\n>>> prompt = \"GPTNeoX20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI.\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```\n\n## Using Flash Attention 2\n\nFlash Attention 2 is an faster, optimized version of the model.\n\n### Installation",
  "First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n\nNext, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n### Usage\n\nTo load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\n```python",
  ">>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n...\n```\n\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `stockmark/gpt-neox-japanese-1.4b` checkpoint and the Flash Attention 2 version of the model using a sequence length of 2048.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/gpt-neox-1.8b-speedup.jpg\">\n</div>\n\n\n## Using Scaled Dot Product Attention (SDPA)\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)",
  "page for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```python\nfrom transformers import GPTNeoXForCausalLM\nmodel = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using `float16` with\n[pythia-410m-deduped](https://huggingface.co/EleutherAI/pythia-410m-deduped), we saw the\nfollowing speedups during training and inference.\n\n### Training\n| Batch size |    Seq len | Time per batch (Eager - s) |    Time per batch (SDPA - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) |    Mem saving (%) |\n|-----------:|-----------:|---------------------------:|-----------------------------:|------------:|--------------------:|-------------------:|------------------:|",
  "|          1 |        128 |                      0.024 |                        0.019 |      28.945 |             1789.95 |            1789.95 |                 0 |\n|          1 |        256 |                      0.039 |                        0.031 |       23.18 |             1845.83 |            1844.84 |             0.053 |\n|          1 |        512 |                       0.08 |                        0.055 |      45.524 |             2278.38 |            1953.76 |            16.615 |\n|          1 |       1024 |                       0.19 |                        0.102 |      86.777 |             4772.36 |            2408.35 |            98.159 |\n|          1 |       2048 |                      0.565 |                        0.204 |     177.098 |             13484.1 |            3882.01 |           247.348 |\n|          2 |        128 |                      0.037 |                        0.032 |      15.121 |             1843.86 |            1844.78 |             -0.05 |\n|          2 |        256 |                      0.067 |                        0.055 |      21.706 |             1999.72 |            1951.67 |             2.462 |",
  "|          2 |        512 |                      0.144 |                        0.096 |      50.046 |             3613.16 |            2406.77 |            50.125 |\n|          2 |       1024 |                      0.366 |                        0.193 |      89.666 |             8707.55 |            3878.86 |           124.487 |\n|          2 |       2048 |                        OOM |                        0.379 |           / |                 OOM |            6825.13 | SDPA does not OOM |\n|          4 |        128 |                       0.06 |                        0.054 |      11.539 |              1947.6 |            1952.06 |            -0.228 |\n|          4 |        256 |                      0.119 |                        0.093 |      28.072 |             3008.39 |            2405.99 |            25.038 |\n|          4 |        512 |                      0.275 |                        0.187 |      47.145 |             6290.58 |            3877.29 |            62.242 |\n|          4 |       1024 |                        OOM |                         0.36 |           / |                 OOM |            6821.98 | SDPA does not OOM |",
  "|          4 |       2048 |                        OOM |                        0.731 |           / |                 OOM |            12705.1 | SDPA does not OOM |\n\n### Inference\n|    Batch size |      Seq len |    Per token latency Eager (ms) |    Per token latency SDPA (ms) |    Speedup (%) |    Mem Eager (MB) |   Mem SDPA (MB) |    Mem saved (%) |\n|--------------:|-------------:|--------------------------------:|-------------------------------:|---------------:|------------------:|----------------:|-----------------:|\n|             1 |          128 |                           6.569 |                          5.858 |          12.14 |           974.831 |         974.826 |                0 |\n|             1 |          256 |                           7.009 |                          5.863 |         19.542 |           1029.01 |         1028.08 |             0.09 |\n|             1 |          512 |                           7.157 |                          5.965 |         19.983 |           1137.54 |         1137.52 |            0.001 |",
  "|             1 |         1024 |                           7.523 |                          6.506 |         15.637 |            1329.3 |         1329.26 |            0.003 |\n|             1 |         2048 |                           9.271 |                          9.205 |          0.713 |           1752.47 |         1734.51 |            1.036 |\n|             2 |          128 |                           7.239 |                          5.959 |         21.493 |            1044.8 |         1028.37 |            1.597 |\n|             2 |          256 |                           7.228 |                          6.036 |         19.757 |           1167.32 |         1137.73 |            2.601 |\n|             2 |          512 |                           7.538 |                          6.693 |         12.628 |           1352.93 |         1329.55 |            1.758 |\n|             2 |         1024 |                           8.916 |                          8.632 |          3.291 |           1752.56 |         1734.62 |            1.034 |",
  "|             2 |         2048 |                          12.628 |                         12.606 |          0.181 |           2558.72 |          2545.8 |            0.508 |\n|             4 |          128 |                           7.278 |                          6.046 |         20.373 |           1168.41 |         1137.79 |            2.691 |\n|             4 |          256 |                           7.614 |                          6.588 |         15.574 |            1353.1 |         1329.79 |            1.753 |\n|             4 |          512 |                           8.798 |                          8.144 |          8.028 |           1752.76 |         1734.85 |            1.032 |\n|             4 |         1024 |                          11.765 |                         11.303 |           4.09 |           2558.96 |         2546.04 |            0.508 |\n|             4 |         2048 |                          19.568 |                         17.735 |          10.33 |            4175.5 |         4165.26 |            0.246 |\n\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## GPTNeoXConfig\n\n[[autodoc]] GPTNeoXConfig\n\n## GPTNeoXTokenizerFast",
  "[[autodoc]] GPTNeoXTokenizerFast\n\n## GPTNeoXModel\n\n[[autodoc]] GPTNeoXModel\n- forward\n\n## GPTNeoXForCausalLM\n\n[[autodoc]] GPTNeoXForCausalLM\n- forward\n\n## GPTNeoXForQuestionAnswering\n\n[[autodoc]] GPTNeoXForQuestionAnswering\n- forward\n\n## GPTNeoXForSequenceClassification\n\n[[autodoc]] GPTNeoXForSequenceClassification\n- forward\n\n## GPTNeoXForTokenClassification\n\n[[autodoc]] GPTNeoXForTokenClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FSMT\n\n## Overview\n\nFSMT (FairSeq MachineTranslation) models were introduced in [Facebook FAIR's WMT19 News Translation Task Submission](https://arxiv.org/abs/1907.06616) by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov.\n\nThe abstract of the paper is the following:\n\n*This paper describes Facebook FAIR's submission to the WMT19 shared news translation task. We participate in two",
  "language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from\nlast year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling\ntoolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,\nas well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific\ndata, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the\nhuman evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.\nThis system improves upon our WMT'18 submission by 4.5 BLEU points.*\n\nThis model was contributed by [stas](https://huggingface.co/stas). The original code can be found\n[here](https://github.com/pytorch/fairseq/tree/master/examples/wmt19).\n\n## Implementation Notes\n\n- FSMT uses source and target vocabulary pairs that aren't combined into one. It doesn't share embeddings tokens\neither. Its tokenizer is very similar to [`XLMTokenizer`] and the main model is derived from\n[`BartModel`].",
  "## FSMTConfig\n\n[[autodoc]] FSMTConfig\n\n## FSMTTokenizer\n\n[[autodoc]] FSMTTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## FSMTModel\n\n[[autodoc]] FSMTModel\n- forward\n\n## FSMTForConditionalGeneration\n\n[[autodoc]] FSMTForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# mT5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe mT5 model was presented in [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\nSiddhant, Aditya Barua, Colin Raffel.\n\nThe abstract from the paper is the following:\n\n*The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain\nstate-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail\nthe design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental translation\" in the zero-shot setting, where a\ngenerative model chooses to (partially) translate its prediction into the wrong language. All of the code and model\ncheckpoints used in this work are publicly available.*\n\nNote: mT5 was only pre-trained on [mC4](https://huggingface.co/datasets/mc4) excluding any supervised training.",
  "Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5 model.\nSince mT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\nGoogle has released the following variants:\n\n- [google/mt5-small](https://huggingface.co/google/mt5-small)\n\n- [google/mt5-base](https://huggingface.co/google/mt5-base)\n\n- [google/mt5-large](https://huggingface.co/google/mt5-large)\n\n- [google/mt5-xl](https://huggingface.co/google/mt5-xl)\n\n- [google/mt5-xxl](https://huggingface.co/google/mt5-xxl).\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/multilingual-t5).\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## MT5Config\n\n[[autodoc]] MT5Config\n\n## MT5Tokenizer\n\n[[autodoc]] MT5Tokenizer\n\nSee [`T5Tokenizer`] for all details.\n\n\n## MT5TokenizerFast\n\n[[autodoc]] MT5TokenizerFast\n\nSee [`T5TokenizerFast`] for all details.\n\n<frameworkcontent>\n<pt>",
  "## MT5Model\n\n[[autodoc]] MT5Model\n\n## MT5ForConditionalGeneration\n\n[[autodoc]] MT5ForConditionalGeneration\n\n## MT5EncoderModel\n\n[[autodoc]] MT5EncoderModel\n\n## MT5ForSequenceClassification\n\n[[autodoc]] MT5ForSequenceClassification\n\n## MT5ForTokenClassification\n\n[[autodoc]] MT5ForTokenClassification\n\n## MT5ForQuestionAnswering\n\n[[autodoc]] MT5ForQuestionAnswering\n\n</pt>\n<tf>\n\n## TFMT5Model\n\n[[autodoc]] TFMT5Model\n\n## TFMT5ForConditionalGeneration\n\n[[autodoc]] TFMT5ForConditionalGeneration\n\n## TFMT5EncoderModel\n\n[[autodoc]] TFMT5EncoderModel\n\n</tf>\n<jax>\n\n## FlaxMT5Model\n\n[[autodoc]] FlaxMT5Model\n\n## FlaxMT5ForConditionalGeneration\n\n[[autodoc]] FlaxMT5ForConditionalGeneration\n\n## FlaxMT5EncoderModel\n\n[[autodoc]] FlaxMT5EncoderModel\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Hiera\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Hiera was proposed in [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/abs/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer\n\nThe paper introduces \"Hiera,\" a hierarchical Vision Transformer that simplifies the architecture of modern hierarchical vision transformers by removing unnecessary components without compromising on accuracy or efficiency. Unlike traditional transformers that add complex vision-specific components to improve supervised classification performance, Hiera demonstrates that such additions, often termed \"bells-and-whistles,\" are not essential for high accuracy. By leveraging a strong visual pretext task (MAE) for pretraining, Hiera retains simplicity and achieves superior accuracy and speed both in inference and training across various image and video recognition tasks. The approach suggests that spatial biases required for vision tasks can be effectively learned through proper pretraining, eliminating the need for added architectural complexity.",
  "The abstract from the paper is the following:\n\n*Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/hiera_overview.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Hiera architecture. Taken from the <a href=\"https://arxiv.org/abs/2306.00989\">original paper.</a> </small>\n\nThis model was a joint contribution by [EduardoPacheco](https://huggingface.co/EduardoPacheco) and [namangarg110](https://huggingface.co/namangarg110). The original code can be found [here] (https://github.com/facebookresearch/hiera).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Hiera. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`HieraForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\n## HieraConfig\n\n[[autodoc]] HieraConfig\n\n## HieraModel\n\n[[autodoc]] HieraModel",
  "- forward\n\n## HieraForPreTraining\n\n[[autodoc]] HieraForPreTraining\n- forward\n\n## HieraForImageClassification\n\n[[autodoc]] HieraForImageClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ConvBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\nYan.\n\nThe abstract from the paper is the following:\n\n*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that",
  "ConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training cost and\nfewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, while\nusing less than 1/4 training cost. Code and pre-trained models will be released.*\n\nThis model was contributed by [abhishek](https://huggingface.co/abhishek). The original implementation can be found\nhere: https://github.com/yitu-opensource/ConvBert\n\n## Usage tips\n\nConvBERT training tips are similar to those of BERT. For usage tips refer to [BERT documentation](bert).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## ConvBertConfig\n\n[[autodoc]] ConvBertConfig\n\n## ConvBertTokenizer\n\n[[autodoc]] ConvBertTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## ConvBertTokenizerFast",
  "[[autodoc]] ConvBertTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## ConvBertModel\n\n[[autodoc]] ConvBertModel\n- forward\n\n## ConvBertForMaskedLM\n\n[[autodoc]] ConvBertForMaskedLM\n- forward\n\n## ConvBertForSequenceClassification\n\n[[autodoc]] ConvBertForSequenceClassification\n- forward\n\n## ConvBertForMultipleChoice\n\n[[autodoc]] ConvBertForMultipleChoice\n- forward\n\n## ConvBertForTokenClassification\n\n[[autodoc]] ConvBertForTokenClassification\n- forward\n\n## ConvBertForQuestionAnswering\n\n[[autodoc]] ConvBertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFConvBertModel\n\n[[autodoc]] TFConvBertModel\n- call\n\n## TFConvBertForMaskedLM\n\n[[autodoc]] TFConvBertForMaskedLM\n- call\n\n## TFConvBertForSequenceClassification\n\n[[autodoc]] TFConvBertForSequenceClassification\n- call\n\n## TFConvBertForMultipleChoice\n\n[[autodoc]] TFConvBertForMultipleChoice\n- call\n\n## TFConvBertForTokenClassification\n\n[[autodoc]] TFConvBertForTokenClassification\n- call\n\n## TFConvBertForQuestionAnswering\n\n[[autodoc]] TFConvBertForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SAM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "SAM (Segment Anything Model) was proposed in [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n\nThe model can be used to predict segmentation masks of any object of interest given an input image.\n\n![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png)\n\nThe abstract from the paper is the following:",
  "*We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at [https://segment-anything.com](https://segment-anything.com) to foster research into foundation models for computer vision.*\n\nTips:\n\n- The model predicts binary masks that states the presence or not of the object of interest given an image.\n- The model predicts much better results if input 2D points and/or input bounding boxes are provided\n- You can prompt multiple points for the same image, and predict a single mask.\n- Fine-tuning the model is not supported yet",
  "- According to the paper, textual input should be also supported. However, at this time of writing this seems not to be supported according to [the official repository](https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844).\n\n\nThis model was contributed by [ybelkada](https://huggingface.co/ybelkada) and [ArthurZ](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/facebookresearch/segment-anything).\n\nBelow is an example on how to run mask generation given an image and a 2D point:\n\n```python\nimport torch\nfrom PIL import Image\nimport requests\nfrom transformers import SamModel, SamProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n\nimg_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\ninput_points = [[[450, 600]]]  # 2D location of a window in the image\n\ninputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(device)",
  "with torch.no_grad():\noutputs = model(**inputs)\n\nmasks = processor.image_processor.post_process_masks(\noutputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n)\nscores = outputs.iou_scores\n```\n\nYou can also process your own masks alongside the input images in the processor to be passed to the model.\n\n```python\nimport torch\nfrom PIL import Image\nimport requests\nfrom transformers import SamModel, SamProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n\nimg_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\nmask_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nsegmentation_map = Image.open(requests.get(mask_url, stream=True).raw).convert(\"1\")\ninput_points = [[[450, 600]]]  # 2D location of a window in the image\n\ninputs = processor(raw_image, input_points=input_points, segmentation_maps=segmentation_map, return_tensors=\"pt\").to(device)",
  "with torch.no_grad():\noutputs = model(**inputs)\n\nmasks = processor.image_processor.post_process_masks(\noutputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n)\nscores = outputs.iou_scores\n```\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SAM.\n\n- [Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb) for using the model.\n- [Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/automatic_mask_generation.ipynb) for using the automatic mask generation pipeline.\n- [Demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb) for inference with MedSAM, a fine-tuned version of SAM on the medical domain. 🌎\n- [Demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb) for fine-tuning the model on custom data. 🌎\n\n## SlimSAM",
  "SlimSAM, a pruned version of SAM, was proposed in [0.1% Data Makes Segment Anything Slim](https://arxiv.org/abs/2312.05284) by Zigeng Chen et al. SlimSAM reduces the size of the SAM models considerably while maintaining the same performance.\n\nCheckpoints can be found on the [hub](https://huggingface.co/models?other=slimsam), and they can be used as a drop-in replacement of SAM.\n\n## Grounded SAM\n\nOne can combine [Grounding DINO](grounding-dino) with SAM for text-based mask generation as introduced in [Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](https://arxiv.org/abs/2401.14159). You can refer to this [demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb) 🌍 for details.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png\"\nalt=\"drawing\" width=\"900\"/>\n\n<small> Grounded SAM overview. Taken from the <a href=\"https://github.com/IDEA-Research/Grounded-Segment-Anything\">original repository</a>. </small>\n\n## SamConfig\n\n[[autodoc]] SamConfig\n\n## SamVisionConfig\n\n[[autodoc]] SamVisionConfig",
  "## SamMaskDecoderConfig\n\n[[autodoc]] SamMaskDecoderConfig\n\n## SamPromptEncoderConfig\n\n[[autodoc]] SamPromptEncoderConfig\n\n\n## SamProcessor\n\n[[autodoc]] SamProcessor\n\n\n## SamImageProcessor\n\n[[autodoc]] SamImageProcessor\n\n\n## SamModel\n\n[[autodoc]] SamModel\n- forward\n\n\n## TFSamModel\n\n[[autodoc]] TFSamModel\n- call",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Falcon\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Falcon is a class of causal decoder-only models built by [TII](https://www.tii.ae/). The largest Falcon checkpoints\nhave been trained on >=1T tokens of text, with a particular emphasis on the [RefinedWeb](https://arxiv.org/abs/2306.01116)\ncorpus. They are made available under the Apache 2.0 license.\n\n\nFalcon's architecture is modern and optimized for inference, with multi-query attention and support for efficient\nattention variants like `FlashAttention`. Both 'base' models trained only as causal language models as well as\n'instruct' models that have received further fine-tuning are available.\n\n\nFalcon models are (as of 2023) some of the largest and most powerful open-source language models,\nand consistently rank highly in the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n\n## Converting custom checkpoints\n\n<Tip>\n\nFalcon models were initially added to the Hugging Face Hub as custom code checkpoints. However, Falcon is now fully\nsupported in the Transformers library. If you fine-tuned a model from a custom code checkpoint, we recommend converting",
  "your checkpoint to the new in-library format, as this should give significant improvements to stability and\nperformance, especially for generation, as well as removing the need to use `trust_remote_code=True`!\n\n</Tip>\n\nYou can convert custom code checkpoints to full Transformers checkpoints using the `convert_custom_code_checkpoint.py`\nscript located in the\n[Falcon model directory](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon)\nof the Transformers library. To use this script, simply call it with\n`python convert_custom_code_checkpoint.py --checkpoint_dir my_model`. This will convert your checkpoint in-place, and\nyou can immediately load it from the directory afterwards with e.g. `from_pretrained()`. If your model hasn't been\nuploaded to the Hub, we recommend making a backup before attempting the conversion, just in case!\n\n\n## FalconConfig\n\n[[autodoc]] FalconConfig\n- all\n\n## FalconModel\n\n[[autodoc]] FalconModel\n- forward\n\n## FalconForCausalLM\n\n[[autodoc]] FalconForCausalLM\n- forward\n\n## FalconForSequenceClassification\n\n[[autodoc]] FalconForSequenceClassification\n- forward\n\n## FalconForTokenClassification",
  "[[autodoc]] FalconForTokenClassification\n- forward\n\n## FalconForQuestionAnswering\n\n[[autodoc]] FalconForQuestionAnswering\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BARThez\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe BARThez model was proposed in [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis on 23 Oct,\n2020.\n\nThe abstract of the paper:\n\n\n*Inductive transfer learning, enabled by self-supervised learning, have taken the entire Natural Language Processing\n(NLP) field by storm, with models such as BERT and BART setting new state of the art on countless natural language\nunderstanding tasks. While there are some notable exceptions, most of the available models and research have been\nconducted for the English language. In this work, we introduce BARThez, the first BART model for the French language\n(to the best of our knowledge). BARThez was pretrained on a very large monolingual French corpus from past research\nthat we adapted to suit BART's perturbation schemes. Unlike already existing BERT-based French language models such as\nCamemBERT and FlauBERT, BARThez is particularly well-suited for generative tasks, since not only its encoder but also",
  "its decoder is pretrained. In addition to discriminative tasks from the FLUE benchmark, we evaluate BARThez on a novel\nsummarization dataset, OrangeSum, that we release with this paper. We also continue the pretraining of an already\npretrained multilingual BART on BARThez's corpus, and we show that the resulting model, which we call mBARTHez,\nprovides a significant boost over vanilla BARThez, and is on par with or outperforms CamemBERT and FlauBERT.*\n\nThis model was contributed by [moussakam](https://huggingface.co/moussakam). The Authors' code can be found [here](https://github.com/moussaKam/BARThez).\n\n<Tip>\n\nBARThez implementation is the same as BART, except for tokenization. Refer to [BART documentation](bart) for information on\nconfiguration classes and their parameters. BARThez-specific tokenizers are documented below.\n\n</Tip>\n\n## Resources\n\n- BARThez can be fine-tuned on sequence-to-sequence tasks in a similar way as BART, check:\n[examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md).\n\n\n## BarthezTokenizer\n\n[[autodoc]] BarthezTokenizer\n\n## BarthezTokenizerFast\n\n[[autodoc]] BarthezTokenizerFast",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LUKE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto.",
  "It is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps\nimprove performance on various downstream tasks involving reasoning about entities such as named entity recognition,\nextractive and cloze-style question answering, entity typing, and relation classification.\n\nThe abstract from the paper is the following:\n\n*Entity representations are useful in natural language tasks involving entities. In this paper, we propose new\npretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed\nmodel treats words and entities in a given text as independent tokens, and outputs contextualized representations of\nthem. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves\npredicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also\npropose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the\ntransformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model",
  "achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains\nstate-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question\nanswering).*\n\nThis model was contributed by [ikuyamada](https://huggingface.co/ikuyamada) and [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/studio-ousia/luke).\n\n## Usage tips\n\n- This implementation is the same as [`RobertaModel`] with the addition of entity embeddings as well\nas an entity-aware self-attention mechanism, which improves performance on tasks involving reasoning about entities.\n- LUKE treats entities as input tokens; therefore, it takes `entity_ids`, `entity_attention_mask`,\n`entity_token_type_ids` and `entity_position_ids` as extra input. You can obtain those using\n[`LukeTokenizer`].\n- [`LukeTokenizer`] takes `entities` and `entity_spans` (character-based start and end\npositions of the entities in the input text) as extra input. `entities` typically consist of [MASK] entities or",
  "Wikipedia entities. The brief description when inputting these entities are as follows:\n\n- *Inputting [MASK] entities to compute entity representations*: The [MASK] entity is used to mask entities to be\npredicted during pretraining. When LUKE receives the [MASK] entity, it tries to predict the original entity by\ngathering the information about the entity from the input text. Therefore, the [MASK] entity can be used to address\ndownstream tasks requiring the information of entities in text such as entity typing, relation classification, and\nnamed entity recognition.\n- *Inputting Wikipedia entities to compute knowledge-enhanced token representations*: LUKE learns rich information\n(or knowledge) about Wikipedia entities during pretraining and stores the information in its entity embedding. By\nusing Wikipedia entities as input tokens, LUKE outputs token representations enriched by the information stored in\nthe embeddings of these entities. This is particularly effective for tasks requiring real-world knowledge, such as\nquestion answering.\n\n- There are three head models for the former use case:",
  "- [`LukeForEntityClassification`], for tasks to classify a single entity in an input text such as\nentity typing, e.g. the [Open Entity dataset](https://www.cs.utexas.edu/~eunsol/html_pages/open_entity.html).\nThis model places a linear head on top of the output entity representation.\n- [`LukeForEntityPairClassification`], for tasks to classify the relationship between two entities\nsuch as relation classification, e.g. the [TACRED dataset](https://nlp.stanford.edu/projects/tacred/). This\nmodel places a linear head on top of the concatenated output representation of the pair of given entities.\n- [`LukeForEntitySpanClassification`], for tasks to classify the sequence of entity spans, such as\nnamed entity recognition (NER). This model places a linear head on top of the output entity representations. You\ncan address NER using this model by inputting all possible entity spans in the text to the model.\n\n[`LukeTokenizer`] has a `task` argument, which enables you to easily create an input to these\nhead models by specifying `task=\"entity_classification\"`, `task=\"entity_pair_classification\"`, or\n`task=\"entity_span_classification\"`. Please refer to the example code of each head models.",
  "Usage example:\n\n```python\n>>> from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification\n\n>>> model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n# Example 1: Computing the contextualized entity representation corresponding to the entity mention \"Beyoncé\"\n\n>>> text = \"Beyoncé lives in Los Angeles.\"\n>>> entity_spans = [(0, 7)]  # character-based entity span corresponding to \"Beyoncé\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Example 2: Inputting Wikipedia entities to obtain enriched contextualized representations\n\n>>> entities = [\n...     \"Beyoncé\",\n...     \"Los Angeles\",\n... ]  # Wikipedia entity titles corresponding to the entity mentions \"Beyoncé\" and \"Los Angeles\"\n>>> entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyoncé\" and \"Los Angeles\"",
  ">>> inputs = tokenizer(text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Example 3: Classifying the relationship between two entities using LukeForEntityPairClassification head model\n\n>>> model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyoncé\" and \"Los Angeles\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> predicted_class_idx = int(logits[0].argmax())\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\n## Resources\n\n- [A demo notebook on how to fine-tune [`LukeForEntityPairClassification`] for relation classification](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LUKE)",
  "- [Notebooks showcasing how you to reproduce the results as reported in the paper with the HuggingFace implementation of LUKE](https://github.com/studio-ousia/luke/tree/master/notebooks)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## LukeConfig\n\n[[autodoc]] LukeConfig\n\n## LukeTokenizer\n\n[[autodoc]] LukeTokenizer\n- __call__\n- save_vocabulary\n\n## LukeModel\n\n[[autodoc]] LukeModel\n- forward\n\n## LukeForMaskedLM\n\n[[autodoc]] LukeForMaskedLM\n- forward\n\n## LukeForEntityClassification\n\n[[autodoc]] LukeForEntityClassification\n- forward\n\n## LukeForEntityPairClassification\n\n[[autodoc]] LukeForEntityPairClassification\n- forward\n\n## LukeForEntitySpanClassification\n\n[[autodoc]] LukeForEntitySpanClassification\n- forward\n\n## LukeForSequenceClassification\n\n[[autodoc]] LukeForSequenceClassification\n- forward\n\n## LukeForMultipleChoice\n\n[[autodoc]] LukeForMultipleChoice\n- forward\n\n## LukeForTokenClassification",
  "[[autodoc]] LukeForTokenClassification\n- forward\n\n## LukeForQuestionAnswering\n\n[[autodoc]] LukeForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FocalNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.",
  "FocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision.\nThe authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation.\n\nThe abstract from the paper is the following:\n\n*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its",
  "content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3.*",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/microsoft/FocalNet).\n\n## FocalNetConfig\n\n[[autodoc]] FocalNetConfig\n\n## FocalNetModel\n\n[[autodoc]] FocalNetModel\n- forward\n\n## FocalNetForMaskedImageModeling\n\n[[autodoc]] FocalNetForMaskedImageModeling\n- forward\n\n## FocalNetForImageClassification\n\n[[autodoc]] FocalNetForImageClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ERNIE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\nERNIE is a series of powerful models proposed by baidu, especially in Chinese tasks,\nincluding [ERNIE1.0](https://arxiv.org/abs/1904.09223), [ERNIE2.0](https://ojs.aaai.org/index.php/AAAI/article/view/6428),",
  "[ERNIE3.0](https://arxiv.org/abs/2107.02137), [ERNIE-Gram](https://arxiv.org/abs/2010.12148), [ERNIE-health](https://arxiv.org/abs/2110.07244), etc.\n\nThese models are contributed by [nghuyong](https://huggingface.co/nghuyong) and the official code can be found in [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) (in PaddlePaddle).\n\n### Usage example\nTake `ernie-1.0-base-zh` as an example:\n\n```Python\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\nmodel = AutoModel.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n```\n\n### Model checkpoints\n\n|     Model Name      | Language |           Description           |\n|:-------------------:|:--------:|:-------------------------------:|\n|  ernie-1.0-base-zh  | Chinese  | Layer:12, Heads:12, Hidden:768  |\n|  ernie-2.0-base-en  | English  | Layer:12, Heads:12, Hidden:768  |\n| ernie-2.0-large-en  | English  | Layer:24, Heads:16, Hidden:1024 |\n|  ernie-3.0-base-zh  | Chinese  | Layer:12, Heads:12, Hidden:768  |\n| ernie-3.0-medium-zh | Chinese  |  Layer:6, Heads:12, Hidden:768  |\n|  ernie-3.0-mini-zh  | Chinese  |  Layer:6, Heads:12, Hidden:384  |",
  "| ernie-3.0-micro-zh  | Chinese  |  Layer:4, Heads:12, Hidden:384  |\n|  ernie-3.0-nano-zh  | Chinese  |  Layer:4, Heads:12, Hidden:312  |\n|   ernie-health-zh   | Chinese  | Layer:12, Heads:12, Hidden:768  |\n|    ernie-gram-zh    | Chinese  | Layer:12, Heads:12, Hidden:768  |\n\nYou can find all the supported models from huggingface's model hub: [huggingface.co/nghuyong](https://huggingface.co/nghuyong), and model details from paddle's official\nrepo: [PaddleNLP](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/ERNIE/contents.html)\nand [ERNIE](https://github.com/PaddlePaddle/ERNIE/blob/repro).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## ErnieConfig\n\n[[autodoc]] ErnieConfig\n- all\n\n## Ernie specific outputs\n\n[[autodoc]] models.ernie.modeling_ernie.ErnieForPreTrainingOutput\n\n## ErnieModel",
  "[[autodoc]] ErnieModel\n- forward\n\n## ErnieForPreTraining\n\n[[autodoc]] ErnieForPreTraining\n- forward\n\n## ErnieForCausalLM\n\n[[autodoc]] ErnieForCausalLM\n- forward\n\n## ErnieForMaskedLM\n\n[[autodoc]] ErnieForMaskedLM\n- forward\n\n## ErnieForNextSentencePrediction\n\n[[autodoc]] ErnieForNextSentencePrediction\n- forward\n\n## ErnieForSequenceClassification\n\n[[autodoc]] ErnieForSequenceClassification\n- forward\n\n## ErnieForMultipleChoice\n\n[[autodoc]] ErnieForMultipleChoice\n- forward\n\n## ErnieForTokenClassification\n\n[[autodoc]] ErnieForTokenClassification\n- forward\n\n## ErnieForQuestionAnswering\n\n[[autodoc]] ErnieForQuestionAnswering\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3.\n\nThe abstract from the paper is the following:\n\n*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This\nrestricted form of supervision limits their generality and usability since additional labeled data is needed to specify",
  "any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a\nmuch broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes\nwith which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet. After pre-training, natural language is used to reference\nlearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study\nthe performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks\nsuch as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need\nfor any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot\nwithout needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained",
  "model weights at this https URL.*\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).\n\n## Usage tips and example\n\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score.\n\nTo feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded. A [CLS] token is added to serve as representation of an entire image. The authors\nalso add absolute position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder.\nThe [`CLIPImageProcessor`] can be used to resize (or rescale) and normalize images for the model.\n\nThe [`CLIPTokenizer`] is used to encode the text. The [`CLIPProcessor`] wraps",
  "[`CLIPImageProcessor`] and [`CLIPTokenizer`] into a single instance to both\nencode the text and prepare the images. The following example shows how to get the image-text similarity scores using\n[`CLIPProcessor`] and [`CLIPModel`].\n\n\n```python\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import CLIPProcessor, CLIPModel\n\n>>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```\n\n\n### Combining CLIP and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```",
  "Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16`)\n\n<Tip warning={true}>\n\nFor small batch sizes, you might notice a slowdown in your model when using flash attention. Refer to the section [Expected speedups with Flash Attention and SDPA](#Expected-speedups-with-Flash-Attention-and-SDPA) below and select an appropriate attention implementation.\n\n</Tip>\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> import requests\n>>> from PIL import Image\n\n>>> from transformers import CLIPProcessor, CLIPModel\n\n>>> device = \"cuda\"\n>>> torch_dtype = torch.float16\n\n>>> model = CLIPModel.from_pretrained(\n...     \"openai/clip-vit-base-patch32\",\n...     attn_implementation=\"flash_attention_2\",\n...     device_map=device,\n...     torch_dtype=torch_dtype,\n... )\n>>> processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)",
  ">>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n>>> inputs.to(device)\n\n>>> with torch.no_grad():\n...     with torch.autocast(device):\n...         outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n>>> print(probs)\ntensor([[0.9946, 0.0052]], device='cuda:0', dtype=torch.float16)\n```\n\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set",
  "`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```python\nfrom transformers import CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\n### Expected speedups with Flash Attention and SDPA\n\nOn a local benchmark (NVIDIA A10G, PyTorch 2.3.1+cu121) with `float16`, we saw the following speedups during inference for `\"openai/clip-vit-large-patch14\"` checkpoint ([code](https://gist.github.com/qubvel/ac691a54e54f9fae8144275f866a7ff8)):\n\n#### CLIPTextModel\n\n|   Num text labels |   Eager (s/iter) |   FA2 (s/iter) |   FA2 speedup |   SDPA (s/iter) |   SDPA speedup |\n|------------------:|-----------------:|---------------:|--------------:|----------------:|---------------:|\n|                 4 |            0.009 |          0.012 |         0.737 |           0.007 |          1.269 |\n|                16 |            0.009 |          0.014 |         0.659 |           0.008 |          1.187 |",
  "|                32 |            0.018 |          0.021 |         0.862 |           0.016 |          1.142 |\n|                64 |            0.034 |          0.034 |         1.001 |           0.03  |          1.163 |\n|               128 |            0.063 |          0.058 |         1.09  |           0.054 |          1.174 |\n\n![clip_text_model_viz_3](https://github.com/user-attachments/assets/e9826b43-4e66-4f4c-952b-af4d90bd38eb)\n\n#### CLIPVisionModel\n\n|   Image batch size |   Eager (s/iter) |   FA2 (s/iter) |   FA2 speedup |   SDPA (s/iter) |   SDPA speedup |\n|-------------------:|-----------------:|---------------:|--------------:|----------------:|---------------:|\n|                  1 |            0.016 |          0.013 |         1.247 |           0.012 |          1.318 |\n|                  4 |            0.025 |          0.021 |         1.198 |           0.021 |          1.202 |\n|                 16 |            0.093 |          0.075 |         1.234 |           0.075 |          1.24  |\n|                 32 |            0.181 |          0.147 |         1.237 |           0.146 |          1.241 |",
  "![clip_image_model_viz_3](https://github.com/user-attachments/assets/50a36206-e3b9-4adc-ac8e-926b8b071d63)\n\n#### CLIPModel\n\n|   Image batch size |   Num text labels |   Eager (s/iter) |   FA2 (s/iter) |   FA2 speedup |   SDPA (s/iter) |   SDPA speedup |\n|-------------------:|------------------:|-----------------:|---------------:|--------------:|----------------:|---------------:|\n|                  1 |                 4 |            0.025 |          0.026 |         0.954 |           0.02  |          1.217 |\n|                  1 |                16 |            0.026 |          0.028 |         0.918 |           0.02  |          1.287 |\n|                  1 |                64 |            0.042 |          0.046 |         0.906 |           0.036 |          1.167 |\n|                  4 |                 4 |            0.028 |          0.033 |         0.849 |           0.024 |          1.189 |\n|                  4 |                16 |            0.034 |          0.035 |         0.955 |           0.029 |          1.169 |\n|                  4 |                64 |            0.059 |          0.055 |         1.072 |           0.05  |          1.179 |",
  "|                 16 |                 4 |            0.096 |          0.088 |         1.091 |           0.078 |          1.234 |\n|                 16 |                16 |            0.102 |          0.09  |         1.129 |           0.083 |          1.224 |\n|                 16 |                64 |            0.127 |          0.11  |         1.157 |           0.105 |          1.218 |\n|                 32 |                 4 |            0.185 |          0.159 |         1.157 |           0.149 |          1.238 |\n|                 32 |                16 |            0.19  |          0.162 |         1.177 |           0.154 |          1.233 |\n|                 32 |                64 |            0.216 |          0.181 |         1.19  |           0.176 |          1.228 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with CLIP.",
  "- [Fine tuning CLIP with Remote Sensing (Satellite) images and captions](https://huggingface.co/blog/fine-tune-clip-rsicd), a blog post about how to fine-tune CLIP with [RSICD dataset](https://github.com/201528014227051/RSICD_optimal) and comparison of performance changes due to data augmentation.\n- This [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) shows how to train a CLIP-like vision-text dual encoder model using a pre-trained vision and text encoder using [COCO dataset](https://cocodataset.org/#home).\n\n<PipelineTag pipeline=\"image-to-text\"/>\n\n- A [notebook](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing) on how to use a pretrained CLIP for inference with beam search for image captioning. 🌎\n\n**Image retrieval**\n\n- A [notebook](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing) on image retrieval using pretrained CLIP and computing MRR(Mean Reciprocal Rank) score. 🌎",
  "- A [notebook](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb) on image retrieval and showing the similarity score. 🌎\n- A [notebook](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing) on how to map images and texts to the same vector space using Multilingual CLIP. 🌎\n- A [notebook](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR) on how to run CLIP on semantic image search using [Unsplash](https://unsplash.com) and [TMDB](https://www.themoviedb.org/) datasets. 🌎\n\n**Explainability**\n\n- A [notebook](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb) on how to visualize similarity between input token and image segment. 🌎\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.\nThe resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## CLIPConfig\n\n[[autodoc]] CLIPConfig\n- from_text_vision_configs\n\n## CLIPTextConfig",
  "[[autodoc]] CLIPTextConfig\n\n## CLIPVisionConfig\n\n[[autodoc]] CLIPVisionConfig\n\n## CLIPTokenizer\n\n[[autodoc]] CLIPTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## CLIPTokenizerFast\n\n[[autodoc]] CLIPTokenizerFast\n\n## CLIPImageProcessor\n\n[[autodoc]] CLIPImageProcessor\n- preprocess\n\n## CLIPImageProcessorFast\n\n[[autodoc]] CLIPImageProcessorFast\n- preprocess\n\n## CLIPFeatureExtractor\n\n[[autodoc]] CLIPFeatureExtractor\n\n## CLIPProcessor\n\n[[autodoc]] CLIPProcessor\n\n<frameworkcontent>\n<pt>\n\n## CLIPModel\n\n[[autodoc]] CLIPModel\n- forward\n- get_text_features\n- get_image_features\n\n## CLIPTextModel\n\n[[autodoc]] CLIPTextModel\n- forward\n\n## CLIPTextModelWithProjection\n\n[[autodoc]] CLIPTextModelWithProjection\n- forward\n\n## CLIPVisionModelWithProjection\n\n[[autodoc]] CLIPVisionModelWithProjection\n- forward\n\n## CLIPVisionModel\n\n[[autodoc]] CLIPVisionModel\n- forward\n\n## CLIPForImageClassification\n\n[[autodoc]] CLIPForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFCLIPModel\n\n[[autodoc]] TFCLIPModel\n- call\n- get_text_features\n- get_image_features\n\n## TFCLIPTextModel\n\n[[autodoc]] TFCLIPTextModel\n- call\n\n## TFCLIPVisionModel",
  "[[autodoc]] TFCLIPVisionModel\n- call\n\n</tf>\n<jax>\n\n## FlaxCLIPModel\n\n[[autodoc]] FlaxCLIPModel\n- __call__\n- get_text_features\n- get_image_features\n\n## FlaxCLIPTextModel\n\n[[autodoc]] FlaxCLIPTextModel\n- __call__\n\n## FlaxCLIPTextModelWithProjection\n\n[[autodoc]] FlaxCLIPTextModelWithProjection\n- __call__\n\n## FlaxCLIPVisionModel\n\n[[autodoc]] FlaxCLIPVisionModel\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TextNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The TextNet model was proposed in [FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation](https://arxiv.org/abs/2111.02394) by Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu. TextNet is a vision backbone useful for text detection tasks. It is the result of neural architecture search (NAS) on backbones with reward function as text detection task (to provide powerful features for text detection).\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/fast_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> TextNet backbone as part of FAST. Taken from the <a href=\"https://arxiv.org/abs/2111.02394\">original paper.</a> </small>\n\nThis model was contributed by [Raghavan](https://huggingface.co/Raghavan), [jadechoghari](https://huggingface.co/jadechoghari) and [nielsr](https://huggingface.co/nielsr).\n\n## Usage tips\n\nTextNet is mainly used as a backbone network for the architecture search of text detection. Each stage of the backbone network is comprised of a stride-2 convolution and searchable blocks.",
  "Specifically, we present a layer-level candidate set, defined as {conv3×3, conv1×3, conv3×1, identity}. As the 1×3 and 3×1 convolutions have asymmetric kernels and oriented structure priors, they may help to capture the features of extreme aspect-ratio and rotated text lines.\n\nTextNet is the backbone for Fast, but can also be used as an efficient text/image classification, we add a `TextNetForImageClassification` as is it would allow people to train an image classifier on top of the pre-trained textnet weights\n\n## TextNetConfig\n\n[[autodoc]] TextNetConfig\n\n## TextNetImageProcessor\n\n[[autodoc]] TextNetImageProcessor\n- preprocess\n\n## TextNetModel\n\n[[autodoc]] TextNetModel\n- forward\n\n## TextNetForImageClassification\n\n[[autodoc]] TextNetForImageClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RT-DETR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\n\nThe RT-DETR model was proposed in [DETRs Beat YOLOs on Real-time Object Detection](https://arxiv.org/abs/2304.08069) by Wenyu Lv, Yian Zhao, Shangliang Xu, Jinman Wei, Guanzhong Wang, Cheng Cui, Yuning Du, Qingqing Dang, Yi Liu.",
  "RT-DETR is an object detection model that stands for \"Real-Time DEtection Transformer.\" This model is designed to perform object detection tasks with a focus on achieving real-time performance while maintaining high accuracy. Leveraging the transformer architecture, which has gained significant popularity in various fields of deep learning, RT-DETR processes images to identify and locate multiple objects within them.\n\nThe abstract from the paper is the following:",
  "*Recently, end-to-end transformer-based detectors (DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/rt_detr_overview.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> RT-DETR performance relative to YOLO models. Taken from the <a href=\"https://arxiv.org/abs/2304.08069\">original paper.</a> </small>\n\nThe model version was contributed by [rafaelpadilla](https://huggingface.co/rafaelpadilla) and [sangbumchoi](https://github.com/SangbumChoi). The original code can be found [here](https://github.com/lyuwenyu/RT-DETR/).\n\n\n## Usage tips\n\nInitially, an image is processed using a pre-trained convolutional neural network, specifically a Resnet-D variant as referenced in the original code. This network extracts features from the final three layers of the architecture. Following this, a hybrid encoder is employed to convert the multi-scale features into a sequential array of image features. Then, a decoder, equipped with auxiliary prediction heads is used to refine the object queries. This process facilitates the direct generation of bounding boxes, eliminating the need for any additional post-processing to acquire the logits and coordinates for the bounding boxes.\n\n```py\n>>> import torch",
  ">>> import requests\n\n>>> from PIL import Image\n>>> from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n>>> model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3)\n\n>>> for result in results:\n...     for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n...         score, label = score.item(), label_id.item()\n...         box = [round(i, 2) for i in box.tolist()]\n...         print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\nsofa: 0.97 [0.14, 0.38, 640.13, 476.21]\ncat: 0.96 [343.38, 24.28, 640.14, 371.5]\ncat: 0.96 [13.23, 54.18, 318.98, 472.22]\nremote: 0.95 [40.11, 73.44, 175.96, 118.48]\nremote: 0.92 [333.73, 76.58, 369.97, 186.99]\n```",
  "## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with RT-DETR.\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- Scripts for finetuning [`RTDetrForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection).\n- Notebooks regarding inference and fine-tuning RT-DETR on a custom dataset can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/RT-DETR). 🌎\n\n## RTDetrConfig\n\n[[autodoc]] RTDetrConfig\n\n## RTDetrResNetConfig\n\n[[autodoc]] RTDetrResNetConfig\n\n## RTDetrImageProcessor\n\n[[autodoc]] RTDetrImageProcessor\n- preprocess\n- post_process_object_detection\n\n## RTDetrImageProcessorFast\n\n[[autodoc]] RTDetrImageProcessorFast\n- preprocess\n- post_process_object_detection\n\n## RTDetrModel\n\n[[autodoc]] RTDetrModel\n- forward\n\n## RTDetrForObjectDetection\n\n[[autodoc]] RTDetrForObjectDetection\n- forward\n\n## RTDetrResNetBackbone\n\n[[autodoc]] RTDetrResNetBackbone\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Falcon3\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nFalcon3 represents a natural evolution from previous releases, emphasizing expanding the models' science, math, and code capabilities. This iteration includes five base models: Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base. In developing these models, we incorporated several key innovations aimed at improving the models' performances while reducing training costs:\n\nOne pre-training: We conducted a single large-scale pretraining run on the 7B model, using 2048 H100 GPU chips, leveraging 14 trillion tokens featuring web, code, STEM, and curated high-quality and multilingual data.\nDepth up-scaling for improved reasoning: Building on recent studies on the effects of model depth, we upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2TT of high-quality data. This yielded Falcon3-10B-Base which achieves state-of-the-art zero-shot and few-shot performance for models under 13B parameters.",
  "Knowledge distillation for better tiny models: To provide compact and efficient alternatives, we developed Falcon3-1B-Base and Falcon3-3B-Base by leveraging pruning and knowledge distillation techniques, using less than 100GT of curated high-quality data, thereby redefining pre-training efficiency.\n\n## Resources\n- [Blog post](https://huggingface.co/blog/falcon3)\n- [Models on Huggingface](https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026)",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Idefics2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Idefics2 model was proposed in [What matters when building vision-language models?](https://arxiv.org/abs/2405.02246) by Léo Tronchon, Hugo Laurencon, Victor Sanh. The accompanying blog post can be found [here](https://huggingface.co/blog/idefics2).\n\nIdefics2 is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text\noutputs. The model can answer questions about images, describe visual content, create stories grounded on multiple\nimages, or simply behave as a pure language model without visual inputs. It improves upon IDEFICS-1, notably on\ndocument understanding, OCR, or visual reasoning. Idefics2 is lightweight (8 billion parameters) and treats\nimages in their native aspect ratio and resolution, which allows for varying inference efficiency.\n\nThe abstract from the paper is the following:",
  "*The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/idefics2_architecture.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Idefics2 architecture. Taken from the <a href=\"https://arxiv.org/abs/2405.02246\">original paper.</a> </small>\n\nThis model was contributed by [amyeroberts](https://huggingface.co/amyeroberts).\nThe original code can be found [here](https://huggingface.co/HuggingFaceM4/idefics2).\n\n## Usage tips\n\n- Each sample can contain multiple images, and the number of images can vary between samples. The processor will pad the inputs to the maximum number of images in a batch for input to the model.\n- The processor has a `do_image_splitting` option. If `True`, each input image will be split into 4 sub-images, and concatenated with the original to form 5 images. This is useful for increasing model performance. Make sure `processor.image_processor.do_image_splitting` is set to `False` if the model was not trained with this option.\n- `text` passed to the processor should have the `<image>` tokens where the images should be inserted. And `<end_of_utterance>` at the end of each utterance if the text is a chat message.\n- The processor has its own `apply_chat_template` method to convert chat messages to text that can then be passed as `text` to the processor.",
  "Example of how to use the processor on chat messages:\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import Idefics2Processor, Idefics2ForConditionalGeneration\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nurl_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nurl_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n\nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\nimages = [image_1, image_2]\n\nmessages = [{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What’s the difference between these two images?\"},\n{\"type\": \"image\"},\n{\"type\": \"image\"},\n],\n}]\n\nprocessor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\nmodel = Idefics2ForConditionalGeneration.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\nmodel.to(device)\n\n# at inference time, one needs to pass `add_generation_prompt=True` in order to make sure the model completes the prompt\ntext = processor.apply_chat_template(messages, add_generation_prompt=True)\nprint(text)\n# 'User: What’s the difference between these two images?<image><image><end_of_utterance>\\nAssistant:'",
  "inputs = processor(images=images, text=text, return_tensors=\"pt\").to(device)\n\ngenerated_text = model.generate(**inputs, max_new_tokens=500)\ngenerated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]\nprint(\"Generated text:\", generated_text)\n```\n\n- During training, it's important to determine which tokens the model should not learn. For Idefics2, this typically comes down to the image and padding tokens. This means that one can create the labels as follows:\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import Idefics2Processor, Idefics2ForConditionalGeneration\nimport torch\n\nurl_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nurl_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n\nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\nimages = [image_1, image_2]\n\nmessages = [{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What’s the difference between these two images?\"},\n{\"type\": \"image\"},\n{\"type\": \"image\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [",
  "{\"type\": \"text\", \"text\": \"The difference is that one image is about dogs and the other one about cats.\"},\n],\n}]\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprocessor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\nmodel = Idefics2ForConditionalGeneration.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\nmodel.to(device)\n\ntext = processor.apply_chat_template(messages, add_generation_prompt=False)\ninputs = processor(images=images, text=text, return_tensors=\"pt\").to(device)\n\nlabels = inputs.input_ids.clone()\nlabels[labels == processor.tokenizer.pad_token_id] = -100\nlabels[labels == model.config.image_token_id] = -100\n\ninputs[\"labels\"] = labels\n\noutputs = model(**inputs)\nloss = outputs.loss\nloss.backward()\n```\n\nDo note that when training Idefics2 on multi-turn conversations between a user and an assistant, one typically also sets all the tokens corresponding to the user messages to -100.\n\n## Model optimizations: Flash Attention",
  "The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). Make also sure to load your model in half-precision (e.g. `torch.float16`)\n\nTo load and run a model using Flash Attention-2, simply change the code snippet above with the following change:\n\n```diff\nmodel = Idefics2ForConditionalGeneration.from_pretrained(\n\"HuggingFaceM4/idefics2-8b\",\n+    torch_dtype=torch.float16,\n+    attn_implementation=\"flash_attention_2\",\n).to(device)\n```\n\n## Shrinking down Idefics2 using quantization",
  "As the Idefics2 model has 8 billion parameters, that would require about 16GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization.md). If the model is quantized to 4 bits (or half a byte per parameter), that requires only about 3.5GB of RAM.\n\nQuantizing a model is as simple as passing a `quantization_config` to the model. One can change the code snippet above with the changes below. We'll leverage the BitsAndyBytes quantization (but refer to [this page](../quantization.md) for other quantization methods):\n\n```diff\n+ from transformers import BitsAndBytesConfig\n\n+ quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_use_double_quant=True,\n+    bnb_4bit_compute_dtype=torch.float16\n+ )\nmodel = Idefics2ForConditionalGeneration.from_pretrained(\n\"HuggingFaceM4/idefics2-8b\",\n+    torch_dtype=torch.float16,\n+    quantization_config=quantization_config,\n).to(device)\n```\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Idefics2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- A notebook on how to fine-tune Idefics2 on a custom dataset using the [Trainer](../main_classes/trainer.md) can be found [here](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing). It supports both full fine-tuning as well as (quantized) LoRa.\n- A script regarding how to fine-tune Idefics2 using the TRL library can be found [here](https://gist.github.com/edbeeching/228652fc6c2b29a1641be5a5778223cb).\n- Demo notebook regarding fine-tuning Idefics2 for JSON extraction use cases can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Idefics2). 🌎\n\n## Idefics2Config\n\n[[autodoc]] Idefics2Config\n\n\n## Idefics2Model\n\n[[autodoc]] Idefics2Model\n- forward\n\n\n## Idefics2ForConditionalGeneration\n\n[[autodoc]] Idefics2ForConditionalGeneration\n- forward\n\n\n## Idefics2ImageProcessor",
  "[[autodoc]] Idefics2ImageProcessor\n- preprocess\n\n\n## Idefics2Processor\n[[autodoc]] Idefics2Processor\n- __call__",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Informer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.",
  "This method introduces a Probabilistic Attention mechanism to select the \"active\" queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention.\n\nThe abstract from the paper is the following:",
  "*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.*",
  "This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).\nThe original code can be found [here](https://github.com/zhouhaoyi/Informer2020).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- Check out the Informer blog-post in HuggingFace blog: [Multivariate Probabilistic Time Series Forecasting with Informer](https://huggingface.co/blog/informer)\n\n## InformerConfig\n\n[[autodoc]] InformerConfig\n\n## InformerModel\n\n[[autodoc]] InformerModel\n- forward\n\n## InformerForPrediction\n\n[[autodoc]] InformerForPrediction\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Granite Vision\n\n## Overview\n\nThe Granite Vision model is a variant of [LLaVA-NeXT](llava_next), leveraging a [Granite](granite) language model alongside a [SigLIP](SigLIP) visual encoder. It utilizes multiple concatenated vision hidden states as its image features, similar to [VipLlava](vipllava). It also uses a larger set of image grid pinpoints than the original LlaVa-NeXT models to support additional aspect ratios.\n\nTips:",
  "- This model is loaded into Transformers as an instance of LlaVA-Next. The usage and tips from [LLaVA-NeXT](llava_next) apply to this model as well.\n\n- You can apply the chat template on the tokenizer / processor in the same way as well. Example chat format:\n```bash\n\"<|user|>\\nWhat’s shown in this image?\\n<|assistant|>\\nThis image shows a red stop sign.<|end_of_text|><|user|>\\nDescribe the image in more details.\\n<|assistant|>\\n\"\n```\n\nSample inference:\n```python\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n\nmodel_path = \"ibm-granite/granite-vision-3.1-2b-preview\"\nprocessor = LlavaNextProcessor.from_pretrained(model_path)\n\nmodel = LlavaNextForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n\n# prepare image and text prompt, using the appropriate prompt template\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": url},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,",
  "return_dict=True,\nreturn_tensors=\"pt\"\n).to(\"cuda\")\n\n\n# autoregressively complete prompt\noutput = model.generate(**inputs, max_new_tokens=100)\n\nprint(processor.decode(output[0], skip_special_tokens=True))\n```\n\nThis model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9944).\n\n## LlavaNextConfig\n\n[[autodoc]] LlavaNextConfig\n\n## LlavaNextImageProcessor\n\n[[autodoc]] LlavaNextImageProcessor\n- preprocess\n\n## LlavaNextProcessor\n\n[[autodoc]] LlavaNextProcessor\n\n## LlavaNextForConditionalGeneration\n\n[[autodoc]] LlavaNextForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# HerBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe HerBERT model was proposed in [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and\nIreneusz Gawlik. It is a BERT-based Language Model trained on Polish Corpora using only MLM objective with dynamic\nmasking of whole words.\n\nThe abstract from the paper is the following:\n\n*In recent years, a series of Transformer-based models unlocked major improvements in general natural language\nunderstanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which\nallow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of\nlanguages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language\nunderstanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing\ndatasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new",
  "sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and\npromote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and\napplications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,\nwhich has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an\nextensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based\nmodels.*\n\nThis model was contributed by [rmroczkowski](https://huggingface.co/rmroczkowski). The original code can be found\n[here](https://github.com/allegro/HerBERT).\n\n\n## Usage example\n\n```python\n>>> from transformers import HerbertTokenizer, RobertaModel\n\n>>> tokenizer = HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n>>> model = RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")\n\n>>> encoded_input = tokenizer.encode(\"Kto ma lepszą sztukę, ma lepszy rząd – to jasne.\", return_tensors=\"pt\")\n>>> outputs = model(encoded_input)",
  ">>> # HerBERT can also be loaded using AutoTokenizer and AutoModel:\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n>>> model = AutoModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")\n```\n\n<Tip>\n\nHerbert implementation is the same as `BERT` except for the tokenization method. Refer to [BERT documentation](bert)\nfor API reference and examples.\n\n</Tip>\n\n## HerbertTokenizer\n\n[[autodoc]] HerbertTokenizer\n\n## HerbertTokenizerFast\n\n[[autodoc]] HerbertTokenizerFast",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# EnCodec\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n\nThe abstract from the paper is the following:",
  "*We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.*",
  "This model was contributed by [Matthijs](https://huggingface.co/Matthijs), [Patrick Von Platen](https://huggingface.co/patrickvonplaten) and [Arthur Zucker](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/facebookresearch/encodec).\n\n## Usage example\n\nHere is a quick example of how to encode and decode an audio using this model:\n\n```python\n>>> from datasets import load_dataset, Audio\n>>> from transformers import EncodecModel, AutoProcessor\n>>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n>>> processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n>>> librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n>>> audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n>>> inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n\n>>> encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])",
  ">>> audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n>>> # or the equivalent with a forward pass\n>>> audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n```\n\n## EncodecConfig\n\n[[autodoc]] EncodecConfig\n\n## EncodecFeatureExtractor\n\n[[autodoc]] EncodecFeatureExtractor\n- __call__\n\n## EncodecModel\n\n[[autodoc]] EncodecModel\n- decode\n- encode\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BLIP-2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by",
  "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer\nencoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198), an 80 billion parameter model, by 8.7%\non zero-shot VQAv2 with 54x fewer trainable parameters.\n\nThe abstract from the paper is the following:",
  "*The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg\"",
  "alt=\"drawing\" width=\"600\"/>\n\n<small> BLIP-2 architecture. Taken from the <a href=\"https://arxiv.org/abs/2301.12597\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207).\n\n## Usage tips\n\n- BLIP-2 can be used for conditional text generation given an image and an optional text prompt. At inference time, it's recommended to use the [`generate`] method.\n- One can use [`Blip2Processor`] to prepare images for the model, and decode the predicted tokens ID's back to text.\n\n> [!NOTE]",
  "> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BLIP-2.",
  "- Demo notebooks for BLIP-2 for image captioning, visual question answering (VQA) and chat-like conversations can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## Blip2Config\n\n[[autodoc]] Blip2Config\n- from_vision_qformer_text_configs\n\n## Blip2VisionConfig\n\n[[autodoc]] Blip2VisionConfig\n\n## Blip2QFormerConfig\n\n[[autodoc]] Blip2QFormerConfig\n\n## Blip2Processor\n\n[[autodoc]] Blip2Processor\n\n## Blip2VisionModel\n\n[[autodoc]] Blip2VisionModel\n- forward\n\n## Blip2QFormerModel\n\n[[autodoc]] Blip2QFormerModel\n- forward\n\n## Blip2Model\n\n[[autodoc]] Blip2Model\n- forward\n- get_text_features\n- get_image_features\n- get_qformer_features\n\n## Blip2ForConditionalGeneration\n\n[[autodoc]] Blip2ForConditionalGeneration\n- forward\n- generate\n\n## Blip2ForImageTextRetrieval\n\n[[autodoc]] Blip2ForImageTextRetrieval\n- forward\n\n## Blip2TextModelWithProjection\n\n[[autodoc]] Blip2TextModelWithProjection\n\n## Blip2VisionModelWithProjection",
  "[[autodoc]] Blip2VisionModelWithProjection",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nThe abstract from the paper is the following:\n\n*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.*",
  "*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/google-research/bert).\n\n## Usage tips\n\n- BERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n- BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.\n- Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\n\n* a special mask token with probability 0.8\n* a random token different from the one masked with probability 0.1",
  "* the same token with probability 0.1\n\n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import BertModel",
  "model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-80GB, CPUx12, RAM 96.6GB, PyTorch 2.2.0, OS Ubuntu 22.04) with `float16`, we saw the\nfollowing speedups during training and inference.\n\n#### Training\n\n|batch_size|seq_len|Time per batch (eager - s)|Time per batch (sdpa - s)|Speedup (%)|Eager peak mem (MB)|sdpa peak mem (MB)|Mem saving (%)|\n|----------|-------|--------------------------|-------------------------|-----------|-------------------|------------------|--------------|\n|4         |256    |0.023                     |0.017                    |35.472     |939.213            |764.834           |22.800        |\n|4         |512    |0.023                     |0.018                    |23.687     |1970.447           |1227.162          |60.569        |\n|8         |256    |0.023                     |0.018                    |23.491     |1594.295           |1226.114          |30.028        |",
  "|8         |512    |0.035                     |0.025                    |43.058     |3629.401           |2134.262          |70.054        |\n|16        |256    |0.030                     |0.024                    |25.583     |2874.426           |2134.262          |34.680        |\n|16        |512    |0.064                     |0.044                    |46.223     |6964.659           |3961.013          |75.830        |\n\n#### Inference\n\n|batch_size|seq_len|Per token latency eager (ms)|Per token latency SDPA (ms)|Speedup (%)|Mem eager (MB)|Mem BT (MB)|Mem saved (%)|\n|----------|-------|----------------------------|---------------------------|-----------|--------------|-----------|-------------|\n|1         |128    |5.736                       |4.987                      |15.022     |282.661       |282.924    |-0.093       |\n|1         |256    |5.689                       |4.945                      |15.055     |298.686       |298.948    |-0.088       |\n|2         |128    |6.154                       |4.982                      |23.521     |314.523       |314.785    |-0.083       |",
  "|2         |256    |6.201                       |4.949                      |25.303     |347.546       |347.033    |0.148        |\n|4         |128    |6.049                       |4.987                      |21.305     |378.895       |379.301    |-0.107       |\n|4         |256    |6.285                       |5.364                      |17.166     |443.209       |444.382    |-0.264       |\n\n\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on [BERT Text Classification in a different language](https://www.philschmid.de/bert-text-classification-in-a-different-language).\n- A notebook for [Finetuning BERT (and friends) for multi-label text classification](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb).",
  "- A notebook on how to [Finetune BERT for multi-label classification using PyTorch](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb). 🌎\n- A notebook on how to [warm-start an EncoderDecoder model with BERT for summarization](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb).\n- [`BertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).\n- [`TFBertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).",
  "- [`FlaxBertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- A blog post on how to use [Hugging Face Transformers with Keras: Fine-tune a non-English BERT for Named Entity Recognition](https://www.philschmid.de/huggingface-transformers-keras-tf).\n- A notebook for [Finetuning BERT for named-entity recognition](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb) using only the first wordpiece of each word in the word label during tokenization. To propagate the label of the word to all wordpieces, see this [version](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb) of the notebook instead.",
  "- [`BertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).\n- [`TFBertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n- [`FlaxBertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).\n- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Token classification task guide](../tasks/token_classification)\n\n<PipelineTag pipeline=\"fill-mask\"/>",
  "- [`BertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFBertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxBertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n<PipelineTag pipeline=\"question-answering\"/>",
  "- [`BertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n- [`TFBertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxBertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n**Multiple choice**",
  "- [`BertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n- [`TFBertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n⚡️ **Inference**\n- A blog post on how to [Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker).\n- A blog post on how to [Accelerate BERT inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/bert-deepspeed-inference).\n\n⚙️ **Pretraining**\n- A blog post on [Pre-Training BERT with Hugging Face Transformers and Habana Gaudi](https://www.philschmid.de/pre-training-bert-habana).\n\n🚀 **Deploy**",
  "- A blog post on how to [Convert Transformers to ONNX with Hugging Face Optimum](https://www.philschmid.de/convert-transformers-to-onnx).\n- A blog post on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi#conclusion).\n- A blog post on [Autoscaling BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker-advanced).\n- A blog post on [Serverless BERT with HuggingFace, AWS Lambda, and Docker](https://www.philschmid.de/serverless-bert-with-huggingface-aws-lambda-docker).\n- A blog post on [Hugging Face Transformers BERT fine-tuning using Amazon SageMaker and Training Compiler](https://www.philschmid.de/huggingface-amazon-sagemaker-training-compiler).\n- A blog post on [Task-specific knowledge distillation for BERT using Transformers & Amazon SageMaker](https://www.philschmid.de/knowledge-distillation-bert-transformers).\n\n## BertConfig\n\n[[autodoc]] BertConfig\n- all\n\n## BertTokenizer\n\n[[autodoc]] BertTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask",
  "- create_token_type_ids_from_sequences\n- save_vocabulary\n\n<frameworkcontent>\n<pt>\n\n## BertTokenizerFast\n\n[[autodoc]] BertTokenizerFast\n\n</pt>\n<tf>\n\n## TFBertTokenizer\n\n[[autodoc]] TFBertTokenizer\n\n</tf>\n</frameworkcontent>\n\n## Bert specific outputs\n\n[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput\n\n[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput\n\n[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput\n\n\n<frameworkcontent>\n<pt>\n\n## BertModel\n\n[[autodoc]] BertModel\n- forward\n\n## BertForPreTraining\n\n[[autodoc]] BertForPreTraining\n- forward\n\n## BertLMHeadModel\n\n[[autodoc]] BertLMHeadModel\n- forward\n\n## BertForMaskedLM\n\n[[autodoc]] BertForMaskedLM\n- forward\n\n## BertForNextSentencePrediction\n\n[[autodoc]] BertForNextSentencePrediction\n- forward\n\n## BertForSequenceClassification\n\n[[autodoc]] BertForSequenceClassification\n- forward\n\n## BertForMultipleChoice\n\n[[autodoc]] BertForMultipleChoice\n- forward\n\n## BertForTokenClassification\n\n[[autodoc]] BertForTokenClassification\n- forward\n\n## BertForQuestionAnswering\n\n[[autodoc]] BertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFBertModel\n\n[[autodoc]] TFBertModel\n- call\n\n## TFBertForPreTraining",
  "[[autodoc]] TFBertForPreTraining\n- call\n\n## TFBertModelLMHeadModel\n\n[[autodoc]] TFBertLMHeadModel\n- call\n\n## TFBertForMaskedLM\n\n[[autodoc]] TFBertForMaskedLM\n- call\n\n## TFBertForNextSentencePrediction\n\n[[autodoc]] TFBertForNextSentencePrediction\n- call\n\n## TFBertForSequenceClassification\n\n[[autodoc]] TFBertForSequenceClassification\n- call\n\n## TFBertForMultipleChoice\n\n[[autodoc]] TFBertForMultipleChoice\n- call\n\n## TFBertForTokenClassification\n\n[[autodoc]] TFBertForTokenClassification\n- call\n\n## TFBertForQuestionAnswering\n\n[[autodoc]] TFBertForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxBertModel\n\n[[autodoc]] FlaxBertModel\n- __call__\n\n## FlaxBertForPreTraining\n\n[[autodoc]] FlaxBertForPreTraining\n- __call__\n\n## FlaxBertForCausalLM\n\n[[autodoc]] FlaxBertForCausalLM\n- __call__\n\n## FlaxBertForMaskedLM\n\n[[autodoc]] FlaxBertForMaskedLM\n- __call__\n\n## FlaxBertForNextSentencePrediction\n\n[[autodoc]] FlaxBertForNextSentencePrediction\n- __call__\n\n## FlaxBertForSequenceClassification\n\n[[autodoc]] FlaxBertForSequenceClassification\n- __call__\n\n## FlaxBertForMultipleChoice\n\n[[autodoc]] FlaxBertForMultipleChoice\n- __call__\n\n## FlaxBertForTokenClassification",
  "[[autodoc]] FlaxBertForTokenClassification\n- __call__\n\n## FlaxBertForQuestionAnswering\n\n[[autodoc]] FlaxBertForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UMT5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The UMT5 model was proposed in [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n\nThe abstract from the paper is the following:",
  "*Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.*\n\nGoogle has released the following variants:\n\n- [google/umt5-small](https://huggingface.co/google/umt5-small)\n- [google/umt5-base](https://huggingface.co/google/umt5-base)",
  "- [google/umt5-xl](https://huggingface.co/google/umt5-xl)\n- [google/umt5-xxl](https://huggingface.co/google/umt5-xxl).\n\nThis model was contributed by [agemagician](https://huggingface.co/agemagician) and [stefan-it](https://huggingface.co/stefan-it). The original code can be\nfound [here](https://github.com/google-research/t5x).\n\n## Usage tips\n\n- UMT5 was only pre-trained on [mC4](https://huggingface.co/datasets/mc4) excluding any supervised training.\nTherefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5 model.\n- Since umT5 was pre-trained in an unsupervised manner, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n## Differences with mT5?\n`UmT5` is based on mT5, with a non-shared relative positional bias that is computed for each layer. This means that the model set `has_relative_bias` for each layer.\nThe conversion script is also different because the model was saved in t5x's latest checkpointing format.\n\n# Sample usage\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer",
  ">>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n\n>>> inputs = tokenizer(\n...     \"A <extra_id_0> walks into a bar and orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\",\n...     return_tensors=\"pt\",\n... )\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs))\n['<pad><extra_id_0>nyone who<extra_id_1> drink<extra_id_2> a<extra_id_3> alcohol<extra_id_4> A<extra_id_5> A. This<extra_id_6> I<extra_id_7><extra_id_52><extra_id_53></s>']\n```\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for more tips, code examples and notebooks.\n</Tip>\n\n## UMT5Config\n\n[[autodoc]] UMT5Config\n\n## UMT5Model\n\n[[autodoc]] UMT5Model\n- forward\n\n## UMT5ForConditionalGeneration\n\n[[autodoc]] UMT5ForConditionalGeneration\n- forward\n\n## UMT5EncoderModel\n\n[[autodoc]] UMT5EncoderModel\n- forward\n\n## UMT5ForSequenceClassification\n\n[[autodoc]] UMT5ForSequenceClassification\n- forward\n\n## UMT5ForTokenClassification\n\n[[autodoc]] UMT5ForTokenClassification\n- forward\n\n## UMT5ForQuestionAnswering\n\n[[autodoc]] UMT5ForQuestionAnswering\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UDOP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe UDOP model was proposed in [Unifying Vision, Text, and Layout for Universal Document Processing](https://arxiv.org/abs/2212.02623) by Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal.",
  "UDOP adopts an encoder-decoder Transformer architecture based on [T5](t5) for document AI tasks like document image classification, document parsing and document visual question answering.\n\nThe abstract from the paper is the following:",
  "We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 9 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark (DUE).*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/udop_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> UDOP architecture. Taken from the <a href=\"https://arxiv.org/abs/2212.02623\">original paper.</a> </small>\n\n## Usage tips\n\n- In addition to *input_ids*, [`UdopForConditionalGeneration`] also expects the input `bbox`, which are\nthe bounding boxes (i.e. 2D-positions) of the input tokens. These can be obtained using an external OCR engine such\nas Google's [Tesseract](https://github.com/tesseract-ocr/tesseract) (there's a [Python wrapper](https://pypi.org/project/pytesseract/) available). Each bounding box should be in (x0, y0, x1, y1) format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1) represents the\nposition of the lower right corner. Note that one first needs to normalize the bounding boxes to be on a 0-1000\nscale. To normalize, you can use the following function:\n\n```python\ndef normalize_bbox(bbox, width, height):\nreturn [\nint(1000 * (bbox[0] / width)),\nint(1000 * (bbox[1] / height)),\nint(1000 * (bbox[2] / width)),\nint(1000 * (bbox[3] / height)),\n]\n```",
  "Here, `width` and `height` correspond to the width and height of the original document in which the token\noccurs. Those can be obtained using the Python Image Library (PIL) library for example, as follows:\n\n```python\nfrom PIL import Image\n\n# Document can be a png, jpg, etc. PDFs must be converted to images.\nimage = Image.open(name_of_your_document).convert(\"RGB\")\n\nwidth, height = image.size\n```\n\nOne can use [`UdopProcessor`] to prepare images and text for the model, which takes care of all of this. By default, this class uses the Tesseract engine to extract a list of words and boxes (coordinates) from a given document. Its functionality is equivalent to that of [`LayoutLMv3Processor`], hence it supports passing either `apply_ocr=False` in case you prefer to use your own OCR engine or `apply_ocr=True` in case you want the default OCR engine to be used. Refer to the [usage guide of LayoutLMv2](layoutlmv2#usage-layoutlmv2processor) regarding all possible use cases (the functionality of `UdopProcessor` is identical).",
  "- If using an own OCR engine of choice, one recommendation is Azure's [Read API](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/call-read-api), which supports so-called line segments. Use of segment position embeddings typically results in better performance.\n- At inference time, it's recommended to use the `generate` method to autoregressively generate text given a document image.\n- The model has been pre-trained on both self-supervised and supervised objectives. One can use the various task prefixes (prompts) used during pre-training to test out the out-of-the-box capabilities. For instance, the model can be prompted with \"Question answering. What is the date?\", as \"Question answering.\" is the task prefix used during pre-training for DocVQA. Refer to the [paper](https://arxiv.org/abs/2212.02623) (table 1) for all task prefixes.\n- One can also fine-tune [`UdopEncoderModel`], which is the encoder-only part of UDOP, which can be seen as a LayoutLMv3-like Transformer encoder. For discriminative tasks, one can just add a linear classifier on top of it and fine-tune it on a labeled dataset.",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/microsoft/UDOP).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with UDOP. If\nyou're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll\nreview it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- Demo notebooks regarding UDOP can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UDOP) that show how\nto fine-tune UDOP on a custom dataset as well as inference. 🌎\n- [Document question answering task guide](../tasks/document_question_answering)\n\n## UdopConfig\n\n[[autodoc]] UdopConfig\n\n## UdopTokenizer\n\n[[autodoc]] UdopTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## UdopTokenizerFast\n\n[[autodoc]] UdopTokenizerFast\n\n## UdopProcessor\n\n[[autodoc]] UdopProcessor\n- __call__\n\n## UdopModel\n\n[[autodoc]] UdopModel\n- forward\n\n## UdopForConditionalGeneration",
  "[[autodoc]] UdopForConditionalGeneration\n- forward\n\n## UdopEncoderModel\n\n[[autodoc]] UdopEncoderModel\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Llama3\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n```py3\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B\"\n\npipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\npipeline(\"Hey how are you doing today?\")\n```\n\n## Overview\n\nThe Llama3 model was proposed in [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) by the meta AI team.\n\nThe abstract from the blogpost is the following:",
  "*Today, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their class, period. In support of our longstanding open approach, we’re putting Llama 3 in the hands of the community. We want to kickstart the next wave of innovation in AI across the stack—from applications to developer tools to evals to inference optimizations and more. We can’t wait to see what you build and look forward to your feedback.*\n\nCheckout all Llama3 model checkpoints [here](https://huggingface.co/models?search=llama3).\nThe original code of the authors can be found [here](https://github.com/meta-llama/llama3).\n\n## Usage tips\n\n<Tip warning={true}>",
  "The `Llama3` models were trained using `bfloat16`, but the original inference uses `float16`. The checkpoints uploaded on the Hub use `torch_dtype = 'float16'`, which will be\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\n\nThe `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online), then it will be casted to the default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is a `torch_dtype` provided in the config, it will be used.\n\nTraining the model in `float16` is not recommended and is known to produce `nan`; as such, the model should be trained in `bfloat16`.\n\n</Tip>\n\nTips:\n\n- Weights for the Llama3 models can be obtained by filling out [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n- The architecture is exactly the same as Llama2.",
  "- The tokenizer is a BPE model based on [tiktoken](https://github.com/openai/tiktoken) (vs the one based on sentencepiece implementation for Llama2). The main difference that it ignores BPE merge rules when an input token is part of the vocab. This means that if no merge exist to produce `\"hugging\"`, instead of having the smallest units, like `[\"hug\",\"ging\"] form 2 tokens, if `\"hugging\"` is part of the vocab, it will be automatically returned as a token.\n- The original model uses `pad_id = -1` which means that there is no padding token. We can't have the same logic, make sure to add a padding token using `tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})` and resize the token embedding accordingly. You should also set the `model.config.pad_token_id`. The `embed_tokens` layer of the model is initialized with `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`, which makes sure that encoding the padding token will output zeros, so passing it when initializing is recommended.",
  "- The original checkpoint can be converted using the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n--input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path --llama_version 3\n```\n\n- After conversion, the model and tokenizer can be loaded via:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\nmodel = AutoModelForCausalLM.from_pretrained(\"/output/path\")\n```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\ncome in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 75B model, it's thus 145GB of RAM needed.",
  "- When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `torch_dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n\n## Resources\n\nA ton of cool resources are already available on the documentation page of [Llama2](./llama2), inviting contributors to add new resources curated for Llama3 here! 🤗",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ViTPose\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The ViTPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. ViTPose employs a standard, non-hierarchical [Vision Transformer](vit) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark. The model was further improved in [ViTPose++: Vision Transformer for Generic Body Pose Estimation](https://arxiv.org/abs/2212.04246) where the authors employ\na mixture-of-experts (MoE) module in the ViT backbone along with pre-training on more data, which further enhances the performance.\n\nThe abstract from the paper is the following:",
  "*Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViTPose architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.12484\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr) and [sangbumchoi](https://github.com/SangbumChoi).\nThe original code can be found [here](https://github.com/ViTAE-Transformer/ViTPose).\n\n## Usage Tips\n\nViTPose is a so-called top-down keypoint detection model. This means that one first uses an object detector, like [RT-DETR](rt_detr.md), to detect people (or other instances) in an image. Next, ViTPose takes the cropped images as input and predicts the keypoints for each of them.\n\n```py\nimport torch\nimport requests\nimport numpy as np\n\nfrom PIL import Image\n\nfrom transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nurl = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)",
  "# ------------------------------------------------------------------------\n# Stage 1. Detect humans on the image\n# ------------------------------------------------------------------------\n\n# You can choose any detector of your choice\nperson_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nperson_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n\ninputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\noutputs = person_model(**inputs)\n\nresults = person_image_processor.post_process_object_detection(\noutputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n)\nresult = results[0]  # take first image results\n\n# Human label refers 0 index in COCO dataset\nperson_boxes = result[\"boxes\"][result[\"labels\"] == 0]\nperson_boxes = person_boxes.cpu().numpy()\n\n# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\nperson_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\nperson_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n\n# ------------------------------------------------------------------------",
  "# Stage 2. Detect keypoints for each person found\n# ------------------------------------------------------------------------\n\nimage_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\n\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\noutputs = model(**inputs)\n\npose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\nimage_pose_result = pose_results[0]  # results for first image\n```\n\n### ViTPose++ models\n\nThe best [checkpoints](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335) are those of the [ViTPose++ paper](https://arxiv.org/abs/2212.04246). ViTPose++ models employ a so-called [Mixture-of-Experts (MoE)](https://huggingface.co/blog/moe) architecture for the ViT backbone, resulting in better performance.\n\nThe ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed.\nAn overview of the various dataset indices is provided below:",
  "- 0: [COCO validation 2017](https://cocodataset.org/#overview) dataset, using an object detector that gets 56 AP on the \"person\" class\n- 1: [AiC](https://github.com/fabbrimatteo/AiC-Dataset) dataset\n- 2: [MPII](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/software-and-datasets/mpii-human-pose-dataset) dataset\n- 3: [AP-10K](https://github.com/AlexTheBad/AP-10K) dataset\n- 4: [APT-36K](https://github.com/pandorgan/APT-36K) dataset\n- 5: [COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody) dataset\n\nPass the `dataset_index` argument in the forward of the model to indicate which experts to use for each example in the batch. Example usage is shown below:\n\n```python\nimage_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device=device)\n\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n\ndataset_index = torch.tensor([0], device=device) # must be a tensor of shape (batch_size,)\n\nwith torch.no_grad():\noutputs = model(**inputs, dataset_index=dataset_index)\n```",
  "The ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed.\nAn overview of the various dataset indices is provided below:\n\n- 0: [COCO validation 2017](https://cocodataset.org/#overview) dataset, using an object detector that gets 56 AP on the \"person\" class\n- 1: [AiC](https://github.com/fabbrimatteo/AiC-Dataset) dataset\n- 2: [MPII](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/software-and-datasets/mpii-human-pose-dataset) dataset\n- 3: [AP-10K](https://github.com/AlexTheBad/AP-10K) dataset\n- 4: [APT-36K](https://github.com/pandorgan/APT-36K) dataset\n- 5: [COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody) dataset\n\n\n### Visualization\n\nTo visualize the various keypoints, one can either leverage the `supervision` [library](https://github.com/roboflow/supervision (requires `pip install supervision`):\n\n```python\nimport supervision as sv\n\nxy = torch.stack([pose_result['keypoints'] for pose_result in image_pose_result]).cpu().numpy()\nscores = torch.stack([pose_result['scores'] for pose_result in image_pose_result]).cpu().numpy()\n\nkey_points = sv.KeyPoints(\nxy=xy, confidence=scores\n)\n\nedge_annotator = sv.EdgeAnnotator(",
  "color=sv.Color.GREEN,\nthickness=1\n)\nvertex_annotator = sv.VertexAnnotator(\ncolor=sv.Color.RED,\nradius=2\n)\nannotated_frame = edge_annotator.annotate(\nscene=image.copy(),\nkey_points=key_points\n)\nannotated_frame = vertex_annotator.annotate(\nscene=annotated_frame,\nkey_points=key_points\n)\n```\n\nAlternatively, one can also visualize the keypoints using [OpenCV](https://opencv.org/) (requires `pip install opencv-python`):\n\n```python\nimport math\nimport cv2\n\ndef draw_points(image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight):\nif pose_keypoint_color is not None:\nassert len(pose_keypoint_color) == len(keypoints)\nfor kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\nx_coord, y_coord = int(kpt[0]), int(kpt[1])\nif kpt_score > keypoint_score_threshold:\ncolor = tuple(int(c) for c in pose_keypoint_color[kid])\nif show_keypoint_weight:\ncv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\ntransparency = max(0, min(1, kpt_score))\ncv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\nelse:\ncv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)",
  "def draw_links(image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = 2):\nheight, width, _ = image.shape\nif keypoint_edges is not None and link_colors is not None:\nassert len(link_colors) == len(keypoint_edges)\nfor sk_id, sk in enumerate(keypoint_edges):\nx1, y1, score1 = (int(keypoints[sk[0], 0]), int(keypoints[sk[0], 1]), scores[sk[0]])\nx2, y2, score2 = (int(keypoints[sk[1], 0]), int(keypoints[sk[1], 1]), scores[sk[1]])\nif (\nx1 > 0\nand x1 < width\nand y1 > 0\nand y1 < height\nand x2 > 0\nand x2 < width\nand y2 > 0\nand y2 < height\nand score1 > keypoint_score_threshold\nand score2 > keypoint_score_threshold\n):\ncolor = tuple(int(c) for c in link_colors[sk_id])\nif show_keypoint_weight:\nX = (x1, x2)\nY = (y1, y2)\nmean_x = np.mean(X)\nmean_y = np.mean(Y)\nlength = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\nangle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\npolygon = cv2.ellipse2Poly(\n(int(mean_x), int(mean_y)), (int(length / 2), int(stick_width)), int(angle), 0, 360, 1\n)\ncv2.fillConvexPoly(image, polygon, color)",
  "transparency = max(0, min(1, 0.5 * (keypoints[sk[0], 2] + keypoints[sk[1], 2])))\ncv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\nelse:\ncv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)\n\n\n# Note: keypoint_edges and color palette are dataset-specific\nkeypoint_edges = model.config.edges\n\npalette = np.array(\n[\n[255, 128, 0],\n[255, 153, 51],\n[255, 178, 102],\n[230, 230, 0],\n[255, 153, 255],\n[153, 204, 255],\n[255, 102, 255],\n[255, 51, 255],\n[102, 178, 255],\n[51, 153, 255],\n[255, 153, 153],\n[255, 102, 102],\n[255, 51, 51],\n[153, 255, 153],\n[102, 255, 102],\n[51, 255, 51],\n[0, 255, 0],\n[0, 0, 255],\n[255, 0, 0],\n[255, 255, 255],\n]\n)\n\nlink_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\nkeypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n\nnumpy_image = np.array(image)\n\nfor pose_result in image_pose_result:\nscores = np.array(pose_result[\"scores\"])\nkeypoints = np.array(pose_result[\"keypoints\"])\n\n# draw each point on image\ndraw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=4, show_keypoint_weight=False)\n\n# draw links",
  "draw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)\n\npose_image = Image.fromarray(numpy_image)\npose_image\n```\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-coco.jpg\" alt=\"drawing\" width=\"600\"/>\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViTPose. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- A demo of ViTPose on images and video can be found [here](https://huggingface.co/spaces/hysts/ViTPose-transformers).\n- A notebook illustrating inference and visualization can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTPose/Inference_with_ViTPose_for_human_pose_estimation.ipynb).\n\n## VitPoseImageProcessor\n\n[[autodoc]] VitPoseImageProcessor\n- preprocess\n- post_process_pose_estimation\n\n## VitPoseConfig\n\n[[autodoc]] VitPoseConfig",
  "## VitPoseForPoseEstimation\n\n[[autodoc]] VitPoseForPoseEstimation\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BertJapanese\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe BERT models trained on Japanese text.\n\nThere are models with two different tokenization methods:\n\n- Tokenize with MeCab and WordPiece. This requires some extra dependencies, [fugashi](https://github.com/polm/fugashi) which is a wrapper around [MeCab](https://taku910.github.io/mecab/).\n- Tokenize into characters.\n\nTo use *MecabTokenizer*, you should `pip install transformers[\"ja\"]` (or `pip install -e .[\"ja\"]` if you install\nfrom source) to install dependencies.\n\nSee [details on cl-tohoku repository](https://github.com/cl-tohoku/bert-japanese).\n\nExample of using a model with MeCab and WordPiece tokenization:\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n\n>>> ## Input Japanese Text\n>>> line = \"吾輩は猫である。\"\n\n>>> inputs = tokenizer(line, return_tensors=\"pt\")\n\n>>> print(tokenizer.decode(inputs[\"input_ids\"][0]))\n[CLS] 吾輩 は 猫 で ある 。 [SEP]\n\n>>> outputs = bertjapanese(**inputs)\n```\n\nExample of using a model with Character tokenization:\n\n```python",
  ">>> bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n\n>>> ## Input Japanese Text\n>>> line = \"吾輩は猫である。\"\n\n>>> inputs = tokenizer(line, return_tensors=\"pt\")\n\n>>> print(tokenizer.decode(inputs[\"input_ids\"][0]))\n[CLS] 吾 輩 は 猫 で あ る 。 [SEP]\n\n>>> outputs = bertjapanese(**inputs)\n```\n\nThis model was contributed by [cl-tohoku](https://huggingface.co/cl-tohoku).\n\n<Tip>\n\nThis implementation is the same as BERT, except for tokenization method. Refer to [BERT documentation](bert) for\nAPI reference information.\n\n</Tip>\n\n\n## BertJapaneseTokenizer\n\n[[autodoc]] BertJapaneseTokenizer",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ALIGN\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The ALIGN model was proposed in [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig. ALIGN is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image classification. ALIGN features a dual-encoder architecture with [EfficientNet](efficientnet) as its vision encoder and [BERT](bert) as its text encoder, and learns to align visual and text representations with contrastive learning. Unlike previous work, ALIGN leverages a massive noisy dataset and shows that the scale of the corpus can be used to achieve SOTA representations with a simple recipe.\n\nThe abstract from the paper is the following:",
  "*Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.*",
  "This model was contributed by [Alara Dirik](https://huggingface.co/adirik).\nThe original code is not released, this implementation is based on the Kakao Brain implementation based on the original paper.\n\n## Usage example\n\nALIGN uses EfficientNet to get visual features and BERT to get the text features. Both the text and visual features are then projected to a latent space with identical dimension. The dot product between the projected image and text features is then used as a similarity score.\n\n[`AlignProcessor`] wraps [`EfficientNetImageProcessor`] and [`BertTokenizer`] into a single instance to both encode the text and preprocess the images. The following example shows how to get the image-text similarity scores using [`AlignProcessor`] and [`AlignModel`].\n\n```python\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [\"an image of a cat\", \"an image of a dog\"]",
  "inputs = processor(images=image ,text=candidate_labels, return_tensors=\"pt\")\n\nwith torch.no_grad():\noutputs = model(**inputs)\n\n# this is the image-text similarity score\nlogits_per_image = outputs.logits_per_image\n\n# we can take the softmax to get the label probabilities\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ALIGN.\n\n- A blog post on [ALIGN and the COYO-700M dataset](https://huggingface.co/blog/vit-align).\n- A zero-shot image classification [demo](https://huggingface.co/spaces/adirik/ALIGN-zero-shot-image-classification).\n- [Model card](https://huggingface.co/kakaobrain/align-base) of `kakaobrain/align-base` model.\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it. The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## AlignConfig\n\n[[autodoc]] AlignConfig\n- from_text_vision_configs\n\n## AlignTextConfig\n\n[[autodoc]] AlignTextConfig\n\n## AlignVisionConfig\n\n[[autodoc]] AlignVisionConfig\n\n## AlignProcessor",
  "[[autodoc]] AlignProcessor\n\n## AlignModel\n\n[[autodoc]] AlignModel\n- forward\n- get_text_features\n- get_image_features\n\n## AlignTextModel\n\n[[autodoc]] AlignTextModel\n- forward\n\n## AlignVisionModel\n\n[[autodoc]] AlignVisionModel\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DialoGPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nDialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,\nJianfeng Gao, Jingjing Liu, Bill Dolan. It's a GPT2 Model trained on 147M conversation-like exchanges extracted from\nReddit.\n\nThe abstract from the paper is the following:\n\n*We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained\ntransformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning\nfrom 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human\nboth in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems\nthat leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline\nsystems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response",
  "generation and the development of more intelligent open-domain dialogue systems.*\n\nThe original code can be found [here](https://github.com/microsoft/DialoGPT).\n\n## Usage tips\n\n- DialoGPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left.\n- DialoGPT was trained with a causal language modeling (CLM) objective on conversational data and is therefore powerful\nat response generation in open-domain dialogue systems.\n- DialoGPT enables the user to create a chat bot in just 10 lines of code as shown on [DialoGPT's model card](https://huggingface.co/microsoft/DialoGPT-medium).\n\nTraining:\n\nIn order to train or fine-tune DialoGPT, one can use causal language modeling training. To cite the official paper: *We\nfollow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language\nmodeling. We first concatenate all dialog turns within a dialogue session into a long text x_1,..., x_N (N is the\nsequence length), ended by the end-of-text token.* For more information please confer to the original paper.\n\n<Tip>",
  "DialoGPT's architecture is based on the GPT2 model, refer to [GPT2's documentation page](gpt2) for API reference and examples.\n\n</Tip>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RegNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe RegNet model was proposed in [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.\n\nThe authors design search spaces to perform Neural Architecture Search (NAS). They first start from a high dimensional search space and iteratively reduce the search space by empirically applying constraints based on the best-performing models sampled by the current search space.\n\nThe abstract from the paper is the following:",
  "*In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.*",
  "This model was contributed by [Francesco](https://huggingface.co/Francesco). The TensorFlow version of the model\nwas contributed by [sayakpaul](https://huggingface.co/sayakpaul) and [ariG23498](https://huggingface.co/ariG23498).\nThe original code can be found [here](https://github.com/facebookresearch/pycls).\n\nThe huge 10B model from [Self-supervised Pretraining of Visual Features in the Wild](https://arxiv.org/abs/2103.01988),\ntrained on  one billion Instagram images, is available on the [hub](https://huggingface.co/facebook/regnet-y-10b-seer)\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with RegNet.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`RegNetForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## RegNetConfig\n\n[[autodoc]] RegNetConfig\n\n<frameworkcontent>\n<pt>\n\n## RegNetModel\n\n[[autodoc]] RegNetModel\n- forward\n\n## RegNetForImageClassification\n\n[[autodoc]] RegNetForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFRegNetModel\n\n[[autodoc]] TFRegNetModel\n- call\n\n## TFRegNetForImageClassification\n\n[[autodoc]] TFRegNetForImageClassification\n- call\n\n</tf>\n<jax>\n\n## FlaxRegNetModel\n\n[[autodoc]] FlaxRegNetModel\n- __call__\n\n## FlaxRegNetForImageClassification\n\n[[autodoc]] FlaxRegNetForImageClassification\n- __call__\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Depth Anything\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Depth Anything model was proposed in [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891) by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao. Depth Anything is based on the [DPT](dpt) architecture, trained on ~62 million images, obtaining state-of-the-art results for both relative and absolute depth estimation.\n\n<Tip>\n\n[Depth Anything V2](depth_anything_v2) was released in June 2024. It uses the same architecture as Depth Anything and therefore it is compatible with all code examples and existing workflows. However, it leverages synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions.\n\n</Tip>\n\nThe abstract from the paper is the following:",
  "*This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_anything_overview.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Depth Anything overview. Taken from the <a href=\"https://arxiv.org/abs/2401.10891\">original paper</a>.</small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/LiheYoung/Depth-Anything).\n\n## Usage example\n\nThere are 2 main ways to use Depth Anything: either using the pipeline API, which abstracts away all the complexity for you, or by using the `DepthAnythingForDepthEstimation` class yourself.\n\n### Pipeline API\n\nThe pipeline allows to use the model in a few lines of code:\n\n```python\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> # load pipe\n>>> pipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-small-hf\")\n\n>>> # load image\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # inference\n>>> depth = pipe(image)[\"depth\"]\n```\n\n### Using the model yourself",
  "If you want to do the pre- and postprocessing yourself, here's how to do that:\n\n```python\n>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n>>> import torch\n>>> import numpy as np\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n>>> model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # interpolate to original size and visualize the prediction\n>>> post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     target_sizes=[(image.height, image.width)],\n... )\n\n>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n>>> depth = depth.detach().cpu().numpy() * 255",
  ">>> depth = Image.fromarray(depth.astype(\"uint8\"))\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Depth Anything.\n\n- [Monocular depth estimation task guide](../tasks/monocular_depth_estimation)\n- A notebook showcasing inference with [`DepthAnythingForDepthEstimation`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Depth%20Anything/Predicting_depth_in_an_image_with_Depth_Anything.ipynb). 🌎\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DepthAnythingConfig\n\n[[autodoc]] DepthAnythingConfig\n\n## DepthAnythingForDepthEstimation\n\n[[autodoc]] DepthAnythingForDepthEstimation\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# YOLOS\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The YOLOS model was proposed in [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\nYOLOS proposes to just leverage the plain [Vision Transformer (ViT)](vit) for object detection, inspired by DETR. It turns out that a base-sized encoder-only Transformer can also achieve 42 AP on COCO, similar to DETR and much more complex frameworks such as Faster R-CNN.\n\nThe abstract from the paper is the following:",
  "*Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yolos_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> YOLOS architecture. Taken from the <a href=\"https://arxiv.org/abs/2106.00666\">original paper</a>.</small>",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/hustvl/YOLOS).\n\n## Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import AutoModelForObjectDetection\nmodel = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```",
  "For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `hustvl/yolos-base` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                       106 |                                        76 |                      1.39 |\n|            2 |                                       154 |                                        90 |                      1.71 |\n|            4 |                                       222 |                                       116 |                      1.91 |\n|            8 |                                       368 |                                       168 |                      2.19 |\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with YOLOS.\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- All example notebooks illustrating inference + fine-tuning [`YolosForObjectDetection`] on a custom dataset can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/YOLOS).\n- Scripts for finetuning [`YolosForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<Tip>\n\nUse [`YolosImageProcessor`] for preparing images (and optional targets) for the model. Contrary to [DETR](detr), YOLOS doesn't require a `pixel_mask` to be created.\n\n</Tip>\n\n## YolosConfig\n\n[[autodoc]] YolosConfig\n\n## YolosImageProcessor\n\n[[autodoc]] YolosImageProcessor\n- preprocess\n- pad",
  "- post_process_object_detection\n\n## YolosFeatureExtractor\n\n[[autodoc]] YolosFeatureExtractor\n- __call__\n- pad\n- post_process_object_detection\n\n## YolosModel\n\n[[autodoc]] YolosModel\n- forward\n\n## YolosForObjectDetection\n\n[[autodoc]] YolosForObjectDetection\n- forward",
  "<!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mistral\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nMistral was introduced in the [this blogpost](https://mistral.ai/news/announcing-mistral-7b/) by Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.\n\nThe introduction of the blog post says:\n\n*Mistral AI team is proud to release Mistral 7B, the most powerful language model for its size to date.*\n\nMistral-7B is the first large language model (LLM) released by [mistral.ai](https://mistral.ai/).\n\n### Architectural details\n\nMistral-7B is a decoder-only Transformer with the following architectural choices:\n\n- Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens",
  "- GQA (Grouped Query Attention) - allowing faster inference and lower cache size.\n- Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens.\n\nFor more details refer to the [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n\n### License\n\n`Mistral-7B` is released under the Apache 2.0 license.\n\n## Usage tips\n\nThe Mistral team has released 3 checkpoints:\n\n- a base model, [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1), which has been pre-trained to predict the next token on internet-scale data.\n- an instruction tuned model, [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1), which is the base model optimized for chat purposes using supervised fine-tuning (SFT) and direct preference optimization (DPO).\n- an improved instruction tuned model, [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), which improves upon v1.\n\nThe base model can be used as follows:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")",
  ">>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"My favourite condiment is to ...\"\n```\n\nThe instruction tuned model can be used as follows:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\n>>> messages = [\n...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n... ]\n\n>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")",
  ">>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"Mayonnaise can be made as follows: (...)\"\n```\n\nAs can be seen, the instruction-tuned model requires a [chat template](../chat_templating) to be applied to make sure the inputs are prepared in the right format.\n\n## Speeding up Mistral by using Flash Attention\n\nThe code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). Make also sure to load your model in half-precision (e.g. `torch.float16`)",
  "To load and run a model using Flash Attention-2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"My favourite condiment is to (...)\"\n```\n\n### Expected speedups\n\nBelow is a expected speedup diagram that compares pure inference time between the native implementation in transformers using `mistralai/Mistral-7B-v0.1` checkpoint and the Flash Attention 2 version of the model.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mistral-7b-inference-large-seqlen.png\">\n</div>\n\n### Sliding window Attention",
  "The current implementation supports the sliding window attention mechanism and memory efficient cache management.\nTo enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`).\n\nThe Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding_window`), support batched generation only for `padding_side=\"left\"` and use the absolute position of the current token to compute the positional embedding.\n\n## Shrinking down Mistral using quantization\n\nAs the Mistral model has 7 billion parameters, that would require about 14GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using [quantization](../quantization.md). If the model is quantized to 4 bits (or half a byte per parameter),that requires only about 3.5GB of RAM.",
  "Quantizing a model is as simple as passing a `quantization_config` to the model. Below, we'll leverage the BitsAndyBytes quantization (but refer to [this page](../quantization.md) for other quantization methods):\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n>>> # specify how to quantize the model\n>>> quantization_config = BitsAndBytesConfig(\n...         load_in_4bit=True,\n...         bnb_4bit_quant_type=\"nf4\",\n...         bnb_4bit_compute_dtype=\"torch.float16\",\n... )\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=True, device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\n>>> prompt = \"My favourite condiment is\"\n\n>>> messages = [\n...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n... ]",
  ">>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\n>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n\"The expected output\"\n```\n\nThis model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](https://huggingface.co/ArthurZ) .\nThe original code can be found [here](https://github.com/mistralai/mistral-src).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Mistral. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- A demo notebook to perform supervised fine-tuning (SFT) of Mistral-7B can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb). 🌎",
  "- A [blog post](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl) on how to fine-tune LLMs in 2024 using Hugging Face tooling. 🌎\n- The [Alignment Handbook](https://github.com/huggingface/alignment-handbook) by Hugging Face includes scripts and recipes to perform supervised fine-tuning (SFT) and direct preference optimization with Mistral-7B. This includes scripts for full fine-tuning, QLoRa on a single GPU as well as multi-GPU fine-tuning.\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## MistralConfig\n\n[[autodoc]] MistralConfig\n\n## MistralModel\n\n[[autodoc]] MistralModel\n- forward\n\n## MistralForCausalLM\n\n[[autodoc]] MistralForCausalLM\n- forward\n\n## MistralForSequenceClassification\n\n[[autodoc]] MistralForSequenceClassification\n- forward\n\n## MistralForTokenClassification\n\n[[autodoc]] MistralForTokenClassification\n- forward\n\n## MistralForQuestionAnswering\n\n[[autodoc]] MistralForQuestionAnswering\n- forward\n\n## FlaxMistralModel\n\n[[autodoc]] FlaxMistralModel\n- __call__\n\n## FlaxMistralForCausalLM\n\n[[autodoc]] FlaxMistralForCausalLM\n- __call__\n\n## TFMistralModel\n\n[[autodoc]] TFMistralModel\n- call\n\n## TFMistralForCausalLM\n\n[[autodoc]] TFMistralForCausalLM",
  "- call\n\n## TFMistralForSequenceClassification\n\n[[autodoc]] TFMistralForSequenceClassification\n- call",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Swin2SR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.",
  "Swin2SR improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps between pre-training\nand fine-tuning, and hunger on data.\n\nThe abstract from the paper is the following:\n\n*Compression plays an important role on the efficient transmission and storage of images and videos through band-limited systems such as streaming services, virtual reality or videogames. However, compression unavoidably leads to artifacts and the loss of the original information, which may severely degrade the visual quality. For these reasons, quality enhancement of compressed images has become a popular research topic. While most state-of-the-art image restoration methods are based on convolutional neural networks, other transformers-based methods such as SwinIR, show impressive performance on these tasks.",
  "In this paper, we explore the novel Swin Transformer V2, to improve SwinIR for image super-resolution, and in particular, the compressed input scenario. Using this method we can tackle the major issues in training transformer vision models, such as training instability, resolution gaps between pre-training and fine-tuning, and hunger on data. We conduct experiments on three representative tasks: JPEG compression artifacts removal, image super-resolution (classical and lightweight), and compressed image super-resolution. Experimental results demonstrate that our method, Swin2SR, can improve the training convergence and performance of SwinIR, and is a top-5 solution at the \"AIM 2022 Challenge on Super-Resolution of Compressed Image and Video\".*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/swin2sr_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Swin2SR architecture. Taken from the <a href=\"https://arxiv.org/abs/2209.11345\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/mv-lab/swin2sr).\n\n## Resources",
  "Demo notebooks for Swin2SR can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Swin2SR).\n\nA demo Space for image super-resolution with SwinSR can be found [here](https://huggingface.co/spaces/jjourney1125/swin2sr).\n\n## Swin2SRImageProcessor\n\n[[autodoc]] Swin2SRImageProcessor\n- preprocess\n\n## Swin2SRConfig\n\n[[autodoc]] Swin2SRConfig\n\n## Swin2SRModel\n\n[[autodoc]] Swin2SRModel\n- forward\n\n## Swin2SRForImageSuperResolution\n\n[[autodoc]] Swin2SRForImageSuperResolution\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FLAVA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.\n\nThe paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks.\n\nThe abstract from the paper is the following:\n\n*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate",
  "impressive performance on a wide range of 35 tasks spanning these target modalities.*\n\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava).\n\n## FlavaConfig\n\n[[autodoc]] FlavaConfig\n\n## FlavaTextConfig\n\n[[autodoc]] FlavaTextConfig\n\n## FlavaImageConfig\n\n[[autodoc]] FlavaImageConfig\n\n## FlavaMultimodalConfig\n\n[[autodoc]] FlavaMultimodalConfig\n\n## FlavaImageCodebookConfig\n\n[[autodoc]] FlavaImageCodebookConfig\n\n## FlavaProcessor\n\n[[autodoc]] FlavaProcessor\n\n## FlavaFeatureExtractor\n\n[[autodoc]] FlavaFeatureExtractor\n\n## FlavaImageProcessor\n\n[[autodoc]] FlavaImageProcessor\n- preprocess\n\n## FlavaForPreTraining\n\n[[autodoc]] FlavaForPreTraining\n- forward\n\n## FlavaModel\n\n[[autodoc]] FlavaModel\n- forward\n- get_text_features\n- get_image_features\n\n## FlavaImageCodebook\n\n[[autodoc]] FlavaImageCodebook\n- forward\n- get_codebook_indices\n- get_codebook_probs\n\n## FlavaTextModel\n\n[[autodoc]] FlavaTextModel\n- forward\n\n## FlavaImageModel\n\n[[autodoc]] FlavaImageModel\n- forward\n\n## FlavaMultimodalModel\n\n[[autodoc]] FlavaMultimodalModel\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Dilated Neighborhood Attention Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nDiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)\nby Ali Hassani and Humphrey Shi.",
  "It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to capture global context,\nand shows significant performance improvements over it.\n\nThe abstract from the paper is the following:\n\n*Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities,\ndomains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have\nalso gained significant attention, thanks to their performance and easy integration into existing frameworks.\nThese models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA)\nor Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity,\nlocal attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling,\nand global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and\nefficient extension to NA that can capture more global context and expand receptive fields exponentially at no",
  "additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we\nintroduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both.\nDiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt.\nOur large model is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object detection,\n1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation.\nPaired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.2 PQ)\nand ADE20K (48.5 PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4 AP) (no extra data).\nIt also matches the state of the art specialized semantic segmentation models on ADE20K (58.2 mIoU),\nand ranks second on Cityscapes (84.5 mIoU) (no extra data). *\n\n<img\nsrc=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neighborhood-attention-pattern.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Neighborhood Attention with different dilation values.",
  "Taken from the <a href=\"https://arxiv.org/abs/2209.15001\">original paper</a>.</small>\n\nThis model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).\nThe original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).\n\n## Usage tips\n\nDiNAT can be used as a *backbone*. When `output_hidden_states = True`,\nit will output both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than `(batch_size, height, width, num_channels)`.\n\nNotes:\n- DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention and Dilated Neighborhood Attention.\nYou can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten), or build on your system by running `pip install natten`.\nNote that the latter will likely take time to compile. NATTEN does not support Windows devices yet.\n- Patch size of 4 is only supported at the moment.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DiNAT.",
  "<PipelineTag pipeline=\"image-classification\"/>\n\n- [`DinatForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DinatConfig\n\n[[autodoc]] DinatConfig\n\n## DinatModel\n\n[[autodoc]] DinatModel\n- forward\n\n## DinatForImageClassification\n\n[[autodoc]] DinatForImageClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Wav2Vec2-Conformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Wav2Vec2-Conformer was added to an updated version of [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.",
  "The official results of the model can be found in Table 3 and Table 4 of the paper.\n\nThe Wav2Vec2-Conformer weights were released by the Meta AI team within the [Fairseq library](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md#pre-trained-models).\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/wav2vec).\n\nNote: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert) - it's pretrained on 4.5M hours of audio. We especially recommend using it for fine-tuning tasks, e.g. as per [this guide](https://huggingface.co/blog/fine-tune-w2v2-bert).\n\n## Usage tips\n\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Attention*-block with a *Conformer*-block\nas introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100).\n- For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields\nan improved word error rate.",
  "- Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2.\n- Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or\nrotary position embeddings by setting the correct `config.position_embeddings_type`.\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## Wav2Vec2ConformerConfig\n\n[[autodoc]] Wav2Vec2ConformerConfig\n\n## Wav2Vec2Conformer specific outputs\n\n[[autodoc]] models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput\n\n## Wav2Vec2ConformerModel\n\n[[autodoc]] Wav2Vec2ConformerModel\n- forward\n\n## Wav2Vec2ConformerForCTC\n\n[[autodoc]] Wav2Vec2ConformerForCTC\n- forward\n\n## Wav2Vec2ConformerForSequenceClassification\n\n[[autodoc]] Wav2Vec2ConformerForSequenceClassification\n- forward\n\n## Wav2Vec2ConformerForAudioFrameClassification\n\n[[autodoc]] Wav2Vec2ConformerForAudioFrameClassification\n- forward\n\n## Wav2Vec2ConformerForXVector\n\n[[autodoc]] Wav2Vec2ConformerForXVector\n- forward\n\n## Wav2Vec2ConformerForPreTraining\n\n[[autodoc]] Wav2Vec2ConformerForPreTraining\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MarkupLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but",
  "applied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recognition on web pages)\n\nThe abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and",
  "dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips\n\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```",
  "In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).",
  "**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should",
  "provide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can",
  "initialize the tokenizer with `only_label_first_subword` set to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> node_labels = [1, 2, 2, 1]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq', 'labels'])\n```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>",
  "...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> question = \"What's his name?\"\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> question = \"What's his name?\"\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())",
  "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n- all\n\n## MarkupLMFeatureExtractor\n\n[[autodoc]] MarkupLMFeatureExtractor\n- __call__\n\n## MarkupLMTokenizer\n\n[[autodoc]] MarkupLMTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## MarkupLMTokenizerFast\n\n[[autodoc]] MarkupLMTokenizerFast\n- all\n\n## MarkupLMProcessor\n\n[[autodoc]] MarkupLMProcessor\n- __call__\n\n## MarkupLMModel\n\n[[autodoc]] MarkupLMModel\n- forward\n\n## MarkupLMForSequenceClassification\n\n[[autodoc]] MarkupLMForSequenceClassification\n- forward\n\n## MarkupLMForTokenClassification\n\n[[autodoc]] MarkupLMForTokenClassification\n- forward\n\n## MarkupLMForQuestionAnswering\n\n[[autodoc]] MarkupLMForQuestionAnswering\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BEiT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by\nHangbo Bao, Li Dong and Furu Wei. Inspired by BERT, BEiT is the first paper that makes self-supervised pre-training of\nVision Transformers (ViTs) outperform supervised pre-training. Rather than pre-training the model to predict the class\nof an image (as done in the [original ViT paper](https://arxiv.org/abs/2010.11929)), BEiT models are pre-trained to\npredict visual tokens from the codebook of OpenAI's [DALL-E model](https://arxiv.org/abs/2102.12092) given masked\npatches.\n\nThe abstract from the paper is the following:\n\n*We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation\nfrom Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image\nmodeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image",
  "patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into\nvisual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training\nobjective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we\ndirectly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder.\nExperimental results on image classification and semantic segmentation show that our model achieves competitive results\nwith previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K,\nsignificantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains\n86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%).*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The JAX/FLAX version of this model was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/beit).",
  "## Usage tips\n\n- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than supervised. They\noutperform both the [original model (ViT)](vit) as well as [Data-efficient Image Transformers (DeiT)](deit) when fine-tuned on ImageNet-1K and CIFAR-100. You can check out demo notebooks regarding inference as well as\nfine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer) (you can just replace\n[`ViTFeatureExtractor`] by [`BeitImageProcessor`] and\n[`ViTForImageClassification`] by [`BeitForImageClassification`]).\n- There's also a demo notebook available which showcases how to combine DALL-E's image tokenizer with BEiT for\nperforming masked image modeling. You can find it [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).\n- As the BEiT models expect each image to be of the same size (resolution), one can use\n[`BeitImageProcessor`] to resize (or rescale) and normalize images for the model.\n- Both the patch resolution and image resolution used during pre-training or fine-tuning are reflected in the name of",
  "each checkpoint. For example, `microsoft/beit-base-patch16-224` refers to a base-sized architecture with patch\nresolution of 16x16 and fine-tuning resolution of 224x224. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit).\n- The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/) (a collection of\n14 million images and 22k classes) only, (2) also fine-tuned on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/) (also referred to as ILSVRC 2012, a collection of 1.3 million\nimages and 1,000 classes).\n- BEiT uses relative position embeddings, inspired by the T5 model. During pre-training, the authors shared the\nrelative position bias among the several self-attention layers. During fine-tuning, each layer's relative position\nbias is initialized with the shared relative position bias obtained after pre-training. Note that, if one wants to\npre-train a model from scratch, one needs to either set the `use_relative_position_bias` or the\n`use_relative_position_bias` attribute of [`BeitConfig`] to `True` in order to add\nposition embeddings.",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/beit_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> BEiT pre-training. Taken from the <a href=\"https://arxiv.org/abs/2106.08254\">original paper.</a> </small>\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import BeitForImageClassification",
  "model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.5.1, OS Ubuntu 20.04) with `float16` and\n`microsoft/beit-base-patch16-224` model, we saw the following improvements during training and inference:\n\n#### Training\n\n| num_training_steps | batch_size | image_size   | is_cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) | Mem saving (%) |\n|--------------------|------------|--------------|---------|----------------------------|---------------------------|-------------|----------------------|--------------------|----------------|\n| 50                 | 2          | (1048, 640)  | True    | 0.984                      | 0.746                     | 31.975      | 6738.915            | 4319.886          | 55.998         |\n\n#### Inference",
  "|   Image batch size |   Eager (s/iter) | Eager CI, %   |   Eager memory (MB) |   SDPA (s/iter) | SDPA CI, %   |   SDPA memory (MB) |   SDPA speedup | SDPA memory saved (%) |\n|-------------------:|-----------------:|:--------------|--------------------:|----------------:|:-------------|-------------------:|---------------:|----------------------:|\n|                  1 |            0.012 | ±0.3%         |         3.76657e+08 |           0.011 | ±0.5%        |        3.75739e+08 |          1.05  |                 0.244 |\n|                  4 |            0.013 | ±0.1%         |         4.03147e+08 |           0.011 | ±0.2%        |        3.90554e+08 |          1.178 |                 3.225 |\n|                 16 |            0.045 | ±0.1%         |         4.96697e+08 |           0.035 | ±0.1%        |        4.51232e+08 |          1.304 |                10.076 |\n|                 32 |            0.088 | ±0.1%         |         6.24417e+08 |           0.066 | ±0.1%        |        5.33488e+08 |          1.325 |                17.044 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BEiT.",
  "<PipelineTag pipeline=\"image-classification\"/>\n\n- [`BeitForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\n**Semantic segmentation**\n- [Semantic segmentation task guide](../tasks/semantic_segmentation)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## BEiT specific outputs\n\n[[autodoc]] models.beit.modeling_beit.BeitModelOutputWithPooling\n\n[[autodoc]] models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling\n\n## BeitConfig\n\n[[autodoc]] BeitConfig\n\n## BeitFeatureExtractor\n\n[[autodoc]] BeitFeatureExtractor\n- __call__\n- post_process_semantic_segmentation\n\n## BeitImageProcessor\n\n[[autodoc]] BeitImageProcessor\n- preprocess\n- post_process_semantic_segmentation\n\n<frameworkcontent>\n<pt>\n\n## BeitModel",
  "[[autodoc]] BeitModel\n- forward\n\n## BeitForMaskedImageModeling\n\n[[autodoc]] BeitForMaskedImageModeling\n- forward\n\n## BeitForImageClassification\n\n[[autodoc]] BeitForImageClassification\n- forward\n\n## BeitForSemanticSegmentation\n\n[[autodoc]] BeitForSemanticSegmentation\n- forward\n\n</pt>\n<jax>\n\n## FlaxBeitModel\n\n[[autodoc]] FlaxBeitModel\n- __call__\n\n## FlaxBeitForMaskedImageModeling\n\n[[autodoc]] FlaxBeitForMaskedImageModeling\n- __call__\n\n## FlaxBeitForImageClassification\n\n[[autodoc]] FlaxBeitForImageClassification\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RoCBert\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf)  by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.",
  "It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n\nThe abstract from the paper is the following:\n\n*Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown\nvulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose\nROCBERT: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation,\nsynonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency\nunder different synthesized adversarial examples. The model takes as input multimodal information including the\nsemantic, phonetic and visual features. We show all these features are important to the model robustness since the\nattack can be performed in all the three forms. Across 5 Chinese NLU tasks, ROCBERT outperforms strong baselines under\nthree blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best\nin the toxic content detection task under human-made attacks.*",
  "This model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RoCBertConfig\n\n[[autodoc]] RoCBertConfig\n- all\n\n## RoCBertTokenizer\n\n[[autodoc]] RoCBertTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## RoCBertModel\n\n[[autodoc]] RoCBertModel\n- forward\n\n## RoCBertForPreTraining\n\n[[autodoc]] RoCBertForPreTraining\n- forward\n\n## RoCBertForCausalLM\n\n[[autodoc]] RoCBertForCausalLM\n- forward\n\n## RoCBertForMaskedLM\n\n[[autodoc]] RoCBertForMaskedLM\n- forward\n\n## RoCBertForSequenceClassification\n\n[[autodoc]] transformers.RoCBertForSequenceClassification\n- forward\n\n## RoCBertForMultipleChoice\n\n[[autodoc]] transformers.RoCBertForMultipleChoice\n- forward\n\n## RoCBertForTokenClassification",
  "[[autodoc]] transformers.RoCBertForTokenClassification\n- forward\n\n## RoCBertForQuestionAnswering\n\n[[autodoc]] RoCBertForQuestionAnswering\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SwiftFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The SwiftFormer model was proposed in [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.\n\nThe SwiftFormer paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications. A series of models called 'SwiftFormer' is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2× faster compared to MobileViT-v2.\n\nThe abstract from the paper is the following:",
  "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, its quadratic computational complexity with respect to image resolution limits its use in real-time applications, especially for deployment on resource-constrained mobile devices. Although hybrid approaches have been proposed to combine the advantages of convolutions and self-attention for a better speed-accuracy trade-off, the expensive matrix multiplication operations in self-attention remain a bottleneck. In this work, we introduce a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations with linear element-wise multiplications. Our design shows that the key-value interaction can be replaced with a linear layer without sacrificing any accuracy. Unlike previous state-of-the-art methods, our efficient formulation of self-attention enables its usage at all stages of the network. Using our proposed efficient additive attention, we build a series of models called \"SwiftFormer\" which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster compared to MobileViT-v2.*",
  "This model was contributed by [shehan97](https://huggingface.co/shehan97). The TensorFlow version was contributed by [joaocmd](https://huggingface.co/joaocmd).\nThe original code can be found [here](https://github.com/Amshaker/SwiftFormer).\n\n## SwiftFormerConfig\n\n[[autodoc]] SwiftFormerConfig\n\n## SwiftFormerModel\n\n[[autodoc]] SwiftFormerModel\n- forward\n\n## SwiftFormerForImageClassification\n\n[[autodoc]] SwiftFormerForImageClassification\n- forward\n\n## TFSwiftFormerModel\n\n[[autodoc]] TFSwiftFormerModel\n- call\n\n## TFSwiftFormerForImageClassification\n\n[[autodoc]] TFSwiftFormerForImageClassification\n- call",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# SeamlessM4T-v2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe SeamlessM4T-v2 model was proposed in [Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/) by the Seamless Communication team from Meta AI.",
  "SeamlessM4T-v2 is a collection of models designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text. It is an improvement on the [previous version](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t). For more details on the differences between v1 and v2, refer to section [Difference with SeamlessM4T-v1](#difference-with-seamlessm4t-v1).\n\nSeamlessM4T-v2 enables multiple tasks without relying on separate models:\n\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR)\n\n[`SeamlessM4Tv2Model`] can perform all the above tasks, but each task also has its own dedicated sub-model.\n\nThe abstract from the paper is the following:",
  "*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one’s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a science fiction concept into a real-world technology. Finally, contributions in this work—including models, code, and a watermark detector—are publicly released and accessible at the link below.*",
  "## Usage\n\nIn the following example, we'll load an Arabic audio sample and an English text sample and convert them into Russian speech and French text.\n\nFirst, load the processor and a checkpoint of the model:\n\n```python\n>>> from transformers import AutoProcessor, SeamlessM4Tv2Model\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n>>> model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```\n\nYou can seamlessly use this model on text or on audio, to generated either translated text or translated audio.\n\nHere is how to use the processor to process text and audio:\n\n```python\n>>> # let's load an audio sample from an Arabic speech corpus\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"arabic_speech_corpus\", split=\"test\", streaming=True, trust_remote_code=True)\n>>> audio_sample = next(iter(dataset))[\"audio\"]\n\n>>> # now, process it\n>>> audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\")\n\n>>> # now, process some English text as well\n>>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\n```\n\n\n### Speech",
  "[`SeamlessM4Tv2Model`] can *seamlessly* generate text or speech with few or no changes. Let's target Russian voice translation:\n\n```python\n>>> audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n>>> audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n```\n\nWith basically the same code, I've translated English text and Arabic speech to Russian speech samples.\n\n### Text\n\nSimilarly, you can generate translated text from audio files or from text with the same model. You only have to pass `generate_speech=False` to [`SeamlessM4Tv2Model.generate`].\nThis time, let's translate to French.\n\n```python\n>>> # from audio\n>>> output_tokens = model.generate(**audio_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n\n>>> # from text\n>>> output_tokens = model.generate(**text_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n```\n\n### Tips\n\n\n#### 1. Use dedicated models",
  "[`SeamlessM4Tv2Model`] is transformers top level model to generate speech and text, but you can also use dedicated models that perform the task without additional components, thus reducing the memory footprint.\nFor example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code:\n\n```python\n>>> from transformers import SeamlessM4Tv2ForSpeechToSpeech\n>>> model = SeamlessM4Tv2ForSpeechToSpeech.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```\n\nOr you can replace the text-to-text generation snippet with the model dedicated to the T2TT task, you only have to remove `generate_speech=False`.\n\n```python\n>>> from transformers import SeamlessM4Tv2ForTextToText\n>>> model = SeamlessM4Tv2ForTextToText.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```\n\nFeel free to try out [`SeamlessM4Tv2ForSpeechToText`] and [`SeamlessM4Tv2ForTextToSpeech`] as well.\n\n#### 2. Change the speaker identity\n\nYou have the possibility to change the speaker used for speech synthesis with the `speaker_id` argument. Some `speaker_id` works better than other for some languages!\n\n#### 3. Change the generation strategy",
  "You can use different [generation strategies](../generation_strategies) for text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, text_do_sample=True)` which will perform multinomial beam-search decoding on the text model. Note that speech generation only supports greedy - by default - or multinomial sampling, which can be used with e.g. `.generate(..., speech_do_sample=True, speech_temperature=0.6)`.\n\n#### 4. Generate speech and text at the same time\n\nUse `return_intermediate_token_ids=True` with [`SeamlessM4Tv2Model`] to return both speech and text !\n\n## Model architecture\n\nSeamlessM4T-v2 features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n\nEach modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646) architecture is placed on top of the second seq2seq model.",
  "### Difference with SeamlessM4T-v1\n\nThe architecture of this new version differs from the first in a few aspects:\n\n#### Improvements on the second-pass model\n\nThe second seq2seq model, named text-to-unit model, is now non-auto regressive, meaning that it computes units in a **single forward pass**. This achievement is made possible by:\n- the use of **character-level embeddings**, meaning that each character of the predicted translated text has its own embeddings, which are then used to predict the unit tokens.\n- the use of an intermediate duration predictor, that predicts speech duration at the **character-level** on the predicted translated text.\n- the use of a new text-to-unit decoder mixing convolutions and self-attention to handle longer context.\n\n#### Difference in the speech encoder\n\nThe speech encoder, which is used during the first-pass generation process to predict the translated text, differs mainly from the previous speech encoder through these mechanisms:\n- the use of chunked attention mask to prevent attention across chunks, ensuring that each position attends only to positions within its own chunk and a fixed number of previous chunks.",
  "- the use of relative position embeddings which only considers distance between sequence elements rather than absolute positions. Please refer to [Self-Attentionwith Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155) for more details.\n- the use of a causal depth-wise convolution instead of a non-causal one.\n\n### Generation process\n\nHere's how the generation process works:\n\n- Input text or speech is processed through its specific encoder.\n- A decoder creates text tokens in the desired language.\n- If speech generation is required, the second seq2seq model, generates unit tokens in an non auto-regressive way.\n- These unit tokens are then passed through the final vocoder to produce the actual speech.\n\n\nThis model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/seamless_communication).\n\n## SeamlessM4Tv2Model\n\n[[autodoc]] SeamlessM4Tv2Model\n- generate\n\n\n## SeamlessM4Tv2ForTextToSpeech\n\n[[autodoc]] SeamlessM4Tv2ForTextToSpeech\n- generate\n\n\n## SeamlessM4Tv2ForSpeechToSpeech\n\n[[autodoc]] SeamlessM4Tv2ForSpeechToSpeech\n- generate\n\n\n## SeamlessM4Tv2ForTextToText",
  "[[autodoc]] transformers.SeamlessM4Tv2ForTextToText\n- forward\n- generate\n\n## SeamlessM4Tv2ForSpeechToText\n\n[[autodoc]] transformers.SeamlessM4Tv2ForSpeechToText\n- forward\n- generate\n\n## SeamlessM4Tv2Config\n\n[[autodoc]] SeamlessM4Tv2Config",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ViTMSN\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The ViTMSN model was proposed in [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,\nPascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. The paper presents a joint-embedding architecture to match the prototypes\nof masked patches with that of the unmasked patches. With this setup, their method yields excellent performance in the low-shot and extreme low-shot\nregimes.\n\nThe abstract from the paper is the following:\n\n*We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our\napproach matches the representation of an image view containing randomly masked patches to the representation of the original\nunmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the\nunmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures,\nwhile producing representations of a high semantic level that perform competitively on low-shot image classification. For instance,",
  "on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy,\nand with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark.*\n\n<img src=\"https://i.ibb.co/W6PQMdC/Screenshot-2022-09-13-at-9-08-40-AM.png\" alt=\"drawing\" width=\"600\"/>\n\n<small> MSN architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.07141\">original paper.</a> </small>\n\nThis model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/facebookresearch/msn).\n\n## Usage tips\n\n- MSN (masked siamese networks) is a method for self-supervised pre-training of Vision Transformers (ViTs). The pre-training\nobjective is to match the prototypes assigned to the unmasked views of the images to that of the masked views of the same images.\n- The authors have only released pre-trained weights of the backbone (ImageNet-1k pre-training). So, to use that on your own image classification dataset,\nuse the [`ViTMSNForImageClassification`] class which is initialized from [`ViTMSNModel`]. Follow",
  "[this notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb) for a detailed tutorial on fine-tuning.\n- MSN is particularly useful in the low-shot and extreme low-shot regimes. Notably, it achieves 75.7% top-1 accuracy with only 1% of ImageNet-1K\nlabels when fine-tuned.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import ViTMSNForImageClassification",
  "model = ViTMSNForImageClassification.from_pretrained(\"facebook/vit-msn-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `facebook/vit-msn-base` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                         7 |                                         6 |                      1.17 |\n|            2 |                                         8 |                                         6 |                      1.33 |\n|            4 |                                         8 |                                         6 |                      1.33 |",
  "|            8 |                                         8 |                                         6 |                      1.33 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT MSN.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`ViTMSNForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ViTMSNConfig\n\n[[autodoc]] ViTMSNConfig\n\n## ViTMSNModel\n\n[[autodoc]] ViTMSNModel\n- forward\n\n## ViTMSNForImageClassification\n\n[[autodoc]] ViTMSNForImageClassification\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RT-DETRv2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe RT-DETRv2 model was proposed in [RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](https://arxiv.org/abs/2407.17140) by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu.",
  "RT-DETRv2 refines RT-DETR by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. These changes enhance flexibility and practicality while maintaining real-time performance.\n\nThe abstract from the paper is the following:",
  "*In this report, we present RT-DETRv2, an improved Real-Time DEtection TRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art real-time detector, RT-DETR, and opens up a set of bag-of-freebies for flexibility and practicality, as well as optimizing the training strategy to achieve enhanced performance. To improve the flexibility, we suggest setting a distinct number of sampling points for features at different scales in the deformable attention to achieve selective multi-scale feature extraction by the decoder. To enhance practicality, we propose an optional discrete sampling operator to replace the grid_sample operator that is specific to RT-DETR compared to YOLOs. This removes the deployment constraints typically associated with DETRs. For the training strategy, we propose dynamic data augmentation and scale-adaptive hyperparameters customization to improve performance without loss of speed.*\n\nThis model was contributed by [jadechoghari](https://huggingface.co/jadechoghari).\nThe original code can be found [here](https://github.com/lyuwenyu/RT-DETR).\n\n## Usage tips\n\nThis second version of RT-DETR improves how the decoder finds objects in an image.",
  "- **better sampling** – adjusts offsets so the model looks at the right areas\n- **flexible attention** – can use smooth (bilinear) or fixed (discrete) sampling\n- **optimized processing** – improves how attention weights mix information\n\n```py\n>>> import torch\n>>> import requests\n\n>>> from PIL import Image\n>>> from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n>>> model = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5)\n\n>>> for result in results:\n...     for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n...         score, label = score.item(), label_id.item()\n...         box = [round(i, 2) for i in box.tolist()]",
  "...         print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\ncat: 0.97 [341.14, 25.11, 639.98, 372.89]\ncat: 0.96 [12.78, 56.35, 317.67, 471.34]\nremote: 0.95 [39.96, 73.12, 175.65, 117.44]\nsofa: 0.86 [-0.11, 2.97, 639.89, 473.62]\nsofa: 0.82 [-0.12, 1.78, 639.87, 473.52]\nremote: 0.79 [333.65, 76.38, 370.69, 187.48]\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with RT-DETRv2.\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- Scripts for finetuning [`RTDetrV2ForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection).\n- Notebooks for [inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/RT_DETR_v2_inference.ipynb) and [fine-tuning](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/RT_DETR_v2_finetune_on_a_custom_dataset.ipynb) RT-DETRv2 on a custom dataset (🌎).\n\n\n## RTDetrV2Config\n\n[[autodoc]] RTDetrV2Config\n\n\n## RTDetrV2Model",
  "[[autodoc]] RTDetrV2Model\n- forward\n\n## RTDetrV2ForObjectDetection\n\n[[autodoc]] RTDetrV2ForObjectDetection\n- forward",
  "<!--\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OLMoE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n\nOLMoE is a series of **O**pen **L**anguage **Mo**dels using sparse **M**ixture-**o**f-**E**xperts designed to enable the science of language models. We release all code, checkpoints, logs, and details involved in training these models.\n\nThe abstract from the paper is the following:",
  "*We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.*\n\nThis model was contributed by [Muennighoff](https://hf.co/Muennighoff).\nThe original code can be found [here](https://github.com/allenai/OLMoE).\n\n\n## OlmoeConfig\n\n[[autodoc]] OlmoeConfig\n\n## OlmoeModel\n\n[[autodoc]] OlmoeModel\n- forward\n\n## OlmoeForCausalLM\n\n[[autodoc]] OlmoeForCausalLM\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OneFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The OneFormer model was proposed in [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal image segmentation framework that can be trained on a single panoptic dataset to perform semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference.\n\n<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png\"/>\n\nThe abstract from the paper is the following:",
  "*Universal Image Segmentation is not a new concept. Past attempts to unify image segmentation in the last decades include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*",
  "The figure below illustrates the architecture of OneFormer. Taken from the [original paper](https://arxiv.org/abs/2211.06220).\n\n<img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_architecture.png\"/>\n\nThis model was contributed by [Jitesh Jain](https://huggingface.co/praeclarumjj3). The original code can be found [here](https://github.com/SHI-Labs/OneFormer).\n\n## Usage tips\n\n-  OneFormer requires two inputs during inference: *image* and *task token*.\n- During training, OneFormer only uses panoptic annotations.\n- If you want to train the model in a distributed environment across multiple nodes, then one should update the\n`get_num_masks` function inside in the `OneFormerLoss` class of `modeling_oneformer.py`. When training on multiple nodes, this should be\nset to the average number of target masks across all nodes, as can be seen in the original implementation [here](https://github.com/SHI-Labs/OneFormer/blob/33ebb56ed34f970a30ae103e786c0cb64c653d9a/oneformer/modeling/criterion.py#L287).",
  "- One can use [`OneFormerProcessor`] to prepare input images and task inputs for the model and optional targets for the model. [`OneFormerProcessor`] wraps [`OneFormerImageProcessor`] and [`CLIPTokenizer`] into a single instance to both prepare the images and encode the task inputs.\n- To get the final segmentation, depending on the task, you can call [`~OneFormerProcessor.post_process_semantic_segmentation`] or [`~OneFormerImageProcessor.post_process_instance_segmentation`] or [`~OneFormerImageProcessor.post_process_panoptic_segmentation`]. All three tasks can be solved using [`OneFormerForUniversalSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with OneFormer.\n\n- Demo notebooks regarding inference + fine-tuning on custom data can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.",
  "The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## OneFormer specific outputs\n\n[[autodoc]] models.oneformer.modeling_oneformer.OneFormerModelOutput\n\n[[autodoc]] models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput\n\n## OneFormerConfig\n\n[[autodoc]] OneFormerConfig\n\n## OneFormerImageProcessor\n\n[[autodoc]] OneFormerImageProcessor\n- preprocess\n- encode_inputs\n- post_process_semantic_segmentation\n- post_process_instance_segmentation\n- post_process_panoptic_segmentation\n\n## OneFormerProcessor\n\n[[autodoc]] OneFormerProcessor\n\n## OneFormerModel\n\n[[autodoc]] OneFormerModel\n- forward\n\n## OneFormerForUniversalSegmentation\n\n[[autodoc]] OneFormerForUniversalSegmentation\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SEW\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "SEW (Squeezed and Efficient Wav2Vec) was proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training\nfor Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q.\nWeinberger, Yoav Artzi.\n\nThe abstract from the paper is the following:\n\n*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition\n(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance\nand its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a\npre-trained model architecture with significant improvements along both performance and efficiency dimensions across a\nvariety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference\ntime, SEW reduces word error rate by 25-50% across different model sizes.*\n\nThis model was contributed by [anton-l](https://huggingface.co/anton-l).\n\n## Usage tips",
  "- SEW is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- SEWForCTC is fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded using\n[`Wav2Vec2CTCTokenizer`].\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## SEWConfig\n\n[[autodoc]] SEWConfig\n\n## SEWModel\n\n[[autodoc]] SEWModel\n- forward\n\n## SEWForCTC\n\n[[autodoc]] SEWForCTC\n- forward\n\n## SEWForSequenceClassification\n\n[[autodoc]] SEWForSequenceClassification\n- forward",
  "<!--Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Qwen2.5-VL\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The [Qwen2.5-VL](https://qwenlm.github.io/blog/qwen2_5-vl/) model is an update to [Qwen2-VL](https://arxiv.org/abs/2409.12191) from Qwen team, Alibaba Group.\n\nThe abstract from this update is the following:\n\n*Qwen2.5-VL marks a major step forward from Qwen2-VL, built upon the latest Qwen2.5 LLM. We've accelerated training and testing through the strategic implementation of window attention within the ViT. The ViT architecture itself has been refined with SwiGLU and RMSNorm, aligning it more closely with the LLM's structure. A key innovation is the expansion of native dynamic resolution to encompass the temporal dimension, in addition to spatial aspects. Furthermore, we've upgraded MRoPE, incorporating absolute time alignment on the time axis to allow the model to effectively capture temporal dynamics, regardless of frame rate, leading to superior video understanding.*\n\n## Usage example\n\n### Single Media inference\n\nThe model can accept both images and videos as input. Here's an example code for inference.\n\n```python\n\nimport torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n\n# Load the model in half-precision on the available device(s)",
  "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n\n\nconversation = [\n{\n\"role\":\"user\",\n\"content\":[\n{\n\"type\":\"image\",\n\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n},\n{\n\"type\":\"text\",\n\"text\":\"Describe this image.\"\n}\n]\n}\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n\n\n# Inference: Generation of the output\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(output_text)\n\n# Video\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n{\"type\": \"text\", \"text\": \"What happened in the video?\"},\n],\n}\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nvideo_fps=1,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,",
  "return_tensors=\"pt\"\n).to(model.device)\n\n# Inference: Generation of the output\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(output_text)\n```\n\n### Batch Mixed Media Inference\n\nThe model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.\n\n```python\n# Conversation for the first image\nconversation1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image1.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"}\n]\n}\n]\n\n# Conversation with two images\nconversation2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image2.jpg\"},\n{\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n{\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n]\n}\n]\n\n# Conversation with pure text\nconversation3 = [\n{\n\"role\": \"user\",\n\"content\": \"who are you?\"\n}\n]\n\n\n# Conversation with mixed midia\nconversation4 = [\n{\n\"role\": \"user\",\n\"content\": [",
  "{\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n{\"type\": \"image\", \"path\": \"/path/to/image4.jpg\"},\n{\"type\": \"video\", \"path\": \"/path/to/video.jpg\"},\n{\"type\": \"text\", \"text\": \"What are the common elements in these medias?\"},\n],\n}\n]\n\nconversations = [conversation1, conversation2, conversation3, conversation4]\n# Preparation for batch inference\nipnuts = processor.apply_chat_template(\nconversations,\nvideo_fps=1,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n\n\n# Batch Inference\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(output_text)\n```\n\n### Usage Tips\n\n#### Image Resolution trade-off\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs.\n\n```python",
  "min_pixels = 224*224\nmax_pixels = 2048*2048\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n```\n\nIn case of limited GPU RAM, one can reduce the resolution as follows:\n\n```python\nmin_pixels = 256*28*28\nmax_pixels = 1024*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n```\nThis ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n\n#### Multiple Image Inputs\n\nBy default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:\n\n```python\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Hello, how are you?\"}\n]\n},\n{\n\"role\": \"assistant\",\n\"content\": \"I'm doing well, thank you for asking. How can I assist you today?\"\n},\n{\n\"role\": \"user\",\n\"content\": [",
  "{\"type\": \"text\", \"text\": \"Can you describe these images and video?\"},\n{\"type\": \"image\"},\n{\"type\": \"image\"},\n{\"type\": \"video\"},\n{\"type\": \"text\", \"text\": \"These are from my vacation.\"}\n]\n},\n{\n\"role\": \"assistant\",\n\"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\"\n},\n{\n\"role\": \"user\",\n\"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\"\n}\n]\n\n# default:\nprompt_without_id = processor.apply_chat_template(conversation, add_generation_prompt=True)",
  "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n\n\n# add ids\nprompt_with_id = processor.apply_chat_template(conversation, add_generation_prompt=True, add_vision_id=True)",
  "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n\n```\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```",
  "Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using FlashAttention-2, add `attn_implementation=\"flash_attention_2\"` when loading the model:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-VL-7B-Instruct\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\n)\n```\n\n\n\n## Qwen2_5_VLConfig\n\n[[autodoc]] Qwen2_5_VLConfig\n\n## Qwen2_5_VLProcessor\n\n[[autodoc]] Qwen2_5_VLProcessor\n\n## Qwen2_5_VLModel\n\n[[autodoc]] Qwen2_5_VLModel\n- forward\n\n## Qwen2_5_VLForConditionalGeneration\n\n[[autodoc]] Qwen2_5_VLForConditionalGeneration\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# AltCLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe AltCLIP model was proposed in [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679v2) by Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu. AltCLIP",
  "(Altering the Language Encoder in CLIP) is a neural network trained on a variety of image-text and text-text pairs. By switching CLIP's\ntext encoder with a pretrained multilingual text encoder XLM-R, we could obtain very close performances with CLIP on almost all tasks, and extended original CLIP's capabilities such as multilingual understanding.\n\nThe abstract from the paper is the following:\n\n*In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model.\nStarting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained\nmultilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of\nteacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art\nperformances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with",
  "CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding.*\n\nThis model was contributed by [jongjyh](https://huggingface.co/jongjyh).\n\n## Usage tips and example\n\nThe usage of AltCLIP is very similar to the CLIP. the difference between CLIP is the text encoder. Note that we use bidirectional attention instead of casual attention\nand we take the [CLS] token in XLM-R to represent text embedding.\n\nAltCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. AltCLIP uses a ViT like transformer to get visual features and a bidirectional language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score.\n\nTo feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded. A [CLS] token is added to serve as representation of an entire image. The authors",
  "also add absolute position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder.\nThe [`CLIPImageProcessor`] can be used to resize (or rescale) and normalize images for the model.\n\nThe [`AltCLIPProcessor`] wraps a [`CLIPImageProcessor`] and a [`XLMRobertaTokenizer`] into a single instance to both\nencode the text and prepare the images. The following example shows how to get the image-text similarity scores using\n[`AltCLIPProcessor`] and [`AltCLIPModel`].\n\n```python\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import AltCLIPModel, AltCLIPProcessor\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score",
  ">>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```\n\n<Tip>\n\nThis model is based on `CLIPModel`, use it like you would use the original [CLIP](clip).\n\n</Tip>\n\n## AltCLIPConfig\n\n[[autodoc]] AltCLIPConfig\n- from_text_vision_configs\n\n## AltCLIPTextConfig\n\n[[autodoc]] AltCLIPTextConfig\n\n## AltCLIPVisionConfig\n\n[[autodoc]] AltCLIPVisionConfig\n\n## AltCLIPProcessor\n\n[[autodoc]] AltCLIPProcessor\n\n## AltCLIPModel\n\n[[autodoc]] AltCLIPModel\n- forward\n- get_text_features\n- get_image_features\n\n## AltCLIPTextModel\n\n[[autodoc]] AltCLIPTextModel\n- forward\n\n## AltCLIPVisionModel\n\n[[autodoc]] AltCLIPVisionModel\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PaliGemma\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The PaliGemma model was proposed in [PaliGemma – Google's Cutting-Edge Open Vision Language Model](https://huggingface.co/blog/paligemma) by Google. It is a 3B vision-language model composed by a [SigLIP](siglip) vision encoder and a [Gemma](gemma) language decoder linked by a multimodal linear projection. It cuts an image into a fixed number of VIT tokens and prepends it to an optional prompt. One particularity is that the model uses full block attention on all the image tokens plus the input text tokens. It comes in 3 resolutions, 224x224, 448x448 and 896x896 with 3 base models, with 55 fine-tuned versions for different tasks, and 2 mix models.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/paligemma/paligemma_arch.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> PaliGemma architecture. Taken from the <a href=\"https://huggingface.co/blog/paligemma\">blog post.</a> </small>\n\nThis model was contributed by [Molbap](https://huggingface.co/Molbap).\n\n## Usage tips",
  "- PaliGemma is not meant for conversational use, and it works best when fine-tuning to a specific use case. Some downstream tasks on which PaliGemma can be fine-tuned include image captioning, visual question answering (VQA), object detection, referring expression segmentation and document understanding.\n- One can use `PaliGemmaProcessor` to prepare images, text and optional labels for the model. When fine-tuning a PaliGemma model, the `suffix` argument can be passed to the processor which creates the `labels` for the model:\n\n```python\nprompt = \"What is on the flower?\"\nanswer = \"a bee\"\ninputs = processor(images=raw_image, text=prompt, suffix=answer, return_tensors=\"pt\")\n```\n\n## Usage Example\n\nThe model can accept a single or multiple images. According to the [paper](https://arxiv.org/abs/2407.07726v1), the checkpoint PaliGemma can transfer to tasks which take multiple images as input. NLVR2 is one such task, which asks one question about two images, and requires looking at both to give the correct answer. Here's an example code for single and multi image inference.\n\n### Single-image Inference\n\n```python\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration",
  "model_id = \"google/paligemma-3b-mix-224\"\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nprompt = \"What is on the flower?\"\nimage_file = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(raw_image, prompt, return_tensors=\"pt\")\noutput = model.generate(**inputs, max_new_tokens=20)\n\nprint(processor.decode(output[0], skip_special_tokens=True)[inputs.input_ids.shape[1]: ])\n```\n\n### Multi-image Inference\n\n```python\nmodel_id = \"google/paligemma-3b-ft-nlvr2-448\"  # checkpoint tuned for multiple images\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\nprocessor = PaliGemmaProcessor.from_pretrained(model_id)\n\nprompt = \"answer en Which of the two pictures shows a snowman, first or second?\"\nstop_sign_image = Image.open(\nrequests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw\n)\nsnow_image = Image.open(\nrequests.get(\n\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\", stream=True\n).raw\n)",
  "inputs = processor(images=[[snow_image, stop_sign_image]], text=prompt, return_tensors=\"pt\")\n\noutput = model.generate(**inputs, max_new_tokens=20)\nprint(processor.decode(output[0], skip_special_tokens=True)[inputs.input_ids.shape[1]: ])\n\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with PaliGemma. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- A blog post introducing all the features of PaliGemma can be found [here](https://huggingface.co/blog/paligemma).\n- Demo notebooks on how to fine-tune PaliGemma for VQA with the Trainer API along with inference can be found [here](https://github.com/huggingface/notebooks/tree/main/examples/paligemma).\n- Demo notebooks on how to fine-tune PaliGemma on a custom dataset (receipt image -> JSON) along with inference can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/PaliGemma). 🌎\n\n## PaliGemmaConfig\n\n[[autodoc]] PaliGemmaConfig\n\n## PaliGemmaProcessor",
  "[[autodoc]] PaliGemmaProcessor\n\n## PaliGemmaForConditionalGeneration\n\n[[autodoc]] PaliGemmaForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Encoder Decoder Models\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC",
  "\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe [`EncoderDecoderModel`] can be used to initialize a sequence-to-sequence model with any\npretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.\n\nThe effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks\nwas shown in [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by\nSascha Rothe, Shashi Narayan, Aliaksei Severyn.\n\nAfter such an [`EncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like\nany other models (see the examples for more information).\n\nAn application of this architecture could be to leverage two pretrained [`BertModel`] as the encoder\nand decoder for a summarization model as was shown in: [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345) by Yang Liu and Mirella Lapata.\n\n## Randomly initializing `EncoderDecoderModel` from model configurations.",
  "[`EncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`BertModel`] configuration for the encoder and the default [`BertForCausalLM`] configuration for the decoder.\n\n```python\n>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n\n>>> config_encoder = BertConfig()\n>>> config_decoder = BertConfig()\n\n>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n>>> model = EncoderDecoderModel(config=config)\n```\n\n## Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\n\n[`EncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained auto-encoding model, *e.g.* BERT, can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the decoder.",
  "Depending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.\nInitializing [`EncoderDecoderModel`] from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder).\nTo do so, the `EncoderDecoderModel` class provides a [`EncoderDecoderModel.from_encoder_decoder_pretrained`] method.\n\n```python\n>>> from transformers import EncoderDecoderModel, BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\")\n```\n\n## Loading an existing `EncoderDecoderModel` checkpoint and perform inference.\n\nTo load fine-tuned checkpoints of the `EncoderDecoderModel` class, [`EncoderDecoderModel`] provides the `from_pretrained(...)` method just like any other model architecture in Transformers.",
  "To perform inference, one uses the [`generate`] method, which allows to autoregressively generate text. This method supports various forms of decoding, such as greedy, beam search and multinomial sampling.\n\n```python\n>>> from transformers import AutoTokenizer, EncoderDecoderModel\n\n>>> # load a fine-tuned seq2seq model and corresponding tokenizer\n>>> model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n\n>>> # let's perform inference on a long piece of text\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n... )\n>>> input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids\n\n>>> # autoregressively generate summary (uses greedy decoding by default)\n>>> generated_ids = model.generate(input_ids)",
  ">>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\nnearly 800 thousand customers were affected by the shutoffs. the aim is to reduce the risk of wildfires. nearly 800, 000 customers were expected to be affected by high winds amid dry conditions. pg & e said it scheduled the blackouts to last through at least midday tomorrow.\n```\n\n## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\n\n[`TFEncoderDecoderModel.from_pretrained`] currently doesn't support initializing the model from a\npytorch checkpoint. Passing `from_pt=True` to this method will throw an exception. If there are only pytorch\ncheckpoints for a particular encoder-decoder model, a workaround is:\n\n```python\n>>> # a workaround to load from pytorch checkpoint\n>>> from transformers import EncoderDecoderModel, TFEncoderDecoderModel\n\n>>> _model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n\n>>> _model.encoder.save_pretrained(\"./encoder\")\n>>> _model.decoder.save_pretrained(\"./decoder\")\n\n>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(",
  "...     \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n... )\n>>> # This is only for copying some specific attributes of this particular model.\n>>> model.config = _model.config\n```\n\n## Training\n\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other encoder-decoder model.\nAs you can see, only 2 inputs are required for the model in order to compute a loss: `input_ids` (which are the\n`input_ids` of the encoded input sequence) and `labels` (which are the `input_ids` of the encoded\ntarget sequence).\n\n```python\n>>> from transformers import BertTokenizer, EncoderDecoderModel\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\")\n\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> input_ids = tokenizer(",
  "...     \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was  finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n...     return_tensors=\"pt\",\n... ).input_ids\n\n>>> labels = tokenizer(\n...     \"the eiffel tower surpassed the washington monument to become the tallest structure in the world. it was the first structure to reach a height of 300 metres in paris in 1930. it is now taller than the chrysler building by 5. 2 metres ( 17 ft ) and is the second tallest free - standing structure in paris.\",\n...     return_tensors=\"pt\",\n... ).input_ids",
  ">>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n```\n\nDetailed [colab](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=ZwQIEhKOrJpl) for training.\n\nThis model was contributed by [thomwolf](https://github.com/thomwolf). This model's TensorFlow and Flax versions\nwere contributed by [ydshieh](https://github.com/ydshieh).\n\n\n## EncoderDecoderConfig\n\n[[autodoc]] EncoderDecoderConfig\n\n<frameworkcontent>\n<pt>\n\n## EncoderDecoderModel\n\n[[autodoc]] EncoderDecoderModel\n- forward\n- from_encoder_decoder_pretrained\n\n</pt>\n<tf>\n\n## TFEncoderDecoderModel\n\n[[autodoc]] TFEncoderDecoderModel\n- call\n- from_encoder_decoder_pretrained\n\n</tf>\n<jax>\n\n## FlaxEncoderDecoderModel\n\n[[autodoc]] FlaxEncoderDecoderModel\n- __call__\n- from_encoder_decoder_pretrained\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ColPali\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The *ColPali* model was proposed in [ColPali: Efficient Document Retrieval with Vision Language Models](https://doi.org/10.48550/arXiv.2407.01449) by **Manuel Faysse***, **Hugues Sibille***, **Tony Wu***, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo (* denotes equal contribution). Work lead by ILLUIN Technology.\n\nIn our proposed *ColPali* approach, we leverage VLMs to construct efficient multi-vector embeddings directly from document images (“screenshots”) for document retrieval. We train the model to maximize the similarity between these document embeddings and the corresponding query embeddings, using the late interaction method introduced in ColBERT.\n\nUsing *ColPali* removes the need for potentially complex and brittle layout recognition and OCR pipelines with a single model that can take into account both the textual and visual content (layout, charts, etc.) of a document.\n\n## Resources\n\n- The *ColPali* arXiv paper can be found [here](https://doi.org/10.48550/arXiv.2407.01449). 📄\n- The official blog post detailing ColPali can be found [here](https://huggingface.co/blog/manu/colpali). 📝",
  "- The original model implementation code for the ColPali model and for the `colpali-engine` package can be found [here](https://github.com/illuin-tech/colpali). 🌎\n- Cookbooks for learning to use the transformers-native version of *ColPali*, fine-tuning, and similarity maps generation can be found [here](https://github.com/tonywu71/colpali-cookbooks). 📚\n\nThis model was contributed by [@tonywu71](https://huggingface.co/tonywu71) and [@yonigozlan](https://huggingface.co/yonigozlan).\n\n## Usage\n\nThis example demonstrates how to use *ColPali* to embed both queries and images, calculate their similarity scores, and identify the most relevant matches. For a specific query, you can retrieve the top-k most similar images by selecting the ones with the highest similarity scores.\n\n```python\nimport torch\nfrom PIL import Image\n\nfrom transformers import ColPaliForRetrieval, ColPaliProcessor\n\nmodel_name = \"vidore/colpali-v1.2-hf\"\n\nmodel = ColPaliForRetrieval.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(model_name)",
  "# Your inputs (replace dummy images with screenshots of your documents)\nimages = [\nImage.new(\"RGB\", (32, 32), color=\"white\"),\nImage.new(\"RGB\", (16, 16), color=\"black\"),\n]\nqueries = [\n\"What is the organizational structure for our R&D department?\",\n\"Can you provide a breakdown of last year’s financial performance?\",\n]\n\n# Process the inputs\nbatch_images = processor(images=images).to(model.device)\nbatch_queries = processor(text=queries).to(model.device)\n\n# Forward pass\nwith torch.no_grad():\nimage_embeddings = model(**batch_images).embeddings\nquery_embeddings = model(**batch_queries).embeddings\n\n# Score the queries against the images\nscores = processor.score_retrieval(query_embeddings, image_embeddings)\n```\n\n## ColPaliConfig\n\n[[autodoc]] ColPaliConfig\n\n## ColPaliProcessor\n\n[[autodoc]] ColPaliProcessor\n\n## ColPaliForRetrieval\n\n[[autodoc]] ColPaliForRetrieval\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SigLIP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The SigLIP model was proposed in [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. SigLIP proposes to replace the loss function used in [CLIP](clip) by a simple pairwise sigmoid loss. This results in better performance in terms of zero-shot classification accuracy on ImageNet.\n\nThe abstract from the paper is the following:",
  "*We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient.*\n\n## Usage tips\n\n- Usage of SigLIP is similar to [CLIP](clip). The main difference is the training loss, which does not require a global view of all the pairwise similarities of images and texts within a batch. One needs to apply the sigmoid activation function to the logits, rather than the softmax.",
  "- Training is supported but does not use `torch.distributed` utilities which may limit the scalability of batch size. However, DDP and FDSP works on single-node multi-gpu setup.\n- When using the standalone [`SiglipTokenizer`] or [`SiglipProcessor`], make sure to pass `padding=\"max_length\"` as that's how the model was trained.\n- To get the same results as the pipeline, a prompt template of \"This is a photo of {label}.\" should be used.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/siglip_table.jpeg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> SigLIP evaluation results compared to CLIP. Taken from the <a href=\"https://arxiv.org/abs/2303.15343\">original paper</a>.</small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/google-research/big_vision/tree/main).\n\n## Usage example\n\nThere are 2 main ways to use SigLIP: either using the pipeline API, which abstracts away all the complexity for you, or by using the `SiglipModel` class yourself.\n\n### Pipeline API\n\nThe pipeline allows to use the model in a few lines of code:\n\n```python",
  ">>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> # load pipe\n>>> image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\")\n\n>>> # load image\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # inference\n>>> candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n>>> outputs = image_classifier(image, candidate_labels=candidate_labels)\n>>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n>>> print(outputs)\n[{'score': 0.1979, 'label': '2 cats'}, {'score': 0.0, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n```\n\n### Using the model yourself\n\nIf you want to do the pre- and postprocessing yourself, here's how to do that:\n\n```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"",
  ">>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n# follows the pipeline prompt template to get same results\n>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n# important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n19.8% that image 0 is '2 cats'\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SigLIP.\n\n- [Zero-shot image classification task guide](../tasks/zero_shot_image_classification)\n- Demo notebooks for SigLIP can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SigLIP). 🌎",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n## Combining SigLIP and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> import requests\n>>> from PIL import Image\n>>> from transformers import SiglipProcessor, SiglipModel\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = SiglipModel.from_pretrained(\n...     \"google/siglip-so400m-patch14-384\",\n...     attn_implementation=\"flash_attention_2\",\n...     torch_dtype=torch.float16,\n...     device_map=device,\n... )\n>>> processor = SiglipProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")",
  ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n# follows the pipeline prompt template to get same results\n>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n# important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(device)\n\n>>> with torch.no_grad():\n...     with torch.autocast(device):\n...         outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n19.8% that image 0 is '2 cats'\n```\n\n\n## Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the",
  "[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nYou may set `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used. Make sure you have `torch>=2.1.1`.\n\n```python\n>>> from transformers import SiglipModel\n\n>>> model = SiglipModel.from_pretrained(\n...     \"google/siglip-so400m-patch14-384\",\n...     attn_implementation=\"sdpa\",\n...     torch_dtype=torch.float16,\n...     device_map=device,\n... )\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\n\n## Expected speedups\n\nBelow is an expected speedup diagram that compares inference time between the native implementation in transformers using `google/siglip-so400m-patch14-384` checkpoint in `float16` precision and the Flash Attention 2 / SDPA version of the model using different batch sizes.\n\n<div style=\"text-align: center\">\n<img src=\"https://i.imgur.com/cWm4rsn.png\">\n</div>\n\n\n## SiglipConfig",
  "[[autodoc]] SiglipConfig\n- from_text_vision_configs\n\n## SiglipTextConfig\n\n[[autodoc]] SiglipTextConfig\n\n## SiglipVisionConfig\n\n[[autodoc]] SiglipVisionConfig\n\n## SiglipTokenizer\n\n[[autodoc]] SiglipTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## SiglipImageProcessor\n\n[[autodoc]] SiglipImageProcessor\n- preprocess\n\n## SiglipImageProcessorFast\n\n[[autodoc]] SiglipImageProcessorFast\n- preprocess\n\n## SiglipProcessor\n\n[[autodoc]] SiglipProcessor\n\n## SiglipModel\n\n[[autodoc]] SiglipModel\n- forward\n- get_text_features\n- get_image_features\n\n## SiglipTextModel\n\n[[autodoc]] SiglipTextModel\n- forward\n\n## SiglipVisionModel\n\n[[autodoc]] SiglipVisionModel\n- forward\n\n\n## SiglipForImageClassification\n\n[[autodoc]] SiglipForImageClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PLBart\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe PLBART model was proposed in [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.",
  "This is a BART-like model which can be used to perform code-summarization, code-generation, and code-translation tasks. The pre-trained model `plbart-base` has been trained using multilingual denoising task\non Java, Python and English.\n\nAccording to the abstract\n\n*Code summarization and generation empower conversion between programming language (PL) and natural language (NL),\nwhile code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks.\nPLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding.\nExperiments on code summarization in the English language, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program\nrepair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding.",
  "Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow\n(e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels\neven with limited annotations.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The Authors' code can be found [here](https://github.com/wasiahmad/PLBART).\n\n## Usage examples\n\nPLBart is a multilingual encoder-decoder (sequence-to-sequence) model primarily intended for code-to-text, text-to-code, code-to-code tasks. As the\nmodel is multilingual it expects the sequences in a different format. A special language id token is added in both the\nsource and target text. The source text format is `X [eos, src_lang_code]` where `X` is the source text. The\ntarget text format is `[tgt_lang_code] X [eos]`. `bos` is never used.\n\nHowever, for fine-tuning, in some cases no language token is provided in cases where a single language is used. Please refer to [the paper](https://arxiv.org/abs/2103.06333) to learn more about this.",
  "In cases where the language code is needed, the regular [`~PLBartTokenizer.__call__`] will encode source text format\nwhen you pass texts as the first argument or with the keyword argument `text`, and will encode target text format if\nit's passed with the `text_target` keyword argument.\n\n### Supervised training\n\n```python\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\", src_lang=\"en_XX\", tgt_lang=\"python\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> expected_translation_english = \"Returns the maximum value of a b c.\"\n>>> inputs = tokenizer(example_python_phrase, text_target=expected_translation_english, return_tensors=\"pt\")\n>>> model(**inputs)\n```\n\n### Generation\n\nWhile generating the target text set the `decoder_start_token_id` to the target language id. The following\nexample shows how to translate Python to English using the `uclanlp/plbart-python-en_XX` model.\n\n```python\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer",
  ">>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> inputs = tokenizer(example_python_phrase, return_tensors=\"pt\")\n>>> model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")\n>>> translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"Returns the maximum value of a b c.\"\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## PLBartConfig\n\n[[autodoc]] PLBartConfig\n\n## PLBartTokenizer\n\n[[autodoc]] PLBartTokenizer\n- build_inputs_with_special_tokens\n\n## PLBartModel\n\n[[autodoc]] PLBartModel\n- forward\n\n## PLBartForConditionalGeneration\n\n[[autodoc]] PLBartForConditionalGeneration\n- forward\n\n## PLBartForSequenceClassification\n\n[[autodoc]] PLBartForSequenceClassification\n- forward",
  "## PLBartForCausalLM\n\n[[autodoc]] PLBartForCausalLM\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# T5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) by [Colin Raffel](https://huggingface.co/craffel), Noam Shazeer, [Adam Roberts](https://huggingface.co/adarob), Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, [Peter J. Liu](https://huggingface.co/peterjliu).\n\nThe abstract from the paper is the following:\n\n*Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream\ntask, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning\nhas given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of\ntransfer learning techniques for NLP by introducing a unified framework that converts every language problem into a\ntext-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled datasets, transfer\napproaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration",
  "with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering\nsummarization, question answering, text classification, and more. To facilitate future work on transfer learning for\nNLP, we release our dataset, pre-trained models, and code.*\n\nAll checkpoints can be found on the [hub](https://huggingface.co/models?search=t5).\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/google-research/text-to-text-transfer-transformer).\n\n## Usage tips\n\n- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which\neach task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a\ndifferent prefix to the input corresponding to each task, e.g., for translation: *translate English to German: ...*,\nfor summarization: *summarize: ...*.",
  "- The pretraining includes both supervised and self-supervised training. Supervised training is conducted on downstream tasks provided by the GLUE and SuperGLUE benchmarks (converting them into text-to-text tasks as explained above).\n- Self-supervised training uses corrupted tokens, by randomly removing 15% of the tokens and replacing them with individual sentinel tokens (if several consecutive tokens are marked for removal, the whole group is replaced with a single sentinel token). The input of the encoder is the corrupted sentence, the input of the decoder is the original sentence and the target is then the dropped out tokens delimited by their sentinel tokens.\n\n- T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right.\n\n- See the [training](#training), [inference](#inference) and [resources](#resources) sections below for all details regarding usage.\n\nT5 comes in different sizes:\n\n- [google-t5/t5-small](https://huggingface.co/google-t5/t5-small)\n\n- [google-t5/t5-base](https://huggingface.co/google-t5/t5-base)\n\n- [google-t5/t5-large](https://huggingface.co/google-t5/t5-large)\n\n- [google-t5/t5-3b](https://huggingface.co/google-t5/t5-3b)",
  "- [google-t5/t5-11b](https://huggingface.co/google-t5/t5-11b).\n\nBased on the original T5 model, Google has released some follow-up works:\n\n- **T5v1.1**: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without\nmixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found [here](t5v1.1).\n\n- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\nthe documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\nto the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of\ndatasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.",
  "- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to\nthe documentation of mT5 which can be found [here](umt5).\n\n## Training\n\nT5 is an encoder-decoder model and converts all NLP problems into a text-to-text format. It is trained using teacher\nforcing. This means that for training, we always need an input sequence and a corresponding target sequence. The input\nsequence is fed to the model using `input_ids`. The target sequence is shifted to the right, i.e., prepended by a\nstart-sequence token and fed to the decoder using the `decoder_input_ids`. In teacher-forcing style, the target\nsequence is then appended by the EOS token and corresponds to the `labels`. The PAD token is hereby used as the\nstart-sequence token. T5 can be trained / fine-tuned both in a supervised and unsupervised fashion.\n\nOne can use [`T5ForConditionalGeneration`] (or the Tensorflow/Flax variant), which includes the\nlanguage modeling head on top of the decoder.\n\n- Unsupervised denoising training",
  "In this setup, spans of the input sequence are masked by so-called sentinel tokens (*a.k.a* unique mask tokens) and\nthe output sequence is formed as a concatenation of the same sentinel tokens and the *real* masked tokens. Each\nsentinel token represents a unique mask token for this sentence and should start with `<extra_id_0>`,\n`<extra_id_1>`, ... up to `<extra_id_99>`. As a default, 100 sentinel tokens are available in\n[`T5Tokenizer`].\n\nFor instance, the sentence \"The cute dog walks in the park\" with the masks put on \"cute dog\" and \"the\" should be\nprocessed as follows:\n\n```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n>>> loss.item()\n3.7837\n```",
  "If you're interested in pre-training T5 on a new corpus, check out the [run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling) script in the Examples\ndirectory.\n\n- Supervised training\n\nIn this setup, the input sequence and output sequence are a standard sequence-to-sequence input-output mapping.\nSuppose that we want to fine-tune the model for translation for example, and we have a training example: the input\nsequence \"The house is wonderful.\" and output sequence \"Das Haus ist wunderbar.\", then they should be prepared for\nthe model as follows:\n\n```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n\n>>> input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n>>> loss.item()\n0.2542\n```",
  "As you can see, only 2 inputs are required for the model in order to compute a loss: `input_ids` (which are the\n`input_ids` of the encoded input sequence) and `labels` (which are the `input_ids` of the encoded\ntarget sequence). The model will automatically create the `decoder_input_ids` based on the `labels`, by\nshifting them one position to the right and prepending the `config.decoder_start_token_id`, which for T5 is\nequal to 0 (i.e. the id of the pad token). Also note the task prefix: we prepend the input sequence with 'translate\nEnglish to German: ' before encoding it. This will help in improving the performance, as this task prefix was used\nduring T5's pre-training.\n\nHowever, the example above only shows a single training example. In practice, one trains deep learning models in\nbatches. This entails that we must pad/truncate examples to the same length. For encoder-decoder models, one\ntypically defines a `max_source_length` and `max_target_length`, which determine the maximum length of the\ninput and output sequences respectively (otherwise they are truncated). These should be carefully set depending on\nthe task.",
  "In addition, we must make sure that padding token id's of the `labels` are not taken into account by the loss\nfunction. In PyTorch and Tensorflow, this can be done by replacing them with -100, which is the `ignore_index`\nof the `CrossEntropyLoss`. In Flax, one can use the `decoder_attention_mask` to ignore padded tokens from\nthe loss (see the [Flax summarization script](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization) for details). We also pass\n`attention_mask` as additional input to the model, which makes sure that padding tokens of the inputs are\nignored. The code example below illustrates all of this.\n\n```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n>>> import torch\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n\n>>> # the following 2 hyperparameters are task-specific\n>>> max_source_length = 512\n>>> max_target_length = 128\n\n>>> # Suppose we have the following 2 training examples:\n>>> input_sequence_1 = \"Welcome to NYC\"\n>>> output_sequence_1 = \"Bienvenue à NYC\"\n\n>>> input_sequence_2 = \"HuggingFace is a company\"",
  ">>> output_sequence_2 = \"HuggingFace est une entreprise\"\n\n>>> # encode the inputs\n>>> task_prefix = \"translate English to French: \"\n>>> input_sequences = [input_sequence_1, input_sequence_2]\n\n>>> encoding = tokenizer(\n...     [task_prefix + sequence for sequence in input_sequences],\n...     padding=\"longest\",\n...     max_length=max_source_length,\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n\n>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n\n>>> # encode the targets\n>>> target_encoding = tokenizer(\n...     [output_sequence_1, output_sequence_2],\n...     padding=\"longest\",\n...     max_length=max_target_length,\n...     truncation=True,\n...     return_tensors=\"pt\",\n... )\n>>> labels = target_encoding.input_ids\n\n>>> # replace padding token id's of the labels by -100 so it's ignored by the loss\n>>> labels[labels == tokenizer.pad_token_id] = -100\n\n>>> # forward pass\n>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n>>> loss.item()\n0.188\n```\n\nAdditional training tips:\n\n- T5 models need a slightly higher learning rate than the default one set in the `Trainer` when using the AdamW",
  "optimizer. Typically, 1e-4 and 3e-4 work well for most problems (classification, summarization, translation, question\nanswering, question generation). Note that T5 was pre-trained using the AdaFactor optimizer.\n\nAccording to [this forum post](https://discuss.huggingface.co/t/t5-finetuning-tips/684), task prefixes matter when\n(1) doing multi-task training (2) your task is similar or related to one of the supervised tasks used in T5's\npre-training mixture (see Appendix D of the [paper](https://arxiv.org/pdf/1910.10683.pdf) for the task prefixes\nused).\n\nIf training on TPU, it is recommended to pad all examples of the dataset to the same length or make use of\n*pad_to_multiple_of* to have a small number of predefined bucket sizes to fit all examples in. Dynamically padding\nbatches to the longest example is not recommended on TPU as it triggers a recompilation for every batch shape that is\nencountered during training thus significantly slowing down the training. only padding up to the longest example in a\nbatch) leads to very slow training on TPU.\n\n## Inference\n\nAt inference time, it is recommended to use [`~generation.GenerationMixin.generate`]. This",
  "method takes care of encoding the input and feeding the encoded hidden states via cross-attention layers to the decoder\nand auto-regressively generates the decoder output. Check out [this blog post](https://huggingface.co/blog/how-to-generate) to know all the details about generating text with Transformers.\nThere's also [this blog post](https://huggingface.co/blog/encoder-decoder#encoder-decoder) which explains how\ngeneration works in general in encoder-decoder models.\n\n```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n\n>>> input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\nDas Haus ist wunderbar.\n```\n\nNote that T5 uses the `pad_token_id` as the `decoder_start_token_id`, so when doing generation without using\n[`~generation.GenerationMixin.generate`], make sure you start it with the `pad_token_id`.",
  "The example above only shows a single example. You can also do batched inference, like so:\n\n```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n\n>>> task_prefix = \"translate English to German: \"\n>>> # use different length sentences to test batching\n>>> sentences = [\"The house is wonderful.\", \"I like to work in NYC.\"]\n\n>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n\n>>> output_sequences = model.generate(\n...     input_ids=inputs[\"input_ids\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     do_sample=False,  # disable sampling to test if batching affects output\n... )\n\n>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']\n```\n\nBecause T5 has been trained with the span-mask denoising objective,\nit can be used to predict the sentinel (masked-out) tokens during inference.\nThe predicted tokens will then be placed between the sentinel tokens.\n\n```python",
  ">>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n\n>>> sequence_ids = model.generate(input_ids)\n>>> sequences = tokenizer.batch_decode(sequence_ids)\n>>> sequences\n['<pad> <extra_id_0> park offers <extra_id_1> the <extra_id_2> park.</s>']\n```\n\n## Performance\n\nIf you'd like a faster training and inference performance, install [NVIDIA APEX](https://github.com/NVIDIA/apex#quick-start) for NVIDIA GPUs, or [ROCm APEX](https://github.com/ROCmSoftwarePlatform/apex) for AMD GPUs and then the model will automatically use `apex.normalization.FusedRMSNorm` instead of `T5LayerNorm`. The former uses an optimized fused kernel which is several times faster than the latter.\n\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with T5. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A notebook for how to [finetune T5 for classification and multiple choice](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb).\n- A notebook for how to [finetune T5 for sentiment span extraction](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb). 🌎\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- A notebook for how to [finetune T5 for named entity recognition](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing). 🌎\n\n<PipelineTag pipeline=\"text-generation\"/>",
  "- A notebook for [Finetuning CodeT5 for generating docstrings from Ruby code](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb).\n\n<PipelineTag pipeline=\"summarization\"/>\n\n- A notebook to [Finetune T5-base-dutch to perform Dutch abstractive summarization on a TPU](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb).\n- A notebook for how to [finetune T5 for summarization in PyTorch and track experiments with WandB](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb#scrollTo=OKRpFvYhBauC). 🌎\n- A blog post on [Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq).",
  "- [`T5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb).\n- [`TFT5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n- [`FlaxT5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization).\n- [Summarization](https://huggingface.co/course/chapter7/5?fw=pt#summarization) chapter of the 🤗 Hugging Face course.\n- [Summarization task guide](../tasks/summarization)\n\n<PipelineTag pipeline=\"fill-mask\"/>",
  "- [`FlaxT5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling) for training T5 with a span-masked language model objective. The script also shows how to train a T5 tokenizer. [`FlaxT5ForConditionalGeneration`] is also supported by this [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n\n<PipelineTag pipeline=\"translation\"/>\n\n- [`T5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb).\n- [`TFT5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n- [Translation task guide](../tasks/translation)\n\n<PipelineTag pipeline=\"question-answering\"/>",
  "- A notebook on how to [finetune T5 for question answering with TensorFlow 2](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb). 🌎\n- A notebook on how to [finetune T5 for question answering on a TPU](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil).\n\n🚀 **Deploy**\n- A blog post on how to deploy [T5 11B for inference for less than $500](https://www.philschmid.de/deploy-t5-11b).\n\n## T5Config\n\n[[autodoc]] T5Config\n\n## T5Tokenizer\n\n[[autodoc]] T5Tokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## T5TokenizerFast\n\n[[autodoc]] T5TokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## T5Model\n\n[[autodoc]] T5Model\n- forward\n\n## T5ForConditionalGeneration\n\n[[autodoc]] T5ForConditionalGeneration\n- forward\n\n## T5EncoderModel\n\n[[autodoc]] T5EncoderModel\n- forward\n\n## T5ForSequenceClassification\n\n[[autodoc]] T5ForSequenceClassification\n- forward\n\n## T5ForTokenClassification\n\n[[autodoc]] T5ForTokenClassification\n- forward\n\n## T5ForQuestionAnswering",
  "[[autodoc]] T5ForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFT5Model\n\n[[autodoc]] TFT5Model\n- call\n\n## TFT5ForConditionalGeneration\n\n[[autodoc]] TFT5ForConditionalGeneration\n- call\n\n## TFT5EncoderModel\n\n[[autodoc]] TFT5EncoderModel\n- call\n\n</tf>\n<jax>\n\n## FlaxT5Model\n\n[[autodoc]] FlaxT5Model\n- __call__\n- encode\n- decode\n\n## FlaxT5ForConditionalGeneration\n\n[[autodoc]] FlaxT5ForConditionalGeneration\n- __call__\n- encode\n- decode\n\n## FlaxT5EncoderModel\n\n[[autodoc]] FlaxT5EncoderModel\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ShieldGemma 2\n\n## Overview",
  "The ShieldGemma 2 model was proposed in a forthcoming technical report by Google. ShieldGemma 2 is built on [Gemma 3](https://ai.google.dev/gemma/docs/core/model_card_3), is a 4 billion (4B) parameter model that checks the safety of both synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm as defined below:\n\n-   No Sexually Explicit content: The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).\n-   No Dangerous Content: The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide).\n-   No Violence/Gore content: The image shall not contain content that depicts shocking, sensational, or gratuitous violence (e.g., excessive blood and gore, gratuitous violence against animals, extreme injury or moment of death).",
  "We recommend using ShieldGemma 2 as an input filter to vision language models, or as an output filter of image generation systems. To train a robust image safety model, we curated training datasets of natural and synthetic images and instruction-tuned Gemma 3 to demonstrate strong performance.\n\nThis model was contributed by [Ryan Mullins](https://huggingface.co/RyanMullins).\n\n## Usage Example\n\n- ShieldGemma 2 provides a Processor that accepts a list of `images` and an optional list of `policies` as input, and constructs a batch of prompts as the product of these two lists using the provided chat template.\n- You can extend ShieldGemma's built-in in policies with the `custom_policies` argument to the Processor. Using the same key as one of the built-in policies will overwrite that policy with your custom defintion.\n- ShieldGemma 2 does not support the image cropping capabilities used by Gemma 3.\n\n### Classification against Built-in Policies\n\n```python\nfrom PIL import Image\nimport requests\nfrom transformers import AutoProcessor, ShieldGemma2ForImageClassification\n\nmodel_id = \"google/shieldgemma-2-4b-it\"",
  "model = ShieldGemma2ForImageClassification.from_pretrained(model_id, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(images=[image], return_tensors=\"pt\").to(model.device)\n\noutput = model(**inputs)\nprint(output.probabilities)\n```\n\n### Classification against Custom Policies\n\n```python\nfrom PIL import Image\nimport requests\nfrom transformers import AutoProcessor, ShieldGemma2ForImageClassification\n\nmodel_id = \"google/shieldgemma-2-4b-it\"\nmodel = ShieldGemma2ForImageClassification.from_pretrained(model_id, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ncustom_policies = {\n\"key_a\": \"descrition_a\",\n\"key_b\": \"descrition_b\",\n}\n\ninputs = processor(\nimages=[image],\ncustom_policies=custom_policies,\npolicies=[\"dangerous\", \"key_a\", \"key_b\"],\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutput = model(**inputs)\nprint(output.probabilities)",
  "```\n\n\n## ShieldGemma2Processor\n\n[[autodoc]] ShieldGemma2Processor\n\n## ShieldGemma2Config\n\n[[autodoc]] ShieldGemma2Config\n\n## ShieldGemma2ForImageClassification\n\n[[autodoc]] ShieldGemma2ForImageClassification\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mistral3\n\n## Overview\n\nBuilding upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.\n\nIt is ideal for:\n- Fast-response conversational agents.\n- Low-latency function calling.",
  "- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n- Programming and math reasoning.\n- Long document understanding.\n- Visual understanding.\n\nThis model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez) and [yonigozlan](https://huggingface.co/yonigozlan).\n\nThe original code can be found [here](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/pixtral.py) and [here](https://github.com/mistralai/mistral-common).\n\n## Usage example\n\n### Inference with Pipeline\n\nHere is how you can use the `image-text-to-text` pipeline to perform inference with the `Mistral3` models in just a few lines of code:\n```python\n>>> from transformers import pipeline\n\n>>> messages = [\n...     {\n...         \"role\": \"user\",\n...         \"content\": [\n...             {\n...                 \"type\": \"image\",\n...                 \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n...             },\n...             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n...         ],\n...     },\n... ]",
  ">>> pipe = pipeline(\"image-text-to-text\", model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", torch_dtype=torch.bfloat16)\n>>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)\n>>> outputs[0][\"generated_text\"]\n'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'\n```\n### Inference on a single image\n\nThis example demonstrates how to perform inference on a single image with the Mistral3 models using chat templates.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n...     {\n...         \"role\": \"user\",\n...         \"content\": [\n...             {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},",
  "...             {\"type\": \"text\", \"text\": \"Describe this image\"},\n...         ],\n...     }\n... ]\n\n>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n>>> generate_ids = model.generate(**inputs, max_new_tokens=20)\n>>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n\n>>> decoded_output\n\"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an\"...\n```\n\n### Text-only generation\nThis example shows how to generate text using the Mistral3 model without providing any image input.\n\n\n````python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)",
  ">>> SYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n>>> user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n\n>>> messages = [\n...    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n...    {\"role\": \"user\", \"content\": user_prompt},\n... ]\n\n>>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n>>> inputs = processor(text=text, return_tensors=\"pt\").to(0, dtype=torch.float16)\n>>> generate_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n>>> decoded_output = processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)[0]\n\n>>> print(decoded_output)\n\"1. À plus tard!\n2. Salut, à plus!\n3. À toute!\n4. À la prochaine!\n5. Je me casse, à plus!\n\n```\n/\\_/\\\n( o.o )\n> ^ <\n```\"\n````\n\n### Batched image and text inputs\nMistral3 models also support batched image and text inputs.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"",
  ">>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n...     [\n...         {\n...             \"role\": \"user\",\n...             \"content\": [\n...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n...             ],\n...         },\n...     ],\n...     [\n...         {\n...             \"role\": \"user\",\n...             \"content\": [\n...                 {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n...                 {\"type\": \"text\", \"text\": \"Describe this image\"},\n...             ],\n...         },\n...     ],\n... ]\n\n\n>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n>>> output = model.generate(**inputs, max_new_tokens=25)\n\n>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n>>> decoded_outputs",
  "[\"Write a haiku for this imageCalm waters reflect\\nWhispers of the forest's breath\\nPeace on wooden path\"\n, \"Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese\"]\n```\n\n### Batched multi-image input and quantization with BitsAndBytes\nThis implementation of the Mistral3 models supports batched text-images inputs with different number of images for each text.\nThis example also how to use `BitsAndBytes` to load the model in 4bit quantization.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n>>> model = AutoModelForImageTextToText.from_pretrained(\n...     model_checkpoint, quantization_config=quantization_config\n... )\n\n>>> messages = [\n...     [\n...         {\n...             \"role\": \"user\",\n...             \"content\": [",
  "...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n...             ],\n...         },\n...     ],\n...     [\n...         {\n...             \"role\": \"user\",\n...             \"content\": [\n...                 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n...                 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n...                 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n...             ],\n...         },\n...     ],\n>>> ]\n\n>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n>>> output = model.generate(**inputs, max_new_tokens=25)\n\n>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n>>> decoded_outputs",
  "[\"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's wooden path\\nSilent forest stands guard\\n\", \"These images depict two different landmarks. Can you identify them? Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\"]\n```\n\n\n## Mistral3Config\n\n[[autodoc]] Mistral3Config\n\n\n## Mistral3ForConditionalGeneration\n\n[[autodoc]] Mistral3ForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OpenAI GPT2\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=gpt2\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-gpt2-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/gpt2\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview",
  "OpenAI GPT-2 model was proposed in [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by Alec\nRadford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from [OpenAI](https://huggingface.co/openai). It's a causal (unidirectional)\ntransformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\n*GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million\nweb pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some\ntext. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks\nacross diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than\n10X the amount of data.*\n\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) is a webapp created and hosted by",
  "Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five\ndifferent sizes: small, medium, large, xl and a distilled version of the small checkpoint: *distilgpt-2*.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://openai.com/blog/better-language-models/).\n\n## Usage tips\n\n- GPT-2 is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\ntoken in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\nobserved in the *run_generation.py* example script.\n- The model can take the *past_key_values* (for PyTorch) or *past* (for TF) as input, which is the previously computed\nkey/value attention pairs. Using this (*past_key_values* or *past*) value prevents the model from re-computing\npre-computed values in the context of text generation. For PyTorch, see *past_key_values* argument of the",
  "[`GPT2Model.forward`] method, or for TF the *past* argument of the\n[`TFGPT2Model.call`] method for more information on its usage.\n- Enabling the *scale_attn_by_inverse_layer_idx* and *reorder_and_upcast_attn* flags will apply the training stability\nimprovements from [Mistral](https://github.com/stanford-crfm/mistral/) (for PyTorch only).\n\n## Usage example\n\nThe `generate()` method can be used to generate text using GPT2 model.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n>>> prompt = \"GPT2 is a model developed by OpenAI.\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```\n\n## Using Flash Attention 2\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on `cuda` kernels.\n\n### Installation",
  "First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n\nNext, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n### Usage\n\nTo load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\n```python\n>>> import torch",
  ">>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n>>> prompt = \"def hello_world():\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n>>> tokenizer.batch_decode(generated_ids)[0]\n```\n\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `gpt2` checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/EduardoPacheco/documentation-images/resolve/main/gpt2_flash_attention_2_speedup.jpg\">\n</div>\n\n\n## Using Scaled Dot Product Attention (SDPA)\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function",
  "encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using `float16` with\n[gpt2-large](https://huggingface.co/openai-community/gpt2-large), we saw the\nfollowing speedups during training and inference.\n\n### Training",
  "| Batch size | Seq len |  Time per batch (Eager - s) | Time per batch (SDPA - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) |    Mem saving (%) |\n|-----------:|--------:|----------------------------:|--------------------------:|------------:|--------------------:|-------------------:|------------------:|\n|          1 |     128 |                       0.039 |                     0.032 |      23.042 |             3482.32 |            3494.62 |            -0.352 |\n|          1 |     256 |                       0.073 |                     0.059 |       25.15 |             3546.66 |             3552.6 |            -0.167 |\n|          1 |     512 |                       0.155 |                     0.118 |       30.96 |              4230.1 |            3665.59 |              15.4 |\n|          1 |    1024 |                       0.316 |                     0.209 |      50.839 |             8682.26 |            4881.09 |            77.875 |\n|          2 |     128 |                        0.07 |                      0.06 |      15.324 |              3557.8 |            3545.91 |             0.335 |",
  "|          2 |     256 |                       0.143 |                     0.122 |       16.53 |              3901.5 |            3657.68 |             6.666 |\n|          2 |     512 |                       0.267 |                     0.213 |      25.626 |             7062.21 |            4876.47 |            44.822 |\n|          2 |    1024 |                         OOM |                     0.404 |           / |                 OOM |            8096.35 | SDPA does not OOM |\n|          4 |     128 |                       0.134 |                     0.128 |       4.412 |             3675.79 |            3648.72 |             0.742 |\n|          4 |     256 |                       0.243 |                     0.217 |      12.292 |             6129.76 |            4871.12 |            25.839 |\n|          4 |     512 |                       0.494 |                     0.406 |      21.687 |             12466.6 |            8102.64 |            53.858 |\n|          4 |    1024 |                         OOM |                     0.795 |           / |                 OOM |            14568.2 | SDPA does not OOM |\n\n### Inference",
  "| Batch size | Seq len | Per token latency Eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem Eager (MB) | Mem SDPA (MB) | Mem saved (%) |\n|-----------:|--------:|-----------------------------:|----------------------------:|------------:|---------------:|--------------:|--------------:|\n|          1 |     128 |                        7.991 |                       6.968 |      14.681 |         1685.2 |       1701.32 |        -0.947 |\n|          1 |     256 |                        8.462 |                       7.199 |      17.536 |        1745.49 |       1770.78 |        -1.428 |\n|          1 |     512 |                         8.68 |                       7.853 |      10.529 |        1907.69 |       1921.29 |        -0.708 |\n|          1 |     768 |                        9.101 |                       8.365 |       8.791 |        2032.93 |       2068.12 |        -1.701 |\n|          2 |     128 |                        9.169 |                       9.001 |       1.861 |        1803.84 |        1811.4 |        -0.418 |\n|          2 |     256 |                        9.907 |                        9.78 |       1.294 |        1907.72 |       1921.44 |        -0.714 |",
  "|          2 |     512 |                       11.519 |                      11.644 |      -1.071 |        2176.86 |       2197.75 |        -0.951 |\n|          2 |     768 |                       13.022 |                      13.407 |      -2.873 |         2464.3 |       2491.06 |        -1.074 |\n|          4 |     128 |                       10.097 |                       9.831 |       2.709 |        1942.25 |       1985.13 |         -2.16 |\n|          4 |     256 |                       11.599 |                      11.398 |       1.764 |        2177.28 |       2197.86 |        -0.937 |\n|          4 |     512 |                       14.653 |                       14.45 |       1.411 |        2753.16 |       2772.57 |          -0.7 |\n|          4 |     768 |                       17.846 |                      17.617 |       1.299 |        3327.04 |       3343.97 |        -0.506 |\n\n\n\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GPT2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).\n- A blog on [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) with GPT-2.\n- A blog on [Training CodeParrot 🦜 from Scratch](https://huggingface.co/blog/codeparrot), a large GPT-2 model.\n- A blog on [Faster Text Generation with TensorFlow and XLA](https://huggingface.co/blog/tf-xla-generate) with GPT-2.\n- A blog on [How to train a Language Model with Megatron-LM](https://huggingface.co/blog/megatron-training) with a GPT-2 model.",
  "- A notebook on how to [finetune GPT2 to generate lyrics in the style of your favorite artist](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb). 🌎\n- A notebook on how to [finetune GPT2 to generate tweets in the style of your favorite Twitter user](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb). 🌎\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the 🤗 Hugging Face Course.\n- [`GPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).",
  "- [`TFGPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxGPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## GPT2Config\n\n[[autodoc]] GPT2Config\n\n## GPT2Tokenizer\n\n[[autodoc]] GPT2Tokenizer\n- save_vocabulary\n\n## GPT2TokenizerFast\n\n[[autodoc]] GPT2TokenizerFast\n\n## GPT2 specific outputs\n\n[[autodoc]] models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput\n\n[[autodoc]] models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput\n\n<frameworkcontent>\n<pt>",
  "## GPT2Model\n\n[[autodoc]] GPT2Model\n- forward\n\n## GPT2LMHeadModel\n\n[[autodoc]] GPT2LMHeadModel\n- forward\n\n## GPT2DoubleHeadsModel\n\n[[autodoc]] GPT2DoubleHeadsModel\n- forward\n\n## GPT2ForQuestionAnswering\n\n[[autodoc]] GPT2ForQuestionAnswering\n- forward\n\n## GPT2ForSequenceClassification\n\n[[autodoc]] GPT2ForSequenceClassification\n- forward\n\n## GPT2ForTokenClassification\n\n[[autodoc]] GPT2ForTokenClassification\n- forward\n\n</pt>\n<tf>\n\n## TFGPT2Model\n\n[[autodoc]] TFGPT2Model\n- call\n\n## TFGPT2LMHeadModel\n\n[[autodoc]] TFGPT2LMHeadModel\n- call\n\n## TFGPT2DoubleHeadsModel\n\n[[autodoc]] TFGPT2DoubleHeadsModel\n- call\n\n## TFGPT2ForSequenceClassification\n\n[[autodoc]] TFGPT2ForSequenceClassification\n- call\n\n## TFSequenceClassifierOutputWithPast\n\n[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutputWithPast\n\n## TFGPT2Tokenizer\n\n[[autodoc]] TFGPT2Tokenizer\n\n</tf>\n<jax>\n\n## FlaxGPT2Model\n\n[[autodoc]] FlaxGPT2Model\n- __call__\n\n## FlaxGPT2LMHeadModel\n\n[[autodoc]] FlaxGPT2LMHeadModel\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OLMo2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The OLMo2 model is the successor of the OLMo model, which was proposed in\n[OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838).\n\nThe architectural changes from the original OLMo model to this model are:\n\n- RMSNorm is used instead of standard layer norm.\n- Norm is applied to attention queries and keys.\n- Norm is applied after attention/feedforward layers rather than before.\n\nThis model was contributed by [shanearora](https://huggingface.co/shanearora).\nThe original code can be found [here](https://github.com/allenai/OLMo/tree/main/olmo).\n\n\n## Olmo2Config\n\n[[autodoc]] Olmo2Config\n\n## Olmo2Model\n\n[[autodoc]] Olmo2Model\n- forward\n\n## Olmo2ForCausalLM\n\n[[autodoc]] Olmo2ForCausalLM\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LLaVA-OneVision\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The LLaVA-OneVision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n\nLLaVA-OneVision is a Vision-Language Model that can generate text conditioned on one or several images/videos. The model consists of SigLIP vision encoder and a Qwen2 language backbone. The images are processed with anyres-9 technique where the image is split into 9 patches to better process high resolution images and capture as much details as possible. However, videos are pooled to a total sequence length of 196 tokens each frame for more memory efficient computation. LLaVA-OneVision is available in three sizes: 0.5B, 7B and 72B and achieves remarkable performance on benchmark evaluations.\n\nThe abstract from the paper is the following:\n\n*We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that",
  "LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios:\nsingle-image, multi-image, and video scenarios. Importantly, the design of LLaVAOneVision allows strong transfer learning across different modalities/scenarios,\nyielding new emerging capabilities. In particular, strong video understanding and\ncross-scenario capabilities are demonstrated through task transfer from images to\nvideos.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava-ov-acrhitecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> LLaVA-OneVision architecture. Taken from the <a href=\"https://arxiv.org/abs/2408.03326\">original paper.</a> </small>\n\nTips:\n\n- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n\n<Tip warning={true}>",
  "- Llava-OneVision uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n\n</Tip>\n\n\n### Formatting Prompts with Chat Templates\n\nEach **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.\n\n**Important:**\n- You must construct a conversation history — passing a plain string won't work.\n- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.\n- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.\n\n\nHere’s an example of how to structure your input.\nWe will use [llava-onevision-qwen2-7b-si-hf](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-si-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n\n```python\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-si-hf\")",
  "conversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What’s shown in this image?\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n},\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe the image in more details.\"},\n],\n},\n]\n\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\n# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images\nprint(text_prompt)\n'<|im_start|>user\\n<image>What is shown in this image?<|im_end|>\\n<|im_start|>assistant\\nPage showing the list of options.<|im_end|>'\n```\n\n🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n\n\nThis model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\nThe original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main).\n\n\n## Usage example\n\n### Single image inference",
  "Here's how to load the model and perform inference in half-precision (`torch.float16`):\n\n```python\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\n\"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\ndevice_map=\"cuda:0\"\n)\n\n# prepare image and text prompt, using the appropriate prompt template\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": url},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\ninputs = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\ninputs = inputs.to(\"cuda:0\", torch.float16)\n\n# autoregressively complete prompt\noutput = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(output[0], skip_special_tokens=True))",
  "'user\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, also known as a spider chart or a star chart, which is used to compare multiple quantitative variables. Each axis represents a different variable, and the chart is filled with'\n```\n\n### Multi image inference\n\nLLaVa-OneVision can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). For that you have to use checkpoints with an \"ov\" suffix. Here is how you can do it:\n\n```python\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n\n# Load the model in half-precision\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n\n# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\nconversation_1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},",
  "{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"There is a red stop sign in the image.\"},\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n{\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n],\n},\n]\n\nconversation_2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\n\ninputs = processor.apply_chat_template(\n[conversation_1, conversation_2],\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\npadding=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\n\n# Generate\ngenerate_ids = model.generate(**inputs, max_new_tokens=30)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n['user\\n\\nWhat is shown in this image?\\nassistant\\nThere is a red stop sign in the image.\\nuser\\n\\nWhat about this image? How many cats do you see?\\nassistant\\ntwo', 'user\\n\\nWhat is shown in this image?\\nassistant\\n']\n```",
  "### Video inference\n\nLLaVa-OneVision also can perform inference with videos as input, where video frames are treated as multiple images. Here is how you can do it:\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n\n# Load the model in half-precision\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n\nvideo_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\nconversation = [\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"path\": video_path},\n{\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n],\n},\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nnum_frames=8\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\n\nout = model.generate(**inputs, max_new_tokens=60)\nprocessor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)",
  "[\"user\\n\\nWhy is this video funny?\\nassistant\\nThe video appears to be humorous because it shows a young child, who is wearing glasses and holding a book, seemingly reading with a serious and focused expression. The child's glasses are a bit oversized for their face, which adds a comical touch, as it's a common trope to see children wearing\"]\n```\n\n## Model optimization\n\n### Quantization using bitsandbytes\n\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes` and make sure to have access to a GPU/accelerator that is supported by the library.\n\n<Tip>\n\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit [this link](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend).",
  "We value your feedback to help identify bugs before the full release! Check out [these docs](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends) for more details and feedback links.\n\n</Tip>\n\nSimply change the snippet above with:\n\n```python\nfrom transformers import LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig\n\n# specify how to quantize the model\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n```\n\n### Use Flash-Attention 2 to further speed-up generation\n\nFirst make sure to install flash-attn. Refer to the [original repository of Flash Attention](https://github.com/Dao-AILab/flash-attention) regarding that package installation. Simply change the snippet above with:\n\n```python\nfrom transformers import LlavaOnevisionForConditionalGeneration\n\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\nuse_flash_attention_2=True\n).to(0)\n```\n\n\n## LlavaOnevisionConfig",
  "[[autodoc]] LlavaOnevisionConfig\n\n## LlavaOnevisionProcessor\n\n[[autodoc]] LlavaOnevisionProcessor\n\n## LlavaOnevisionImageProcessor\n\n[[autodoc]] LlavaOnevisionImageProcessor\n\n## LlavaOnevisionImageProcessorFast\n\n[[autodoc]] LlavaOnevisionImageProcessorFast\n- preprocess\n\n## LlavaOnevisionVideoProcessor\n\n[[autodoc]] LlavaOnevisionVideoProcessor\n\n## LlavaOnevisionForConditionalGeneration\n\n[[autodoc]] LlavaOnevisionForConditionalGeneration\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BROS\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BROS model was proposed in [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.",
  "BROS stands for *BERT Relying On Spatiality*. It is an encoder-only Transformer model that takes a sequence of tokens and their bounding boxes as inputs and outputs a sequence of hidden states. BROS encode relative spatial information instead of using absolute spatial information.\n\nIt is pre-trained with two objectives: a token-masked language modeling objective (TMLM) used in BERT, and a novel area-masked language modeling objective (AMLM)\nIn TMLM, tokens are randomly masked, and the model predicts the masked tokens using spatial information and other unmasked tokens.\nAMLM is a 2D version of TMLM. It randomly masks text tokens and predicts with the same information as TMLM, but it masks text blocks (areas).\n\n`BrosForTokenClassification` has a simple linear layer on top of BrosModel. It predicts the label of each token.",
  "`BrosSpadeEEForTokenClassification` has an `initial_token_classifier` and `subsequent_token_classifier` on top of BrosModel. `initial_token_classifier` is used to predict the first token of each entity, and `subsequent_token_classifier` is used to predict the next token of within entity. `BrosSpadeELForTokenClassification` has an `entity_linker` on top of BrosModel. `entity_linker` is used to predict the relation between two entities.\n\n`BrosForTokenClassification` and `BrosSpadeEEForTokenClassification` essentially perform the same job. However, `BrosForTokenClassification` assumes input tokens are perfectly serialized (which is very challenging task since they exist in a 2D space), while `BrosSpadeEEForTokenClassification` allows for more flexibility in handling serialization errors as it predicts next connection tokens from one token.\n\n`BrosSpadeELForTokenClassification` perform the intra-entity linking task. It predicts relation from one token (of one entity) to another token (of another entity) if these two entities share some relation.",
  "BROS achieves comparable or better result on Key Information Extraction (KIE) benchmarks such as FUNSD, SROIE, CORD and SciTSR, without relying on explicit visual features.\n\nThe abstract from the paper is the following:",
  "*Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space. Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout. On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout. Specifically, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy. With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features. This paper also reveals two real-world challenges in KIE tasks-(1) minimizing the error from incorrect text ordering and (2) efficient learning from fewer downstream examples-and demonstrates the superiority of BROS over previous methods.*",
  "This model was contributed by [jinho8345](https://huggingface.co/jinho8345). The original code can be found [here](https://github.com/clovaai/bros).\n\n## Usage tips and examples\n\n- [`~transformers.BrosModel.forward`] requires `input_ids` and `bbox` (bounding box). Each bounding box should be in (x0, y0, x1, y1) format (top-left corner, bottom-right corner). Obtaining of Bounding boxes depends on external OCR system. The `x` coordinate should be normalized by document image width, and the `y` coordinate should be normalized by document image height.\n\n```python\ndef expand_and_normalize_bbox(bboxes, doc_width, doc_height):\n# here, bboxes are numpy array\n\n# Normalize bbox -> 0 ~ 1\nbboxes[:, [0, 2]] = bboxes[:, [0, 2]] / width\nbboxes[:, [1, 3]] = bboxes[:, [1, 3]] / height\n```",
  "- [`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassification.forward`] require not only `input_ids` and `bbox` but also `box_first_token_mask` for loss calculation. It is a mask to filter out non-first tokens of each box. You can obtain this mask by saving start token indices of bounding boxes when creating `input_ids` from words. You can make `box_first_token_mask` with following code,\n\n\n```python\ndef make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):\n\nbox_first_token_mask = np.zeros(max_seq_length, dtype=np.bool_)\n\n# encode(tokenize) each word from words (List[str])\ninput_ids_list: List[List[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]\n\n# get the length of each box\ntokens_length_list: List[int] = [len(l) for l in input_ids_list]\n\nbox_end_token_indices = np.array(list(itertools.accumulate(tokens_length_list)))\nbox_start_token_indices = box_end_token_indices - np.array(tokens_length_list)\n\n# filter out the indices that are out of max_seq_length",
  "box_end_token_indices = box_end_token_indices[box_end_token_indices < max_seq_length - 1]\nif len(box_start_token_indices) > len(box_end_token_indices):\nbox_start_token_indices = box_start_token_indices[: len(box_end_token_indices)]\n\n# set box_start_token_indices to True\nbox_first_token_mask[box_start_token_indices] = True\n\nreturn box_first_token_mask\n\n```\n\n## Resources\n\n- Demo scripts can be found [here](https://github.com/clovaai/bros).\n\n## BrosConfig\n\n[[autodoc]] BrosConfig\n\n## BrosProcessor\n\n[[autodoc]] BrosProcessor\n- __call__\n\n## BrosModel\n\n[[autodoc]] BrosModel\n- forward\n\n\n## BrosForTokenClassification\n\n[[autodoc]] BrosForTokenClassification\n- forward\n\n## BrosSpadeEEForTokenClassification\n\n[[autodoc]] BrosSpadeEEForTokenClassification\n- forward\n\n## BrosSpadeELForTokenClassification\n\n[[autodoc]] BrosSpadeELForTokenClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n# Zamba2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>",
  "Zamba2 is a large language model (LLM) trained by Zyphra, and made available under an Apache 2.0 license. Please see the [Zyphra Hugging Face](https://huggingface.co/collections/zyphra/) repository for model weights.\n\nThis model was contributed by [pglo](https://huggingface.co/pglo).\n\n\n## Model details\n\nZamba2-1.2B, Zamba2-2.7B and Zamba2-7B are hybrid models combining state-space models (Specifically [Mamba](https://github.com/state-spaces/mamba)) and transformer, and were trained using next-token prediction. Zamba2 uses shared transformer layers after every 6 mamba blocks. It uses the [Mistral v0.1 tokenizer](https://huggingface.co/mistralai/Mistral-7B-v0.1). We came to this architecture after a series of ablations at small scales. Zamba2-1.2B, Zamba2-2.7B and Zamba2-7B were pre-trained on 2T and 3T tokens, respectively.\n\n<img src=https://github.com/user-attachments/assets/c2cff209-b901-483c-87aa-774b82a0769f width=30% height=40% />\n\n## Quick start\n\n\n### Presequities\n\nZamba2 requires you use `transformers` version 4.48.0 or higher:\n```bash\npip install transformers>=4.48.0\n```\n\n## Inference\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch",
  "tokenizer = AutoTokenizer.from_pretrained(\"Zyphra/Zamba2-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba2-7B\", device_map=\"cuda\", torch_dtype=torch.bfloat16)\n\ninput_text = \"What factors contributed to the fall of the Roman Empire?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0]))\n```\n\n\n## Model card\n\nThe model cards can be found at:\n* [Zamba2-1.2B](https://huggingface.co/Zyphra/Zamba2-1.2B)\n* [Zamba2-2.7B](https://huggingface.co/Zyphra/Zamba2-2.7B)\n* [Zamba2-7B](https://huggingface.co/Zyphra/Zamba2-7B)\n\n\n## Issues\nFor issues with model output, or community discussion, please use the Hugging Face community [forum](https://huggingface.co/Zyphra/Zamba2-7B/discussions)\n\n\n## License\n\nThe model weights are open-sourced via an Apache 2.0 license.\n\n\n## Zamba2Config\n\n[[autodoc]] Zamba2Config\n\n\n## Zamba2Model\n\n[[autodoc]] Zamba2Model\n- forward\n\n\n## Zamba2ForCausalLM\n\n[[autodoc]] Zamba2ForCausalLM\n- forward\n\n\n## Zamba2ForSequenceClassification\n\n[[autodoc]] transformers.Zamba2ForSequenceClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RoBERTa\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n## Overview\n\nThe RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, [Myle Ott](https://huggingface.co/myleott), Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google's BERT model released in 2018.\n\nIt builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with\nmuch larger mini-batches and learning rates.\n\nThe abstract from the paper is the following:\n\n*Language model pretraining has led to significant performance gains but careful comparison between different\napproaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the final results. We present a replication\nstudy of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and",
  "training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every\nmodel published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise questions about the source of recently\nreported improvements. We release our models and code.*\n\nThis model was contributed by [julien-c](https://huggingface.co/julien-c). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/roberta).\n\n## Usage tips\n\n- This implementation is the same as [`BertModel`] with a minor tweak to the embeddings, as well as a setup\nfor RoBERTa pretrained models.\n- RoBERTa has the same architecture as BERT but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a\ndifferent pretraining scheme.\n- RoBERTa doesn't have `token_type_ids`, so you don't need to indicate which token belongs to which segment. Just\nseparate your segments with the separation token `tokenizer.sep_token` (or `</s>`).\n- RoBERTa is similar to BERT but with better pretraining techniques:",
  "* Dynamic masking: tokens are masked differently at each epoch, whereas BERT does it once and for all.\n* Sentence packing: Sentences are packed together to reach 512 tokens (so the sentences are in an order that may span several documents).\n* Larger batches: Training uses larger batches.\n* Byte-level BPE vocabulary: Uses BPE with bytes as a subunit instead of characters, accommodating Unicode characters.\n- [CamemBERT](camembert) is a wrapper around RoBERTa. Refer to its model page for usage examples.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog on [Getting Started with Sentiment Analysis on Twitter](https://huggingface.co/blog/sentiment-analysis-twitter) using RoBERTa and the [Inference API](https://huggingface.co/inference-api).",
  "- A blog on [Opinion Classification with Kili and Hugging Face AutoTrain](https://huggingface.co/blog/opinion-classification-with-kili) using RoBERTa.\n- A notebook on how to [finetune RoBERTa for sentiment analysis](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb). 🌎\n- [`RobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).\n- [`TFRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).",
  "- [`FlaxRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- [`RobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).\n- [`TFRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n- [`FlaxRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).",
  "- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Token classification task guide](../tasks/token_classification)\n\n<PipelineTag pipeline=\"fill-mask\"/>\n\n- A blog on [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train) with RoBERTa.\n- [`RobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).",
  "- [`FlaxRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- A blog on [Accelerated Inference with Optimum and Transformers Pipelines](https://huggingface.co/blog/optimum-inference) with RoBERTa for question answering.\n- [`RobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).",
  "- [`TFRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n**Multiple choice**\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).",
  "- [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RobertaConfig\n\n[[autodoc]] RobertaConfig\n\n## RobertaTokenizer\n\n[[autodoc]] RobertaTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## RobertaTokenizerFast\n\n[[autodoc]] RobertaTokenizerFast\n- build_inputs_with_special_tokens\n\n<frameworkcontent>\n<pt>\n\n## RobertaModel\n\n[[autodoc]] RobertaModel\n- forward\n\n## RobertaForCausalLM\n\n[[autodoc]] RobertaForCausalLM\n- forward\n\n## RobertaForMaskedLM\n\n[[autodoc]] RobertaForMaskedLM\n- forward\n\n## RobertaForSequenceClassification\n\n[[autodoc]] RobertaForSequenceClassification\n- forward\n\n## RobertaForMultipleChoice\n\n[[autodoc]] RobertaForMultipleChoice\n- forward\n\n## RobertaForTokenClassification\n\n[[autodoc]] RobertaForTokenClassification\n- forward\n\n## RobertaForQuestionAnswering\n\n[[autodoc]] RobertaForQuestionAnswering",
  "- forward\n\n</pt>\n<tf>\n\n## TFRobertaModel\n\n[[autodoc]] TFRobertaModel\n- call\n\n## TFRobertaForCausalLM\n\n[[autodoc]] TFRobertaForCausalLM\n- call\n\n## TFRobertaForMaskedLM\n\n[[autodoc]] TFRobertaForMaskedLM\n- call\n\n## TFRobertaForSequenceClassification\n\n[[autodoc]] TFRobertaForSequenceClassification\n- call\n\n## TFRobertaForMultipleChoice\n\n[[autodoc]] TFRobertaForMultipleChoice\n- call\n\n## TFRobertaForTokenClassification\n\n[[autodoc]] TFRobertaForTokenClassification\n- call\n\n## TFRobertaForQuestionAnswering\n\n[[autodoc]] TFRobertaForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxRobertaModel\n\n[[autodoc]] FlaxRobertaModel\n- __call__\n\n## FlaxRobertaForCausalLM\n\n[[autodoc]] FlaxRobertaForCausalLM\n- __call__\n\n## FlaxRobertaForMaskedLM\n\n[[autodoc]] FlaxRobertaForMaskedLM\n- __call__\n\n## FlaxRobertaForSequenceClassification\n\n[[autodoc]] FlaxRobertaForSequenceClassification\n- __call__\n\n## FlaxRobertaForMultipleChoice\n\n[[autodoc]] FlaxRobertaForMultipleChoice\n- __call__\n\n## FlaxRobertaForTokenClassification\n\n[[autodoc]] FlaxRobertaForTokenClassification\n- __call__\n\n## FlaxRobertaForQuestionAnswering\n\n[[autodoc]] FlaxRobertaForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Swin Transformer V2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Swin Transformer V2 model was proposed in [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\n\nThe abstract from the paper is the following:",
  "*Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.*",
  "This model was contributed by [nandwalritik](https://huggingface.co/nandwalritik).\nThe original code can be found [here](https://github.com/microsoft/Swin-Transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`Swinv2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nBesides that:\n\n- [`Swinv2ForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## Swinv2Config\n\n[[autodoc]] Swinv2Config\n\n## Swinv2Model",
  "[[autodoc]] Swinv2Model\n- forward\n\n## Swinv2ForMaskedImageModeling\n\n[[autodoc]] Swinv2ForMaskedImageModeling\n- forward\n\n## Swinv2ForImageClassification\n\n[[autodoc]] transformers.Swinv2ForImageClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LED\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview\n\nThe LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz",
  "Beltagy, Matthew E. Peters, Arman Cohan.\n\nThe abstract from the paper is the following:\n\n*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our\npretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on\nWikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting",
  "long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization\ndataset.*\n\n## Usage tips\n\n- [`LEDForConditionalGeneration`] is an extension of\n[`BartForConditionalGeneration`] exchanging the traditional *self-attention* layer with\n*Longformer*'s *chunked self-attention* layer. [`LEDTokenizer`] is an alias of\n[`BartTokenizer`].\n- LED works very well on long-range *sequence-to-sequence* tasks where the `input_ids` largely exceed a length of\n1024 tokens.\n- LED pads the `input_ids` to be a multiple of `config.attention_window` if required. Therefore a small speed-up is\ngained, when [`LEDTokenizer`] is used with the `pad_to_multiple_of` argument.\n- LED makes use of *global attention* by means of the `global_attention_mask` (see\n[`LongformerModel`]). For summarization, it is advised to put *global attention* only on the first\n`<s>` token. For question answering, it is advised to put *global attention* on all tokens of the question.\n- To fine-tune LED on all 16384, *gradient checkpointing* can be enabled in case training leads to out-of-memory (OOM)\nerrors. This can be done by executing `model.gradient_checkpointing_enable()`.",
  "Moreover, the `use_cache=False`\nflag can be used to disable the caching mechanism to save memory.\n- LED is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n## Resources\n\n- [A notebook showing how to evaluate LED](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing).\n- [A notebook showing how to fine-tune LED](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing).\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## LEDConfig\n\n[[autodoc]] LEDConfig\n\n## LEDTokenizer\n\n[[autodoc]] LEDTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## LEDTokenizerFast\n\n[[autodoc]] LEDTokenizerFast\n\n## LED specific outputs\n\n[[autodoc]] models.led.modeling_led.LEDEncoderBaseModelOutput",
  "[[autodoc]] models.led.modeling_led.LEDSeq2SeqModelOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqLMOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput\n\n[[autodoc]] models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput\n\n[[autodoc]] models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput\n\n[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput\n\n[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput\n\n<frameworkcontent>\n<pt>\n\n## LEDModel\n\n[[autodoc]] LEDModel\n- forward\n\n## LEDForConditionalGeneration\n\n[[autodoc]] LEDForConditionalGeneration\n- forward\n\n## LEDForSequenceClassification\n\n[[autodoc]] LEDForSequenceClassification\n- forward\n\n## LEDForQuestionAnswering\n\n[[autodoc]] LEDForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFLEDModel\n\n[[autodoc]] TFLEDModel\n- call\n\n## TFLEDForConditionalGeneration\n\n[[autodoc]] TFLEDForConditionalGeneration\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Phi-3\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Phi-3 model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\n\n### Summary\n\nThe abstract from the Phi-3 paper is the following:\n\nWe introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).\n\nThe original code for Phi-3 can be found [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct).",
  "## Usage tips\n\n- This model is very similar to `Llama` with the main difference of [`Phi3SuScaledRotaryEmbedding`] and [`Phi3YarnScaledRotaryEmbedding`], where they are used to extend the context of the rotary embeddings. The query, key and values are fused, and the MLP's up and gate projection layers are also fused.\n- The tokenizer used for this model is identical to the [`LlamaTokenizer`], with the exception of additional tokens.\n\n## How to use Phi-3\n\n<Tip warning={true}>\n\nPhi-3 has been integrated in the development version (4.40.0.dev) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\n</Tip>\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer",
  ">>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n\n>>> messages = [{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}]\n>>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n\n>>> outputs = model.generate(inputs, max_new_tokens=32)\n>>> text = tokenizer.batch_decode(outputs)[0]\n>>> print(text)\n<|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> Certainly! Bananas and dragonfruits can be combined in various delicious ways. Here are some creative ideas for incorporating both fruits\n```\n\n## Phi3Config\n\n[[autodoc]] Phi3Config\n\n<frameworkcontent>\n<pt>\n\n## Phi3Model\n\n[[autodoc]] Phi3Model\n- forward\n\n## Phi3ForCausalLM\n\n[[autodoc]] Phi3ForCausalLM\n- forward\n- generate\n\n## Phi3ForSequenceClassification\n\n[[autodoc]] Phi3ForSequenceClassification\n- forward\n\n## Phi3ForTokenClassification\n\n[[autodoc]] Phi3ForTokenClassification\n- forward\n\n</pt>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UPerNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)\nby Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a general framework to effectively segment",
  "a wide range of concepts from images, leveraging any vision backbone like [ConvNeXt](convnext) or [Swin](swin).\n\nThe abstract from the paper is the following:\n\n*Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/upernet_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> UPerNet framework. Taken from the <a href=\"https://arxiv.org/abs/1807.10221\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code is based on OpenMMLab's mmsegmentation [here](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py).\n\n## Usage examples\n\nUPerNet is a general framework for semantic segmentation. It can be used with any vision backbone, like so:\n\n```py\nfrom transformers import SwinConfig, UperNetConfig, UperNetForSemanticSegmentation\n\nbackbone_config = SwinConfig(out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n\nconfig = UperNetConfig(backbone_config=backbone_config)\nmodel = UperNetForSemanticSegmentation(config)\n```\n\nTo use another vision backbone, like [ConvNeXt](convnext), simply instantiate the model with the appropriate backbone:\n\n```py\nfrom transformers import ConvNextConfig, UperNetConfig, UperNetForSemanticSegmentation\n\nbackbone_config = ConvNextConfig(out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n\nconfig = UperNetConfig(backbone_config=backbone_config)\nmodel = UperNetForSemanticSegmentation(config)\n```",
  "Note that this will randomly initialize all the weights of the model.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with UPerNet.\n\n- Demo notebooks for UPerNet can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet).\n- [`UperNetForSemanticSegmentation`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb).\n- See also: [Semantic segmentation task guide](../tasks/semantic_segmentation)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## UperNetConfig\n\n[[autodoc]] UperNetConfig\n\n## UperNetForSemanticSegmentation\n\n[[autodoc]] UperNetForSemanticSegmentation\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SigLIP2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The SigLIP2 model was proposed in [SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://huggingface.co/papers/2502.14786) by Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin,\nNikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen,\nAndreas Steiner and Xiaohua Zhai.\n\nThe model comes in two variants\n\n1) FixRes - model works with fixed resolution images (backward compatible with SigLIP v1)\n2) NaFlex - model works with variable image aspect ratios and resolutions (SigLIP2 in `transformers`)\n\nThe abstract from the paper is the following:\n\n*We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success\nof the original SigLIP. In this second iteration, we extend the original image-text training objective with\nseveral prior, independently developed techniques into a unified recipe—this includes decoder-based\npretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With",
  "these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification (best SigLIP 2 ViT-g/16 achieves 85.0% ImageNet zero-shot\naccuracy), image-text retrieval, and transfer performance when extracting visual representations for\nVision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which support multiple resolutions\nand preserve the input’s native aspect ratio. Finally, we train on a more diverse data-mixture that\nincludes de-biasing techniques, leading to much better multilingual understanding and improved fair-\nness. To provide users with the ability to trade-off inference cost with performance, we release model\ncheckpoints at four sizes (ViT-B/86M, L/303M, So400m/400M, and g/1B).*\n\n## Usage tips",
  "- Usage of SigLIP2 is similar to [SigLIP](siglip) and [CLIP](clip). The main difference from CLIP is the training loss, which does not require a global view of all the pairwise similarities of images and texts within a batch. One needs to apply the sigmoid activation function to the logits, rather than the softmax.\n- Training is supported but does not use `torch.distributed` utilities which may limit the scalability of batch size. However, DDP and FDSP works on single-node multi-gpu setup.\n- When using the standalone [`GemmaTokenizerFast`] make sure to pass `padding=\"max_length\"` and `max_length=64` as that's how the model was trained.\n- Model was trained with *lowercased* text, make sure you make the same preprocessing for your text labels.\n- To get the same results as the pipeline, a prompt template of \"this is a photo of {label}\" should be used.\n- The NaFlex variant supports processing images at higher resolutions by adjusting the `max_num_patches` parameter in the `Processor`. The default value is `max_num_patches=256`. Increasing `max_num_patches` to 1024 (4x) will approximately double processed image height and width, while preserving the aspect ratio.",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/siglip2_metrics_table.png\"\nalt=\"drawing\" width=\"600\"/>\n\nThis model was contributed by [qubvel](https://huggingface.co/qubvel-hf).\nThe original code can be found [here](https://github.com/google-research/big_vision/tree/main).\n\n## Usage example\n\nThere are 2 main ways to use SigLIP2: either using the pipeline API, which abstracts away all the complexity for you, or by using the `Siglip2Model` class yourself.\n\n### FixRes variant\n\n**Pipeline API**\n\nThe pipeline allows to use the model in a few lines of code:\n\n```python\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> # load pipe\n>>> image_classifier = pipeline(\n...     task=\"zero-shot-image-classification\",\n...     model=\"google/siglip2-base-patch16-224\",\n... )\n\n>>> # load image\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # inference\n>>> candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n>>> outputs = image_classifier(image, candidate_labels=candidate_labels)",
  ">>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n>>> print(outputs)\n[{'score': 0.1499, 'label': '2 cats'}, {'score': 0.0008, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n```\n\n**Using the model yourself**\n\nIf you want to do the pre- and postprocessing yourself, here's how to do that:\n\n```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n# follows the pipeline prompt template to get same results\n>>> texts = [f\"This is a photo of {label}.\" for label in candidate_labels]\n\n# IMPORTANT: we pass `padding=max_length` and `max_length=64` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n\n>>> with torch.no_grad():",
  "...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n15.0% that image 0 is '2 cats'\n```\n\n### NaFlex variant\n\nNaFlex combines ideas from FlexiViT, i.e. supporting multiple, predefined sequence lengths\nwith a single ViT model, and NaViT, namely processing images at their native aspect ratio.\nThis enables processing different types of images at appropriate resolution, e.g. using a\nlarger resolution to process document images, while at the same time minimizing the impact\nof aspect ratio distortion on certain inference tasks, e.g. on OCR.\n\nGiven a patch size and target sequence length, NaFlex preprocesses the data by first resizing\nthe input image such that the height and width after resizing are multiples of the patch size,\nwhile\n\n1. keeping the aspect ratio distortion as small as possible\n2. producing a sequence length of at most the desired target sequence length (`max_num_patches`)\n\nThe resulting distortion in width and height is at most `(patch_size - 1) / width` and",
  "`(patch_size - 1) / height`, respectively, which tends to be small for common resolutions and aspect ratios.\nAfter resizing, the image is split into a sequence of patches, and a mask with padding information is added.\n\n```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n# follows the pipeline prompt template to get same results\n>>> texts = [f\"This is a photo of {label}.\" for label in candidate_labels]\n\n# default value for `max_num_patches` is 256, but you can increase resulted image resolution providing\n# higher values e.g. `max_num_patches=512`\n>>> inputs = processor(text=texts, images=image, max_num_patches=256, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image",
  ">>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n21.1% that image 0 is '2 cats'\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SigLIP2.\n\n- [Zero-shot image classification task guide](../tasks/zero_shot_image_classification)\n- Demo notebook for SigLIP2 can be found [here](https://github.com/qubvel/transformers-notebooks/tree/master/notebooks/SigLIP2_inference.ipynb). 🌎\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n## Combining SigLIP2 and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)",
  "To load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> import requests\n>>> from PIL import Image\n>>> from transformers import AutoProcessor, AutoModel\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModel.from_pretrained(\n...     \"google/siglip2-so400m-patch14-384\",\n...     attn_implementation=\"flash_attention_2\",\n...     torch_dtype=torch.float16,\n...     device_map=device,\n... )\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip2-so400m-patch14-384\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n# follows the pipeline prompt template to get same results\n>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n# important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(device)\n\n>>> with torch.no_grad():\n...     with torch.autocast(device):\n...         outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image",
  ">>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n19.8% that image 0 is '2 cats'\n```\n\n## Siglip2Config\n\n[[autodoc]] Siglip2Config\n\n## Siglip2TextConfig\n\n[[autodoc]] Siglip2TextConfig\n\n## Siglip2VisionConfig\n\n[[autodoc]] Siglip2VisionConfig\n\n## Siglip2ImageProcessor\n\n[[autodoc]] Siglip2ImageProcessor\n- preprocess\n\n## Siglip2ImageProcessorFast\n\n[[autodoc]] Siglip2ImageProcessorFast\n- preprocess\n\n## Siglip2Processor\n\n[[autodoc]] Siglip2Processor\n\n## Siglip2Model\n\n[[autodoc]] Siglip2Model\n- forward\n- get_text_features\n- get_image_features\n\n## Siglip2TextModel\n\n[[autodoc]] Siglip2TextModel\n- forward\n\n## Siglip2VisionModel\n\n[[autodoc]] Siglip2VisionModel\n- forward\n\n## Siglip2ForImageClassification\n\n[[autodoc]] Siglip2ForImageClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Blenderbot\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that\nscaling neural models in the number of parameters and the size of the data they are trained on gives improved results,\nwe show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of\nskills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to\ntheir partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent\npersona. We show that large scale models can learn these skills when given appropriate training data and choice of\ngeneration strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models",
  "and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn\ndialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing\nfailure cases of our models.*\n\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer). The authors' code can be found [here](https://github.com/facebookresearch/ParlAI) .\n\n## Usage tips and example\n\nBlenderbot is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\nrather than the left.\n\nAn example:\n\n```python\n>>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot-400M-distill\"\n>>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n>>> reply_ids = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(reply_ids))\n[\"<s> That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s>\"]\n```",
  "## Implementation Notes\n\n- Blenderbot uses a standard [seq2seq model transformer](https://arxiv.org/pdf/1706.03762.pdf) based architecture.\n- Available checkpoints can be found in the [model hub](https://huggingface.co/models?search=blenderbot).\n- This is the *default* Blenderbot model class. However, some smaller checkpoints, such as\n`facebook/blenderbot_small_90M`, have a different architecture and consequently should be used with\n[BlenderbotSmall](blenderbot-small).\n\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## BlenderbotConfig\n\n[[autodoc]] BlenderbotConfig\n\n## BlenderbotTokenizer\n\n[[autodoc]] BlenderbotTokenizer\n- build_inputs_with_special_tokens\n\n## BlenderbotTokenizerFast\n\n[[autodoc]] BlenderbotTokenizerFast\n- build_inputs_with_special_tokens\n\n\n<frameworkcontent>\n<pt>\n\n## BlenderbotModel\n\nSee [`~transformers.BartModel`] for arguments to *forward* and *generate*\n\n[[autodoc]] BlenderbotModel\n- forward\n\n## BlenderbotForConditionalGeneration\n\nSee [`~transformers.BartForConditionalGeneration`] for arguments to *forward* and *generate*",
  "[[autodoc]] BlenderbotForConditionalGeneration\n- forward\n\n## BlenderbotForCausalLM\n\n[[autodoc]] BlenderbotForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFBlenderbotModel\n\n[[autodoc]] TFBlenderbotModel\n- call\n\n## TFBlenderbotForConditionalGeneration\n\n[[autodoc]] TFBlenderbotForConditionalGeneration\n- call\n\n</tf>\n<jax>\n\n## FlaxBlenderbotModel\n\n[[autodoc]] FlaxBlenderbotModel\n- __call__\n- encode\n- decode\n\n## FlaxBlenderbotForConditionalGeneration\n\n[[autodoc]] FlaxBlenderbotForConditionalGeneration\n- __call__\n- encode\n- decode\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Splinter\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Splinter model was proposed in [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy. Splinter",
  "is an encoder-only transformer (similar to BERT) pretrained using the recurring span selection task on a large corpus\ncomprising Wikipedia and the Toronto Book Corpus.\n\nThe abstract from the paper is the following:\n\nIn several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order\nof 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred\ntraining examples are available, and observe that standard models perform poorly, highlighting the discrepancy between\ncurrent pretraining objectives and question answering. We propose a new pretraining scheme tailored for question\nanswering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all\nrecurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans\nare replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select\nthe answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD",
  "with only 128 training examples), while maintaining competitive performance in the high-resource setting.\n\nThis model was contributed by [yuvalkirstain](https://huggingface.co/yuvalkirstain) and [oriram](https://huggingface.co/oriram). The original code can be found [here](https://github.com/oriram/splinter).\n\n## Usage tips\n\n- Splinter was trained to predict answers spans conditioned on a special [QUESTION] token. These tokens contextualize\nto question representations which are used to predict the answers. This layer is called QASS, and is the default\nbehaviour in the [`SplinterForQuestionAnswering`] class. Therefore:\n- Use [`SplinterTokenizer`] (rather than [`BertTokenizer`]), as it already\ncontains this special token. Also, its default behavior is to use this token when two sequences are given (for\nexample, in the *run_qa.py* script).\n- If you plan on using Splinter outside *run_qa.py*, please keep in mind the question token - it might be important for\nthe success of your model, especially in a few-shot setting.\n- Please note there are two different checkpoints for each size of Splinter. Both are basically the same, except that",
  "one also has the pretrained weights of the QASS layer (*tau/splinter-base-qass* and *tau/splinter-large-qass*) and one\ndoesn't (*tau/splinter-base* and *tau/splinter-large*). This is done to support randomly initializing this layer at\nfine-tuning, as it is shown to yield better results for some cases in the paper.\n\n## Resources\n\n- [Question answering task guide](../tasks/question-answering)\n\n## SplinterConfig\n\n[[autodoc]] SplinterConfig\n\n## SplinterTokenizer\n\n[[autodoc]] SplinterTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## SplinterTokenizerFast\n\n[[autodoc]] SplinterTokenizerFast\n\n## SplinterModel\n\n[[autodoc]] SplinterModel\n- forward\n\n## SplinterForQuestionAnswering\n\n[[autodoc]] SplinterForQuestionAnswering\n- forward\n\n## SplinterForPreTraining\n\n[[autodoc]] SplinterForPreTraining\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SEW-D\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nSEW-D (Squeezed and Efficient Wav2Vec with Disentangled attention) was proposed in [Performance-Efficiency Trade-offs\nin Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim,",
  "Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n\nThe abstract from the paper is the following:\n\n*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition\n(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance\nand its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a\npre-trained model architecture with significant improvements along both performance and efficiency dimensions across a\nvariety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference\ntime, SEW reduces word error rate by 25-50% across different model sizes.*\n\nThis model was contributed by [anton-l](https://huggingface.co/anton-l).\n\n## Usage tips\n\n- SEW-D is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- SEWDForCTC is fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded",
  "using [`Wav2Vec2CTCTokenizer`].\n\n## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## SEWDConfig\n\n[[autodoc]] SEWDConfig\n\n## SEWDModel\n\n[[autodoc]] SEWDModel\n- forward\n\n## SEWDForCTC\n\n[[autodoc]] SEWDForCTC\n- forward\n\n## SEWDForSequenceClassification\n\n[[autodoc]] SEWDForSequenceClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TAPAS\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview\n\nThe TAPAS model was proposed in [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://www.aclweb.org/anthology/2020.acl-main.398)",
  "by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos. It's a BERT-based model specifically\ndesigned (and pre-trained) for answering questions about tabular data. Compared to BERT, TAPAS uses relative position embeddings and has 7\ntoken types that encode tabular structure. TAPAS is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising\nmillions of tables from English Wikipedia and corresponding texts.\n\nFor question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for (optionally) performing aggregations (such as counting or summing) among selected cells. TAPAS has been fine-tuned on several datasets:\n- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n- [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\n- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce).\n\nIt achieves state-of-the-art on both SQA and WTQ, while having comparable performance to SOTA on WikiSQL, with a much simpler architecture.",
  "The abstract from the paper is the following:",
  "*Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.*",
  "In addition, the authors have further pre-trained TAPAS to recognize **table entailment**, by creating a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. The authors of TAPAS call this further pre-training intermediate pre-training (since TAPAS is first pre-trained on MLM, and then on another dataset). They found that intermediate pre-training further improves performance on SQA, achieving a new state-of-the-art as well as state-of-the-art on [TabFact](https://github.com/wenhuchen/Table-Fact-Checking), a large-scale dataset with 16k Wikipedia tables for table entailment (a binary classification task). For more details, see their follow-up paper: [Understanding tables with intermediate pre-training](https://www.aclweb.org/anthology/2020.findings-emnlp.27/) by Julian Martin Eisenschlos, Syrine Krichene and Thomas Müller.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tapas_architecture.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> TAPAS architecture. Taken from the <a href=\"https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html\">original blog post</a>.</small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The Tensorflow version of this model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/tapas).\n\n## Usage tips",
  "- TAPAS is a model that uses relative position embeddings by default (restarting the position embeddings at every cell of the table). Note that this is something that was added after the publication of the original TAPAS paper. According to the authors, this usually results in a slightly better performance, and allows you to encode longer sequences without running out of embeddings. This is reflected in the `reset_position_index_per_cell` parameter of [`TapasConfig`], which is set to `True` by default. The default versions of the models available on the [hub](https://huggingface.co/models?search=tapas) all use relative position embeddings. You can still use the ones with absolute position embeddings by passing in an additional argument `revision=\"no_reset\"` when calling the `from_pretrained()` method. Note that it's usually advised to pad the inputs on the right rather than the left.",
  "- TAPAS is based on BERT, so `TAPAS-base` for example corresponds to a `BERT-base` architecture. Of course, `TAPAS-large` will result in the best performance (the results reported in the paper are from `TAPAS-large`). Results of the various sized models are shown on the [original GitHub repository](https://github.com/google-research/tapas).\n- TAPAS has checkpoints fine-tuned on SQA, which are capable of answering questions related to a table in a conversational set-up. This means that you can ask follow-up questions such as \"what is his age?\" related to the previous question. Note that the forward pass of TAPAS is a bit different in case of a conversational set-up: in that case, you have to feed every table-question pair one by one to the model, such that the `prev_labels` token type ids can be overwritten by the predicted `labels` of the model to the previous question. See \"Usage\" section for more info.",
  "- TAPAS is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained with a causal language modeling (CLM) objective are better in that regard. Note that TAPAS can be used as an encoder in the EncoderDecoderModel framework, to combine it with an autoregressive text decoder such as GPT-2.\n\n## Usage: fine-tuning\n\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your own dataset.\n\n**STEP 1: Choose one of the 3 ways in which you can use TAPAS - or experiment**\n\nBasically, there are 3 different ways in which one can fine-tune [`TapasForQuestionAnswering`], corresponding to the different datasets on which Tapas was fine-tuned:\n\n1. SQA: if you're interested in asking follow-up questions related to a table, in a conversational set-up. For example if you first ask \"what's the name of the first actor?\" then you can ask a follow-up question such as \"how old is he?\". Here, questions do not involve any aggregation (all questions are cell selection questions).",
  "2. WTQ: if you're not interested in asking questions in a conversational set-up, but rather just asking questions related to a table, which might involve aggregation, such as counting a number of rows, summing up cell values or averaging cell values. You can then for example ask \"what's the total number of goals Cristiano Ronaldo made in his career?\". This case is also called **weak supervision**, since the model itself must learn the appropriate aggregation operator (SUM/COUNT/AVERAGE/NONE) given only the answer to the question as supervision.\n3. WikiSQL-supervised: this dataset is based on WikiSQL with the model being given the ground truth aggregation operator during training. This is also called **strong supervision**. Here, learning the appropriate aggregation operator is much easier.\n\nTo summarize:\n\n| **Task**                            | **Example dataset** | **Description**                                                                                         |\n|-------------------------------------|---------------------|---------------------------------------------------------------------------------------------------------|",
  "| Conversational                      | SQA                 | Conversational, only cell selection questions                                                           |\n| Weak supervision for aggregation    | WTQ                 | Questions might involve aggregation, and the model must learn this given only the answer as supervision |\n| Strong supervision for aggregation  | WikiSQL-supervised  | Questions might involve aggregation, and the model must learn this given the gold aggregation operator  |\n\n<frameworkcontent>\n<pt>\nInitializing a model with a pre-trained base and randomly initialized classification heads from the hub can be done as shown below.\n\n```py\n>>> from transformers import TapasConfig, TapasForQuestionAnswering\n\n>>> # for example, the base sized model with default SQA configuration\n>>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\")\n\n>>> # or, the base sized model with WTQ configuration\n>>> config = TapasConfig.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n>>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n\n>>> # or, the base sized model with WikiSQL configuration",
  ">>> config = TapasConfig(\"google-base-finetuned-wikisql-supervised\")\n>>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n```\n\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters you want when initializing [`TapasConfig`], and then create a [`TapasForQuestionAnswering`] based on that configuration. For example, if you have a dataset that has both conversational questions and questions that might involve aggregation, then you can do it this way. Here's an example:\n\n```py\n>>> from transformers import TapasConfig, TapasForQuestionAnswering\n\n>>> # you can initialize the classification heads any way you want (see docs of TapasConfig)\n>>> config = TapasConfig(num_aggregation_labels=3, average_logits_per_cell=True)\n>>> # initializing the pre-trained base sized model with our custom classification heads\n>>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n```\n</pt>\n<tf>",
  "Initializing a model with a pre-trained base and randomly initialized classification heads from the hub can be done as shown below. Be sure to have installed the [tensorflow_probability](https://github.com/tensorflow/probability) dependency:\n\n```py\n>>> from transformers import TapasConfig, TFTapasForQuestionAnswering\n\n>>> # for example, the base sized model with default SQA configuration\n>>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\")\n\n>>> # or, the base sized model with WTQ configuration\n>>> config = TapasConfig.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n>>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n\n>>> # or, the base sized model with WikiSQL configuration\n>>> config = TapasConfig(\"google-base-finetuned-wikisql-supervised\")\n>>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n```",
  "Of course, you don't necessarily have to follow one of these three ways in which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters you want when initializing [`TapasConfig`], and then create a [`TFTapasForQuestionAnswering`] based on that configuration. For example, if you have a dataset that has both conversational questions and questions that might involve aggregation, then you can do it this way. Here's an example:\n\n```py\n>>> from transformers import TapasConfig, TFTapasForQuestionAnswering\n\n>>> # you can initialize the classification heads any way you want (see docs of TapasConfig)\n>>> config = TapasConfig(num_aggregation_labels=3, average_logits_per_cell=True)\n>>> # initializing the pre-trained base sized model with our custom classification heads\n>>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n```\n</tf>\n</frameworkcontent>",
  "What you can also do is start from an already fine-tuned checkpoint. A note here is that the already fine-tuned checkpoint on WTQ has some issues due to the L2-loss which is somewhat brittle. See [here](https://github.com/google-research/tapas/issues/91#issuecomment-735719340) for more info.\n\nFor a list of all pre-trained and fine-tuned TAPAS checkpoints available on HuggingFace's  hub, see [here](https://huggingface.co/models?search=tapas).\n\n**STEP 2: Prepare your data in the SQA format**\n\nSecond, no matter what you picked above, you should prepare your dataset in the [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) format. This format is a TSV/CSV file with the following columns:\n\n- `id`: optional, id of the table-question pair, for bookkeeping purposes.\n- `annotator`: optional, id of the person who annotated the table-question pair, for bookkeeping purposes.\n- `position`: integer indicating if the question is the first, second, third,... related to the table. Only required in case of conversational setup (SQA). You don't need this column in case you're going for WTQ/WikiSQL-supervised.\n- `question`: string",
  "- `table_file`: string, name of a csv file containing the tabular data\n- `answer_coordinates`: list of one or more tuples (each tuple being a cell coordinate, i.e. row, column pair that is part of the answer)\n- `answer_text`: list of one or more strings (each string being a cell value that is part of the answer)\n- `aggregation_label`: index of the aggregation operator. Only required in case of strong supervision for aggregation (the WikiSQL-supervised case)\n- `float_answer`: the float answer to the question, if there is one (np.nan if there isn't). Only required in case of weak supervision for aggregation (such as WTQ and WikiSQL)",
  "The tables themselves should be present in a folder, each table being a separate csv file. Note that the authors of the TAPAS algorithm used conversion scripts with some automated logic to convert the other datasets (WTQ, WikiSQL) into the SQA format. The author explains this [here](https://github.com/google-research/tapas/issues/50#issuecomment-705465960). A conversion of this script that works with HuggingFace's implementation can be found [here](https://github.com/NielsRogge/tapas_utils). Interestingly, these conversion scripts are not perfect (the `answer_coordinates` and `float_answer` fields are populated based on the `answer_text`), meaning that WTQ and WikiSQL results could actually be improved.\n\n**STEP 3: Convert your data into tensors using TapasTokenizer**\n\n<frameworkcontent>\n<pt>\nThird, given that you've prepared your data in this TSV/CSV format (and corresponding CSV files containing the tabular data), you can then use [`TapasTokenizer`] to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids` and so on. Again, based on which of the three cases you picked above, [`TapasForQuestionAnswering`] requires different\ninputs to be fine-tuned:",
  "| **Task**                           | **Required inputs**                                                                                                 |\n|------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n| Conversational                     | `input_ids`, `attention_mask`, `token_type_ids`, `labels`                                                           |\n|  Weak supervision for aggregation  | `input_ids`, `attention_mask`, `token_type_ids`, `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |\n| Strong supervision for aggregation | `input ids`, `attention mask`, `token type ids`, `labels`, `aggregation_labels`                                     |\n\n[`TapasTokenizer`] creates the `labels`, `numeric_values` and `numeric_values_scale` based on the `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer` and `aggregation_labels` are already in the TSV file of step 2. Here's an example:\n\n```py\n>>> from transformers import TapasTokenizer\n>>> import pandas as pd\n\n>>> model_name = \"google/tapas-base\"",
  ">>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> queries = [\n...     \"What is the name of the first actor?\",\n...     \"How many movies has George Clooney played in?\",\n...     \"What is the total number of movies?\",\n... ]\n>>> answer_coordinates = [[(0, 0)], [(2, 1)], [(0, 1), (1, 1), (2, 1)]]\n>>> answer_text = [[\"Brad Pitt\"], [\"69\"], [\"209\"]]\n>>> table = pd.DataFrame.from_dict(data)\n>>> inputs = tokenizer(\n...     table=table,\n...     queries=queries,\n...     answer_coordinates=answer_coordinates,\n...     answer_text=answer_text,\n...     padding=\"max_length\",\n...     return_tensors=\"pt\",\n... )\n>>> inputs\n{'input_ids': tensor([[ ... ]]), 'attention_mask': tensor([[...]]), 'token_type_ids': tensor([[[...]]]),\n'numeric_values': tensor([[ ... ]]), 'numeric_values_scale: tensor([[ ... ]]), labels: tensor([[ ... ]])}\n```\n\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.astype(str)` on a dataframe to turn it into text-only data.",
  "Of course, this only shows how to encode a single training example. It is advised to create a dataloader to iterate over batches:\n\n```py\n>>> import torch\n>>> import pandas as pd\n\n>>> tsv_path = \"your_path_to_the_tsv_file\"\n>>> table_csv_path = \"your_path_to_a_directory_containing_all_csv_files\"\n\n\n>>> class TableDataset(torch.utils.data.Dataset):\n...     def __init__(self, data, tokenizer):\n...         self.data = data\n...         self.tokenizer = tokenizer\n\n...     def __getitem__(self, idx):\n...         item = data.iloc[idx]\n...         table = pd.read_csv(table_csv_path + item.table_file).astype(\n...             str\n...         )  # be sure to make your table data text only\n...         encoding = self.tokenizer(\n...             table=table,\n...             queries=item.question,\n...             answer_coordinates=item.answer_coordinates,\n...             answer_text=item.answer_text,\n...             truncation=True,\n...             padding=\"max_length\",\n...             return_tensors=\"pt\",\n...         )\n...         # remove the batch dimension which the tokenizer adds by default\n...         encoding = {key: val.squeeze(0) for key, val in encoding.items()}",
  "...         # add the float_answer which is also required (weak supervision for aggregation case)\n...         encoding[\"float_answer\"] = torch.tensor(item.float_answer)\n...         return encoding\n\n...     def __len__(self):\n...         return len(self.data)\n\n\n>>> data = pd.read_csv(tsv_path, sep=\"\\t\")\n>>> train_dataset = TableDataset(data, tokenizer)\n>>> train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n```\n</pt>\n<tf>\nThird, given that you've prepared your data in this TSV/CSV format (and corresponding CSV files containing the tabular data), you can then use [`TapasTokenizer`] to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids` and so on. Again, based on which of the three cases you picked above, [`TFTapasForQuestionAnswering`] requires different\ninputs to be fine-tuned:\n\n| **Task**                           | **Required inputs**                                                                                                 |\n|------------------------------------|---------------------------------------------------------------------------------------------------------------------|",
  "| Conversational                     | `input_ids`, `attention_mask`, `token_type_ids`, `labels`                                                           |\n|  Weak supervision for aggregation  | `input_ids`, `attention_mask`, `token_type_ids`, `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |\n| Strong supervision for aggregation | `input ids`, `attention mask`, `token type ids`, `labels`, `aggregation_labels`                                     |\n\n[`TapasTokenizer`] creates the `labels`, `numeric_values` and `numeric_values_scale` based on the `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer` and `aggregation_labels` are already in the TSV file of step 2. Here's an example:\n\n```py\n>>> from transformers import TapasTokenizer\n>>> import pandas as pd\n\n>>> model_name = \"google/tapas-base\"\n>>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> queries = [\n...     \"What is the name of the first actor?\",\n...     \"How many movies has George Clooney played in?\",\n...     \"What is the total number of movies?\",\n... ]",
  ">>> answer_coordinates = [[(0, 0)], [(2, 1)], [(0, 1), (1, 1), (2, 1)]]\n>>> answer_text = [[\"Brad Pitt\"], [\"69\"], [\"209\"]]\n>>> table = pd.DataFrame.from_dict(data)\n>>> inputs = tokenizer(\n...     table=table,\n...     queries=queries,\n...     answer_coordinates=answer_coordinates,\n...     answer_text=answer_text,\n...     padding=\"max_length\",\n...     return_tensors=\"tf\",\n... )\n>>> inputs\n{'input_ids': tensor([[ ... ]]), 'attention_mask': tensor([[...]]), 'token_type_ids': tensor([[[...]]]),\n'numeric_values': tensor([[ ... ]]), 'numeric_values_scale: tensor([[ ... ]]), labels: tensor([[ ... ]])}\n```\n\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.astype(str)` on a dataframe to turn it into text-only data.\nOf course, this only shows how to encode a single training example. It is advised to create a dataloader to iterate over batches:\n\n```py\n>>> import tensorflow as tf\n>>> import pandas as pd\n\n>>> tsv_path = \"your_path_to_the_tsv_file\"\n>>> table_csv_path = \"your_path_to_a_directory_containing_all_csv_files\"\n\n\n>>> class TableDataset:\n...     def __init__(self, data, tokenizer):\n...         self.data = data",
  "...         self.tokenizer = tokenizer\n\n...     def __iter__(self):\n...         for idx in range(self.__len__()):\n...             item = self.data.iloc[idx]\n...             table = pd.read_csv(table_csv_path + item.table_file).astype(\n...                 str\n...             )  # be sure to make your table data text only\n...             encoding = self.tokenizer(\n...                 table=table,\n...                 queries=item.question,\n...                 answer_coordinates=item.answer_coordinates,\n...                 answer_text=item.answer_text,\n...                 truncation=True,\n...                 padding=\"max_length\",\n...                 return_tensors=\"tf\",\n...             )\n...             # remove the batch dimension which the tokenizer adds by default\n...             encoding = {key: tf.squeeze(val, 0) for key, val in encoding.items()}\n...             # add the float_answer which is also required (weak supervision for aggregation case)\n...             encoding[\"float_answer\"] = tf.convert_to_tensor(item.float_answer, dtype=tf.float32)\n...             yield encoding[\"input_ids\"], encoding[\"attention_mask\"], encoding[\"numeric_values\"], encoding[",
  "...                 \"numeric_values_scale\"\n...             ], encoding[\"token_type_ids\"], encoding[\"labels\"], encoding[\"float_answer\"]\n\n...     def __len__(self):\n...         return len(self.data)\n\n\n>>> data = pd.read_csv(tsv_path, sep=\"\\t\")\n>>> train_dataset = TableDataset(data, tokenizer)\n>>> output_signature = (\n...     tf.TensorSpec(shape=(512,), dtype=tf.int32),\n...     tf.TensorSpec(shape=(512,), dtype=tf.int32),\n...     tf.TensorSpec(shape=(512,), dtype=tf.float32),\n...     tf.TensorSpec(shape=(512,), dtype=tf.float32),\n...     tf.TensorSpec(shape=(512, 7), dtype=tf.int32),\n...     tf.TensorSpec(shape=(512,), dtype=tf.int32),\n...     tf.TensorSpec(shape=(512,), dtype=tf.float32),\n... )\n>>> train_dataloader = tf.data.Dataset.from_generator(train_dataset, output_signature=output_signature).batch(32)\n```\n</tf>\n</frameworkcontent>\n\nNote that here, we encode each table-question pair independently. This is fine as long as your dataset is **not conversational**. In case your dataset involves conversational questions (such as in SQA), then you should first group together the `queries`, `answer_coordinates` and `answer_text` per table (in the order of their `position`",
  "index) and batch encode each table with its questions. This will make sure that the `prev_labels` token types (see docs of [`TapasTokenizer`]) are set correctly. See [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) for more info. See [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) for more info regarding using the TensorFlow model.\n\n**STEP 4: Train (fine-tune) the model\n\n<frameworkcontent>\n<pt>\nYou can then fine-tune [`TapasForQuestionAnswering`] as follows (shown here for the weak supervision for aggregation case):\n\n```py\n>>> from transformers import TapasConfig, TapasForQuestionAnswering, AdamW\n\n>>> # this is the default WTQ configuration\n>>> config = TapasConfig(\n...     num_aggregation_labels=4,\n...     use_answer_as_supervision=True,\n...     answer_loss_cutoff=0.664694,\n...     cell_selection_preference=0.207951,\n...     huber_loss_delta=0.121194,\n...     init_cell_selection_weights_to_zero=True,\n...     select_one_column=True,\n...     allow_empty_column_selection=False,\n...     temperature=0.0352513,\n... )",
  ">>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n\n>>> optimizer = AdamW(model.parameters(), lr=5e-5)\n\n>>> model.train()\n>>> for epoch in range(2):  # loop over the dataset multiple times\n...     for batch in train_dataloader:\n...         # get the inputs;\n...         input_ids = batch[\"input_ids\"]\n...         attention_mask = batch[\"attention_mask\"]\n...         token_type_ids = batch[\"token_type_ids\"]\n...         labels = batch[\"labels\"]\n...         numeric_values = batch[\"numeric_values\"]\n...         numeric_values_scale = batch[\"numeric_values_scale\"]\n...         float_answer = batch[\"float_answer\"]\n\n...         # zero the parameter gradients\n...         optimizer.zero_grad()\n\n...         # forward + backward + optimize\n...         outputs = model(\n...             input_ids=input_ids,\n...             attention_mask=attention_mask,\n...             token_type_ids=token_type_ids,\n...             labels=labels,\n...             numeric_values=numeric_values,\n...             numeric_values_scale=numeric_values_scale,\n...             float_answer=float_answer,\n...         )\n...         loss = outputs.loss\n...         loss.backward()",
  "...         optimizer.step()\n```\n</pt>\n<tf>\nYou can then fine-tune [`TFTapasForQuestionAnswering`] as follows (shown here for the weak supervision for aggregation case):\n\n```py\n>>> import tensorflow as tf\n>>> from transformers import TapasConfig, TFTapasForQuestionAnswering\n\n>>> # this is the default WTQ configuration\n>>> config = TapasConfig(\n...     num_aggregation_labels=4,\n...     use_answer_as_supervision=True,\n...     answer_loss_cutoff=0.664694,\n...     cell_selection_preference=0.207951,\n...     huber_loss_delta=0.121194,\n...     init_cell_selection_weights_to_zero=True,\n...     select_one_column=True,\n...     allow_empty_column_selection=False,\n...     temperature=0.0352513,\n... )\n>>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n\n>>> optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n\n>>> for epoch in range(2):  # loop over the dataset multiple times\n...     for batch in train_dataloader:\n...         # get the inputs;\n...         input_ids = batch[0]\n...         attention_mask = batch[1]\n...         token_type_ids = batch[4]\n...         labels = batch[-1]\n...         numeric_values = batch[2]",
  "...         numeric_values_scale = batch[3]\n...         float_answer = batch[6]\n\n...         # forward + backward + optimize\n...         with tf.GradientTape() as tape:\n...             outputs = model(\n...                 input_ids=input_ids,\n...                 attention_mask=attention_mask,\n...                 token_type_ids=token_type_ids,\n...                 labels=labels,\n...                 numeric_values=numeric_values,\n...                 numeric_values_scale=numeric_values_scale,\n...                 float_answer=float_answer,\n...             )\n...         grads = tape.gradient(outputs.loss, model.trainable_weights)\n...         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n```\n</tf>\n</frameworkcontent>\n\n## Usage: inference\n\n<frameworkcontent>\n<pt>",
  "Here we explain how you can use [`TapasForQuestionAnswering`] or [`TFTapasForQuestionAnswering`] for inference (i.e. making predictions on new data). For inference, only `input_ids`, `attention_mask` and `token_type_ids` (which you can obtain using [`TapasTokenizer`]) have to be provided to the model to obtain the logits. Next, you can use the handy [`~models.tapas.tokenization_tapas.convert_logits_to_predictions`] method to convert these into predicted coordinates and optional aggregation indices.\n\nHowever, note that inference is **different** depending on whether or not the setup is conversational. In a non-conversational set-up, inference can be done in parallel on all table-question pairs of a batch. Here's an example of that:\n\n```py\n>>> from transformers import TapasTokenizer, TapasForQuestionAnswering\n>>> import pandas as pd\n\n>>> model_name = \"google/tapas-base-finetuned-wtq\"\n>>> model = TapasForQuestionAnswering.from_pretrained(model_name)\n>>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> queries = [",
  "...     \"What is the name of the first actor?\",\n...     \"How many movies has George Clooney played in?\",\n...     \"What is the total number of movies?\",\n... ]\n>>> table = pd.DataFrame.from_dict(data)\n>>> inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n...     inputs, outputs.logits.detach(), outputs.logits_aggregation.detach()\n... )\n\n>>> # let's print out the results:\n>>> id2aggregation = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3: \"COUNT\"}\n>>> aggregation_predictions_string = [id2aggregation[x] for x in predicted_aggregation_indices]\n\n>>> answers = []\n>>> for coordinates in predicted_answer_coordinates:\n...     if len(coordinates) == 1:\n...         # only a single cell:\n...         answers.append(table.iat[coordinates[0]])\n...     else:\n...         # multiple cells\n...         cell_values = []\n...         for coordinate in coordinates:\n...             cell_values.append(table.iat[coordinate])\n...         answers.append(\", \".join(cell_values))\n\n>>> display(table)\n>>> print(\"\")",
  ">>> for query, answer, predicted_agg in zip(queries, answers, aggregation_predictions_string):\n...     print(query)\n...     if predicted_agg == \"NONE\":\n...         print(\"Predicted answer: \" + answer)\n...     else:\n...         print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)\nWhat is the name of the first actor?\nPredicted answer: Brad Pitt\nHow many movies has George Clooney played in?\nPredicted answer: COUNT > 69\nWhat is the total number of movies?\nPredicted answer: SUM > 87, 53, 69\n```\n</pt>\n<tf>\nHere we explain how you can use [`TFTapasForQuestionAnswering`] for inference (i.e. making predictions on new data). For inference, only `input_ids`, `attention_mask` and `token_type_ids` (which you can obtain using [`TapasTokenizer`]) have to be provided to the model to obtain the logits. Next, you can use the handy [`~models.tapas.tokenization_tapas.convert_logits_to_predictions`] method to convert these into predicted coordinates and optional aggregation indices.",
  "However, note that inference is **different** depending on whether or not the setup is conversational. In a non-conversational set-up, inference can be done in parallel on all table-question pairs of a batch. Here's an example of that:\n\n```py\n>>> from transformers import TapasTokenizer, TFTapasForQuestionAnswering\n>>> import pandas as pd\n\n>>> model_name = \"google/tapas-base-finetuned-wtq\"\n>>> model = TFTapasForQuestionAnswering.from_pretrained(model_name)\n>>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> queries = [\n...     \"What is the name of the first actor?\",\n...     \"How many movies has George Clooney played in?\",\n...     \"What is the total number of movies?\",\n... ]\n>>> table = pd.DataFrame.from_dict(data)\n>>> inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n>>> predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n...     inputs, outputs.logits, outputs.logits_aggregation\n... )\n\n>>> # let's print out the results:",
  ">>> id2aggregation = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3: \"COUNT\"}\n>>> aggregation_predictions_string = [id2aggregation[x] for x in predicted_aggregation_indices]\n\n>>> answers = []\n>>> for coordinates in predicted_answer_coordinates:\n...     if len(coordinates) == 1:\n...         # only a single cell:\n...         answers.append(table.iat[coordinates[0]])\n...     else:\n...         # multiple cells\n...         cell_values = []\n...         for coordinate in coordinates:\n...             cell_values.append(table.iat[coordinate])\n...         answers.append(\", \".join(cell_values))\n\n>>> display(table)\n>>> print(\"\")\n>>> for query, answer, predicted_agg in zip(queries, answers, aggregation_predictions_string):\n...     print(query)\n...     if predicted_agg == \"NONE\":\n...         print(\"Predicted answer: \" + answer)\n...     else:\n...         print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)\nWhat is the name of the first actor?\nPredicted answer: Brad Pitt\nHow many movies has George Clooney played in?\nPredicted answer: COUNT > 69\nWhat is the total number of movies?\nPredicted answer: SUM > 87, 53, 69\n```\n</tf>\n</frameworkcontent>",
  "In case of a conversational set-up, then each table-question pair must be provided **sequentially** to the model, such that the `prev_labels` token types can be overwritten by the predicted `labels` of the previous table-question pair. Again, more info can be found in [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) (for PyTorch) and [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) (for TensorFlow).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n## TAPAS specific outputs\n[[autodoc]] models.tapas.modeling_tapas.TableQuestionAnsweringOutput\n\n## TapasConfig\n[[autodoc]] TapasConfig\n\n## TapasTokenizer\n[[autodoc]] TapasTokenizer\n- __call__\n- convert_logits_to_predictions\n- save_vocabulary\n\n<frameworkcontent>\n<pt>\n\n## TapasModel\n[[autodoc]] TapasModel\n- forward\n\n## TapasForMaskedLM\n[[autodoc]] TapasForMaskedLM\n- forward\n\n## TapasForSequenceClassification\n[[autodoc]] TapasForSequenceClassification",
  "- forward\n\n## TapasForQuestionAnswering\n[[autodoc]] TapasForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFTapasModel\n[[autodoc]] TFTapasModel\n- call\n\n## TFTapasForMaskedLM\n[[autodoc]] TFTapasForMaskedLM\n- call\n\n## TFTapasForSequenceClassification\n[[autodoc]] TFTapasForSequenceClassification\n- call\n\n## TFTapasForQuestionAnswering\n[[autodoc]] TFTapasForQuestionAnswering\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LXMERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\n(one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\ncombination of masked language modeling, visual-language text alignment, ROI-feature regression, masked\nvisual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining\nconsists of multiple multi-modal datasets: MSCOCO, Visual-Genome + Visual-Genome Question Answering, VQA 2.0, and GQA.\n\nThe abstract from the paper is the following:\n\n*Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly,\nthe alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality\nEncoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we\nbuild a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language",
  "encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language\nsemantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative\npretraining tasks: masked language modeling, masked object prediction (feature regression and label classification),\ncross-modality matching, and image question answering. These tasks help in learning both intra-modality and\ncross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art\nresults on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our\npretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR, and improve the previous\nbest result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel\nmodel components and pretraining strategies significantly contribute to our strong results; and also present several\nattention visualizations for the different encoders*",
  "This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). The original code can be found [here](https://github.com/airsplay/lxmert).\n\n## Usage tips\n\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any kind of visual-spacial features\nwill work.\n- Both the language hidden states and the visual hidden states that LXMERT outputs are passed through the\ncross-modality layer, so they contain information from both modalities. To access a modality that only attends to\nitself, select the vision/language hidden states from the first input in the tuple.\n- The bidirectional cross-modality encoder attention only returns attention values when the language modality is used\nas the input and the vision modality is used as the context vector. Further, while the cross-modality encoder\ncontains self-attention for each respective modality and cross-attention, only the cross attention is returned and\nboth self attention outputs are disregarded.\n\n## Resources\n\n- [Question answering task guide](../tasks/question_answering)\n\n## LxmertConfig\n\n[[autodoc]] LxmertConfig\n\n## LxmertTokenizer\n\n[[autodoc]] LxmertTokenizer\n\n## LxmertTokenizerFast",
  "[[autodoc]] LxmertTokenizerFast\n\n## Lxmert specific outputs\n\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertModelOutput\n\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput\n\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput\n\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\n\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## LxmertModel\n\n[[autodoc]] LxmertModel\n- forward\n\n## LxmertForPreTraining\n\n[[autodoc]] LxmertForPreTraining\n- forward\n\n## LxmertForQuestionAnswering\n\n[[autodoc]] LxmertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFLxmertModel\n\n[[autodoc]] TFLxmertModel\n- call\n\n## TFLxmertForPreTraining\n\n[[autodoc]] TFLxmertForPreTraining\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The DPT model was proposed in [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\nDPT is a model that leverages the [Vision Transformer (ViT)](vit) as backbone for dense prediction tasks like semantic segmentation and depth estimation.\n\nThe abstract from the paper is the following:",
  "*We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dpt_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> DPT architecture. Taken from the <a href=\"https://arxiv.org/abs/2103.13413\" target=\"_blank\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/isl-org/DPT).\n\n## Usage tips\n\nDPT is compatible with the [`AutoBackbone`] class. This allows to use the DPT framework with various computer vision backbones available in the library, such as [`VitDetBackbone`] or [`Dinov2Backbone`]. One can create it as follows:\n\n```python\nfrom transformers import Dinov2Config, DPTConfig, DPTForDepthEstimation\n\n# initialize with a Transformer-based backbone such as DINOv2\n# in that case, we also specify `reshape_hidden_states=False` to get feature maps of shape (batch_size, num_channels, height, width)\nbackbone_config = Dinov2Config.from_pretrained(\"facebook/dinov2-base\", out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"], reshape_hidden_states=False)\n\nconfig = DPTConfig(backbone_config=backbone_config)\nmodel = DPTForDepthEstimation(config=config)\n```",
  "## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DPT.\n\n- Demo notebooks for [`DPTForDepthEstimation`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DPT).\n\n- [Semantic segmentation task guide](../tasks/semantic_segmentation)\n- [Monocular depth estimation task guide](../tasks/monocular_depth_estimation)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DPTConfig\n\n[[autodoc]] DPTConfig\n\n## DPTFeatureExtractor\n\n[[autodoc]] DPTFeatureExtractor\n- __call__\n- post_process_semantic_segmentation\n\n## DPTImageProcessor\n\n[[autodoc]] DPTImageProcessor\n- preprocess\n- post_process_semantic_segmentation\n\n## DPTModel\n\n[[autodoc]] DPTModel\n- forward\n\n## DPTForDepthEstimation\n\n[[autodoc]] DPTForDepthEstimation\n- forward\n\n## DPTForSemanticSegmentation\n\n[[autodoc]] DPTForSemanticSegmentation\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CANINE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe CANINE model was proposed in [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language\nRepresentation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting. It's",
  "among the first papers that trains a Transformer without using an explicit tokenization step (such as Byte Pair\nEncoding (BPE), WordPiece or SentencePiece). Instead, the model is trained directly at a Unicode character-level.\nTraining at a character-level inevitably comes with a longer sequence length, which CANINE solves with an efficient\ndownsampling strategy, before applying a deep Transformer encoder.\n\nThe abstract from the paper is the following:\n\n*Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models\nstill require an explicit tokenization step. While recent tokenization approaches based on data-derived subword\nlexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all\nlanguages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE,\na neural encoder that operates directly on character sequences, without explicit tokenization or vocabulary, and a\npre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias.",
  "To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input\nsequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by\n2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28% fewer model parameters.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/google-research/language/tree/master/language/canine).\n\n## Usage tips\n\n- CANINE uses no less than 3 Transformer encoders internally: 2 \"shallow\" encoders (which only consist of a single\nlayer) and 1 \"deep\" encoder (which is a regular BERT encoder). First, a \"shallow\" encoder is used to contextualize\nthe character embeddings, using local attention. Next, after downsampling, a \"deep\" encoder is applied. Finally,\nafter upsampling, a \"shallow\" encoder is used to create the final character embeddings. Details regarding up- and\ndownsampling can be found in the paper.\n- CANINE uses a max sequence length of 2048 characters by default. One can use [`CanineTokenizer`]\nto prepare text for the model.",
  "- Classification can be done by placing a linear layer on top of the final hidden state of the special [CLS] token\n(which has a predefined Unicode code point). For token classification tasks however, the downsampled sequence of\ntokens needs to be upsampled again to match the length of the original character sequence (which is 2048). The\ndetails for this can be found in the paper.\n\nModel checkpoints:\n\n- [google/canine-c](https://huggingface.co/google/canine-c): Pre-trained with autoregressive character loss,\n12-layer, 768-hidden, 12-heads, 121M parameters (size ~500 MB).\n- [google/canine-s](https://huggingface.co/google/canine-s): Pre-trained with subword loss, 12-layer,\n768-hidden, 12-heads, 121M parameters (size ~500 MB).\n\n\n## Usage example\n\nCANINE works on raw characters, so it can be used **without a tokenizer**:\n\n```python\n>>> from transformers import CanineModel\n>>> import torch\n\n>>> model = CanineModel.from_pretrained(\"google/canine-c\")  # model pre-trained with autoregressive character loss\n\n>>> text = \"hello world\"\n>>> # use Python's built-in ord() function to turn each character into its unicode code point id\n>>> input_ids = torch.tensor([[ord(char) for char in text]])",
  ">>> outputs = model(input_ids)  # forward pass\n>>> pooled_output = outputs.pooler_output\n>>> sequence_output = outputs.last_hidden_state\n```\n\nFor batched inference and training, it is however recommended to make use of the tokenizer (to pad/truncate all\nsequences to the same length):\n\n```python\n>>> from transformers import CanineTokenizer, CanineModel\n\n>>> model = CanineModel.from_pretrained(\"google/canine-c\")\n>>> tokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")\n\n>>> inputs = [\"Life is like a box of chocolates.\", \"You never know what you gonna get.\"]\n>>> encoding = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)  # forward pass\n>>> pooled_output = outputs.pooler_output\n>>> sequence_output = outputs.last_hidden_state\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## CanineConfig\n\n[[autodoc]] CanineConfig\n\n## CanineTokenizer\n\n[[autodoc]] CanineTokenizer",
  "- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n\n## CANINE specific outputs\n\n[[autodoc]] models.canine.modeling_canine.CanineModelOutputWithPooling\n\n## CanineModel\n\n[[autodoc]] CanineModel\n- forward\n\n## CanineForSequenceClassification\n\n[[autodoc]] CanineForSequenceClassification\n- forward\n\n## CanineForMultipleChoice\n\n[[autodoc]] CanineForMultipleChoice\n- forward\n\n## CanineForTokenClassification\n\n[[autodoc]] CanineForTokenClassification\n- forward\n\n## CanineForQuestionAnswering\n\n[[autodoc]] CanineForQuestionAnswering\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# VipLlava\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The VipLlava model was proposed in [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.\n\nVipLlava enhances the training protocol of Llava by marking images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\" during training.\n\nThe abstract from the paper is the following:",
  "*While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.*\n\nThe original code can be found [here](https://github.com/mu-cai/ViP-LLaVA).",
  "This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)\n\n\n## Usage tips:\n\n- The architecture is similar than llava architecture except that the multi-modal projector takes a set of concatenated vision hidden states and has an additional layernorm layer on that module.\n\n- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n\n- Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n\n> [!NOTE]\n> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.",
  "Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n\n\n- For better results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:\n\n```python\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/vip-llava-7b-hf\")",
  "conversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What’s shown in this image?\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n},\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe the image in more details.\"},\n],\n},\n]\n\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\n# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images\nprint(text_prompt)\n>>> \"###Human: <image>\\nWhat’s shown in this image?###Assistant: This image shows a red stop sign.###Human: Describe the image in more details.###Assistant:\"\n```\n\n- If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by VipLLaVa checkpoints:\n```bash\nA chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n<prompt>###Assistant:\n```\n\nFor multiple turns conversation:\n```bash",
  "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n<prompt1>###Assistant: <answer1>###Human: <prompt2>###Assistant:\n```\n\n\n## VipLlavaConfig\n\n[[autodoc]] VipLlavaConfig\n\n## VipLlavaForConditionalGeneration\n\n[[autodoc]] VipLlavaForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# NLLB\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Updated tokenizer behavior",
  "**DISCLAIMER:** The default behaviour for the tokenizer was fixed and thus changed in April 2023.\nThe previous version adds `[self.eos_token_id, self.cur_lang_code]` at the end of the token sequence for both target and source tokenization. This is wrong as the NLLB paper mentions (page 48, 6.1.1. Model Architecture) :\n\n*Note that we prefix the source sequence with the source language, as opposed to the target\nlanguage as previously done in several works (Arivazhagan et al., 2019; Johnson et al.,\n2017). This is primarily because we prioritize optimizing zero-shot performance of our\nmodel on any pair of 200 languages at a minor cost to supervised performance.*\n\nPrevious behaviour:\n\n```python\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> tokenizer(\"How was your day?\").input_ids\n[13374, 1398, 4260, 4039, 248130, 2, 256047]\n\n>>> # 2: '</s>'\n>>> # 256047 : 'eng_Latn'\n```\nNew behaviour\n\n```python\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> tokenizer(\"How was your day?\").input_ids",
  "[256047, 13374, 1398, 4260, 4039, 248130, 2]\n```\n\nEnabling the old behaviour can be done as follows:\n```python\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", legacy_behaviour=True)\n```\n\nFor more details, feel free to check the linked [PR](https://github.com/huggingface/transformers/pull/22313) and [Issue](https://github.com/huggingface/transformers/issues/19943).\n\n## Overview\n\nThe NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\nLoic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\nNecip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.",
  "The abstract of the paper is the following:\n\n*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.\nHowever, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the\n200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by\nfirst contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed\nat narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of\nExperts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training",
  "improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using\na human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.\nOur model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*\n\nThis implementation contains the dense models available on release.\n\n**The sparse model NLLB-MoE (Mixture of Expert) is now available! More details [here](nllb-moe)**\n\nThis model was contributed by [Lysandre](https://huggingface.co/lysandre). The authors' code can be found [here](https://github.com/facebookresearch/fairseq/tree/nllb).\n\n## Generating with NLLB\n\nWhile generating the target text set the `forced_bos_token_id` to the target language id. The following\nexample shows how to translate English to French using the *facebook/nllb-200-distilled-600M* model.",
  "Note that we're using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\nfor the list of all BCP-47 in the Flores 200 dataset.\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n\n>>> article = \"UN Chief says there is no military solution in Syria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nLe chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\n```\n\n### Generating from any other language than English\n\nEnglish (`eng_Latn`) is set as the default language from which to translate. In order to specify that you'd like to translate from a different language,\nyou should specify the BCP-47 code in the `src_lang` keyword argument of the tokenizer initialization.",
  "See example below for a translation from romanian to german:\n\n```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\n...     \"facebook/nllb-200-distilled-600M\", token=True, src_lang=\"ron_Latn\"\n... )\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", token=True)\n\n>>> article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nUN-Chef sagt, es gibt keine militärische Lösung in Syrien\n```\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## NllbTokenizer\n\n[[autodoc]] NllbTokenizer\n- build_inputs_with_special_tokens\n\n## NllbTokenizerFast\n\n[[autodoc]] NllbTokenizerFast\n\n## Using Flash Attention 2\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on `cuda` kernels.\n\n### Installation",
  "First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n\nNext, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\n### Usage\n\nTo load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). You can use either `torch.float16` or `torch.bfloat16` precision.\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\").eval()\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n\n>>> article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"",
  ">>> inputs = tokenizer(article, return_tensors=\"pt\").to(\"cuda\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"UN-Chef sagt, es gibt keine militärische Lösung in Syrien\"\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation and the Flash Attention 2.\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/visheratin/documentation-images/resolve/main/nllb-speedup.webp\">\n</div>\n\n## Using Scaled Dot Product Attention (SDPA)\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)",
  "page for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DeiT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">",
  "<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, Hervé Jégou. The [Vision Transformer (ViT)](vit) introduced in [Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929) has shown that one can match or even outperform existing convolutional neural\nnetworks using a Transformer encoder (BERT-like). However, the ViT models introduced in that paper required training on\nexpensive infrastructure for multiple weeks, using external data. DeiT (data-efficient image transformers) are more\nefficiently trained transformers for image classification, requiring far less data and far less computing resources\ncompared to the original ViT models.\n\nThe abstract from the paper is the following:\n\n*Recently, neural networks purely based on attention were shown to address image understanding tasks such as image",
  "classification. However, these visual transformers are pre-trained with hundreds of millions of images using an\nexpensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free\ntransformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision\ntransformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external\ndata. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation\ntoken ensuring that the student learns from the teacher through attention. We show the interest of this token-based\ndistillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets\nfor both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and\nmodels.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version of this model was added by [amyeroberts](https://huggingface.co/amyeroberts).\n\n## Usage tips",
  "- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teacher (which, in the\nDeiT paper, is a ResNet like-model). The distillation token is learned through backpropagation, by interacting with\nthe class ([CLS]) and patch tokens through the self-attention layers.\n- There are 2 ways to fine-tune distilled models, either (1) in a classic way, by only placing a prediction head on top\nof the final hidden state of the class token and not using the distillation signal, or (2) by placing both a\nprediction head on top of the class token and on top of the distillation token. In that case, the [CLS] prediction\nhead is trained using regular cross-entropy between the prediction of the head and the ground-truth label, while the\ndistillation prediction head is trained using hard distillation (cross-entropy between the prediction of the\ndistillation head and the label predicted by the teacher). At inference time, one takes the average prediction\nbetween both heads as final prediction. (2) is also called \"fine-tuning with distillation\", because one relies on a",
  "teacher that has already been fine-tuned on the downstream dataset. In terms of models, (1) corresponds to\n[`DeiTForImageClassification`] and (2) corresponds to\n[`DeiTForImageClassificationWithTeacher`].\n- Note that the authors also did try soft distillation for (2) (in which case the distillation prediction head is\ntrained using KL divergence to match the softmax output of the teacher), but hard distillation gave the best results.\n- All released checkpoints were pre-trained and fine-tuned on ImageNet-1k only. No external data was used. This is in\ncontrast with the original ViT model, which used external data like the JFT-300M dataset/Imagenet-21k for\npre-training.\n- The authors of DeiT also released more efficiently trained ViT models, which you can directly plug into\n[`ViTModel`] or [`ViTForImageClassification`]. Techniques like data\naugmentation, optimization, and regularization were used in order to simulate training on a much larger dataset\n(while only using ImageNet-1k for pre-training). There are 4 variants available (in 3 different sizes):\n*facebook/deit-tiny-patch16-224*, *facebook/deit-small-patch16-224*, *facebook/deit-base-patch16-224* and",
  "*facebook/deit-base-patch16-384*. Note that one should use [`DeiTImageProcessor`] in order to\nprepare images for the model.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import DeiTForImageClassification\nmodel = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```",
  "For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `facebook/deit-base-distilled-patch16-224` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                         8 |                                         6 |                      1.33 |\n|            2 |                                         9 |                                         6 |                      1.5  |\n|            4 |                                         9 |                                         6 |                      1.5  |\n|            8 |                                         8 |                                         6 |                      1.33 |\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DeiT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`DeiTForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nBesides that:\n\n- [`DeiTForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DeiTConfig\n\n[[autodoc]] DeiTConfig\n\n## DeiTFeatureExtractor\n\n[[autodoc]] DeiTFeatureExtractor\n- __call__\n\n## DeiTImageProcessor\n\n[[autodoc]] DeiTImageProcessor\n- preprocess\n\n## DeiTImageProcessorFast\n\n[[autodoc]] DeiTImageProcessorFast\n- preprocess",
  "<frameworkcontent>\n<pt>\n\n## DeiTModel\n\n[[autodoc]] DeiTModel\n- forward\n\n## DeiTForMaskedImageModeling\n\n[[autodoc]] DeiTForMaskedImageModeling\n- forward\n\n## DeiTForImageClassification\n\n[[autodoc]] DeiTForImageClassification\n- forward\n\n## DeiTForImageClassificationWithTeacher\n\n[[autodoc]] DeiTForImageClassificationWithTeacher\n- forward\n\n</pt>\n<tf>\n\n## TFDeiTModel\n\n[[autodoc]] TFDeiTModel\n- call\n\n## TFDeiTForMaskedImageModeling\n\n[[autodoc]] TFDeiTForMaskedImageModeling\n- call\n\n## TFDeiTForImageClassification\n\n[[autodoc]] TFDeiTForImageClassification\n- call\n\n## TFDeiTForImageClassificationWithTeacher\n\n[[autodoc]] TFDeiTForImageClassificationWithTeacher\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PEGASUS-X\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe PEGASUS-X model was proposed in [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)  by Jason Phang, Yao Zhao and Peter J. Liu.",
  "PEGASUS-X (PEGASUS eXtended) extends the PEGASUS models for long input summarization through additional long input pretraining and using staggered block-local attention with global tokens in the encoder.\n\nThe abstract from the paper is the following:",
  "*While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs continues to be a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most pretrained models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms can most efficiently adapt a pretrained Transformer for long input summarization. We find that a staggered, block-local Transformer with global encoder tokens strikes a good balance of performance and efficiency, and that an additional pretraining phase on long sequences meaningfully improves downstream summarization performance. Based on our findings, we introduce PEGASUS-X, an extension of the PEGASUS model with additional long input pretraining to handle inputs of up to 16K tokens. PEGASUS-X achieves strong performance on long input summarization tasks comparable with much larger models while adding few additional parameters and not requiring model parallelism to train.*",
  "This model was contributed by [zphang](https://huggingface.co/zphang). The original code can be found [here](https://github.com/google-research/pegasus).\n\n## Documentation resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n<Tip>\n\nPEGASUS-X uses the same tokenizer as [PEGASUS](pegasus).\n\n</Tip>\n\n## PegasusXConfig\n\n[[autodoc]] PegasusXConfig\n\n## PegasusXModel\n\n[[autodoc]] PegasusXModel\n- forward\n\n## PegasusXForConditionalGeneration\n\n[[autodoc]] PegasusXForConditionalGeneration\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Wav2Vec2-BERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Wav2Vec2-BERT model was proposed in [Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/) by the Seamless Communication team from Meta AI.\n\nThis model was pre-trained on 4.5M hours of unlabeled audio data covering more than 143 languages. It requires finetuning to be used for downstream tasks such as Automatic Speech Recognition (ASR), or Audio Classification.\n\nThe official results of the model can be found in Section 3.2.1 of the paper.\n\nThe abstract from the paper is the following:",
  "*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one’s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a science fiction concept into a real-world technology. Finally, contributions in this work—including models, code, and a watermark detector—are publicly released and accessible at the link below.*",
  "This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/seamless_communication).\n\n## Usage tips\n\n- Wav2Vec2-BERT follows the same architecture as Wav2Vec2-Conformer, but employs a causal depthwise convolutional layer and uses as input a mel-spectrogram representation of the audio instead of the raw waveform.\n- Wav2Vec2-BERT can use either no relative position embeddings, Shaw-like position embeddings, Transformer-XL-like position embeddings, or\nrotary position embeddings by setting the correct `config.position_embeddings_type`.\n- Wav2Vec2-BERT also introduces a Conformer-based adapter network instead of a simple convolutional network.\n\n## Resources\n\n<PipelineTag pipeline=\"automatic-speech-recognition\"/>\n\n- [`Wav2Vec2BertForCTC`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).",
  "- You can also adapt these notebooks on [how to finetune a speech recognition model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb), and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).\n\n<PipelineTag pipeline=\"audio-classification\"/>\n\n- [`Wav2Vec2BertForSequenceClassification`] can be used by adapting this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification).\n- See also: [Audio classification task guide](../tasks/audio_classification)\n\n\n## Wav2Vec2BertConfig\n\n[[autodoc]] Wav2Vec2BertConfig\n\n## Wav2Vec2BertProcessor\n\n[[autodoc]] Wav2Vec2BertProcessor\n- __call__\n- pad\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## Wav2Vec2BertModel\n\n[[autodoc]] Wav2Vec2BertModel\n- forward\n\n## Wav2Vec2BertForCTC\n\n[[autodoc]] Wav2Vec2BertForCTC\n- forward\n\n## Wav2Vec2BertForSequenceClassification\n\n[[autodoc]] Wav2Vec2BertForSequenceClassification\n- forward\n\n## Wav2Vec2BertForAudioFrameClassification",
  "[[autodoc]] Wav2Vec2BertForAudioFrameClassification\n- forward\n\n## Wav2Vec2BertForXVector\n\n[[autodoc]] Wav2Vec2BertForXVector\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RoFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe RoFormer model was proposed in [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864v1.pdf) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n\nThe abstract from the paper is the following:\n\n*Position encoding in transformer architecture provides supervision for dependency modeling between elements at\ndifferent positions in the sequence. We investigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named Rotary Position Embedding(RoPE). The\nproposed RoPE encodes absolute positional information with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with valuable properties such as flexibility of\nbeing expand to any sequence lengths, decaying inter-token dependency with increasing relative distances, and\ncapability of equipping the linear self-attention with relative position encoding. As a result, the enhanced\ntransformer with rotary position embedding, or RoFormer, achieves superior performance in tasks with long texts. We",
  "release the theoretical analysis along with some preliminary experiment results on Chinese data. The undergoing\nexperiment for English benchmark will soon be updated.*\n\nThis model was contributed by [junnyu](https://huggingface.co/junnyu). The original code can be found [here](https://github.com/ZhuiyiTechnology/roformer).\n\n## Usage tips\nRoFormer is a BERT-like autoencoding model with rotary position embeddings. Rotary position embeddings have shown\nimproved performance on classification tasks with long texts.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RoFormerConfig\n\n[[autodoc]] RoFormerConfig\n\n## RoFormerTokenizer\n\n[[autodoc]] RoFormerTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## RoFormerTokenizerFast",
  "[[autodoc]] RoFormerTokenizerFast\n- build_inputs_with_special_tokens\n\n<frameworkcontent>\n<pt>\n\n## RoFormerModel\n\n[[autodoc]] RoFormerModel\n- forward\n\n## RoFormerForCausalLM\n\n[[autodoc]] RoFormerForCausalLM\n- forward\n\n## RoFormerForMaskedLM\n\n[[autodoc]] RoFormerForMaskedLM\n- forward\n\n## RoFormerForSequenceClassification\n\n[[autodoc]] RoFormerForSequenceClassification\n- forward\n\n## RoFormerForMultipleChoice\n\n[[autodoc]] RoFormerForMultipleChoice\n- forward\n\n## RoFormerForTokenClassification\n\n[[autodoc]] RoFormerForTokenClassification\n- forward\n\n## RoFormerForQuestionAnswering\n\n[[autodoc]] RoFormerForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFRoFormerModel\n\n[[autodoc]] TFRoFormerModel\n- call\n\n## TFRoFormerForMaskedLM\n\n[[autodoc]] TFRoFormerForMaskedLM\n- call\n\n## TFRoFormerForCausalLM\n\n[[autodoc]] TFRoFormerForCausalLM\n- call\n\n## TFRoFormerForSequenceClassification\n\n[[autodoc]] TFRoFormerForSequenceClassification\n- call\n\n## TFRoFormerForMultipleChoice\n\n[[autodoc]] TFRoFormerForMultipleChoice\n- call\n\n## TFRoFormerForTokenClassification\n\n[[autodoc]] TFRoFormerForTokenClassification\n- call\n\n## TFRoFormerForQuestionAnswering\n\n[[autodoc]] TFRoFormerForQuestionAnswering\n- call\n\n</tf>\n<jax>",
  "## FlaxRoFormerModel\n\n[[autodoc]] FlaxRoFormerModel\n- __call__\n\n## FlaxRoFormerForMaskedLM\n\n[[autodoc]] FlaxRoFormerForMaskedLM\n- __call__\n\n## FlaxRoFormerForSequenceClassification\n\n[[autodoc]] FlaxRoFormerForSequenceClassification\n- __call__\n\n## FlaxRoFormerForMultipleChoice\n\n[[autodoc]] FlaxRoFormerForMultipleChoice\n- __call__\n\n## FlaxRoFormerForTokenClassification\n\n[[autodoc]] FlaxRoFormerForTokenClassification\n- __call__\n\n## FlaxRoFormerForQuestionAnswering\n\n[[autodoc]] FlaxRoFormerForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OmDet-Turbo\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The OmDet-Turbo model was proposed in [Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head](https://arxiv.org/abs/2403.06892) by Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee. OmDet-Turbo incorporates components from RT-DETR and introduces a swift multimodal fusion module to achieve real-time open-vocabulary object detection capabilities while maintaining high accuracy. The base model achieves performance of up to 100.2 FPS and 53.4 AP on COCO zero-shot.\n\nThe abstract from the paper is the following:",
  "*End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/omdet_turbo_architecture.jpeg\" alt=\"drawing\" width=\"600\"/>\n\n<small> OmDet-Turbo architecture overview. Taken from the <a href=\"https://arxiv.org/abs/2403.06892\">original paper</a>. </small>\n\nThis model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\nThe original code can be found [here](https://github.com/om-ai-lab/OmDet).\n\n## Usage tips\n\nOne unique property of OmDet-Turbo compared to other zero-shot object detection models, such as [Grounding DINO](grounding-dino), is the decoupled classes and prompt embedding structure that allows caching of text embeddings. This means that the model needs both classes and task as inputs, where classes is a list of objects we want to detect and task is the grounded text used to guide open-vocabulary detection. This approach limits the scope of the open-vocabulary detection and makes the decoding process faster.",
  "[`OmDetTurboProcessor`] is used to prepare the classes, task and image triplet. The task input is optional, and when not provided, it will default to `\"Detect [class1], [class2], [class3], ...\"`. To process the results from the model, one can use `post_process_grounded_object_detection` from [`OmDetTurboProcessor`]. Notably, this function takes in the input classes, as unlike other zero-shot object detection models, the decoupling of classes and task embeddings means that no decoding of the predicted class embeddings is needed in the post-processing step, and the predicted classes can be matched to the inputted ones directly.\n\n## Usage example\n\n### Single image inference\n\nHere's how to load the model and prepare the inputs to perform zero-shot object detection on a single image:\n\n```python\n>>> import torch\n>>> import requests\n>>> from PIL import Image\n\n>>> from transformers import AutoProcessor, OmDetTurboForObjectDetection\n\n>>> processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n>>> model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"",
  ">>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text_labels = [\"cat\", \"remote\"]\n>>> inputs = processor(image, text=text_labels, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # convert outputs (bounding boxes and class logits)\n>>> results = processor.post_process_grounded_object_detection(\n...     outputs,\n...     target_sizes=[(image.height, image.width)],\n...     text_labels=text_labels,\n...     threshold=0.3,\n...     nms_threshold=0.3,\n... )\n>>> result = results[0]\n>>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n>>> for box, score, text_label in zip(boxes, scores, text_labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\nDetected remote with confidence 0.768 at location [39.89, 70.35, 176.74, 118.04]\nDetected cat with confidence 0.72 at location [11.6, 54.19, 314.8, 473.95]\nDetected remote with confidence 0.563 at location [333.38, 75.77, 370.7, 187.03]\nDetected cat with confidence 0.552 at location [345.15, 23.95, 639.75, 371.67]\n```\n\n### Multi image inference",
  "OmDet-Turbo can perform batched multi-image inference, with support for different text prompts and classes in the same batch:\n\n```python\n>>> import torch\n>>> import requests\n>>> from io import BytesIO\n>>> from PIL import Image\n>>> from transformers import AutoProcessor, OmDetTurboForObjectDetection\n\n>>> processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n>>> model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n\n>>> url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image1 = Image.open(BytesIO(requests.get(url1).content)).convert(\"RGB\")\n>>> text_labels1 = [\"cat\", \"remote\"]\n>>> task1 = \"Detect {}.\".format(\", \".join(text_labels1))\n\n>>> url2 = \"http://images.cocodataset.org/train2017/000000257813.jpg\"\n>>> image2 = Image.open(BytesIO(requests.get(url2).content)).convert(\"RGB\")\n>>> text_labels2 = [\"boat\"]\n>>> task2 = \"Detect everything that looks like a boat.\"\n\n>>> url3 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n>>> image3 = Image.open(BytesIO(requests.get(url3).content)).convert(\"RGB\")\n>>> text_labels3 = [\"statue\", \"trees\"]",
  ">>> task3 = \"Focus on the foreground, detect statue and trees.\"\n\n>>> inputs = processor(\n...     images=[image1, image2, image3],\n...     text=[text_labels1, text_labels2, text_labels3],\n...     task=[task1, task2, task3],\n...     return_tensors=\"pt\",\n... )\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # convert outputs (bounding boxes and class logits)\n>>> results = processor.post_process_grounded_object_detection(\n...     outputs,\n...     text_labels=[text_labels1, text_labels2, text_labels3],\n...     target_sizes=[(image.height, image.width) for image in [image1, image2, image3]],\n...     threshold=0.2,\n...     nms_threshold=0.3,\n... )\n\n>>> for i, result in enumerate(results):\n...     for score, text_label, box in zip(\n...         result[\"scores\"], result[\"text_labels\"], result[\"boxes\"]\n...     ):\n...         box = [round(i, 1) for i in box.tolist()]\n...         print(\n...             f\"Detected {text_label} with confidence \"\n...             f\"{round(score.item(), 2)} at location {box} in image {i}\"\n...         )\nDetected remote with confidence 0.77 at location [39.9, 70.4, 176.7, 118.0] in image 0",
  "Detected cat with confidence 0.72 at location [11.6, 54.2, 314.8, 474.0] in image 0\nDetected remote with confidence 0.56 at location [333.4, 75.8, 370.7, 187.0] in image 0\nDetected cat with confidence 0.55 at location [345.2, 24.0, 639.8, 371.7] in image 0\nDetected boat with confidence 0.32 at location [146.9, 219.8, 209.6, 250.7] in image 1\nDetected boat with confidence 0.3 at location [319.1, 223.2, 403.2, 238.4] in image 1\nDetected boat with confidence 0.27 at location [37.7, 220.3, 84.0, 235.9] in image 1\nDetected boat with confidence 0.22 at location [407.9, 207.0, 441.7, 220.2] in image 1\nDetected statue with confidence 0.73 at location [544.7, 210.2, 651.9, 502.8] in image 2\nDetected trees with confidence 0.25 at location [3.9, 584.3, 391.4, 785.6] in image 2\nDetected trees with confidence 0.25 at location [1.4, 621.2, 118.2, 787.8] in image 2\nDetected statue with confidence 0.2 at location [428.1, 205.5, 767.3, 759.5] in image 2\n\n```\n\n## OmDetTurboConfig\n\n[[autodoc]] OmDetTurboConfig\n\n## OmDetTurboProcessor\n\n[[autodoc]] OmDetTurboProcessor\n- post_process_grounded_object_detection\n\n## OmDetTurboForObjectDetection\n\n[[autodoc]] OmDetTurboForObjectDetection\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OWL-ViT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text.\n\nThe abstract from the paper is the following:",
  "*Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/owlvit_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> OWL-ViT architecture. Taken from the <a href=\"https://arxiv.org/abs/2205.06230\">original paper</a>. </small>\n\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit).\n\n## Usage tips\n\nOWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](clip) as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.",
  "[`OwlViTImageProcessor`] can be used to resize (or rescale) and normalize images for the model and [`CLIPTokenizer`] is used to encode the text. [`OwlViTProcessor`] wraps [`OwlViTImageProcessor`] and [`CLIPTokenizer`] into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using [`OwlViTProcessor`] and [`OwlViTForObjectDetection`].\n\n```python\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n\n>>> from transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n>>> processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n>>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n>>> target_sizes = torch.tensor([(image.height, image.width)])",
  ">>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> results = processor.post_process_grounded_object_detection(\n...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n... )\n>>> # Retrieve predictions for the first image for the corresponding text queries\n>>> result = results[0]\n>>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n>>> for box, score, text_label in zip(boxes, scores, text_labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]\nDetected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]\n```\n\n## Resources\n\nA demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object detection can be found [here](https://github.com/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb).\n\n## OwlViTConfig\n\n[[autodoc]] OwlViTConfig\n- from_text_vision_configs\n\n## OwlViTTextConfig",
  "[[autodoc]] OwlViTTextConfig\n\n## OwlViTVisionConfig\n\n[[autodoc]] OwlViTVisionConfig\n\n## OwlViTImageProcessor\n\n[[autodoc]] OwlViTImageProcessor\n- preprocess\n- post_process_object_detection\n- post_process_image_guided_detection\n\n## OwlViTProcessor\n\n[[autodoc]] OwlViTProcessor\n- __call__\n- post_process_grounded_object_detection\n- post_process_image_guided_detection\n\n## OwlViTModel\n\n[[autodoc]] OwlViTModel\n- forward\n- get_text_features\n- get_image_features\n\n## OwlViTTextModel\n\n[[autodoc]] OwlViTTextModel\n- forward\n\n## OwlViTVisionModel\n\n[[autodoc]] OwlViTVisionModel\n- forward\n\n## OwlViTForObjectDetection\n\n[[autodoc]] OwlViTForObjectDetection\n- forward\n- image_guided_detection",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GraniteMoe\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n\nPowerMoE-3B is a 3B sparse Mixture-of-Experts (sMoE) language model trained with the Power learning rate scheduler. It sparsely activates 800M parameters for each token. It is trained on a mix of open-source and proprietary datasets. PowerMoE-3B has shown promising results compared to other dense models with 2x activate parameters across various benchmarks, including natural language multi-choices, code generation, and math reasoning.\n\nThe abstract from the paper is the following:\n\n*Finding the optimal learning rate for language model pretraining is a challenging task.",
  "This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored.",
  "In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler, that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (\\mup) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models.\nWe [open source](https://huggingface.co/collections/ibm/power-lm-66be64ae647ddf11b9808000) these pretrained models.*\n\nTips:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"ibm/PowerMoE-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# drop device_map if running on CPU",
  "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\nmodel.eval()\n\n# change input text as desired\nprompt = \"Write a code to find the maximum value in a list of numbers.\"\n\n# tokenize the text\ninput_tokens = tokenizer(prompt, return_tensors=\"pt\")\n# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# loop over the batch to print, in this example the batch size is 1\nfor i in output:\nprint(i)\n```\n\nThis model was contributed by [mayank-mishra](https://huggingface.co/mayank-mishra).\n\n\n## GraniteMoeConfig\n\n[[autodoc]] GraniteMoeConfig\n\n## GraniteMoeModel\n\n[[autodoc]] GraniteMoeModel\n- forward\n\n## GraniteMoeForCausalLM\n\n[[autodoc]] GraniteMoeForCausalLM\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RWKV\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe RWKV model was proposed in [this repo](https://github.com/BlinkDL/RWKV-LM)",
  "It suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n\nThis can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model uses a fixed context length for training).\n\nThis model was contributed by [sgugger](https://huggingface.co/sgugger).\nThe original code can be found [here](https://github.com/BlinkDL/RWKV-LM).\n\n## Usage example\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, RwkvConfig, RwkvModel\n\nmodel = RwkvModel.from_pretrained(\"sgugger/rwkv-430M-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"sgugger/rwkv-430M-pile\")\n\ninputs = tokenizer(\"This is an example.\", return_tensors=\"pt\")\n# Feed everything to the model\noutputs = model(inputs[\"input_ids\"])\noutput_whole = outputs.last_hidden_state\n\noutputs = model(inputs[\"input_ids\"][:, :2])\noutput_one = outputs.last_hidden_state\n\n# Using the state computed on the first inputs, we will get the same output",
  "outputs = model(inputs[\"input_ids\"][:, 2:], state=outputs.state)\noutput_two = outputs.last_hidden_state\n\ntorch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)\n```\n\nIf you want to make sure the model stops generating when `'\\n\\n'` is detected, we recommend using the following stopping criteria:\n\n```python\nfrom transformers import StoppingCriteria\n\nclass RwkvStoppingCriteria(StoppingCriteria):\ndef __init__(self, eos_sequence = [187,187], eos_token_id = 537):\nself.eos_sequence = eos_sequence\nself.eos_token_id = eos_token_id\n\ndef __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\nlast_2_ids = input_ids[:,-2:].tolist()\nreturn self.eos_sequence in last_2_ids\n\n\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=64, stopping_criteria = [RwkvStoppingCriteria()])\n```\n\n## RwkvConfig\n\n[[autodoc]] RwkvConfig\n\n## RwkvModel\n\n[[autodoc]] RwkvModel\n- forward\n\n## RwkvLMHeadModel\n\n[[autodoc]] RwkvForCausalLM\n- forward\n\n## Rwkv attention and the recurrent formulas\n\nIn a traditional auto-regressive Transformer, attention is written as\n\n$$O = \\hbox{softmax}(QK^{T} / \\sqrt{d}) V$$",
  "with \\\\(Q\\\\), \\\\(K\\\\) and \\\\(V\\\\) are matrices of shape `seq_len x hidden_size` named query, key and value (they are actually bigger matrices with a batch dimension and an attention head dimension but we're only interested in the last two, which is where the matrix product is taken, so for the sake of simplicity we only consider those two). The product \\\\(QK^{T}\\\\) then has shape `seq_len x seq_len` and we can take the matrix product with \\\\(V\\\\) to get the output \\\\(O\\\\) of the same shape as the others.\n\nReplacing the softmax by its value gives:\n\n$$O_{i} = \\frac{\\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \\sqrt{d}} V_{j}}{\\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \\sqrt{d}}}$$\n\nNote that the entries in \\\\(QK^{T}\\\\) corresponding to \\\\(j > i\\\\) are masked (the sum stops at j) because the attention is not allowed to look at future tokens (only past ones).\n\nIn comparison, the RWKV attention is given by\n\n$$O_{i} = \\sigma(R_{i}) \\frac{\\sum_{j=1}^{i} e^{W_{i-j} + K_{j}} V_{j}}{\\sum_{j=1}^{i} e^{W_{i-j} + K_{j}}}$$",
  "where \\\\(R\\\\) is a new matrix called receptance by the author, \\\\(K\\\\) and \\\\(V\\\\) are still the key and value (\\\\(\\sigma\\\\) here is the sigmoid function). \\\\(W\\\\) is a new vector that represents the position of the token and is given by\n\n$$W_{0} = u \\hbox{  and  } W_{k} = (k-1)w \\hbox{ for } k \\geq 1$$\n\nwith \\\\(u\\\\) and \\\\(w\\\\) learnable parameters called in the code `time_first` and `time_decay` respectively. The numerator and denominator can both be expressed recursively. Naming them \\\\(N_{i}\\\\) and \\\\(D_{i}\\\\) we have:\n\n$$N_{i} = e^{u + K_{i}} V_{i} + \\hat{N}_{i} \\hbox{  where  } \\hat{N}_{i} = e^{K_{i-1}} V_{i-1} + e^{w + K_{i-2}} V_{i-2} \\cdots + e^{(i-2)w + K_{1}} V_{1}$$\n\nso \\\\(\\hat{N}_{i}\\\\) (called `numerator_state` in the code) satisfies\n\n$$\\hat{N}_{0} = 0 \\hbox{  and  } \\hat{N}_{j+1} = e^{K_{j}} V_{j} + e^{w} \\hat{N}_{j}$$\n\nand\n\n$$D_{i} = e^{u + K_{i}} + \\hat{D}_{i} \\hbox{  where  } \\hat{D}_{i} = e^{K_{i-1}} + e^{w + K_{i-2}} \\cdots + e^{(i-2)w + K_{1}}$$\n\nso \\\\(\\hat{D}_{i}\\\\) (called `denominator_state` in the code) satisfies\n\n$$\\hat{D}_{0} = 0 \\hbox{  and  } \\hat{D}_{j+1} = e^{K_{j}} + e^{w} \\hat{D}_{j}$$",
  "The actual recurrent formula used are a tiny bit more complex, as for numerical stability we don't want to compute exponentials of big numbers. Usually the softmax is not computed as is, but the exponential of the maximum term is divided of the numerator and denominator:\n\n$$\\frac{e^{x_{i}}}{\\sum_{j=1}^{n} e^{x_{j}}} = \\frac{e^{x_{i} - M}}{\\sum_{j=1}^{n} e^{x_{j} - M}}$$\n\nwith \\\\(M\\\\) the maximum of all \\\\(x_{j}\\\\). So here on top of saving the numerator state (\\\\(\\hat{N}\\\\)) and the denominator state (\\\\(\\hat{D}\\\\)) we also keep track of the maximum of all terms encountered in the exponentials. So we actually use\n\n$$\\tilde{N}_{i} = e^{-M_{i}} \\hat{N}_{i} \\hbox{  and  } \\tilde{D}_{i} = e^{-M_{i}} \\hat{D}_{i}$$\n\ndefined by the following recurrent formulas:\n\n$$\\tilde{N}_{0} = 0 \\hbox{  and  } \\tilde{N}_{j+1} = e^{K_{j} - q} V_{j} + e^{w + M_{j} - q} \\tilde{N}_{j} \\hbox{  where  } q = \\max(K_{j}, w + M_{j})$$\n\nand\n\n$$\\tilde{D}_{0} = 0 \\hbox{  and  } \\tilde{D}_{j+1} = e^{K_{j} - q} + e^{w + M_{j} - q} \\tilde{D}_{j} \\hbox{  where  } q = \\max(K_{j}, w + M_{j})$$\n\nand \\\\(M_{j+1} = q\\\\). With those, we can then compute",
  "$$N_{i} = e^{u + K_{i} - q} V_{i} + e^{M_{i}} \\tilde{N}_{i} \\hbox{  where  } q = \\max(u + K_{i}, M_{i})$$\n\nand\n\n$$D_{i} = e^{u + K_{i} - q} + e^{M_{i}} \\tilde{D}_{i} \\hbox{  where  } q = \\max(u + K_{i}, M_{i})$$\n\nwhich finally gives us\n\n$$O_{i} = \\sigma(R_{i}) \\frac{N_{i}}{D_{i}}$$",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# DBRX\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nDBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction.",
  "It uses a *fine-grained* mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input.\nIt was pre-trained on 12T tokens of text and code data.\nCompared to other open MoE models like Mixtral-8x7B and Grok-1, DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, while Mixtral-8x7B and Grok-1 have 8 experts and choose 2.\nThis provides 65x more possible combinations of experts and we found that this improves model quality.\nDBRX uses rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA).\nIt is a BPE based model and uses the GPT-4 tokenizer as described in the [tiktoken](https://github.com/openai/tiktoken) repository.\nWe made these choices based on exhaustive evaluation and scaling experiments.\n\nDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32K tokens.\nWe estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models.",
  "This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and Databricks notebooks for data processing, and Unity Catalog for data management and governance.\nWe used curriculum learning for pretraining, changing the data mix during training in ways we found to substantially improve model quality.\n\n\nMore detailed information about DBRX Instruct and DBRX Base can be found in our [technical blog post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm).\n\nThis model was contributed by [eitan-turok](https://huggingface.co/eitanturok) and [abhi-db](https://huggingface.co/abhi-db). The original code can be found [here](https://github.com/databricks/dbrx-instruct), though this may not be up to date.\n\n## Usage Examples\n\nThe `generate()` method can be used to generate text using DBRX. You can generate using the standard attention implementation, flash-attention, and the PyTorch scaled dot product attention. The last two attention implementations give speed ups.\n\n```python\nfrom transformers import DbrxForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOUR_HF_TOKEN\")",
  "model = DbrxForCausalLM.from_pretrained(\n\"databricks/dbrx-instruct\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\ntoken=\"YOUR_HF_TOKEN\",\n)\n\ninput_text = \"What does it take to build a great LLM?\"\nmessages = [{\"role\": \"user\", \"content\": input_text}]\ninput_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0]))\n```\n\nIf you have flash-attention installed (`pip install flash-attn`), it is possible to generate faster. (The HuggingFace documentation for flash-attention can be found [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).)\n```python\nfrom transformers import DbrxForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOUR_HF_TOKEN\")\nmodel = DbrxForCausalLM.from_pretrained(\n\"databricks/dbrx-instruct\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\ntoken=\"YOUR_HF_TOKEN\",\nattn_implementation=\"flash_attention_2\",\n)\n\ninput_text = \"What does it take to build a great LLM?\"",
  "messages = [{\"role\": \"user\", \"content\": input_text}]\ninput_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0]))\n```\n\nYou can also generate faster using the PyTorch scaled dot product attention. (The HuggingFace documentation for scaled dot product attention can be found [here](https://huggingface.co/docs/transformers/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).)\n```python\nfrom transformers import DbrxForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOUR_HF_TOKEN\")\nmodel = DbrxForCausalLM.from_pretrained(\n\"databricks/dbrx-instruct\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\ntoken=\"YOUR_HF_TOKEN\",\nattn_implementation=\"sdpa\",\n)\n\ninput_text = \"What does it take to build a great LLM?\"\nmessages = [{\"role\": \"user\", \"content\": input_text}]\ninput_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")",
  "outputs = model.generate(**input_ids, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0]))\n```\n\n## DbrxConfig\n\n[[autodoc]] DbrxConfig\n\n\n## DbrxModel\n\n[[autodoc]] DbrxModel\n- forward\n\n\n## DbrxForCausalLM\n\n[[autodoc]] DbrxForCausalLM\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ByT5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:\n\n*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.\nEncoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with",
  "minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\nexperiments.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5).\n\n<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example",
  "ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n\n>>> num_special_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n\n>>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> labels = torch.tensor([list(\"La vie est comme une boîte de chocolat.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> loss = model(input_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```\n\nFor batched inference and training it is however recommended to make use of the tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n\n>>> model_inputs = tokenizer(\n...     [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\"\n... )",
  ">>> labels_dict = tokenizer(\n...     [\"La vie est comme une boîte de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\"\n... )\n>>> labels = labels_dict.input_ids\n\n>>> loss = model(**model_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```\n\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,\nsince the model works directly on characters, the pretraining task is a bit\ndifferent. Let's corrupt some characters of the\ninput sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them\nfor us.\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")\n\n>>> input_ids_prompt = \"The dog chases a ball in the park.\"\n>>> input_ids = tokenizer(input_ids_prompt).input_ids\n\n>>> # Note that we cannot add \"{extra_id_...}\" to the string directly\n>>> # as the Byte tokenizer would incorrectly merge the tokens\n>>> # For ByT5, we need to work directly on the character level\n>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead",
  ">>> # uses final utf character ids.\n>>> # UTF-8 is represented by 8 bits and ByT5 has 3 special tokens.\n>>> # => There are 2**8+2 = 259 input ids and mask tokens count down from index 258.\n>>> # => mask to \"The dog [258]a ball [257]park.\"\n\n>>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\n>>> input_ids\ntensor([[ 87, 107, 104,  35, 103, 114, 106,  35, 258,  35, 100,  35, 101, 100, 111, 111, 257,  35, 115, 100, 117, 110,  49,   1]])\n\n>>> # ByT5 produces only one char at a time so we need to produce many more output characters here -> set `max_length=100`.\n>>> output_ids = model.generate(input_ids, max_length=100)[0].tolist()\n>>> output_ids\n[0, 258, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118, 257,  35, 108, 113,  35, 119, 107, 104,  35, 103, 108, 118, 102, 114, 256, 108, 113,  35, 119, 107, 104, 35, 115, 100, 117, 110,  49,  35,  87, 107, 104,  35, 103, 114, 106, 35, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118,  35, 100,  35, 101, 100, 111, 111,  35, 108, 113, 255,  35, 108, 113,  35, 119, 107, 104,  35, 115, 100, 117, 110,  49]",
  ">>> # ^- Note how 258 descends to 257, 256, 255\n\n>>> # Now we need to split on the sentinel tokens, let's write a short loop for this\n>>> output_ids_list = []\n>>> start_token = 0\n>>> sentinel_token = 258\n>>> while sentinel_token in output_ids:\n...     split_idx = output_ids.index(sentinel_token)\n...     output_ids_list.append(output_ids[start_token:split_idx])\n...     start_token = split_idx\n...     sentinel_token -= 1\n\n>>> output_ids_list.append(output_ids[start_token:])\n>>> output_string = tokenizer.batch_decode(output_ids_list)\n>>> output_string\n['<pad>', 'is the one who does', ' in the disco', 'in the park. The dog is the one who does a ball in', ' in the park.']\n```\n\n\n## ByT5Tokenizer\n\n[[autodoc]] ByT5Tokenizer\n\nSee [`ByT5Tokenizer`] for all details.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Qwen2-VL\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n</div>\n\n## Overview",
  "The [Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/) model is a major update to [Qwen-VL](https://arxiv.org/pdf/2308.12966) from the Qwen team at Alibaba Research.\n\nThe abstract from the blog is the following:\n\n*This blog introduces Qwen2-VL, an advanced version of the Qwen-VL model that has undergone significant enhancements over the past year. Key improvements include enhanced image comprehension, advanced video understanding, integrated visual agent functionality, and expanded multilingual support. The model architecture has been optimized for handling arbitrary image resolutions through Naive Dynamic Resolution support and utilizes Multimodal Rotary Position Embedding (M-ROPE) to effectively process both 1D textual and multi-dimensional visual data. This updated model demonstrates competitive performance against leading AI systems like GPT-4o and Claude 3.5 Sonnet in vision-related tasks and ranks highly among open-source models in text capabilities. These advancements make Qwen2-VL a versatile tool for various applications requiring robust multimodal processing and reasoning abilities.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/qwen2_vl_architecture.jpeg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Qwen2-VL architecture. Taken from the <a href=\"https://qwenlm.github.io/blog/qwen2-vl/\">blog post.</a> </small>\n\nThis model was contributed by [simonJJJ](https://huggingface.co/simonJJJ).\n\n## Usage example\n\n### Single Media inference\n\nThe model can accept both images and videos as input. Here's an example code for inference.\n\n```python\n\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n\n# Load the model in half-precision on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n\nconversation = [\n{\n\"role\":\"user\",\n\"content\":[\n{\n\"type\":\"image\",\n\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n},\n{\n\"type\":\"text\",\n\"text\":\"Describe this image.\"\n}\n]\n}\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"",
  ").to(model.device)\n\n# Inference: Generation of the output\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(output_text)\n\n\n\n# Video\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n{\"type\": \"text\", \"text\": \"What happened in the video?\"},\n],\n}\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nvideo_fps=1,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n\n\n# Inference: Generation of the output\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(output_text)\n```\n\n### Batch Mixed Media Inference\n\nThe model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.",
  "```python\n\n# Conversation for the first image\nconversation1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image1.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"}\n]\n}\n]\n\n# Conversation with two images\nconversation2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image2.jpg\"},\n{\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n{\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n]\n}\n]\n\n# Conversation with pure text\nconversation3 = [\n{\n\"role\": \"user\",\n\"content\": \"who are you?\"\n}\n]\n\n\n# Conversation with mixed midia\nconversation4 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n{\"type\": \"image\", \"path\": \"/path/to/image4.jpg\"},\n{\"type\": \"video\", \"path\": \"/path/to/video.jpg\"},\n{\"type\": \"text\", \"text\": \"What are the common elements in these medias?\"},\n],\n}\n]\n\nconversations = [conversation1, conversation2, conversation3, conversation4]\n# Preparation for batch inference\nipnuts = processor.apply_chat_template(\nconversations,\nvideo_fps=1,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n\n\n# Batch Inference",
  "output_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(output_text)\n```\n\n### Usage Tips\n\n#### Image Resolution trade-off\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs.\n\n```python\nmin_pixels = 224*224\nmax_pixels = 2048*2048\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n```\n\nIn case of limited GPU RAM, one can reduce the resolution as follows:\n\n```python\nmin_pixels = 256*28*28\nmax_pixels = 1024*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n```",
  "This ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n\n\n#### Multiple Image Inputs\n\nBy default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:\n\n```python\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Hello, how are you?\"}\n]\n},\n{\n\"role\": \"assistant\",\n\"content\": \"I'm doing well, thank you for asking. How can I assist you today?\"\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Can you describe these images and video?\"},\n{\"type\": \"image\"},\n{\"type\": \"image\"},\n{\"type\": \"video\"},\n{\"type\": \"text\", \"text\": \"These are from my vacation.\"}\n]\n},\n{\n\"role\": \"assistant\",\n\"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\"\n},\n{\n\"role\": \"user\",\n\"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\"\n}\n]",
  "# default:\nprompt_without_id = processor.apply_chat_template(conversation, add_generation_prompt=True)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n\n\n# add ids\nprompt_with_id = processor.apply_chat_template(conversation, add_generation_prompt=True, add_vision_id=True)",
  "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n\n```\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```",
  "Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using Flash Attention-2, simply add `attn_implementation=\"flash_attention_2\"` when loading the model as follows:\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2-VL-7B-Instruct\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\n)\n```\n\n## Qwen2VLConfig\n\n[[autodoc]] Qwen2VLConfig\n\n## Qwen2VLImageProcessor\n\n[[autodoc]] Qwen2VLImageProcessor\n- preprocess\n\n## Qwen2VLImageProcessorFast\n\n[[autodoc]] Qwen2VLImageProcessorFast\n- preprocess\n\n## Qwen2VLProcessor\n\n[[autodoc]] Qwen2VLProcessor\n\n## Qwen2VLModel\n\n[[autodoc]] Qwen2VLModel\n- forward\n\n## Qwen2VLForConditionalGeneration\n\n[[autodoc]] Qwen2VLForConditionalGeneration\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the MIT License; you may not use this file except in compliance with\nthe License.\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n\n-->\n\n# SuperPoint\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe SuperPoint model was proposed\nin [SuperPoint: Self-Supervised Interest Point Detection and Description](https://arxiv.org/abs/1712.07629) by Daniel\nDeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n\nThis model is the result of a self-supervised training of a fully-convolutional network for interest point detection and",
  "description. The model is able to detect interest points that are repeatable under homographic transformations and\nprovide a descriptor for each point. The use of the model in its own is limited, but it can be used as a feature\nextractor for other tasks such as homography estimation, image matching, etc.\n\nThe abstract from the paper is the following:\n\n*This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a\nlarge number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our\nfully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and\nassociated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography\napproach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g.,\nsynthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able\nto repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other",
  "traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches\nwhen compared to LIFT, SIFT and ORB.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/superpoint_architecture.png\"\nalt=\"drawing\" width=\"500\"/>\n\n<small> SuperPoint overview. Taken from the <a href=\"https://arxiv.org/abs/1712.07629v4\">original paper.</a> </small>\n\n## Usage tips\n\nHere is a quick example of using the model to detect interest points in an image:\n\n```python\nfrom transformers import AutoImageProcessor, SuperPointForKeypointDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\nmodel = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n\ninputs = processor(image, return_tensors=\"pt\")\noutputs = model(**inputs)\n```\n\nThe outputs contain the list of keypoint coordinates with their respective score and description (a 256-long vector).",
  "You can also feed multiple images to the model. Due to the nature of SuperPoint, to output a dynamic number of keypoints,\nyou will need to use the mask attribute to retrieve the respective information :\n\n```python\nfrom transformers import AutoImageProcessor, SuperPointForKeypointDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl_image_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage_1 = Image.open(requests.get(url_image_1, stream=True).raw)\nurl_image_2 = \"http://images.cocodataset.org/test-stuff2017/000000000568.jpg\"\nimage_2 = Image.open(requests.get(url_image_2, stream=True).raw)\n\nimages = [image_1, image_2]\n\nprocessor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\nmodel = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n\ninputs = processor(images, return_tensors=\"pt\")\noutputs = model(**inputs)\nimage_sizes = [(image.height, image.width) for image in images]\noutputs = processor.post_process_keypoint_detection(outputs, image_sizes)\n\nfor output in outputs:\nfor keypoints, scores, descriptors in zip(output[\"keypoints\"], output[\"scores\"], output[\"descriptors\"]):",
  "print(f\"Keypoints: {keypoints}\")\nprint(f\"Scores: {scores}\")\nprint(f\"Descriptors: {descriptors}\")\n```\n\nYou can then print the keypoints on the image of your choice to visualize the result:\n```python\nimport matplotlib.pyplot as plt\n\nplt.axis(\"off\")\nplt.imshow(image_1)\nplt.scatter(\noutputs[0][\"keypoints\"][:, 0],\noutputs[0][\"keypoints\"][:, 1],\nc=outputs[0][\"scores\"] * 100,\ns=outputs[0][\"scores\"] * 50,\nalpha=0.8\n)\nplt.savefig(f\"output_image.png\")\n```\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/ZtFmphEhx8tcbEQqOolyE.png)\n\nThis model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\nThe original code can be found [here](https://github.com/magicleap/SuperPointPretrainedNetwork).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with SuperPoint. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
  "- A notebook showcasing inference and visualization with SuperPoint can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SuperPoint/Inference_with_SuperPoint_to_detect_interest_points_in_an_image.ipynb). 🌎\n\n## SuperPointConfig\n\n[[autodoc]] SuperPointConfig\n\n## SuperPointImageProcessor\n\n[[autodoc]] SuperPointImageProcessor\n\n- preprocess\n- post_process_keypoint_detection\n\n## SuperPointForKeypointDetection\n\n[[autodoc]] SuperPointForKeypointDetection\n\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FLAN-T5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nFLAN-T5 was released in the paper [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) - it is an enhanced version of T5 that has been finetuned in a mixture of tasks.\n\nOne can directly use FLAN-T5 weights without finetuning the model:\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n\n>>> inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Pour a cup of bolognese into a large bowl and add the pasta']\n```\n\nFLAN-T5 includes the same improvements as T5 version 1.1 (see [here](https://huggingface.co/docs/transformers/model_doc/t5v1.1) for the full details of the model's improvements.)\n\nGoogle has released the following variants:\n\n- [google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n\n- [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)",
  "- [google/flan-t5-large](https://huggingface.co/google/flan-t5-large)\n\n- [google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)\n\n- [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl).\n\nThe original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints).\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for all API reference, code examples and notebooks. For more details regarding training and evaluation of the FLAN-T5, refer to the model card.\n\n</Tip>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CTRL\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "CTRL model was proposed in [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and\nRichard Socher. It's a causal (unidirectional) transformer pre-trained using language modeling on a very large corpus\nof ~140 GB of text data with the first token reserved as a control code (such as Links, Books, Wikipedia etc.).\n\nThe abstract from the paper is the following:\n\n*Large-scale language models show promising text generation capabilities, but users cannot easily control particular\naspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model,\ntrained to condition on control codes that govern style, content, and task-specific behavior. Control codes were\nderived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while\nproviding more explicit control over text generation. These codes also allow CTRL to predict which parts of the\ntraining data are most likely given a sequence. This provides a potential method for analyzing large amounts of data",
  "via model-based source attribution.*\n\nThis model was contributed by [keskarnitishr](https://huggingface.co/keskarnitishr). The original code can be found\n[here](https://github.com/salesforce/ctrl).\n\n## Usage tips\n\n- CTRL makes use of control codes to generate text: it requires generations to be started by certain words, sentences\nor links to generate coherent text. Refer to the [original implementation](https://github.com/salesforce/ctrl) for\nmore information.\n- CTRL is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n- CTRL was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\ntoken in a sequence. Leveraging this feature allows CTRL to generate syntactically coherent text as it can be\nobserved in the *run_generation.py* example script.\n- The PyTorch models can take the `past_key_values` as input, which is the previously computed key/value attention pairs.\nTensorFlow models accepts `past` as input. Using the `past_key_values` value prevents the model from re-computing",
  "pre-computed values in the context of text generation. See the [`forward`](model_doc/ctrl#transformers.CTRLModel.forward)\nmethod for more information on the usage of this argument.\n\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## CTRLConfig\n\n[[autodoc]] CTRLConfig\n\n## CTRLTokenizer\n\n[[autodoc]] CTRLTokenizer\n- save_vocabulary\n\n<frameworkcontent>\n<pt>\n\n## CTRLModel\n\n[[autodoc]] CTRLModel\n- forward\n\n## CTRLLMHeadModel\n\n[[autodoc]] CTRLLMHeadModel\n- forward\n\n## CTRLForSequenceClassification\n\n[[autodoc]] CTRLForSequenceClassification\n- forward\n\n</pt>\n<tf>\n\n## TFCTRLModel\n\n[[autodoc]] TFCTRLModel\n- call\n\n## TFCTRLLMHeadModel\n\n[[autodoc]] TFCTRLLMHeadModel\n- call\n\n## TFCTRLForSequenceClassification\n\n[[autodoc]] TFCTRLForSequenceClassification\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GIT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe GIT model was proposed in [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by",
  "Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. GIT is a decoder-only Transformer\nthat leverages [CLIP](clip)'s vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on\nimage captioning and visual question answering benchmarks.\n\nThe abstract from the paper is the following:",
  "*In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/git_architecture.jpg\"",
  "alt=\"drawing\" width=\"600\"/>\n\n<small> GIT architecture. Taken from the <a href=\"https://arxiv.org/abs/2205.14100\" target=\"_blank\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/microsoft/GenerativeImage2Text).\n\n## Usage tips\n\n- GIT is implemented in a very similar way to GPT-2, the only difference being that the model is also conditioned on `pixel_values`.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GIT.\n\n- Demo notebooks regarding inference + fine-tuning GIT on custom data can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GIT).\n- See also: [Causal language modeling task guide](../tasks/language_modeling)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.\nThe resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## GitVisionConfig\n\n[[autodoc]] GitVisionConfig\n\n## GitVisionModel\n\n[[autodoc]] GitVisionModel\n- forward\n\n## GitConfig",
  "[[autodoc]] GitConfig\n- all\n\n## GitProcessor\n\n[[autodoc]] GitProcessor\n- __call__\n\n## GitModel\n\n[[autodoc]] GitModel\n- forward\n\n## GitForCausalLM\n\n[[autodoc]] GitForCausalLM\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLAP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe CLAP model was proposed in [Large Scale Contrastive Language-Audio pretraining with\nfeature fusion and keyword-to-caption augmentation](https://arxiv.org/pdf/2211.06687.pdf) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.",
  "CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on a variety of (audio, text) pairs. It can be instructed in to predict the most relevant text snippet, given an audio, without directly optimizing for the task. The CLAP model uses a SWINTransformer to get audio features from a log-Mel spectrogram input, and a RoBERTa model to get text features. Both the text and audio features are then projected to a latent space with identical dimension. The dot product between the projected audio and text features is then used as a similar score.\n\nThe abstract from the paper is the following:",
  "*Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zeroshot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-6*",
  "This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](https://huggingface.co/ArthurZ) .\nThe original code can be found [here](https://github.com/LAION-AI/Clap).\n\n## ClapConfig\n\n[[autodoc]] ClapConfig\n- from_text_audio_configs\n\n## ClapTextConfig\n\n[[autodoc]] ClapTextConfig\n\n## ClapAudioConfig\n\n[[autodoc]] ClapAudioConfig\n\n## ClapFeatureExtractor\n\n[[autodoc]] ClapFeatureExtractor\n\n## ClapProcessor\n\n[[autodoc]] ClapProcessor\n\n## ClapModel\n\n[[autodoc]] ClapModel\n- forward\n- get_text_features\n- get_audio_features\n\n## ClapTextModel\n\n[[autodoc]] ClapTextModel\n- forward\n\n## ClapTextModelWithProjection\n\n[[autodoc]] ClapTextModelWithProjection\n- forward\n\n## ClapAudioModel\n\n[[autodoc]] ClapAudioModel\n- forward\n\n## ClapAudioModelWithProjection\n\n[[autodoc]] ClapAudioModelWithProjection\n- forward",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Moonshine\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Moonshine model was proposed in [Moonshine: Speech Recognition for Live Transcription and Voice Commands\n](https://arxiv.org/abs/2410.15608) by Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, Pete Warden.\n\nThe abstract from the paper is the following:\n\n*This paper introduces Moonshine, a family of speech recognition models optimized for live transcription and voice command processing. Moonshine is based on an encoder-decoder transformer architecture and employs Rotary Position Embedding (RoPE) instead of traditional absolute position embeddings. The model is trained on speech segments of various lengths, but without using zero-padding, leading to greater efficiency for the encoder during inference time. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny demonstrates a 5x reduction in compute requirements for transcribing a 10-second speech segment while incurring no increase in word error rates across standard evaluation datasets. These results highlight Moonshine's potential for real-time and resource-constrained applications.*\n\nTips:\n\n- Moonshine improves upon Whisper's architecture:",
  "1. It uses SwiGLU activation instead of GELU in the decoder layers\n2. Most importantly, it replaces absolute position embeddings with Rotary Position Embeddings (RoPE). This allows Moonshine to handle audio inputs of any length, unlike Whisper which is restricted to fixed 30-second windows.\n\nThis model was contributed by [Eustache Le Bihan (eustlb)](https://huggingface.co/eustlb).\nThe original code can be found [here](https://github.com/usefulsensors/moonshine).\n\n## Resources\n\n- [Automatic speech recognition task guide](../tasks/asr)\n\n## MoonshineConfig\n\n[[autodoc]] MoonshineConfig\n\n## MoonshineModel\n\n[[autodoc]] MoonshineModel\n- forward\n- _mask_input_features\n\n## MoonshineForConditionalGeneration\n\n[[autodoc]] MoonshineForConditionalGeneration\n- forward\n- generate",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLVP\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed in [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243) by James Betker.\n\nThe abstract from the paper is the following:",
  "*In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise - an expressive, multi-voice text-to-speech system.*\n\n\nThis model was contributed by [Susnato Dhar](https://huggingface.co/susnato).\nThe original code can be found [here](https://github.com/neonbjb/tortoise-tts).\n\n\n## Usage tips\n\n1. CLVP is an integral part of the Tortoise TTS model.\n2. CLVP can be used to compare different generated speech candidates with the provided text, and the best speech tokens are forwarded to the diffusion model.\n3. The use of the [`ClvpModelForConditionalGeneration.generate()`] method is strongly recommended for tortoise usage.\n4. Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary to other audio models which expects 16 kHz.",
  "## Brief Explanation:\n\n- The [`ClvpTokenizer`] tokenizes the text input, and the [`ClvpFeatureExtractor`] extracts the log mel-spectrogram from the desired audio.\n- [`ClvpConditioningEncoder`] takes those text tokens and audio representations and converts them into embeddings conditioned on the text and audio.\n- The [`ClvpForCausalLM`] uses those embeddings to generate multiple speech candidates.\n- Each speech candidate is passed through the speech encoder ([`ClvpEncoder`]) which converts them into a vector representation, and the text encoder ([`ClvpEncoder`]) converts the text tokens into the same latent space.\n- At the end, we compare each speech vector with the text vector to see which speech vector is most similar to the text vector.\n- [`ClvpModelForConditionalGeneration.generate()`] compresses all of the logic described above into a single method.\n\n\nExample :\n\n```python\n>>> import datasets\n>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration\n\n>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library).\n>>> text = \"This is an example text.\"",
  ">>> ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n>>> sample = ds[0][\"audio\"]\n\n>>> # Define processor and model.\n>>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n>>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n\n>>> # Generate processor output and model output.\n>>> processor_output = processor(raw_speech=sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], text=text, return_tensors=\"pt\")\n>>> generated_output = model.generate(**processor_output)\n```\n\n\n## ClvpConfig\n\n[[autodoc]] ClvpConfig\n- from_sub_model_configs\n\n## ClvpEncoderConfig\n\n[[autodoc]] ClvpEncoderConfig\n\n## ClvpDecoderConfig\n\n[[autodoc]] ClvpDecoderConfig\n\n## ClvpTokenizer\n\n[[autodoc]] ClvpTokenizer\n- save_vocabulary\n\n## ClvpFeatureExtractor\n\n[[autodoc]] ClvpFeatureExtractor\n- __call__\n\n## ClvpProcessor\n\n[[autodoc]] ClvpProcessor\n- __call__\n- decode\n- batch_decode\n\n## ClvpModelForConditionalGeneration\n\n[[autodoc]] ClvpModelForConditionalGeneration\n- forward\n- generate\n- get_text_features\n- get_speech_features\n\n## ClvpForCausalLM",
  "[[autodoc]] ClvpForCausalLM\n\n## ClvpModel\n\n[[autodoc]] ClvpModel\n\n## ClvpEncoder\n\n[[autodoc]] ClvpEncoder\n\n## ClvpDecoder\n\n[[autodoc]] ClvpDecoder",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RecurrentGemma\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Recurrent Gemma model was proposed in [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf) by the Griffin, RLHF and Gemma Teams of Google.",
  "The abstract from the paper is the following:\n\n*We introduce RecurrentGemma, an open language model which uses Google’s novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script [`src/transformers/models/recurrent_gemma/convert_recurrent_gemma_weights_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py).\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/google-deepmind/recurrentgemma).\n\n\n## RecurrentGemmaConfig\n\n[[autodoc]] RecurrentGemmaConfig\n\n\n## RecurrentGemmaModel\n\n[[autodoc]] RecurrentGemmaModel\n- forward\n\n## RecurrentGemmaForCausalLM",
  "[[autodoc]] RecurrentGemmaForCausalLM\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DETR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov and Sergey Zagoruyko. DETR",
  "consists of a convolutional backbone followed by an encoder-decoder Transformer which can be trained end-to-end for\nobject detection. It greatly simplifies a lot of the complexity of models like Faster-R-CNN and Mask-R-CNN, which use\nthings like region proposals, non-maximum suppression procedure and anchor generation. Moreover, DETR can also be\nnaturally extended to perform panoptic segmentation, by simply adding a mask head on top of the decoder outputs.\n\nThe abstract from the paper is the following:\n\n*We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the\ndetection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression\nprocedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the\nnew framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via\nbipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries,",
  "DETR reasons about the relations of the objects and the global image context to directly output the final set of\npredictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many\nother modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and\nhighly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily\ngeneralized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive\nbaselines.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/facebookresearch/detr).\n\n## How DETR works\n\nHere's a TLDR explaining how [`~transformers.DetrForObjectDetection`] works:\n\nFirst, an image is sent through a pre-trained convolutional backbone (in the paper, the authors use\nResNet-50/ResNet-101). Let's assume we also add a batch dimension. This means that the input to the backbone is a\ntensor of shape `(batch_size, 3, height, width)`, assuming the image has 3 color channels (RGB). The CNN backbone",
  "outputs a new lower-resolution feature map, typically of shape `(batch_size, 2048, height/32, width/32)`. This is\nthen projected to match the hidden dimension of the Transformer of DETR, which is `256` by default, using a\n`nn.Conv2D` layer. So now, we have a tensor of shape `(batch_size, 256, height/32, width/32).` Next, the\nfeature map is flattened and transposed to obtain a tensor of shape `(batch_size, seq_len, d_model)` =\n`(batch_size, width/32*height/32, 256)`. So a difference with NLP models is that the sequence length is actually\nlonger than usual, but with a smaller `d_model` (which in NLP is typically 768 or higher).\n\nNext, this is sent through the encoder, outputting `encoder_hidden_states` of the same shape (you can consider\nthese as image features). Next, so-called **object queries** are sent through the decoder. This is a tensor of shape\n`(batch_size, num_queries, d_model)`, with `num_queries` typically set to 100 and initialized with zeros.\nThese input embeddings are learnt positional encodings that the authors refer to as object queries, and similarly to",
  "the encoder, they are added to the input of each attention layer. Each object query will look for a particular object\nin the image. The decoder updates these embeddings through multiple self-attention and encoder-decoder attention layers\nto output `decoder_hidden_states` of the same shape: `(batch_size, num_queries, d_model)`. Next, two heads\nare added on top for object detection: a linear layer for classifying each object query into one of the objects or \"no\nobject\", and a MLP to predict bounding boxes for each query.\n\nThe model is trained using a **bipartite matching loss**: so what we actually do is compare the predicted classes +\nbounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N\n(so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as\nbounding box). The [Hungarian matching algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm) is used to find\nan optimal one-to-one mapping of each of the N queries to each of the N annotations. Next, standard cross-entropy (for",
  "the classes) and a linear combination of the L1 and [generalized IoU loss](https://giou.stanford.edu/) (for the\nbounding boxes) are used to optimize the parameters of the model.\n\nDETR can be naturally extended to perform panoptic segmentation (which unifies semantic segmentation and instance\nsegmentation). [`~transformers.DetrForSegmentation`] adds a segmentation mask head on top of\n[`~transformers.DetrForObjectDetection`]. The mask head can be trained either jointly, or in a two steps process,\nwhere one first trains a [`~transformers.DetrForObjectDetection`] model to detect bounding boxes around both\n\"things\" (instances) and \"stuff\" (background things like trees, roads, sky), then freeze all the weights and train only\nthe mask head for 25 epochs. Experimentally, these two approaches give similar results. Note that predicting boxes is\nrequired for the training to be possible, since the Hungarian matching is computed using distances between boxes.\n\n## Usage tips\n\n- DETR uses so-called **object queries** to detect objects in an image. The number of queries determines the maximum\nnumber of objects that can be detected in a single image, and is set to 100 by default (see parameter",
  "`num_queries` of [`~transformers.DetrConfig`]). Note that it's good to have some slack (in COCO, the\nauthors used 100, while the maximum number of objects in a COCO image is ~70).\n- The decoder of DETR updates the query embeddings in parallel. This is different from language models like GPT-2,\nwhich use autoregressive decoding instead of parallel. Hence, no causal attention mask is used.\n- DETR adds position embeddings to the hidden states at each self-attention and cross-attention layer before projecting\nto queries and keys. For the position embeddings of the image, one can choose between fixed sinusoidal or learned\nabsolute position embeddings. By default, the parameter `position_embedding_type` of\n[`~transformers.DetrConfig`] is set to `\"sine\"`.\n- During training, the authors of DETR did find it helpful to use auxiliary losses in the decoder, especially to help\nthe model output the correct number of objects of each class. If you set the parameter `auxiliary_loss` of\n[`~transformers.DetrConfig`] to `True`, then prediction feedforward neural networks and Hungarian losses\nare added after each decoder layer (with the FFNs sharing parameters).",
  "- If you want to train the model in a distributed environment across multiple nodes, then one should update the\n_num_boxes_ variable in the _DetrLoss_ class of _modeling_detr.py_. When training on multiple nodes, this should be\nset to the average number of target boxes across all nodes, as can be seen in the original implementation [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232).\n- [`~transformers.DetrForObjectDetection`] and [`~transformers.DetrForSegmentation`] can be initialized with\nany convolutional backbone available in the [timm library](https://github.com/rwightman/pytorch-image-models).\nInitializing with a MobileNet backbone for example can be done by setting the `backbone` attribute of\n[`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`, and then initializing the model with that\nconfig.\n- DETR resizes the input images such that the shortest side is at least a certain amount of pixels while the longest is\nat most 1333 pixels. At training time, scale augmentation is used such that the shortest side is randomly set to at",
  "least 480 and at most 800 pixels. At inference time, the shortest side is set to 800. One can use\n[`~transformers.DetrImageProcessor`] to prepare images (and optional annotations in COCO format) for the\nmodel. Due to this resizing, images in a batch can have different sizes. DETR solves this by padding images up to the\nlargest size in a batch, and by creating a pixel mask that indicates which pixels are real/which are padding.\nAlternatively, one can also define a custom `collate_fn` in order to batch images together, using\n[`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`].\n- The size of the images will determine the amount of memory being used, and will thus determine the `batch_size`.\nIt is advised to use a batch size of 2 per GPU. See [this Github thread](https://github.com/facebookresearch/detr/issues/150) for more info.\n\nThere are three ways to instantiate a DETR model (depending on what you prefer):\n\nOption 1: Instantiate DETR with pre-trained weights for entire model\n```py\n>>> from transformers import DetrForObjectDetection\n\n>>> model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n```",
  "Option 2: Instantiate DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone\n```py\n>>> from transformers import DetrConfig, DetrForObjectDetection\n\n>>> config = DetrConfig()\n>>> model = DetrForObjectDetection(config)\n```\nOption 3: Instantiate DETR with randomly initialized weights for backbone + Transformer\n```py\n>>> config = DetrConfig(use_pretrained_backbone=False)\n>>> model = DetrForObjectDetection(config)\n```\n\nAs a summary, consider the following table:\n\n| Task | Object detection | Instance segmentation | Panoptic segmentation |\n|------|------------------|-----------------------|-----------------------|\n| **Description** | Predicting bounding boxes and class labels around objects in an image | Predicting masks around objects (i.e. instances) in an image | Predicting masks around both objects (i.e. instances) as well as \"stuff\" (i.e. background things like trees and roads) in an image |\n| **Model** | [`~transformers.DetrForObjectDetection`] | [`~transformers.DetrForSegmentation`] | [`~transformers.DetrForSegmentation`] |",
  "| **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO panoptic  |                                                                        |\n| **Format of annotations to provide to**  [`~transformers.DetrImageProcessor`] | {'image_id': `int`, 'annotations': `List[Dict]`} each Dict being a COCO object annotation  | {'image_id': `int`, 'annotations': `List[Dict]`}  (in case of COCO detection) or {'file_name': `str`, 'image_id': `int`, 'segments_info': `List[Dict]`} (in case of COCO panoptic) | {'file_name': `str`, 'image_id': `int`, 'segments_info': `List[Dict]`} and masks_path (path to directory containing PNG files of the masks) |\n| **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformers.DetrImageProcessor.post_process`] | [`~transformers.DetrImageProcessor.post_process_segmentation`] | [`~transformers.DetrImageProcessor.post_process_segmentation`], [`~transformers.DetrImageProcessor.post_process_panoptic`] |\n| **evaluators** | `CocoEvaluator` with `iou_types=\"bbox\"` | `CocoEvaluator` with `iou_types=\"bbox\"` or `\"segm\"` | `CocoEvaluator` with `iou_tupes=\"bbox\"` or `\"segm\"`, `PanopticEvaluator` |",
  "In short, one should prepare the data either in COCO detection or COCO panoptic format, then use\n[`~transformers.DetrImageProcessor`] to create `pixel_values`, `pixel_mask` and optional\n`labels`, which can then be used to train (or fine-tune) a model. For evaluation, one should first convert the\noutputs of the model using one of the postprocessing methods of [`~transformers.DetrImageProcessor`]. These can\nbe provided to either `CocoEvaluator` or `PanopticEvaluator`, which allow you to calculate metrics like\nmean Average Precision (mAP) and Panoptic Quality (PQ). The latter objects are implemented in the [original repository](https://github.com/facebookresearch/detr). See the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR) for more info regarding evaluation.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DETR.\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- All example notebooks illustrating fine-tuning [`DetrForObjectDetection`] and [`DetrForSegmentation`] on a custom dataset can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR).",
  "- Scripts for finetuning [`DetrForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DetrConfig\n\n[[autodoc]] DetrConfig\n\n## DetrImageProcessor\n\n[[autodoc]] DetrImageProcessor\n- preprocess\n- post_process_object_detection\n- post_process_semantic_segmentation\n- post_process_instance_segmentation\n- post_process_panoptic_segmentation\n\n## DetrImageProcessorFast\n\n[[autodoc]] DetrImageProcessorFast\n- preprocess\n- post_process_object_detection\n- post_process_semantic_segmentation\n- post_process_instance_segmentation\n- post_process_panoptic_segmentation\n\n## DetrFeatureExtractor\n\n[[autodoc]] DetrFeatureExtractor\n- __call__\n- post_process_object_detection\n- post_process_semantic_segmentation\n- post_process_instance_segmentation",
  "- post_process_panoptic_segmentation\n\n## DETR specific outputs\n\n[[autodoc]] models.detr.modeling_detr.DetrModelOutput\n\n[[autodoc]] models.detr.modeling_detr.DetrObjectDetectionOutput\n\n[[autodoc]] models.detr.modeling_detr.DetrSegmentationOutput\n\n## DetrModel\n\n[[autodoc]] DetrModel\n- forward\n\n## DetrForObjectDetection\n\n[[autodoc]] DetrForObjectDetection\n- forward\n\n## DetrForSegmentation\n\n[[autodoc]] DetrForSegmentation\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nQuoc V. Le. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn\nbidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization\norder.\n\nThe abstract from the paper is the following:\n\n*With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves\nbetter performance than pretraining approaches based on autoregressive language modeling. However, relying on\ncorrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a\npretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all\npermutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive",
  "formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into\npretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large\nmargin, including question answering, natural language inference, sentiment analysis, and document ranking.*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/zihangdai/xlnet/).\n\n## Usage tips\n\n- The specific attention pattern can be controlled at training and test time using the `perm_mask` input.\n- Due to the difficulty of training a fully auto-regressive model over various factorization order, XLNet is pretrained\nusing only a sub-set of the output tokens as target which are selected with the `target_mapping` input.\n- To use XLNet for sequential decoding (i.e. not in fully bi-directional setting), use the `perm_mask` and\n`target_mapping` inputs to control the attention span and outputs (see examples in\n*examples/pytorch/text-generation/run_generation.py*)\n- XLNet is one of the few models that has no sequence length limit.",
  "- XLNet is not a traditional autoregressive model but uses a training strategy that builds on that. It permutes the tokens in the sentence, then allows the model to use the last n tokens to predict the token n+1. Since this is all done with a mask, the sentence is actually fed in the model in the right order, but instead of masking the first n tokens for n+1, XLNet uses a mask that hides the previous tokens in some given permutation of 1,…,sequence length.\n- XLNet also uses the same recurrence mechanism as Transformer-XL to build long-term dependencies.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## XLNetConfig\n\n[[autodoc]] XLNetConfig\n\n## XLNetTokenizer\n\n[[autodoc]] XLNetTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## XLNetTokenizerFast\n\n[[autodoc]] XLNetTokenizerFast\n\n## XLNet specific outputs",
  "[[autodoc]] models.xlnet.modeling_xlnet.XLNetModelOutput\n\n[[autodoc]] models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput\n\n[[autodoc]] models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput\n\n[[autodoc]] models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput\n\n[[autodoc]] models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput\n\n[[autodoc]] models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput\n\n[[autodoc]] models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput\n\n[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput\n\n[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput\n\n[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput\n\n[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput\n\n[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput\n\n[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput\n\n<frameworkcontent>\n<pt>\n\n## XLNetModel\n\n[[autodoc]] XLNetModel\n- forward\n\n## XLNetLMHeadModel\n\n[[autodoc]] XLNetLMHeadModel\n- forward\n\n## XLNetForSequenceClassification\n\n[[autodoc]] XLNetForSequenceClassification\n- forward",
  "## XLNetForMultipleChoice\n\n[[autodoc]] XLNetForMultipleChoice\n- forward\n\n## XLNetForTokenClassification\n\n[[autodoc]] XLNetForTokenClassification\n- forward\n\n## XLNetForQuestionAnsweringSimple\n\n[[autodoc]] XLNetForQuestionAnsweringSimple\n- forward\n\n## XLNetForQuestionAnswering\n\n[[autodoc]] XLNetForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFXLNetModel\n\n[[autodoc]] TFXLNetModel\n- call\n\n## TFXLNetLMHeadModel\n\n[[autodoc]] TFXLNetLMHeadModel\n- call\n\n## TFXLNetForSequenceClassification\n\n[[autodoc]] TFXLNetForSequenceClassification\n- call\n\n## TFXLNetForMultipleChoice\n\n[[autodoc]] TFXLNetForMultipleChoice\n- call\n\n## TFXLNetForTokenClassification\n\n[[autodoc]] TFXLNetForTokenClassification\n- call\n\n## TFXLNetForQuestionAnsweringSimple\n\n[[autodoc]] TFXLNetForQuestionAnsweringSimple\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# I-JEPA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The I-JEPA model was proposed in [Image-based Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2301.08243) by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas.\nI-JEPA is a self-supervised learning method that predicts the representations of one part of an image based on other parts of the same image. This approach focuses on learning semantic features without relying on pre-defined invariances from hand-crafted data transformations, which can bias specific tasks, or on filling in pixel-level details, which often leads to less meaningful representations.\n\nThe abstract from the paper is the following:",
  "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image- based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample tar- get blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transform- ers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/ijepa_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> I-JEPA architecture. Taken from the <a href=\"https://arxiv.org/abs/2301.08243\">original paper.</a> </small>\n\nThis model was contributed by [jmtzt](https://huggingface.co/jmtzt).\nThe original code can be found [here](https://github.com/facebookresearch/ijepa).\n\n## How to use\n\nHere is how to use this model for image feature extraction:\n\n```python\nimport requests\nimport torch\nfrom PIL import Image\nfrom torch.nn.functional import cosine_similarity\n\nfrom transformers import AutoModel, AutoProcessor\n\nurl_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nurl_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nmodel_id = \"facebook/ijepa_vith14_1k\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModel.from_pretrained(model_id)\n\n@torch.no_grad()\ndef infer(image):\ninputs = processor(image, return_tensors=\"pt\")\noutputs = model(**inputs)\nreturn outputs.last_hidden_state.mean(dim=1)\n\n\nembed_1 = infer(image_1)\nembed_2 = infer(image_2)\n\nsimilarity = cosine_similarity(embed_1, embed_2)\nprint(similarity)\n```\n\n## Resources",
  "A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with I-JEPA.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`IJepaForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\n## IJepaConfig\n\n[[autodoc]] IJepaConfig\n\n## IJepaModel\n\n[[autodoc]] IJepaModel\n- forward\n\n## IJepaForImageClassification\n\n[[autodoc]] IJepaForImageClassification\n- forward",
  "<!--Copyright 2024 The GLM & ZhipuAI team and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The GLM Model was proposed\nin [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/html/2406.12793v1)\nby GLM Team, THUDM & ZhipuAI.\n\nThe abstract from the paper is the following:\n\n*We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report\nprimarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most\ncapable models that are trained with all the insights and lessons gained from the preceding three generations of\nChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with\na small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment\nis achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human\nfeedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU,\nGSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3)",
  "matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as\nmeasured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide\nwhen and which tool(s) to use—including web browser, Python interpreter, text-to-image model, and user-defined\nfunctions—to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All\nTools in tasks like accessing online information via web browsing and solving math problems using Python interpreter.\nOver the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M),\nGLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone.*\n\nTips:\n\n- This model was contributed by [THUDM](https://huggingface.co/THUDM). The most recent code can be\nfound [here](https://github.com/thudm/GLM-4).\n\n\n## Usage tips\n\n`GLM-4` can be found on the [Huggingface Hub](https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7)",
  "In the following, we demonstrate how to use `glm-4-9b-chat` for the inference. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"THUDM/glm-4-9b-chat\", device_map=\"auto\", trust_remote_code=True)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat\")\n\n>>> prompt = \"Give me a short introduction to large language model.\"\n\n>>> messages = [{\"role\": \"user\", \"content\": prompt}]\n\n>>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n>>> model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n\n>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n\n>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## GlmConfig\n\n[[autodoc]] GlmConfig\n\n## GlmModel\n\n[[autodoc]] GlmModel",
  "- forward\n\n## GlmForCausalLM\n\n[[autodoc]] GlmForCausalLM\n- forward\n\n## GlmForSequenceClassification\n\n[[autodoc]] GlmForSequenceClassification\n- forward\n\n## GlmForTokenClassification\n\n[[autodoc]] GlmForTokenClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ESM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview\n\nThis page provides code and pre-trained weights for Transformer protein language models from Meta AI's Fundamental",
  "AI Research Team, providing the state-of-the-art ESMFold and ESM-2, and the previously released ESM-1b and ESM-1v.\nTransformer protein language models were introduced in the paper [Biological structure and function emerge from scaling\nunsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott,\nC. Lawrence Zitnick, Jerry Ma, and Rob Fergus.\nThe first version of this paper was [preprinted in 2019](https://www.biorxiv.org/content/10.1101/622803v1?versioned=true).\n\nESM-2 outperforms all tested single-sequence protein language models across a range of structure prediction tasks,\nand enables atomic resolution structure prediction.\nIt was released with the paper [Language models of protein sequences at the scale of evolution enable accurate\nstructure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie,\nZhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido and Alexander Rives.",
  "Also introduced in this paper was ESMFold. It uses an ESM-2 stem with a head that can predict folded protein\nstructures with state-of-the-art accuracy. Unlike [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2),\nit relies on the token embeddings from the large pre-trained protein language model stem and does not perform a multiple\nsequence alignment (MSA) step at inference time, which means that ESMFold checkpoints are fully \"standalone\" -\nthey do not require a database of known protein sequences and structures with associated external query tools\nto make predictions, and are much faster as a result.\n\n\nThe abstract from\n\"Biological structure and function emerge from scaling unsupervised learning to 250\nmillion protein sequences\" is\n\n\n*In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised\nlearning has led to major advances in representation learning and statistical generation. In the life sciences, the\nanticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling",
  "at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To\nthis end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250\nmillion protein sequences spanning evolutionary diversity. The resulting model contains information about biological\nproperties in its representations. The representations are learned from sequence data alone. The learned representation\nspace has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to\nremote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and\ncan be identified by linear projections. Representation learning produces features that generalize across a range of\napplications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and\nimproving state-of-the-art features for long-range contact prediction.*\n\n\nThe abstract from\n\"Language models of protein sequences at the scale of evolution enable accurate structure prediction\" is",
  "*Large language models have recently been shown to develop emergent capabilities with scale, going beyond\nsimple pattern matching to perform higher level reasoning and generate lifelike images and text. While\nlanguage models trained on protein sequences have been studied at a smaller scale, little is known about\nwhat they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters,\nthe largest language models of proteins to be evaluated to date. We find that as models are scaled they learn\ninformation enabling the prediction of the three-dimensional structure of a protein at the resolution of\nindividual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly\nfrom the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for\nsequences with low perplexity that are well understood by the language model. ESMFold inference is an\norder of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic\nproteins in practical timescales.*\n\nThe original code can be found [here](https://github.com/facebookresearch/esm) and was",
  "was developed by the Fundamental AI Research team at Meta AI.\nESM-1b, ESM-1v and ESM-2 were contributed to huggingface by [jasonliu](https://huggingface.co/jasonliu)\nand [Matt](https://huggingface.co/Rocketknight1).\n\nESMFold was contributed to huggingface by [Matt](https://huggingface.co/Rocketknight1) and\n[Sylvain](https://huggingface.co/sgugger), with a big thank you to Nikita Smetanin, Roshan Rao and Tom Sercu for their\nhelp throughout the process!\n\n## Usage tips\n\n- ESM models are trained with a masked language modeling (MLM) objective.\n- The HuggingFace port of ESMFold uses portions of the [openfold](https://github.com/aqlaboratory/openfold) library. The `openfold` library is licensed under the Apache License 2.0.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n## EsmConfig\n\n[[autodoc]] EsmConfig\n- all\n\n## EsmTokenizer\n\n[[autodoc]] EsmTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n<frameworkcontent>\n<pt>\n\n## EsmModel",
  "[[autodoc]] EsmModel\n- forward\n\n## EsmForMaskedLM\n\n[[autodoc]] EsmForMaskedLM\n- forward\n\n## EsmForSequenceClassification\n\n[[autodoc]] EsmForSequenceClassification\n- forward\n\n## EsmForTokenClassification\n\n[[autodoc]] EsmForTokenClassification\n- forward\n\n## EsmForProteinFolding\n\n[[autodoc]] EsmForProteinFolding\n- forward\n\n</pt>\n<tf>\n\n## TFEsmModel\n\n[[autodoc]] TFEsmModel\n- call\n\n## TFEsmForMaskedLM\n\n[[autodoc]] TFEsmForMaskedLM\n- call\n\n## TFEsmForSequenceClassification\n\n[[autodoc]] TFEsmForSequenceClassification\n- call\n\n## TFEsmForTokenClassification\n\n[[autodoc]] TFEsmForTokenClassification\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Pyramid Vision Transformer (PVT)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe PVT model was proposed in\n[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)\nby Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. The PVT is a type of",
  "vision transformer that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks. Specifically\nit allows for more fine-grained inputs (4 x 4 pixels per patch) to be used, while simultaneously shrinking the sequence length\nof the Transformer as it deepens - reducing the computational cost. Additionally, a spatial-reduction attention (SRA) layer\nis used to further reduce the resource consumption when learning high-resolution features.\n\nThe abstract from the paper is the following:\n\n*Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a\nsimpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the recently proposed Vision\nTransformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer\n(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several\nmerits compared to current state of the arts. Different from ViT that typically yields low resolution outputs and",
  "incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high\noutput resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the\ncomputations of large feature maps. PVT inherits the advantages of both CNN and Transformer, making it a unified\nbackbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones.\nWe validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including\nobject detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet\nachieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope\nthat PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future research.*\n\nThis model was contributed by [Xrenya](https://huggingface.co/Xrenya). The original code can be found [here](https://github.com/whai362/PVT).\n\n\n- PVTv1 on ImageNet-1K\n\n| **Model variant**  |**Size** |**Acc@1**|**Params (M)**|",
  "|--------------------|:-------:|:-------:|:------------:|\n| PVT-Tiny           |    224  |   75.1  |     13.2     |\n| PVT-Small          |    224  |   79.8  |     24.5     |\n| PVT-Medium         |    224  |   81.2  |     44.2     |\n| PVT-Large          |    224  |   81.7  |     61.4     |\n\n\n## PvtConfig\n\n[[autodoc]] PvtConfig\n\n## PvtImageProcessor\n\n[[autodoc]] PvtImageProcessor\n- preprocess\n\n## PvtForImageClassification\n\n[[autodoc]] PvtForImageClassification\n- forward\n\n## PvtModel\n\n[[autodoc]] PvtModel\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Reformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.\n\nThe abstract from the paper is the following:",
  "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can\nbe prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\nTransformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\ncomplexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual\nlayers instead of the standard residuals, which allows storing activations only once in the training process instead of\nN times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models\nwhile being much more memory-efficient and much faster on long sequences.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Authors' code can be\nfound [here](https://github.com/google/trax/tree/master/trax/models/reformer).\n\n## Usage tips\n\n- Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).",
  "- Use Axial position encoding (see below for more details). It’s a mechanism to avoid having a huge positional encoding matrix (when the sequence length is very big) by factorizing it into smaller matrices.\n- Replace traditional attention by LSH (local-sensitive hashing) attention (see below for more details). It’s a technique to avoid computing the full product query-key in the attention layers.\n- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them for results inside a given layer (less efficient than storing them but saves memory).\n- Compute the feedforward operations by chunks and not on the whole batch.\n\n### Axial Positional Encodings\n\nAxial Positional Encodings were first implemented in Google's [trax library](https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29)\nand developed by the authors of this model's paper. In models that are treating very long input sequences, the",
  "conventional position id encodings store an embeddings vector of size \\\\(d\\\\) being the `config.hidden_size` for\nevery position \\\\(i, \\ldots, n_s\\\\), with \\\\(n_s\\\\) being `config.max_embedding_size`. This means that having\na sequence length of \\\\(n_s = 2^{19} \\approx 0.5M\\\\) and a `config.hidden_size` of \\\\(d = 2^{10} \\approx 1000\\\\)\nwould result in a position encoding matrix:\n\n$$X_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s\\right]$$\n\nwhich alone has over 500M parameters to store. Axial positional encodings factorize \\\\(X_{i,j}\\\\) into two matrices:\n\n$$X^{1}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^1\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s^1\\right]$$\n\nand\n\n$$X^{2}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^2\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s^2\\right]$$\n\nwith:\n\n$$d = d^1 + d^2 \\text{ and } n_s = n_s^1 \\times n_s^2 .$$\n\nTherefore the following holds:\n\n$$X_{i,j} = \\begin{cases}\nX^{1}_{i, k}, & \\text{if }\\ i < d^1 \\text{ with } k = j \\mod n_s^1 \\\\\nX^{2}_{i - d^1, l}, & \\text{if } i \\ge d^1 \\text{ with } l = \\lfloor\\frac{j}{n_s^1}\\rfloor\n\\end{cases}$$",
  "Intuitively, this means that a position embedding vector \\\\(x_j \\in \\mathbb{R}^{d}\\\\) is now the composition of two\nfactorized embedding vectors: \\\\(x^1_{k, l} + x^2_{l, k}\\\\), where as the `config.max_embedding_size` dimension\n\\\\(j\\\\) is factorized into \\\\(k \\text{ and } l\\\\). This design ensures that each position embedding vector\n\\\\(x_j\\\\) is unique.\n\nUsing the above example again, axial position encoding with \\\\(d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}\\\\)\ncan drastically reduced the number of parameters from 500 000 000 to \\\\(2^{18} + 2^{19} \\approx 780 000\\\\) parameters, this means 85% less memory usage.\n\nIn practice, the parameter `config.axial_pos_embds_dim` is set to a tuple \\\\((d^1, d^2)\\\\) which sum has to be\nequal to `config.hidden_size` and `config.axial_pos_shape` is set to a tuple \\\\((n_s^1, n_s^2)\\\\) which\nproduct has to be equal to `config.max_embedding_size`, which during training has to be equal to the *sequence\nlength* of the `input_ids`.\n\n\n### LSH Self Attention\n\nIn Locality sensitive hashing (LSH) self attention the key and query projection weights are tied. Therefore, the key",
  "query embedding vectors are also tied. LSH self attention uses the locality sensitive hashing mechanism proposed in\n[Practical and Optimal LSH for Angular Distance](https://arxiv.org/abs/1509.02897) to assign each of the tied key\nquery embedding vectors to one of `config.num_buckets` possible buckets. The premise is that the more \"similar\"\nkey query embedding vectors (in terms of *cosine similarity*) are to each other, the more likely they are assigned to\nthe same bucket.\n\nThe accuracy of the LSH mechanism can be improved by increasing `config.num_hashes` or directly the argument\n`num_hashes` of the forward function so that the output of the LSH self attention better approximates the output\nof the \"normal\" full self attention. The buckets are then sorted and chunked into query key embedding vector chunks\neach of length `config.lsh_chunk_length`. For each chunk, the query embedding vectors attend to its key vectors\n(which are tied to themselves) and to the key embedding vectors of `config.lsh_num_chunks_before` previous\nneighboring chunks and `config.lsh_num_chunks_after` following neighboring chunks.",
  "For more information, see the [original Paper](https://arxiv.org/abs/2001.04451) or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).\n\nNote that `config.num_buckets` can also be factorized into a list \\\\((n_{\\text{buckets}}^1,\nn_{\\text{buckets}}^2)\\\\). This way instead of assigning the query key embedding vectors to one of \\\\((1,\\ldots,\nn_{\\text{buckets}})\\\\) they are assigned to one of \\\\((1-1,\\ldots, n_{\\text{buckets}}^1-1, \\ldots,\n1-n_{\\text{buckets}}^2, \\ldots, n_{\\text{buckets}}^1-n_{\\text{buckets}}^2)\\\\). This is crucial for very long sequences to\nsave memory.\n\nWhen training a model from scratch, it is recommended to leave `config.num_buckets=None`, so that depending on the\nsequence length a good value for `num_buckets` is calculated on the fly. This value will then automatically be\nsaved in the config and should be reused for inference.\n\nUsing LSH self attention, the memory and time complexity of the query-key matmul operation can be reduced from\n\\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to \\\\(\\mathcal{O}(n_s \\times \\log(n_s))\\\\), which usually represents the memory\nand time bottleneck in a transformer model, with \\\\(n_s\\\\) being the sequence length.",
  "### Local Self Attention\n\nLocal self attention is essentially a \"normal\" self attention layer with key, query and value projections, but is\nchunked so that in each chunk of length `config.local_chunk_length` the query embedding vectors only attends to\nthe key embedding vectors in its chunk and to the key embedding vectors of `config.local_num_chunks_before`\nprevious neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.\n\nUsing Local self attention, the memory and time complexity of the query-key matmul operation can be reduced from\n\\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to \\\\(\\mathcal{O}(n_s \\times \\log(n_s))\\\\), which usually represents the memory\nand time bottleneck in a transformer model, with \\\\(n_s\\\\) being the sequence length.\n\n\n### Training\n\nDuring training, we must ensure that the sequence length is set to a value that can be divided by the least common\nmultiple of `config.lsh_chunk_length` and `config.local_chunk_length` and that the parameters of the Axial\nPositional Encodings are correctly set as described above. Reformer is very memory efficient so that the model can\neasily be trained on sequences as long as 64000 tokens.",
  "For training, the [`ReformerModelWithLMHead`] should be used as follows:\n\n```python\ninput_ids = tokenizer.encode(\"This is a sentence from the training data\", return_tensors=\"pt\")\nloss = model(input_ids, labels=input_ids)[0]\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n## ReformerConfig\n\n[[autodoc]] ReformerConfig\n\n## ReformerTokenizer\n\n[[autodoc]] ReformerTokenizer\n- save_vocabulary\n\n## ReformerTokenizerFast\n\n[[autodoc]] ReformerTokenizerFast\n\n## ReformerModel\n\n[[autodoc]] ReformerModel\n- forward\n\n## ReformerModelWithLMHead\n\n[[autodoc]] ReformerModelWithLMHead\n- forward\n\n## ReformerForMaskedLM\n\n[[autodoc]] ReformerForMaskedLM\n- forward\n\n## ReformerForSequenceClassification\n\n[[autodoc]] ReformerForSequenceClassification\n- forward\n\n## ReformerForQuestionAnswering\n\n[[autodoc]] ReformerForQuestionAnswering\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CamemBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The CamemBERT model was proposed in [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by\n[Louis Martin](https://huggingface.co/louismartin), [Benjamin Muller](https://huggingface.co/benjamin-mlr), [Pedro Javier Ortiz Suárez](https://huggingface.co/pjox), Yoann Dupont, Laurent Romary, Éric Villemonte de la\nClergerie, [Djamé Seddah](https://huggingface.co/Djame), and [Benoît Sagot](https://huggingface.co/sagot). It is based on Facebook's RoBERTa model released in 2019. It is a model\ntrained on 138GB of French text.\n\nThe abstract from the paper is the following:\n\n*Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available\nmodels have either been trained on English data or on the concatenation of data in multiple languages. This makes\npractical use of such models --in all languages except English-- very limited. Aiming to address this issue for French,\nwe release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the\nperformance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,",
  "dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art\nfor most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and\ndownstream applications for French NLP.*\n\nThis model was contributed by [the ALMAnaCH team (Inria)](https://huggingface.co/almanach). The original code can be found [here](https://camembert-model.fr/).\n\n<Tip>\n\nThis implementation is the same as RoBERTa. Refer to the [documentation of RoBERTa](roberta) for usage examples as well\nas the information relative to the inputs and outputs.\n\n</Tip>\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## CamembertConfig\n\n[[autodoc]] CamembertConfig\n\n## CamembertTokenizer\n\n[[autodoc]] CamembertTokenizer\n- build_inputs_with_special_tokens",
  "- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## CamembertTokenizerFast\n\n[[autodoc]] CamembertTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## CamembertModel\n\n[[autodoc]] CamembertModel\n\n## CamembertForCausalLM\n\n[[autodoc]] CamembertForCausalLM\n\n## CamembertForMaskedLM\n\n[[autodoc]] CamembertForMaskedLM\n\n## CamembertForSequenceClassification\n\n[[autodoc]] CamembertForSequenceClassification\n\n## CamembertForMultipleChoice\n\n[[autodoc]] CamembertForMultipleChoice\n\n## CamembertForTokenClassification\n\n[[autodoc]] CamembertForTokenClassification\n\n## CamembertForQuestionAnswering\n\n[[autodoc]] CamembertForQuestionAnswering\n\n</pt>\n<tf>\n\n## TFCamembertModel\n\n[[autodoc]] TFCamembertModel\n\n## TFCamembertForCausalLM\n\n[[autodoc]] TFCamembertForCausalLM\n\n## TFCamembertForMaskedLM\n\n[[autodoc]] TFCamembertForMaskedLM\n\n## TFCamembertForSequenceClassification\n\n[[autodoc]] TFCamembertForSequenceClassification\n\n## TFCamembertForMultipleChoice\n\n[[autodoc]] TFCamembertForMultipleChoice\n\n## TFCamembertForTokenClassification\n\n[[autodoc]] TFCamembertForTokenClassification\n\n## TFCamembertForQuestionAnswering\n\n[[autodoc]] TFCamembertForQuestionAnswering\n\n</tf>\n</frameworkcontent>",
  "",
  "<!--Copyright 2024 StepFun and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GOT-OCR2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The GOT-OCR2 model was proposed in [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/abs/2409.01704) by Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang.\n\nThe abstract from the paper is the following:",
  "*Traditional OCR systems (OCR-1.0) are increasingly unable to meet people’snusage due to the growing demand for intelligent processing of man-made opticalncharacters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multipage OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/got_ocr_overview.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> GOT-OCR2 training stages. Taken from the <a href=\"https://arxiv.org/abs/2409.01704\">original paper.</a> </small>\n\n\nTips:\n\nGOT-OCR2 works on a wide range of tasks, including plain document OCR, scene text OCR, formatted document OCR, and even OCR for tables, charts, mathematical formulas, geometric shapes, molecular formulas and sheet music. While this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like `pdftex`, `mathpix`, `matplotlib`, `tikz`, `verovio` or `pyecharts`.\nThe model can also be used for interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box.\n\nThis model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\nThe original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2.0).\n\n## Usage example\n\n### Plain text inference\n\n```python",
  ">>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n>>> inputs = processor(image, return_tensors=\"pt\", device=device).to(device)\n\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"R&D QUALITY IMPROVEMENT\\nSUGGESTION/SOLUTION FORM\\nName/Phone Ext. : (...)\"\n```\n\n### Plain text inference batched\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)",
  ">>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n>>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n\n>>> inputs = processor([image1, image2], return_tensors=\"pt\", device=device).to(device)\n\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4,\n... )\n\n>>> processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n[\"Reducing the number\", \"R&D QUALITY\"]\n```\n\n### Formatted text inference\n\nGOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an example of how to generate formatted text:\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)",
  ">>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/latex.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", format=True, device=device).to(device)\n\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"\\\\author{\\nHanwen Jiang* \\\\(\\\\quad\\\\) Arjun Karpur \\\\({ }^{\\\\dagger} \\\\quad\\\\) Bingyi Cao \\\\({ }^{\\\\dagger} \\\\quad\\\\) (...)\"\n```\n\n### Inference on multiple pages\n\nAlthough it might be reasonable in most cases to use a “for loop” for multi-page processing, some text data with formatting across several pages make it necessary to process all pages at once. GOT introduces a multi-page OCR (without “for loop”) feature, where multiple pages can be processed by the model at once, whith the output being one continuous text.\nHere is an example of how to process multiple pages at once:\n\n\n```python",
  ">>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page1.png\"\n>>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page2.png\"\n>>> inputs = processor([image1, image2], return_tensors=\"pt\", multi_page=True, format=True, device=device).to(device)\n\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"\\\\title{\\nGeneral OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\\n}\\n\\\\author{\\nHaoran Wei (...)\"\n```\n\n### Inference on cropped patches",
  "GOT supports a 1024×1024 input resolution, which is sufficient for most OCR tasks, such as scene OCR or processing A4-sized PDF pages. However, certain scenarios, like horizontally stitched two-page PDFs commonly found in academic papers or images with unusual aspect ratios, can lead to accuracy issues when processed as a single image. To address this, GOT can dynamically crop an image into patches, process them all at once, and merge the results for better accuracy with such inputs.\nHere is an example of how to process cropped patches:\n\n```python\n>>> import torch\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", torch_dtype=torch.bfloat16, device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", format=True, crop_to_patches=True, max_patches=3, device=device).to(device)",
  ">>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"on developing architectural improvements to make learnable matching methods generalize.\\nMotivated by the above observations, (...)\"\n```\n\n### Inference on a specific region\n\nGOT supports interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box. Here is an example of how to process a specific region:\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"",
  ">>> inputs = processor(image, return_tensors=\"pt\", color=\"green\", device=device).to(device) # or box=[x1, y1, x2, y2] for coordinates (image pixels)\n\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"You should keep in mind what features from the module should be used, especially \\nwhen you’re planning to sell a template.\"\n```\n\n### Inference on general OCR data example: sheet music\n\nAlthough this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like `pdftex`, `mathpix`, `matplotlib`, `tikz`, `verovio` or `pyecharts`.\nHere is an example of how to process sheet music:\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n>>> import verovio\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)",
  ">>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/sheet_music.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", format=True, device=device).to(device)\n\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n\n>>> outputs = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n>>> tk = verovio.toolkit()\n>>> tk.loadData(outputs)\n>>> tk.setOptions(\n...     {\n...         \"pageWidth\": 2100,\n...         \"pageHeight\": 800,\n...         \"footer\": \"none\",\n...         \"barLineWidth\": 0.5,\n...         \"beamMaxSlope\": 15,\n...         \"staffLineWidth\": 0.2,\n...         \"spacingStaff\": 6,\n...     }\n... )\n>>> tk.getPageCount()\n>>> svg = tk.renderToSVG()\n>>> svg = svg.replace('overflow=\"inherit\"', 'overflow=\"visible\"')\n>>> with open(\"output.svg\", \"w\") as f:\n>>>     f.write(svg)\n```",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sheet_music.svg\"\nalt=\"drawing\" width=\"600\"/>\n\n## GotOcr2Config\n\n[[autodoc]] GotOcr2Config\n\n## GotOcr2VisionConfig\n\n[[autodoc]] GotOcr2VisionConfig\n\n## GotOcr2ImageProcessor\n\n[[autodoc]] GotOcr2ImageProcessor\n\n## GotOcr2ImageProcessorFast\n\n[[autodoc]] GotOcr2ImageProcessorFast\n\n## GotOcr2Processor\n\n[[autodoc]] GotOcr2Processor\n\n## GotOcr2ForConditionalGeneration\n\n[[autodoc]] GotOcr2ForConditionalGeneration\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPT-NeoX-Japanese\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n</div>\n\n## Overview",
  "We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\nJapanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts.\nTo address this distinct structure of the Japanese language, we use a [special sub-word tokenizer](https://github.com/tanreinama/Japanese-BPEEncoder_V2). We are very grateful to *tanreinama* for open-sourcing this incredibly helpful tokenizer.\nFollowing the recommendations from Google's research on [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), we have removed bias parameters from transformer blocks, achieving better model performance. Please refer [this article](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4) in detail.",
  "Development of the model was led by [Shinya Otani](https://github.com/SO0529), [Takayoshi Makabe](https://github.com/spider-man-tm), [Anuj Arora](https://github.com/Anuj040), and [Kyo Hattori](https://github.com/go5paopao) from [ABEJA, Inc.](https://www.abejainc.com/). For more information on this model-building activity, please refer [here (ja)](https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207).\n\n### Usage example\n\nThe `generate()` method can be used to generate text using GPT NeoX Japanese model.\n\n```python\n>>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer\n\n>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n\n>>> prompt = \"人とAIが協調するためには、\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n\n>>> print(gen_text)\n人とAIが協調するためには、AIと人が共存し、AIを正しく理解する必要があります。\n```\n\n## Resources",
  "- [Causal language modeling task guide](../tasks/language_modeling)\n\n## GPTNeoXJapaneseConfig\n\n[[autodoc]] GPTNeoXJapaneseConfig\n\n## GPTNeoXJapaneseTokenizer\n\n[[autodoc]] GPTNeoXJapaneseTokenizer\n\n## GPTNeoXJapaneseModel\n\n[[autodoc]] GPTNeoXJapaneseModel\n- forward\n\n## GPTNeoXJapaneseForCausalLM\n\n[[autodoc]] GPTNeoXJapaneseForCausalLM\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MRA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MRA model was proposed in [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh.\n\nThe abstract from the paper is the following:",
  "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified sparsity patterns, low-rank basis expansions and combinations thereof. In this paper, we revisit classical Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value in this setting remains underexplored thus far. We show that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest. We undertake an extensive set of experiments and demonstrate that this multi-resolution scheme outperforms most efficient self-attention proposals and is favorable for both short and long sequences. Code is available at https://github.com/mlpen/mra-attention.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03).",
  "The original code can be found [here](https://github.com/mlpen/mra-attention).\n\n## MraConfig\n\n[[autodoc]] MraConfig\n\n## MraModel\n\n[[autodoc]] MraModel\n- forward\n\n## MraForMaskedLM\n\n[[autodoc]] MraForMaskedLM\n- forward\n\n## MraForSequenceClassification\n\n[[autodoc]] MraForSequenceClassification\n- forward\n\n## MraForMultipleChoice\n\n[[autodoc]] MraForMultipleChoice\n- forward\n\n## MraForTokenClassification\n\n[[autodoc]] MraForTokenClassification\n- forward\n\n## MraForQuestionAnswering\n\n[[autodoc]] MraForQuestionAnswering\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Pop2Piano\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.\n\nPiano covers of pop music are widely enjoyed, but generating them from music is not a trivial task. It requires great\nexpertise with playing piano as well as knowing different characteristics and melodies of a song. With Pop2Piano you",
  "can directly generate a cover from a song's audio waveform. It is the first model to directly generate a piano cover\nfrom pop audio without melody and chord extraction modules.\n\nPop2Piano is an encoder-decoder Transformer model based on [T5](https://arxiv.org/pdf/1910.10683.pdf). The input audio\nis transformed to its waveform and passed to the encoder, which transforms it to a latent representation. The decoder\nuses these latent representations to generate token ids in an autoregressive way. Each token id corresponds to one of four\ndifferent token types: time, velocity, note and 'special'. The token ids are then decoded to their equivalent MIDI file.\n\nThe abstract from the paper is the following:\n\n*Piano covers of pop music are enjoyed by many people. However, the\ntask of automatically generating piano covers of pop music is still\nunderstudied. This is partly due to the lack of synchronized\n{Pop, Piano Cover} data pairs, which made it challenging to apply\nthe latest data-intensive deep learning-based methods. To leverage\nthe power of the data-driven approach, we make a large amount of\npaired and synchronized {Pop, Piano Cover} data using an automated",
  "pipeline. In this paper, we present Pop2Piano, a Transformer network\nthat generates piano covers given waveforms of pop music. To the best\nof our knowledge, this is the first model to generate a piano cover\ndirectly from pop audio without using melody and chord extraction\nmodules. We show that Pop2Piano, trained with our dataset, is capable\nof producing plausible piano covers.*\n\nThis model was contributed by [Susnato Dhar](https://huggingface.co/susnato).\nThe original code can be found [here](https://github.com/sweetcocoa/pop2piano).\n\n## Usage tips\n\n* To use Pop2Piano, you will need to install the 🤗 Transformers library, as well as the following third party modules:\n```bash\npip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\n```\nPlease note that you may need to restart your runtime after installation.\n* Pop2Piano is an Encoder-Decoder based model like T5.\n* Pop2Piano can be used to generate midi-audio files for a given audio sequence.\n* Choosing different composers in `Pop2PianoForConditionalGeneration.generate()` can lead to variety of different results.\n* Setting the sampling rate to 44.1 kHz when loading the audio file can give good performance.",
  "* Though Pop2Piano was mainly trained on Korean Pop music, it also does pretty well on other Western Pop or Hip Hop songs.\n\n## Examples\n\n- Example using HuggingFace Dataset:\n\n```python\n>>> from datasets import load_dataset\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor\n\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> processor = Pop2PianoProcessor.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> ds = load_dataset(\"sweetcocoa/pop2piano_ci\", split=\"test\")\n\n>>> inputs = processor(\n...     audio=ds[\"audio\"][0][\"array\"], sampling_rate=ds[\"audio\"][0][\"sampling_rate\"], return_tensors=\"pt\"\n... )\n>>> model_output = model.generate(input_features=inputs[\"input_features\"], composer=\"composer1\")\n>>> tokenizer_output = processor.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"][0]\n>>> tokenizer_output.write(\"./Outputs/midi_output.mid\")\n```\n\n- Example using your own audio file:\n\n```python\n>>> import librosa\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor",
  ">>> audio, sr = librosa.load(\"<your_audio_file_here>\", sr=44100)  # feel free to change the sr to a suitable value.\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> processor = Pop2PianoProcessor.from_pretrained(\"sweetcocoa/pop2piano\")\n\n>>> inputs = processor(audio=audio, sampling_rate=sr, return_tensors=\"pt\")\n>>> model_output = model.generate(input_features=inputs[\"input_features\"], composer=\"composer1\")\n>>> tokenizer_output = processor.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"][0]\n>>> tokenizer_output.write(\"./Outputs/midi_output.mid\")\n```\n\n- Example of processing multiple audio files in batch:\n\n```python\n>>> import librosa\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor\n\n>>> # feel free to change the sr to a suitable value.\n>>> audio1, sr1 = librosa.load(\"<your_first_audio_file_here>\", sr=44100)\n>>> audio2, sr2 = librosa.load(\"<your_second_audio_file_here>\", sr=44100)\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> processor = Pop2PianoProcessor.from_pretrained(\"sweetcocoa/pop2piano\")",
  ">>> inputs = processor(audio=[audio1, audio2], sampling_rate=[sr1, sr2], return_attention_mask=True, return_tensors=\"pt\")\n>>> # Since we now generating in batch(2 audios) we must pass the attention_mask\n>>> model_output = model.generate(\n...     input_features=inputs[\"input_features\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     composer=\"composer1\",\n... )\n>>> tokenizer_output = processor.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"]\n\n>>> # Since we now have 2 generated MIDI files\n>>> tokenizer_output[0].write(\"./Outputs/midi_output1.mid\")\n>>> tokenizer_output[1].write(\"./Outputs/midi_output2.mid\")\n```\n\n\n- Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor` and `Pop2PianoTokenizer`):\n\n```python\n>>> import librosa\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoFeatureExtractor, Pop2PianoTokenizer\n\n>>> # feel free to change the sr to a suitable value.\n>>> audio1, sr1 = librosa.load(\"<your_first_audio_file_here>\", sr=44100)\n>>> audio2, sr2 = librosa.load(\"<your_second_audio_file_here>\", sr=44100)",
  ">>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> feature_extractor = Pop2PianoFeatureExtractor.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> tokenizer = Pop2PianoTokenizer.from_pretrained(\"sweetcocoa/pop2piano\")\n\n>>> inputs = feature_extractor(\n...     audio=[audio1, audio2],\n...     sampling_rate=[sr1, sr2],\n...     return_attention_mask=True,\n...     return_tensors=\"pt\",\n... )\n>>> # Since we now generating in batch(2 audios) we must pass the attention_mask\n>>> model_output = model.generate(\n...     input_features=inputs[\"input_features\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     composer=\"composer1\",\n... )\n>>> tokenizer_output = tokenizer.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"]\n\n>>> # Since we now have 2 generated MIDI files\n>>> tokenizer_output[0].write(\"./Outputs/midi_output1.mid\")\n>>> tokenizer_output[1].write(\"./Outputs/midi_output2.mid\")\n```\n\n\n## Pop2PianoConfig\n\n[[autodoc]] Pop2PianoConfig\n\n## Pop2PianoFeatureExtractor\n\n[[autodoc]] Pop2PianoFeatureExtractor\n- __call__\n\n## Pop2PianoForConditionalGeneration\n\n[[autodoc]] Pop2PianoForConditionalGeneration",
  "- forward\n- generate\n\n## Pop2PianoTokenizer\n\n[[autodoc]] Pop2PianoTokenizer\n- __call__\n\n## Pop2PianoProcessor\n\n[[autodoc]] Pop2PianoProcessor\n- __call__",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FalconMamba\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe FalconMamba model was proposed by TII UAE (Technology Innovation Institute) in their release.\n\nThe abstract from the paper is the following:",
  "*We present FalconMamba, a new base large language model based on the novel Mamba architecture. FalconMamba is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, FalconMamba surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B. Currently, FalconMamba is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models.\nDue to its architecture, FalconMamba is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we argue and demonstrate that the pure Mamba design can achieve similar, even superior results compared to the hybrid design. We make the weights of our implementation of FalconMamba publicly available under a permissive license.*\n\nTips:",
  "- FalconMamba is mostly based on Mamba architecture, the same [tips and best practices](./mamba) would be relevant here.\n\nThe model has been trained on approximtely 6T tokens consisting a mixture of many data sources such as RefineWeb, Cosmopedia and Math data.\n\nFor more details about the training procedure and the architecture, have a look at [the technical paper of FalconMamba]() (coming soon).\n\n# Usage\n\nBelow we demonstrate how to use the model:\n\n```python\nfrom transformers import FalconMambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\nmodel = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nThe architecture is also compatible with `torch.compile` for faster generation:\n\n```python\nfrom transformers import FalconMambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")",
  "model = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\", torch_dtype=torch.bfloat16).to(0)\nmodel = torch.compile(model)\n\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nIf you have access to a GPU that is compatible with `bitsandbytes`, you can also quantize the model in 4-bit precision:\n\n```python\nfrom transformers import FalconMambaForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\nmodel = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\", quantization_config=quantization_config)\n\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nYou can also play with the instruction fine-tuned model:\n\n```python\nfrom transformers import FalconMambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b-instruct\")",
  "model = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b-instruct\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True).input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n## FalconMambaConfig\n\n[[autodoc]] FalconMambaConfig\n\n## FalconMambaModel\n\n[[autodoc]] FalconMambaModel\n- forward\n\n## FalconMambaLMHeadModel\n\n[[autodoc]] FalconMambaForCausalLM\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ConvNeXt V2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The ConvNeXt V2 model was proposed in [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\nConvNeXt V2 is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, and a successor of [ConvNeXT](convnext).\n\nThe abstract from the paper is the following:",
  "*Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked  autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnextv2_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ConvNeXt V2 architecture. Taken from the <a href=\"https://arxiv.org/abs/2301.00808\">original paper</a>.</small>\n\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt-V2).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ConvNeXt V2.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`ConvNextV2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ConvNextV2Config\n\n[[autodoc]] ConvNextV2Config",
  "## ConvNextV2Model\n\n[[autodoc]] ConvNextV2Model\n- forward\n\n## ConvNextV2ForImageClassification\n\n[[autodoc]] ConvNextV2ForImageClassification\n- forward\n\n## TFConvNextV2Model\n\n[[autodoc]] TFConvNextV2Model\n- call\n\n\n## TFConvNextV2ForImageClassification\n\n[[autodoc]] TFConvNextV2ForImageClassification\n- call",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GraniteMoeShared\n\n## Overview\n\n\nThe GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n\nAdditionally this class GraniteMoeSharedModel adds shared experts for Moe.\n\n```python\nimport torch",
  "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"ibm-research/moe-7b-1b-active-shared-experts\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\nmodel.eval()\n\n# change input text as desired\nprompt = \"Write a code to find the maximum value in a list of numbers.\"\n\n# tokenize the text\ninput_tokens = tokenizer(prompt, return_tensors=\"pt\")\n# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# loop over the batch to print, in this example the batch size is 1\nfor i in output:\nprint(i)\n```\n\nThis HF implementation is contributed by [Mayank Mishra](https://huggingface.co/mayank-mishra), [Shawn Tan](https://huggingface.co/shawntan) and [Sukriti Sharma](https://huggingface.co/SukritiSharma).\n\n\n## GraniteMoeSharedConfig\n\n[[autodoc]] GraniteMoeSharedConfig\n\n## GraniteMoeSharedModel\n\n[[autodoc]] GraniteMoeSharedModel\n- forward\n\n## GraniteMoeSharedForCausalLM\n\n[[autodoc]] GraniteMoeSharedForCausalLM\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\nLicense. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\nspecific language governing permissions and limitations under the License. -->\n\n# Donut\n\n## Overview\n\nThe Donut model was proposed in [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by\nGeewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\nDonut consists of an image Transformer encoder and an autoregressive text Transformer decoder to perform document understanding",
  "tasks such as document image classification, form understanding and visual question answering.\n\nThe abstract from the paper is the following:",
  "*Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/donut_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Donut high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2111.15664\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n[here](https://github.com/clovaai/donut).\n\n## Usage tips\n\n- The quickest way to get started with Donut is by checking the [tutorial\nnotebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut), which show how to use the model\nat inference time as well as fine-tuning on custom data.\n- Donut is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n\n## Inference examples\n\nDonut's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.\n\nThe [`DonutImageProcessor`] class is responsible for preprocessing the input image and\n[`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`] decodes the generated target tokens to the target string. The",
  "[`DonutProcessor`] wraps [`DonutImageProcessor`] and [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]\ninto a single instance to both extract the input features and decode the predicted token ids.\n\n- Step-by-step Document Image Classification\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE_RESULT\n\n>>> # load document image\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[1][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_rvlcdip>\"\n>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(\n...     pixel_values.to(device),",
  "...     decoder_input_ids=decoder_input_ids.to(device),\n...     max_length=model.decoder.config.max_position_embeddings,\n...     pad_token_id=processor.tokenizer.pad_token_id,\n...     eos_token_id=processor.tokenizer.eos_token_id,\n...     use_cache=True,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n...     return_dict_in_generate=True,\n... )\n\n>>> sequence = processor.batch_decode(outputs.sequences)[0]\n>>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n>>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n>>> print(processor.token2json(sequence))\n{'class': 'advertisement'}\n```\n\n- Step-by-step Document Parsing\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE_RESULT",
  ">>> # load document image\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[2][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_cord-v2>\"\n>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(\n...     pixel_values.to(device),\n...     decoder_input_ids=decoder_input_ids.to(device),\n...     max_length=model.decoder.config.max_position_embeddings,\n...     pad_token_id=processor.tokenizer.pad_token_id,\n...     eos_token_id=processor.tokenizer.eos_token_id,\n...     use_cache=True,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n...     return_dict_in_generate=True,\n... )\n\n>>> sequence = processor.batch_decode(outputs.sequences)[0]\n>>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n>>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n>>> print(processor.token2json(sequence))",
  "{'menu': {'nm': 'CINNAMON SUGAR', 'unitprice': '17,000', 'cnt': '1 x', 'price': '17,000'}, 'sub_total': {'subtotal_price': '17,000'}, 'total': {'total_price': '17,000', 'cashprice': '20,000', 'changeprice': '3,000'}}\n```\n\n- Step-by-step Document Visual Question Answering (DocVQA)\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE_RESULT\n\n>>> # load document image from the DocVQA dataset\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[0][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n>>> question = \"When is the coffee break?\"\n>>> prompt = task_prompt.replace(\"{user_input}\", question)",
  ">>> decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(\n...     pixel_values.to(device),\n...     decoder_input_ids=decoder_input_ids.to(device),\n...     max_length=model.decoder.config.max_position_embeddings,\n...     pad_token_id=processor.tokenizer.pad_token_id,\n...     eos_token_id=processor.tokenizer.eos_token_id,\n...     use_cache=True,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n...     return_dict_in_generate=True,\n... )\n\n>>> sequence = processor.batch_decode(outputs.sequences)[0]\n>>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n>>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n>>> print(processor.token2json(sequence))\n{'question': 'When is the coffee break?', 'answer': '11-14 to 11:39 a.m.'}\n```\n\nSee the [model hub](https://huggingface.co/models?filter=donut) to look for Donut checkpoints.\n\n## Training",
  "We refer to the [tutorial notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut).\n\n## DonutSwinConfig\n\n[[autodoc]] DonutSwinConfig\n\n## DonutImageProcessor\n\n[[autodoc]] DonutImageProcessor\n- preprocess\n\n## DonutFeatureExtractor\n\n[[autodoc]] DonutFeatureExtractor\n- __call__\n\n## DonutProcessor\n\n[[autodoc]] DonutProcessor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n## DonutSwinModel\n\n[[autodoc]] DonutSwinModel\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# mLUKE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension",
  "of the [LUKE model](https://arxiv.org/abs/2010.01057) trained on the basis of XLM-RoBERTa.\n\nIt is based on XLM-RoBERTa and adds entity embeddings, which helps improve performance on various downstream tasks\ninvolving reasoning about entities such as named entity recognition, extractive question answering, relation\nclassification, cloze-style knowledge completion.\n\nThe abstract from the paper is the following:\n\n*Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual\nalignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining\nand do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging\nentity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages\nwith entity representations and show the model consistently outperforms word-based pretrained models in various\ncross-lingual transfer tasks. We also analyze the model and the key insight is that incorporating entity",
  "representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a\nmultilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual\nknowledge more likely than using only word representations.*\n\nThis model was contributed by [ryo0634](https://huggingface.co/ryo0634). The original code can be found [here](https://github.com/studio-ousia/luke).\n\n## Usage tips\n\nOne can directly plug in the weights of mLUKE into a LUKE model, like so:\n\n```python\nfrom transformers import LukeModel\n\nmodel = LukeModel.from_pretrained(\"studio-ousia/mluke-base\")\n```\n\nNote that mLUKE has its own tokenizer, [`MLukeTokenizer`]. You can initialize it as follows:\n\n```python\nfrom transformers import MLukeTokenizer\n\ntokenizer = MLukeTokenizer.from_pretrained(\"studio-ousia/mluke-base\")\n```\n\n<Tip>\n\nAs mLUKE's architecture is equivalent to that of LUKE, one can refer to [LUKE's documentation page](luke) for all\ntips, code examples and notebooks.\n\n</Tip>\n\n## MLukeTokenizer\n\n[[autodoc]] MLukeTokenizer\n- __call__\n- save_vocabulary",
  "<!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# QDQBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe QDQBERT model can be referenced in [Integer Quantization for Deep Learning Inference: Principles and Empirical\nEvaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius\nMicikevicius.\n\nThe abstract from the paper is the following:\n\n*Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by\ntaking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of\nquantization parameters and evaluate their choices on a wide range of neural network models for different application\ndomains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration\nby processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is\nable to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are\nmore difficult to quantize, such as MobileNets and BERT-large.*",
  "This model was contributed by [shangz](https://huggingface.co/shangz).\n\n## Usage tips\n\n- QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to (i) linear layer\ninputs and weights, (ii) matmul inputs, (iii) residual add inputs, in BERT model.\n- QDQBERT requires the dependency of [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). To install `pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com`\n- QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example *google-bert/bert-base-uncased*), and\nperform Quantization Aware Training/Post Training Quantization.\n- A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for\nSQUAD task can be found at https://github.com/huggingface/transformers-research-projects/tree/main/quantization-qdqbert.\n\n### Set default quantizers\n\nQDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to BERT by",
  "`TensorQuantizer` in [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). `TensorQuantizer` is the module\nfor quantizing tensors, with `QuantDescriptor` defining how the tensor should be quantized. Refer to [Pytorch\nQuantization Toolkit userguide](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html) for more details.\n\nBefore creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor quantizers.\n\nExample:\n\n```python\n>>> import pytorch_quantization.nn as quant_nn\n>>> from pytorch_quantization.tensor_quant import QuantDescriptor\n\n>>> # The default tensor quantizer is set to use Max calibration method\n>>> input_desc = QuantDescriptor(num_bits=8, calib_method=\"max\")\n>>> # The default tensor quantizer is set to be per-channel quantization for weights\n>>> weight_desc = QuantDescriptor(num_bits=8, axis=((0,)))\n>>> quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)\n>>> quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc)\n```\n\n### Calibration",
  "Calibration is the terminology of passing data samples to the quantizer and deciding the best scaling factors for\ntensors. After setting up the tensor quantizers, one can use the following example to calibrate the model:\n\n```python\n>>> # Find the TensorQuantizer and enable calibration\n>>> for name, module in model.named_modules():\n...     if name.endswith(\"_input_quantizer\"):\n...         module.enable_calib()\n...         module.disable_quant()  # Use full precision data to calibrate\n\n>>> # Feeding data samples\n>>> model(x)\n>>> # ...\n\n>>> # Finalize calibration\n>>> for name, module in model.named_modules():\n...     if name.endswith(\"_input_quantizer\"):\n...         module.load_calib_amax()\n...         module.enable_quant()\n\n>>> # If running on GPU, it needs to call .cuda() again because new tensors will be created by calibration process\n>>> model.cuda()\n\n>>> # Keep running the quantized model\n>>> # ...\n```\n\n### Export to ONNX\n\nThe goal of exporting to ONNX is to deploy inference by [TensorRT](https://developer.nvidia.com/tensorrt). Fake\nquantization will be broken into a pair of QuantizeLinear/DequantizeLinear ONNX ops. After setting static member of",
  "TensorQuantizer to use Pytorch’s own fake quantization functions, fake quantized model can be exported to ONNX, follow\nthe instructions in [torch.onnx](https://pytorch.org/docs/stable/onnx.html). Example:\n\n```python\n>>> from pytorch_quantization.nn import TensorQuantizer\n\n>>> TensorQuantizer.use_fb_fake_quant = True\n\n>>> # Load the calibrated model\n>>> ...\n>>> # ONNX export\n>>> torch.onnx.export(...)\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## QDQBertConfig\n\n[[autodoc]] QDQBertConfig\n\n## QDQBertModel\n\n[[autodoc]] QDQBertModel\n- forward\n\n## QDQBertLMHeadModel\n\n[[autodoc]] QDQBertLMHeadModel\n- forward\n\n## QDQBertForMaskedLM\n\n[[autodoc]] QDQBertForMaskedLM\n- forward\n\n## QDQBertForSequenceClassification\n\n[[autodoc]] QDQBertForSequenceClassification\n- forward\n\n## QDQBertForNextSentencePrediction",
  "[[autodoc]] QDQBertForNextSentencePrediction\n- forward\n\n## QDQBertForMultipleChoice\n\n[[autodoc]] QDQBertForMultipleChoice\n- forward\n\n## QDQBertForTokenClassification\n\n[[autodoc]] QDQBertForTokenClassification\n- forward\n\n## QDQBertForQuestionAnswering\n\n[[autodoc]] QDQBertForQuestionAnswering\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Aria\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Aria model was proposed in [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://huggingface.co/papers/2410.05993) by Li et al. from the Rhymes.AI team.\n\nAria is an open multimodal-native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. It has a Mixture-of-Experts architecture, with respectively 3.9B and 3.5B activated parameters per visual token and text token.\n\nThe abstract from the paper is the following:",
  "*Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.*\n\nThis model was contributed by [m-ric](https://huggingface.co/m-ric).",
  "The original code can be found [here](https://github.com/rhymes-ai/Aria).\n\n## Usage tips\n\nHere's how to use the model for vision tasks:\n```python\nimport requests\nimport torch\nfrom PIL import Image\n\nfrom transformers import AriaProcessor, AriaForConditionalGeneration\n\nmodel_id_or_path = \"rhymes-ai/Aria\"\n\nmodel = AriaForConditionalGeneration.from_pretrained(\nmodel_id_or_path, device_map=\"auto\"\n)\n\nprocessor = AriaProcessor.from_pretrained(model_id_or_path)\n\nimage = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"text\": \"what is the image?\", \"type\": \"text\"},\n],\n}\n]\n\ntext = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=text, images=image, return_tensors=\"pt\")\ninputs.to(model.device)\n\noutput = model.generate(\n**inputs,\nmax_new_tokens=15,\nstop_strings=[\"<|im_end|>\"],\ntokenizer=processor.tokenizer,\ndo_sample=True,\ntemperature=0.9,\n)\noutput_ids = output[0][inputs[\"input_ids\"].shape[1]:]\nresponse = processor.decode(output_ids, skip_special_tokens=True)\n```\n\n\n## AriaImageProcessor\n\n[[autodoc]] AriaImageProcessor\n\n## AriaProcessor",
  "[[autodoc]] AriaProcessor\n\n## AriaTextConfig\n\n[[autodoc]] AriaTextConfig\n\n## AriaConfig\n\n[[autodoc]] AriaConfig\n\n## AriaTextModel\n\n[[autodoc]] AriaTextModel\n\n## AriaTextForCausalLM\n\n[[autodoc]] AriaTextForCausalLM\n\n## AriaForConditionalGeneration\n\n[[autodoc]] AriaForConditionalGeneration\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BertGeneration\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BertGeneration model is a BERT model that can be leveraged for sequence-to-sequence tasks using\n[`EncoderDecoderModel`] as proposed in [Leveraging Pre-trained Checkpoints for Sequence Generation",
  "Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n\nThe abstract from the paper is the following:\n\n*Unsupervised pretraining of large neural models has recently revolutionized Natural Language Processing. By\nwarm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language\nUnderstanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT,\nGPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both\nencoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation,\nText Summarization, Sentence Splitting, and Sentence Fusion.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be",
  "found [here](https://tfhub.dev/s?module-type=text-generation&subtype=module,placeholder).\n\n## Usage examples and tips\n\nThe model can be used in combination with the [`EncoderDecoderModel`] to leverage two pretrained BERT checkpoints for\nsubsequent fine-tuning:\n\n```python\n>>> # leverage checkpoints for Bert2Bert model...\n>>> # use BERT's cls token as BOS token and sep token as EOS token\n>>> encoder = BertGenerationEncoder.from_pretrained(\"google-bert/bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n>>> # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n>>> decoder = BertGenerationDecoder.from_pretrained(\n...     \"google-bert/bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n... )\n>>> bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n\n>>> # create tokenizer...\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")\n\n>>> input_ids = tokenizer(\n...     \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n>>> labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n\n>>> # train...",
  ">>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n>>> loss.backward()\n```\n\nPretrained [`EncoderDecoderModel`] are also directly available in the model hub, e.g.:\n\n```python\n>>> # instantiate sentence fusion model\n>>> sentence_fuser = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n\n>>> input_ids = tokenizer(\n...     \"This is the first sentence. This is the second sentence.\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n\n>>> outputs = sentence_fuser.generate(input_ids)\n\n>>> print(tokenizer.decode(outputs[0]))\n```\n\nTips:\n\n- [`BertGenerationEncoder`] and [`BertGenerationDecoder`] should be used in\ncombination with [`EncoderDecoder`].\n- For summarization, sentence splitting, sentence fusion and translation, no special tokens are required for the input.\nTherefore, no EOS token should be added to the end of the input.\n\n## BertGenerationConfig\n\n[[autodoc]] BertGenerationConfig\n\n## BertGenerationTokenizer\n\n[[autodoc]] BertGenerationTokenizer\n- save_vocabulary\n\n## BertGenerationEncoder\n\n[[autodoc]] BertGenerationEncoder",
  "- forward\n\n## BertGenerationDecoder\n\n[[autodoc]] BertGenerationDecoder\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Transformer XL\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n<Tip warning={true}>",
  "This model is in maintenance mode only, so we won't accept any new PRs changing its code. This model was deprecated due to security issues linked to `pickle.load`.\n\nWe recommend switching to more recent models for improved security.\n\nIn case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub.\n\nYou will need to set the environment variable `TRUST_REMOTE_CODE` to `True` in order to allow the\nusage of `pickle.load()`:\n\n```python\nimport os\nfrom transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n\nos.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n\ncheckpoint = 'transfo-xl/transfo-xl-wt103'\nrevision = '40a186da79458c9f9de846edfaea79c412137f97'\n\ntokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\nmodel = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n```\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.35.0.\nYou can do so by running the following command: `pip install -U transformers==4.35.0`.\n\n</Tip>",
  "<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=transfo-xl\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-transfo--xl-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/transfo-xl-wt103\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\nSalakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinusoïdal) embeddings which can\nreuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax\ninputs and outputs (tied).\n\nThe abstract from the paper is the following:\n\n*Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the\nsetting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency",
  "beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a\nnovel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the\ncontext fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450%\nlonger than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+\ntimes faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of\nbpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn\nTreebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably\ncoherent, novel text articles with thousands of tokens.*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\n\n## Usage tips\n\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done on the left or on the right. The",
  "original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.\n- Transformer-XL is one of the few models that has no sequence length limit.\n- Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive segments (similar to a regular RNNs with two consecutive inputs). In this context, a segment is a number of consecutive tokens (for instance 512) that may span across multiple documents, and segments are fed in order to the model.\n- Basically, the hidden states of the previous segment are concatenated to the current input to compute the attention scores. This allows the model to pay attention to information that was in the previous segment as well as the current one. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.\n- This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.\n\n\n<Tip warning={true}>",
  "TransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)\n\n</Tip>\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## TransfoXLConfig\n\n[[autodoc]] TransfoXLConfig\n\n## TransfoXLTokenizer\n\n[[autodoc]] TransfoXLTokenizer\n- save_vocabulary\n\n## TransfoXL specific outputs\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput\n\n<frameworkcontent>\n<pt>\n\n## TransfoXLModel\n\n[[autodoc]] TransfoXLModel\n- forward\n\n## TransfoXLLMHeadModel\n\n[[autodoc]] TransfoXLLMHeadModel\n- forward\n\n## TransfoXLForSequenceClassification\n\n[[autodoc]] TransfoXLForSequenceClassification\n- forward\n\n</pt>\n<tf>\n\n## TFTransfoXLModel\n\n[[autodoc]] TFTransfoXLModel\n- call\n\n## TFTransfoXLLMHeadModel",
  "[[autodoc]] TFTransfoXLLMHeadModel\n- call\n\n## TFTransfoXLForSequenceClassification\n\n[[autodoc]] TFTransfoXLForSequenceClassification\n- call\n\n</tf>\n</frameworkcontent>\n\n## Internal Layers\n\n[[autodoc]] AdaptiveEmbedding\n\n[[autodoc]] TFAdaptiveEmbedding",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DETA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe DETA model was proposed in [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.\nDETA (short for Detection Transformers with Assignment) improves [Deformable DETR](deformable_detr) by replacing the one-to-one bipartite Hungarian matching loss\nwith one-to-many label assignments used in traditional detectors with non-maximum suppression (NMS). This leads to significant gains of up to 2.5 mAP.\n\nThe abstract from the paper is the following:",
  "*Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object detection. Recently, these models have surpassed traditional detectors on COCO with undeniable elegance. However, they differ from traditional detectors in multiple designs, including model architecture and training schedules, and thus the effectiveness of one-to-one matching is not fully understood. In this work, we conduct a strict comparison between the one-to-one Hungarian matching in DETRs and the one-to-many label assignments in traditional detectors with non-maximum supervision (NMS). Surprisingly, we observe one-to-many assignments with NMS consistently outperform standard one-to-one matching under the same setting, with a significant gain of up to 2.5 mAP. Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50.2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone, outperforming all existing traditional or transformer-based detectors in this setting. On multiple datasets, schedules, and architectures, we consistently show bipartite matching is unnecessary for performant detection transformers. Furthermore, we attribute the success of detection transformers to their expressive transformer architecture.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/deta_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> DETA overview. Taken from the <a href=\"https://arxiv.org/abs/2212.06137\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/jozhang97/DETA).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DETA.\n\n- Demo notebooks for DETA can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETA).\n- Scripts for finetuning [`DetaForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection).",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DetaConfig\n\n[[autodoc]] DetaConfig\n\n## DetaImageProcessor\n\n[[autodoc]] DetaImageProcessor\n- preprocess\n- post_process_object_detection\n\n## DetaModel\n\n[[autodoc]] DetaModel\n- forward\n\n## DetaForObjectDetection\n\n[[autodoc]] DetaForObjectDetection\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Starcoder2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "StarCoder2 is a family of open LLMs for code and comes in 3 different sizes with 3B, 7B and 15B parameters. The flagship StarCoder2-15B model is trained on over 4 trillion tokens and 600+ programming languages from The Stack v2. All models use Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and were trained using the Fill-in-the-Middle objective. The models have been released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.",
  "The abstract of the paper is the following:",
  "> The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
  "## License\n\nThe models are licensed under the [BigCode OpenRAIL-M v1 license agreement](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement).\n\n## Usage tips\n\nThe StarCoder2 models can be found in the [HuggingFace hub](https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a). You can find some examples for inference and fine-tuning in StarCoder2's [GitHub repo](https://github.com/bigcode-project/starcoder2).\n\nThese ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-7b\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-7b\")\n\n>>> prompt = \"def print_hello_world():\"\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=False)\n>>> tokenizer.batch_decode(generated_ids)[0]\n'def print_hello_world():\\n    print(\"Hello World!\")\\n\\ndef print'\n```\n\n## Starcoder2Config\n\n[[autodoc]] Starcoder2Config\n\n## Starcoder2Model\n\n[[autodoc]] Starcoder2Model",
  "- forward\n\n## Starcoder2ForCausalLM\n\n[[autodoc]] Starcoder2ForCausalLM\n- forward\n\n## Starcoder2ForSequenceClassification\n\n[[autodoc]] Starcoder2ForSequenceClassification\n- forward\n\n## Starcoder2ForTokenClassification\n\n[[autodoc]] Starcoder2ForTokenClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ELECTRA\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe ELECTRA model was proposed in the paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\nGenerators](https://openreview.net/pdf?id=r1xMH1BtvB). ELECTRA is a new pretraining approach which trains two\ntransformer models: the generator and the discriminator. The generator's role is to replace tokens in a sequence, and\nis therefore trained as a masked language model. The discriminator, which is the model we're interested in, tries to\nidentify which tokens were replaced by the generator in the sequence.\n\nThe abstract from the paper is the following:\n\n*Masked language modeling (MLM) pretraining methods such as BERT corrupt the input by replacing some tokens with [MASK]\nand then train a model to reconstruct the original tokens. While they produce good results when transferred to\ndownstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a\nmore sample-efficient pretraining task called replaced token detection. Instead of masking the input, our approach\ncorrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead",
  "of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that\npredicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments\ndemonstrate this new pretraining task is more efficient than MLM because the task is defined over all input tokens\nrather than just the small subset that was masked out. As a result, the contextual representations learned by our\napproach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are\nparticularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained\nusing 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale,\nwhere it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when\nusing the same amount of compute.*\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). The original code can be found [here](https://github.com/google-research/electra).\n\n## Usage tips",
  "- ELECTRA is the pretraining approach, therefore there is nearly no changes done to the underlying model: BERT. The\nonly change is the separation of the embedding size and the hidden size: the embedding size is generally smaller,\nwhile the hidden size is larger. An additional projection layer (linear) is used to project the embeddings from their\nembedding size to the hidden size. In the case where the embedding size is the same as the hidden size, no projection\nlayer is used.\n- ELECTRA is a transformer model pretrained with the use of another (small) masked language model. The inputs are corrupted by that language model, which takes an input text that is randomly masked and outputs a text in which ELECTRA has to predict which token is an original and which one has been replaced. Like for GAN training, the small language model is trained for a few steps (but with the original texts as objective, not to fool the ELECTRA model like in a traditional GAN setting) then the ELECTRA model is trained for a few steps.\n- The ELECTRA checkpoints saved using [Google Research's implementation](https://github.com/google-research/electra)",
  "contain both the generator and discriminator. The conversion script requires the user to name which model to export\ninto the correct architecture. Once converted to the HuggingFace format, these checkpoints may be loaded into all\navailable ELECTRA models, however. This means that the discriminator may be loaded in the\n[`ElectraForMaskedLM`] model, and the generator may be loaded in the\n[`ElectraForPreTraining`] model (the classification head will be randomly initialized as it\ndoesn't exist in the generator).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## ElectraConfig\n\n[[autodoc]] ElectraConfig\n\n## ElectraTokenizer\n\n[[autodoc]] ElectraTokenizer\n\n## ElectraTokenizerFast\n\n[[autodoc]] ElectraTokenizerFast\n\n## Electra specific outputs\n\n[[autodoc]] models.electra.modeling_electra.ElectraForPreTrainingOutput",
  "[[autodoc]] models.electra.modeling_tf_electra.TFElectraForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## ElectraModel\n\n[[autodoc]] ElectraModel\n- forward\n\n## ElectraForPreTraining\n\n[[autodoc]] ElectraForPreTraining\n- forward\n\n## ElectraForCausalLM\n\n[[autodoc]] ElectraForCausalLM\n- forward\n\n## ElectraForMaskedLM\n\n[[autodoc]] ElectraForMaskedLM\n- forward\n\n## ElectraForSequenceClassification\n\n[[autodoc]] ElectraForSequenceClassification\n- forward\n\n## ElectraForMultipleChoice\n\n[[autodoc]] ElectraForMultipleChoice\n- forward\n\n## ElectraForTokenClassification\n\n[[autodoc]] ElectraForTokenClassification\n- forward\n\n## ElectraForQuestionAnswering\n\n[[autodoc]] ElectraForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFElectraModel\n\n[[autodoc]] TFElectraModel\n- call\n\n## TFElectraForPreTraining\n\n[[autodoc]] TFElectraForPreTraining\n- call\n\n## TFElectraForMaskedLM\n\n[[autodoc]] TFElectraForMaskedLM\n- call\n\n## TFElectraForSequenceClassification\n\n[[autodoc]] TFElectraForSequenceClassification\n- call\n\n## TFElectraForMultipleChoice\n\n[[autodoc]] TFElectraForMultipleChoice\n- call\n\n## TFElectraForTokenClassification\n\n[[autodoc]] TFElectraForTokenClassification\n- call\n\n## TFElectraForQuestionAnswering",
  "[[autodoc]] TFElectraForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxElectraModel\n\n[[autodoc]] FlaxElectraModel\n- __call__\n\n## FlaxElectraForPreTraining\n\n[[autodoc]] FlaxElectraForPreTraining\n- __call__\n\n## FlaxElectraForCausalLM\n\n[[autodoc]] FlaxElectraForCausalLM\n- __call__\n\n## FlaxElectraForMaskedLM\n\n[[autodoc]] FlaxElectraForMaskedLM\n- __call__\n\n## FlaxElectraForSequenceClassification\n\n[[autodoc]] FlaxElectraForSequenceClassification\n- __call__\n\n## FlaxElectraForMultipleChoice\n\n[[autodoc]] FlaxElectraForMultipleChoice\n- __call__\n\n## FlaxElectraForTokenClassification\n\n[[autodoc]] FlaxElectraForTokenClassification\n- __call__\n\n## FlaxElectraForQuestionAnswering\n\n[[autodoc]] FlaxElectraForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RoBERTa-PreLayerNorm\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe RoBERTa-PreLayerNorm model was proposed in [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\nIt is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n\nThe abstract from the paper is the following:\n\n*fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs.*\n\nThis model was contributed by [andreasmaden](https://huggingface.co/andreasmadsen).\nThe original code can be found [here](https://github.com/princeton-nlp/DinkyTrain).\n\n## Usage tips",
  "- The implementation is the same as [Roberta](roberta) except instead of using _Add and Norm_ it does _Norm and Add_. _Add_ and _Norm_ refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n- This is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RobertaPreLayerNormConfig\n\n[[autodoc]] RobertaPreLayerNormConfig\n\n<frameworkcontent>\n<pt>\n\n## RobertaPreLayerNormModel\n\n[[autodoc]] RobertaPreLayerNormModel\n- forward\n\n## RobertaPreLayerNormForCausalLM\n\n[[autodoc]] RobertaPreLayerNormForCausalLM\n- forward\n\n## RobertaPreLayerNormForMaskedLM\n\n[[autodoc]] RobertaPreLayerNormForMaskedLM\n- forward\n\n## RobertaPreLayerNormForSequenceClassification",
  "[[autodoc]] RobertaPreLayerNormForSequenceClassification\n- forward\n\n## RobertaPreLayerNormForMultipleChoice\n\n[[autodoc]] RobertaPreLayerNormForMultipleChoice\n- forward\n\n## RobertaPreLayerNormForTokenClassification\n\n[[autodoc]] RobertaPreLayerNormForTokenClassification\n- forward\n\n## RobertaPreLayerNormForQuestionAnswering\n\n[[autodoc]] RobertaPreLayerNormForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFRobertaPreLayerNormModel\n\n[[autodoc]] TFRobertaPreLayerNormModel\n- call\n\n## TFRobertaPreLayerNormForCausalLM\n\n[[autodoc]] TFRobertaPreLayerNormForCausalLM\n- call\n\n## TFRobertaPreLayerNormForMaskedLM\n\n[[autodoc]] TFRobertaPreLayerNormForMaskedLM\n- call\n\n## TFRobertaPreLayerNormForSequenceClassification\n\n[[autodoc]] TFRobertaPreLayerNormForSequenceClassification\n- call\n\n## TFRobertaPreLayerNormForMultipleChoice\n\n[[autodoc]] TFRobertaPreLayerNormForMultipleChoice\n- call\n\n## TFRobertaPreLayerNormForTokenClassification\n\n[[autodoc]] TFRobertaPreLayerNormForTokenClassification\n- call\n\n## TFRobertaPreLayerNormForQuestionAnswering\n\n[[autodoc]] TFRobertaPreLayerNormForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxRobertaPreLayerNormModel\n\n[[autodoc]] FlaxRobertaPreLayerNormModel\n- __call__",
  "## FlaxRobertaPreLayerNormForCausalLM\n\n[[autodoc]] FlaxRobertaPreLayerNormForCausalLM\n- __call__\n\n## FlaxRobertaPreLayerNormForMaskedLM\n\n[[autodoc]] FlaxRobertaPreLayerNormForMaskedLM\n- __call__\n\n## FlaxRobertaPreLayerNormForSequenceClassification\n\n[[autodoc]] FlaxRobertaPreLayerNormForSequenceClassification\n- __call__\n\n## FlaxRobertaPreLayerNormForMultipleChoice\n\n[[autodoc]] FlaxRobertaPreLayerNormForMultipleChoice\n- __call__\n\n## FlaxRobertaPreLayerNormForTokenClassification\n\n[[autodoc]] FlaxRobertaPreLayerNormForTokenClassification\n- __call__\n\n## FlaxRobertaPreLayerNormForQuestionAnswering\n\n[[autodoc]] FlaxRobertaPreLayerNormForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MobileViTV2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MobileViTV2 model was proposed in [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.",
  "MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-attention in MobileViT with separable self-attention.\n\nThe abstract from the paper is the following:",
  "*Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires O(k2) time complexity with respect to the number of tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. O(k). A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTV2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTV2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running 3.2× faster on a mobile device.*",
  "This model was contributed by [shehan97](https://huggingface.co/shehan97).\nThe original code can be found [here](https://github.com/apple/ml-cvnets).\n\n## Usage tips\n\n- MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map.\n- One can use [`MobileViTImageProcessor`] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).\n- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).\n- The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n\n## MobileViTV2Config\n\n[[autodoc]] MobileViTV2Config\n\n## MobileViTV2Model\n\n[[autodoc]] MobileViTV2Model\n- forward\n\n## MobileViTV2ForImageClassification\n\n[[autodoc]] MobileViTV2ForImageClassification\n- forward",
  "## MobileViTV2ForSemanticSegmentation\n\n[[autodoc]] MobileViTV2ForSemanticSegmentation\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Grounding DINO\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Grounding DINO model was proposed in [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection. The model achieves remarkable results, such as 52.5 AP on COCO zero-shot.\n\nThe abstract from the paper is the following:",
  "*In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a 52.5 AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean 26.1 AP.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grouding_dino_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Grounding DINO overview. Taken from the <a href=\"https://arxiv.org/abs/2303.05499\">original paper</a>. </small>\n\nThis model was contributed by [EduardoPacheco](https://huggingface.co/EduardoPacheco) and [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/IDEA-Research/GroundingDINO).\n\n## Usage tips\n\n- One can use [`GroundingDinoProcessor`] to prepare image-text pairs for the model.\n- To separate classes in the text use a period e.g. \"a cat. a dog.\"\n- When using multiple classes (e.g. `\"a cat. a dog.\"`), use `post_process_grounded_object_detection` from [`GroundingDinoProcessor`] to post process outputs. Since, the labels returned from `post_process_object_detection` represent the indices from the model dimension where prob > threshold.\n\nHere's how to use the model for zero-shot object detection:\n\n```python\n>>> import requests\n\n>>> import torch\n>>> from PIL import Image\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection",
  ">>> model_id = \"IDEA-Research/grounding-dino-tiny\"\n>>> device = \"cuda\"\n\n>>> processor = AutoProcessor.from_pretrained(model_id)\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n\n>>> image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(image_url, stream=True).raw)\n>>> # Check for cats and remote controls\n>>> text_labels = [[\"a cat\", \"a remote control\"]]\n\n>>> inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> results = processor.post_process_grounded_object_detection(\n...     outputs,\n...     inputs.input_ids,\n...     box_threshold=0.4,\n...     text_threshold=0.3,\n...     target_sizes=[image.size[::-1]]\n... )\n\n# Retrieve the first image result\n>>> result = results[0]\n>>> for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n...     box = [round(x, 2) for x in box.tolist()]\n...     print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a cat with confidence 0.468 at location [344.78, 22.9, 637.3, 373.62]",
  "Detected a cat with confidence 0.426 at location [11.74, 51.55, 316.51, 473.22]\n```\n\n## Grounded SAM\n\nOne can combine Grounding DINO with the [Segment Anything](sam) model for text-based mask generation as introduced in [Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](https://arxiv.org/abs/2401.14159). You can refer to this [demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb) 🌍 for details.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png\"\nalt=\"drawing\" width=\"900\"/>\n\n<small> Grounded SAM overview. Taken from the <a href=\"https://github.com/IDEA-Research/Grounded-Segment-Anything\">original repository</a>. </small>\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Grounding DINO. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
  "- Demo notebooks regarding inference with Grounding DINO as well as combining it with [SAM](sam) can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Grounding%20DINO). 🌎\n\n## GroundingDinoImageProcessor\n\n[[autodoc]] GroundingDinoImageProcessor\n- preprocess\n- post_process_object_detection\n\n## GroundingDinoProcessor\n\n[[autodoc]] GroundingDinoProcessor\n- post_process_grounded_object_detection\n\n## GroundingDinoConfig\n\n[[autodoc]] GroundingDinoConfig\n\n## GroundingDinoModel\n\n[[autodoc]] GroundingDinoModel\n- forward\n\n## GroundingDinoForObjectDetection\n\n[[autodoc]] GroundingDinoForObjectDetection\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MarianMT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nA framework for translation models, using the same models as BART. Translations should be similar, but not identical to output in the test set linked to in each model card.\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer).\n\n\n## Implementation Notes\n\n- Each model is about 298 MB on disk, there are more than 1,000 models.\n- The list of supported language pairs can be found [here](https://huggingface.co/Helsinki-NLP).\n- Models were originally trained by [Jörg Tiedemann](https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann) using the [Marian](https://marian-nmt.github.io/) C++ library, which supports fast training and translation.\n- All models are transformer encoder-decoders with 6 layers in each component. Each model's performance is documented\nin a model card.\n- The 80 opus models that require BPE preprocessing are not supported.\n- The modeling code is the same as [`BartForConditionalGeneration`] with a few minor modifications:\n\n- static (sinusoid) positional embeddings (`MarianConfig.static_position_embeddings=True`)\n- no layernorm_embedding (`MarianConfig.normalize_embedding=False`)",
  "- the model starts generating with `pad_token_id` (which has 0 as a token_embedding) as the prefix (Bart uses\n`<s/>`),\n- Code to bulk convert models can be found in `convert_marian_to_pytorch.py`.\n\n\n## Naming\n\n- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`\n- The language codes used to name models are inconsistent. Two digit codes can usually be found [here](https://developers.google.com/admin-sdk/directory/v1/languages), three digit codes require googling \"language\ncode {code}\".\n- Codes formatted like `es_AR` are usually `code_{region}`. That one is Spanish from Argentina.\n- The models were converted in two stages. The first 1000 models use ISO-639-2 codes to identify languages, the second\ngroup use a combination of ISO-639-5 codes and ISO-639-2 codes.\n\n\n## Examples\n\n- Since Marian models are smaller than many other translation models available in the library, they can be useful for\nfine-tuning experiments and integration tests.\n- [Fine-tune on GPU](https://github.com/huggingface/transformers/blob/master/examples/legacy/seq2seq/train_distil_marian_enro.sh)\n\n## Multilingual Models",
  "- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`:\n- If a model can output multiple languages, and you should specify a language code by prepending the desired output\nlanguage to the `src_text`.\n- You can see a models's supported language codes in its model card, under target constituents, like in [opus-mt-en-roa](https://huggingface.co/Helsinki-NLP/opus-mt-en-roa).\n- Note that if a model is only multilingual on the source side, like `Helsinki-NLP/opus-mt-roa-en`, no language\ncodes are required.\n\nNew multi-lingual models from the [Tatoeba-Challenge repo](https://github.com/Helsinki-NLP/Tatoeba-Challenge)\nrequire 3 character language codes:\n\n```python\n>>> from transformers import MarianMTModel, MarianTokenizer\n\n>>> src_text = [\n...     \">>fra<< this is a sentence in english that we want to translate to french\",\n...     \">>por<< This should go to portuguese\",\n...     \">>esp<< And this to Spanish\",\n... ]\n\n>>> model_name = \"Helsinki-NLP/opus-mt-en-roa\"\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\n>>> print(tokenizer.supported_language_codes)",
  "['>>zlm_Latn<<', '>>mfe<<', '>>hat<<', '>>pap<<', '>>ast<<', '>>cat<<', '>>ind<<', '>>glg<<', '>>wln<<', '>>spa<<', '>>fra<<', '>>ron<<', '>>por<<', '>>ita<<', '>>oci<<', '>>arg<<', '>>min<<']\n\n>>> model = MarianMTModel.from_pretrained(model_name)\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n>>> [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n[\"c'est une phrase en anglais que nous voulons traduire en français\",\n'Isto deve ir para o português.',\n'Y esto al español']\n```\n\nHere is the code to see all available pretrained models on the hub:\n\n```python\nfrom huggingface_hub import list_models\n\nmodel_list = list_models()\norg = \"Helsinki-NLP\"\nmodel_ids = [x.id for x in model_list if x.id.startswith(org)]\nsuffix = [x.split(\"/\")[1] for x in model_ids]\nold_style_multi_models = [f\"{org}/{s}\" for s in suffix if s != s.lower()]\n```\n\n## Old Style Multi-Lingual Models\n\nThese are the old style multi-lingual models ported from the OPUS-MT-Train repo: and the members of each language\ngroup:\n\n```python no-style\n['Helsinki-NLP/opus-mt-NORTH_EU-NORTH_EU',\n'Helsinki-NLP/opus-mt-ROMANCE-en',\n'Helsinki-NLP/opus-mt-SCANDINAVIA-SCANDINAVIA',",
  "'Helsinki-NLP/opus-mt-de-ZH',\n'Helsinki-NLP/opus-mt-en-CELTIC',\n'Helsinki-NLP/opus-mt-en-ROMANCE',\n'Helsinki-NLP/opus-mt-es-NORWAY',\n'Helsinki-NLP/opus-mt-fi-NORWAY',\n'Helsinki-NLP/opus-mt-fi-ZH',\n'Helsinki-NLP/opus-mt-fi_nb_no_nn_ru_sv_en-SAMI',\n'Helsinki-NLP/opus-mt-sv-NORWAY',\n'Helsinki-NLP/opus-mt-sv-ZH']\nGROUP_MEMBERS = {\n'ZH': ['cmn', 'cn', 'yue', 'ze_zh', 'zh_cn', 'zh_CN', 'zh_HK', 'zh_tw', 'zh_TW', 'zh_yue', 'zhs', 'zht', 'zh'],\n'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lmo', 'es', 'es_AR', 'es_CL', 'es_CO', 'es_CR', 'es_DO', 'es_EC', 'es_ES', 'es_GT', 'es_HN', 'es_MX', 'es_NI', 'es_PA', 'es_PE', 'es_PR', 'es_SV', 'es_UY', 'es_VE', 'pt', 'pt_br', 'pt_BR', 'pt_PT', 'gl', 'lad', 'an', 'mwl', 'it', 'it_IT', 'co', 'nap', 'scn', 'vec', 'sc', 'ro', 'la'],\n'NORTH_EU': ['de', 'nl', 'fy', 'af', 'da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\n'SCANDINAVIA': ['da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\n'SAMI': ['se', 'sma', 'smj', 'smn', 'sms'],\n'NORWAY': ['nb_NO', 'nb', 'nn_NO', 'nn', 'nog', 'no_nb', 'no'],\n'CELTIC': ['ga', 'cy', 'br', 'gd', 'kw', 'gv']\n}\n```",
  "Example of translating english to many romance languages, using old-style 2 character language codes\n\n\n```python\n>>> from transformers import MarianMTModel, MarianTokenizer\n\n>>> src_text = [\n...     \">>fr<< this is a sentence in english that we want to translate to french\",\n...     \">>pt<< This should go to portuguese\",\n...     \">>es<< And this to Spanish\",\n... ]\n\n>>> model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\n\n>>> model = MarianMTModel.from_pretrained(model_name)\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n>>> tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n[\"c'est une phrase en anglais que nous voulons traduire en français\",\n'Isto deve ir para o português.',\n'Y esto al español']\n```\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## MarianConfig\n\n[[autodoc]] MarianConfig\n\n## MarianTokenizer\n\n[[autodoc]] MarianTokenizer\n- build_inputs_with_special_tokens\n\n<frameworkcontent>\n<pt>\n\n## MarianModel",
  "[[autodoc]] MarianModel\n- forward\n\n## MarianMTModel\n\n[[autodoc]] MarianMTModel\n- forward\n\n## MarianForCausalLM\n\n[[autodoc]] MarianForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFMarianModel\n\n[[autodoc]] TFMarianModel\n- call\n\n## TFMarianMTModel\n\n[[autodoc]] TFMarianMTModel\n- call\n\n</tf>\n<jax>\n\n## FlaxMarianModel\n\n[[autodoc]] FlaxMarianModel\n- __call__\n\n## FlaxMarianMTModel\n\n[[autodoc]] FlaxMarianMTModel\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# VisionTextDualEncoder\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe [`VisionTextDualEncoderModel`] can be used to initialize a vision-text dual encoder model with\nany pretrained vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.* [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both the vision and text encoder to project the output embeddings\nto a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a\ndownstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\ntraining and then can be used for zero-shot vision tasks such image-classification or retrieval.\n\nIn [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how",
  "leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\nnew zero-shot vision tasks such as image classification or retrieval.\n\n## VisionTextDualEncoderConfig\n\n[[autodoc]] VisionTextDualEncoderConfig\n\n## VisionTextDualEncoderProcessor\n\n[[autodoc]] VisionTextDualEncoderProcessor\n\n<frameworkcontent>\n<pt>\n\n## VisionTextDualEncoderModel\n\n[[autodoc]] VisionTextDualEncoderModel\n- forward\n\n</pt>\n<tf>\n\n## FlaxVisionTextDualEncoderModel\n\n[[autodoc]] FlaxVisionTextDualEncoderModel\n- __call__\n\n</tf>\n<jax>\n\n## TFVisionTextDualEncoderModel\n\n[[autodoc]] TFVisionTextDualEncoderModel\n- call\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# NLLB-MOE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-jussà, James Cross, Onur Çelebi,",
  "Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\nLoic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\nNecip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n\nThe abstract of the paper is the following:\n\n*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.\nHowever, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the\n200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by",
  "first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed\nat narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of\nExperts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training\nimprovements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using\na human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.\nOur model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/facebookresearch/fairseq).",
  "## Usage tips\n\n- M2M100ForConditionalGeneration is the base model for both NLLB and NLLB MoE\n- The NLLB-MoE is very similar to the NLLB model, but it's feed forward layer is based on the implementation of SwitchTransformers.\n- The tokenizer is the same as the NLLB models.\n\n## Implementation differences with SwitchTransformers\n\nThe biggest difference is the way the tokens are routed. NLLB-MoE uses a `top-2-gate` which means that for each input, only the top two experts are selected based on the\nhighest predicted probabilities from the gating network, and the remaining experts are ignored. In `SwitchTransformers`, only the top-1 probabilities are computed,\nwhich means that tokens have less probability of being forwarded. Moreover, if a token is not routed to any expert, `SwitchTransformers` still adds its unmodified hidden\nstates (kind of like a residual connection) while they are masked in `NLLB`'s top-2 routing mechanism.\n\n## Generating with NLLB-MoE\n\nThe available checkpoints require around 350GB of storage. Make sure to use `accelerate` if you do not have enough RAM on your machine.",
  "While generating the target text set the `forced_bos_token_id` to the target language id. The following\nexample shows how to translate English to French using the *facebook/nllb-200-distilled-600M* model.\n\nNote that we're using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\nfor the list of all BCP-47 in the Flores 200 dataset.\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> article = \"Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage.\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"], max_length=50\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]",
  "\"Auparavant, le PDG de Ring, Jamie Siminoff, a fait remarquer que la société avait commencé lorsque sa sonnette n'était pas audible depuis son magasin dans son garage.\"\n```\n\n### Generating from any other language than English\n\nEnglish (`eng_Latn`) is set as the default language from which to translate. In order to specify that you'd like to translate from a different language,\nyou should specify the BCP-47 code in the `src_lang` keyword argument of the tokenizer initialization.\n\nSee example below for a translation from romanian to german:\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\", src_lang=\"ron_Latn\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n```\n\n## Resources\n\n- [Translation task guide](../tasks/translation)",
  "- [Summarization task guide](../tasks/summarization)\n\n\n## NllbMoeConfig\n\n[[autodoc]] NllbMoeConfig\n\n## NllbMoeTop2Router\n\n[[autodoc]] NllbMoeTop2Router\n- route_tokens\n- forward\n\n## NllbMoeSparseMLP\n\n[[autodoc]] NllbMoeSparseMLP\n- forward\n\n## NllbMoeModel\n\n[[autodoc]] NllbMoeModel\n- forward\n\n## NllbMoeForConditionalGeneration\n\n[[autodoc]] NllbMoeForConditionalGeneration\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPTSAN-japanese\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama).\n\nGPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\nin the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\nfine-tune for translation or summarization.\n\n### Usage example\n\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model.\n\n```python\n>>> from transformers import AutoModel, AutoTokenizer\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").cuda()\n>>> x_tok = tokenizer(\"は、\", prefix_text=\"織田信長\", return_tensors=\"pt\")\n>>> torch.manual_seed(0)\n>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)\n>>> tokenizer.decode(gen_tok[0])\n'織田信長は、2004年に『戦国BASARA』のために、豊臣秀吉'\n```\n\n## GPTSAN Features",
  "GPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.\nThe Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.\nGPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details.\n\n### Prefix-LM Model\n\nGPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The original GPTSAN repository calls it `hybrid`)\nIn GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.\nArbitrary lengths can also be specified differently for each batch.\nThis length applies to the text entered in `prefix_text` for the tokenizer.\nThe tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token_type_ids`.",
  "The model treats the part where `token_type_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after.\n\n## Usage tips\n\nSpecifying the Prefix part is done with a mask passed to self-attention.\nWhen token_type_ids=None or all zero, it is equivalent to regular causal mask\n\nfor example:\n\n>>> x_token = tokenizer(\"ｱｲｳｴ\")\ninput_ids:      | SOT | SEG | ｱ | ｲ | ｳ | ｴ |\ntoken_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |\nprefix_lm_mask:\nSOT | 1 0 0 0 0 0 |\nSEG | 1 1 0 0 0 0 |\nｱ   | 1 1 1 0 0 0 |\nｲ   | 1 1 1 1 0 0 |\nｳ   | 1 1 1 1 1 0 |\nｴ   | 1 1 1 1 1 1 |\n\n>>> x_token = tokenizer(\"\", prefix_text=\"ｱｲｳｴ\")\ninput_ids:      | SOT | ｱ | ｲ | ｳ | ｴ | SEG |\ntoken_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |\nprefix_lm_mask:\nSOT | 1 1 1 1 1 0 |\nｱ   | 1 1 1 1 1 0 |\nｲ   | 1 1 1 1 1 0 |\nｳ   | 1 1 1 1 1 0 |\nｴ   | 1 1 1 1 1 0 |\nSEG | 1 1 1 1 1 1 |\n\n>>> x_token = tokenizer(\"ｳｴ\", prefix_text=\"ｱｲ\")\ninput_ids:      | SOT | ｱ | ｲ | SEG | ｳ | ｴ |\ntoken_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |\nprefix_lm_mask:\nSOT | 1 1 1 0 0 0 |\nｱ   | 1 1 1 0 0 0 |\nｲ   | 1 1 1 0 0 0 |\nSEG | 1 1 1 1 0 0 |\nｳ   | 1 1 1 1 1 0 |\nｴ   | 1 1 1 1 1 1 |\n\n### Spout Vector",
  "A Spout Vector is a special vector for controlling text generation.\nThis vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens.\nIn the pre-trained model published from `Tanrei/GPTSAN-japanese`, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention.\nThe Spout Vector projected by the fully connected layer is split to be passed to all self-attentions.\n\n## GPTSanJapaneseConfig\n\n[[autodoc]] GPTSanJapaneseConfig\n\n## GPTSanJapaneseTokenizer\n\n[[autodoc]] GPTSanJapaneseTokenizer\n\n## GPTSanJapaneseModel\n\n[[autodoc]] GPTSanJapaneseModel\n\n## GPTSanJapaneseForConditionalGeneration\n\n[[autodoc]] GPTSanJapaneseForConditionalGeneration\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Neighborhood Attention Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nNAT was proposed in [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143)\nby Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n\nIt is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n\nThe abstract from the paper is the following:\n\n*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision.\nNA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a\nlinear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\nreceptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\nSwin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package\nwith efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less",
  "memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA\nthat boosts image classification and downstream vision performance. Experimental results on NAT are competitive;\nNAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9%\nImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. *\n\n<img\nsrc=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/neighborhood-attention-pattern.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Neighborhood Attention compared to other attention patterns.\nTaken from the <a href=\"https://arxiv.org/abs/2204.07143\">original paper</a>.</small>\n\nThis model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).\nThe original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).\n\n## Usage tips\n\n- One can use the [`AutoImageProcessor`] API to prepare images for the model.\n- NAT can be used as a *backbone*. When `output_hidden_states = True`,\nit will output both `hidden_states` and `reshaped_hidden_states`.",
  "The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than\n`(batch_size, height, width, num_channels)`.\n\nNotes:\n- NAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention.\nYou can install it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten),\nor build on your system by running `pip install natten`.\nNote that the latter will likely take time to compile. NATTEN does not support Windows devices yet.\n- Patch size of 4 is only supported at the moment.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with NAT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`NatForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## NatConfig\n\n[[autodoc]] NatConfig\n\n## NatModel\n\n[[autodoc]] NatModel\n- forward\n\n## NatForImageClassification\n\n[[autodoc]] NatForImageClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Pixtral\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Pixtral model was released by the Mistral AI team in a [blog post](https://mistral.ai/news/pixtral-12b/). Pixtral is a multimodal version of [Mistral](mistral), incorporating a 400 million parameter vision encoder trained from scratch.",
  "The intro from the blog says the following:\n\n*Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/pixtral_architecture.webp\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Pixtral architecture. Taken from the <a href=\"https://mistral.ai/news/pixtral-12b/\">blog post.</a> </small>\n\nTips:\n\n- Pixtral is a multimodal model, taking images and text as input, and producing text as output.",
  "- This model follows the [Llava](llava) architecture. The model uses [`PixtralVisionModel`] for its vision encoder, and [`MistralForCausalLM`] for its language decoder.\n- The main contribution is the 2d ROPE (rotary position embeddings) on the images, and support for arbitrary image sizes (the images are not padded together nor are they resized).\n- Similar to [Llava](llava), the model internally replaces the `[IMG]` token placeholders by image embeddings from the vision encoder. The format for one or multiple prompts is the following:\n```\n\"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n```\nThen, the processor will replace each `[IMG]` token with a number of `[IMG]` tokens that depend on the height and the width of each image. Each *row* of the image is separated by an `[IMG_BREAK]` token, and each image is separated by an `[IMG_END]` token. It's advised to use the `apply_chat_template` method of the processor, which takes care of all of this and formats the text for you. If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the [usage section](#usage) for more info.",
  "This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/vllm-project/vllm/pull/8377).\n\n\n## Usage\n\nAt inference time, it's advised to use the processor's `apply_chat_template` method, which correctly formats the prompt for the model:\n\n```python\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id = \"mistral-community/pixtral-12b\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"cuda\")\n\nchat = [\n{\n\"role\": \"user\", \"content\": [\n{\"type\": \"text\", \"content\": \"Can this animal\"},\n{\"type\": \"image\", \"url\": \"https://picsum.photos/id/237/200/300\"},\n{\"type\": \"text\", \"content\": \"live here?\"},\n{\"type\": \"image\", \"url\": \"https://picsum.photos/seed/picsum/200/300\"}\n]\n}\n]\n\ninputs = processor.apply_chat_template(\nchat,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n\ngenerate_ids = model.generate(**inputs, max_new_tokens=500)",
  "output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n\n## PixtralVisionConfig\n\n[[autodoc]] PixtralVisionConfig\n\n## PixtralVisionModel\n\n[[autodoc]] PixtralVisionModel\n- forward\n\n## PixtralImageProcessor\n\n[[autodoc]] PixtralImageProcessor\n- preprocess\n\n## PixtralImageProcessorFast\n\n[[autodoc]] PixtralImageProcessorFast\n- preprocess\n\n## PixtralProcessor\n\n[[autodoc]] PixtralProcessor",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Depth Anything V2\n\n## Overview\n\nDepth Anything V2 was introduced in [the paper of the same name](https://arxiv.org/abs/2406.09414) by Lihe Yang et al. It uses the same architecture as the original [Depth Anything model](depth_anything), but uses synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions.\n\nThe abstract from the paper is the following:",
  "*This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_anything_overview.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Depth Anything overview. Taken from the <a href=\"https://arxiv.org/abs/2401.10891\">original paper</a>.</small>\n\nThe Depth Anything models were contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/DepthAnything/Depth-Anything-V2).\n\n## Usage example\n\nThere are 2 main ways to use Depth Anything V2: either using the pipeline API, which abstracts away all the complexity for you, or by using the `DepthAnythingForDepthEstimation` class yourself.\n\n### Pipeline API\n\nThe pipeline allows to use the model in a few lines of code:\n\n```python\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> # load pipe\n>>> pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n\n>>> # load image\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # inference\n>>> depth = pipe(image)[\"depth\"]\n```\n\n### Using the model yourself",
  "If you want to do the pre- and post-processing yourself, here's how to do that:\n\n```python\n>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n>>> import torch\n>>> import numpy as np\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n>>> model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # interpolate to original size and visualize the prediction\n>>> post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     target_sizes=[(image.height, image.width)],\n... )\n\n>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n>>> depth = depth.detach().cpu().numpy() * 255",
  ">>> depth = Image.fromarray(depth.astype(\"uint8\"))\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Depth Anything.\n\n- [Monocular depth estimation task guide](../tasks/monocular_depth_estimation)\n- [Depth Anything V2 demo](https://huggingface.co/spaces/depth-anything/Depth-Anything-V2).\n- A notebook showcasing inference with [`DepthAnythingForDepthEstimation`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Depth%20Anything/Predicting_depth_in_an_image_with_Depth_Anything.ipynb). 🌎\n- [Core ML conversion of the `small` variant for use on Apple Silicon](https://huggingface.co/apple/coreml-depth-anything-v2-small).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## DepthAnythingConfig\n\n[[autodoc]] DepthAnythingConfig\n\n## DepthAnythingForDepthEstimation\n\n[[autodoc]] DepthAnythingForDepthEstimation\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ALBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows",
  "that our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers.",
  "- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)",
  "or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import AlbertModel\nmodel = AlbertModel.from_pretrained(\"albert/albert-base-v1\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16`, we saw the\nfollowing speedups during training and inference.\n\n#### Training for 100 iterations\n\n|batch_size|seq_len|Time per batch (eager - s)| Time per batch (sdpa - s)| Speedup (%)| Eager peak mem (MB)| sdpa peak mem (MB)| Mem saving (%)|\n|----------|-------|--------------------------|--------------------------|------------|--------------------|-------------------|---------------|",
  "|2         |256    |0.028                     |0.024                     |14.388      |358.411             |321.088            |11.624         |\n|2         |512    |0.049                     |0.041                     |17.681      |753.458             |602.660            |25.022         |\n|4         |256    |0.044                     |0.039                     |12.246      |679.534             |602.660            |12.756         |\n|4         |512    |0.090                     |0.076                     |18.472      |1434.820            |1134.140           |26.512         |\n|8         |256    |0.081                     |0.072                     |12.664      |1283.825            |1134.140           |13.198         |\n|8         |512    |0.170                     |0.143                     |18.957      |2820.398            |2219.695           |27.062         |\n\n#### Inference with 50 batches\n\n|batch_size|seq_len|Per token latency eager (ms)|Per token latency SDPA (ms)|Speedup (%) |Mem eager (MB)|Mem BT (MB)|Mem saved (%)|\n|----------|-------|----------------------------|---------------------------|------------|--------------|-----------|-------------|",
  "|4         |128    |0.083                       |0.071                      |16.967      |48.319        |48.45      |-0.268       |\n|4         |256    |0.148                       |0.127                      |16.37       |63.4          |63.922     |-0.817       |\n|4         |512    |0.31                        |0.247                      |25.473      |110.092       |94.343     |16.693       |\n|8         |128    |0.137                       |0.124                      |11.102      |63.4          |63.66      |-0.409       |\n|8         |256    |0.271                       |0.231                      |17.271      |91.202        |92.246     |-1.132       |\n|8         |512    |0.602                       |0.48                       |25.47       |186.159       |152.564    |22.021       |\n|16        |128    |0.252                       |0.224                      |12.506      |91.202        |91.722     |-0.567       |\n|16        |256    |0.526                       |0.448                      |17.604      |148.378       |150.467    |-1.388       |\n|16        |512    |1.203                       |0.96                       |25.365      |338.293       |271.102    |24.784       |",
  "This model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n\n## Resources\n\n\nThe resources provided in the following sections consist of a list of official Hugging Face and community (indicated by 🌎) resources to help you get started with AlBERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n\n- [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).\n\n\n- [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification).",
  "- [`FlaxAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n- Check the [Text classification task guide](../tasks/sequence_classification) on how to use the model.\n\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n\n- [`AlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification).\n\n\n- [`TFAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n\n\n\n- [`FlaxAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).",
  "- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n- Check the [Token classification task guide](../tasks/token_classification) on how to use the model.\n\n<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`AlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFAlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxAlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).",
  "- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- Check the [Masked language modeling task guide](../tasks/masked_language_modeling) on how to use the model.\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- [`AlbertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n- [`TFAlbertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxAlbertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.",
  "- Check the [Question answering task guide](../tasks/question_answering) on how to use the model.\n\n**Multiple choice**\n\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n- [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n\n- Check the  [Multiple choice task guide](../tasks/multiple_choice) on how to use the model.\n\n\n## AlbertConfig\n\n[[autodoc]] AlbertConfig\n\n## AlbertTokenizer\n\n[[autodoc]] AlbertTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## AlbertTokenizerFast\n\n[[autodoc]] AlbertTokenizerFast\n\n## Albert specific outputs\n\n[[autodoc]] models.albert.modeling_albert.AlbertForPreTrainingOutput",
  "[[autodoc]] models.albert.modeling_tf_albert.TFAlbertForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## AlbertModel\n\n[[autodoc]] AlbertModel\n- forward\n\n## AlbertForPreTraining\n\n[[autodoc]] AlbertForPreTraining\n- forward\n\n## AlbertForMaskedLM\n\n[[autodoc]] AlbertForMaskedLM\n- forward\n\n## AlbertForSequenceClassification\n\n[[autodoc]] AlbertForSequenceClassification\n- forward\n\n## AlbertForMultipleChoice\n\n[[autodoc]] AlbertForMultipleChoice\n\n## AlbertForTokenClassification\n\n[[autodoc]] AlbertForTokenClassification\n- forward\n\n## AlbertForQuestionAnswering\n\n[[autodoc]] AlbertForQuestionAnswering\n- forward\n\n</pt>\n\n<tf>\n\n## TFAlbertModel\n\n[[autodoc]] TFAlbertModel\n- call\n\n## TFAlbertForPreTraining\n\n[[autodoc]] TFAlbertForPreTraining\n- call\n\n## TFAlbertForMaskedLM\n\n[[autodoc]] TFAlbertForMaskedLM\n- call\n\n## TFAlbertForSequenceClassification\n\n[[autodoc]] TFAlbertForSequenceClassification\n- call\n\n## TFAlbertForMultipleChoice\n\n[[autodoc]] TFAlbertForMultipleChoice\n- call\n\n## TFAlbertForTokenClassification\n\n[[autodoc]] TFAlbertForTokenClassification\n- call\n\n## TFAlbertForQuestionAnswering\n\n[[autodoc]] TFAlbertForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxAlbertModel",
  "[[autodoc]] FlaxAlbertModel\n- __call__\n\n## FlaxAlbertForPreTraining\n\n[[autodoc]] FlaxAlbertForPreTraining\n- __call__\n\n## FlaxAlbertForMaskedLM\n\n[[autodoc]] FlaxAlbertForMaskedLM\n- __call__\n\n## FlaxAlbertForSequenceClassification\n\n[[autodoc]] FlaxAlbertForSequenceClassification\n- __call__\n\n## FlaxAlbertForMultipleChoice\n\n[[autodoc]] FlaxAlbertForMultipleChoice\n- __call__\n\n## FlaxAlbertForTokenClassification\n\n[[autodoc]] FlaxAlbertForTokenClassification\n- __call__\n\n## FlaxAlbertForQuestionAnswering\n\n[[autodoc]] FlaxAlbertForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# DINOv2 with Registers\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DINOv2 with Registers model was proposed in [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588) by Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski.",
  "The [Vision Transformer](vit) (ViT) is a transformer encoder model (BERT-like) originally introduced to do supervised image classification on ImageNet.\n\nNext, people figured out ways to make ViT work really well on self-supervised image feature extraction (i.e. learning meaningful features, also called embeddings) on images without requiring any labels. Some example papers here include [DINOv2](dinov2) and [MAE](vit_mae).\n\nThe authors of DINOv2 noticed that ViTs have artifacts in attention maps. It’s due to the model using some image patches as “registers”. The authors propose a fix: just add some new tokens (called \"register\" tokens), which you only use during pre-training (and throw away afterwards). This results in:\n- no artifacts\n- interpretable attention maps\n- and improved performances.\n\nThe abstract from the paper is the following:",
  "*Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dinov2_with_registers_visualization.png\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Visualization of attention maps of various models trained with vs. without registers. Taken from the <a href=\"https://arxiv.org/abs/2309.16588\">original paper</a>. </small>\n\nTips:\n\n- Usage of DINOv2 with Registers is identical to DINOv2 without, you'll just get better performance.\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/facebookresearch/dinov2).\n\n\n## Dinov2WithRegistersConfig\n\n[[autodoc]] Dinov2WithRegistersConfig\n\n## Dinov2WithRegistersModel\n\n[[autodoc]] Dinov2WithRegistersModel\n- forward\n\n## Dinov2WithRegistersForImageClassification\n\n[[autodoc]] Dinov2WithRegistersForImageClassification\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ViTDet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ViTDet model was proposed in [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.\nVitDet leverages the plain [Vision Transformer](vit) for the task of object detection.\n\nThe abstract from the paper is the following:",
  "*We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet).\n\nTips:",
  "- At the moment, only the backbone is available.\n\n## VitDetConfig\n\n[[autodoc]] VitDetConfig\n\n## VitDetModel\n\n[[autodoc]] VitDetModel\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Speech2Text\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The Speech2Text model was proposed in [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It's a\ntransformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech\nTranslation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are\nfed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the\ntranscripts/translations autoregressively. Speech2Text has been fine-tuned on several datasets for ASR and ST:\n[LibriSpeech](http://www.openslr.org/12), [CoVoST 2](https://github.com/facebookresearch/covost), [MuST-C](https://ict.fbk.eu/must-c/).\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text).\n\n## Inference\n\nSpeech2Text is a speech model that accepts a float tensor of log-mel filter-bank features extracted from the speech",
  "signal. It's a transformer-based seq2seq model, so the transcripts/translations are generated autoregressively. The\n`generate()` method can be used for inference.\n\nThe [`Speech2TextFeatureExtractor`] class is responsible for extracting the log-mel filter-bank\nfeatures. The [`Speech2TextProcessor`] wraps [`Speech2TextFeatureExtractor`] and\n[`Speech2TextTokenizer`] into a single instance to both extract the input features and decode the\npredicted token ids.\n\nThe feature extractor depends on `torchaudio` and the tokenizer depends on `sentencepiece` so be sure to\ninstall those packages before running the examples. You could either install those as extra speech dependencies with\n`pip install transformers\"[speech, sentencepiece]\"` or install the packages separately with `pip install torchaudio sentencepiece`. Also `torchaudio` requires the development version of the [libsndfile](http://www.mega-nerd.com/libsndfile/) package which can be installed via a system package manager. On Ubuntu it can\nbe installed as follows: `apt install libsndfile1-dev`\n\n- ASR and Speech Translation\n\n```python\n>>> import torch",
  ">>> from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n>>> processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n>>> generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n\n>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> transcription\n['mister quilter is the apostle of the middle classes and we are glad to welcome his gospel']\n```\n\n- Multilingual speech translation\n\nFor multilingual speech translation models, `eos_token_id` is used as the `decoder_start_token_id` and\nthe target language id is forced as the first generated token. To force the target language id as the first\ngenerated token, pass the `forced_bos_token_id` parameter to the `generate()` method. The following",
  "example shows how to translate English speech to French text using the *facebook/s2t-medium-mustc-multilingual-st*\ncheckpoint.\n\n```python\n>>> import torch\n>>> from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-medium-mustc-multilingual-st\")\n>>> processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-medium-mustc-multilingual-st\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n\n>>> inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n>>> generated_ids = model.generate(\n...     inputs[\"input_features\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     forced_bos_token_id=processor.tokenizer.lang_code_to_id[\"fr\"],\n... )\n\n>>> translation = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> translation\n[\"(Vidéo) Si M. Kilder est l'apossible des classes moyennes, et nous sommes heureux d'être accueillis dans son évangile.\"]\n```",
  "See the [model hub](https://huggingface.co/models?filter=speech_to_text) to look for Speech2Text checkpoints.\n\n## Speech2TextConfig\n\n[[autodoc]] Speech2TextConfig\n\n## Speech2TextTokenizer\n\n[[autodoc]] Speech2TextTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## Speech2TextFeatureExtractor\n\n[[autodoc]] Speech2TextFeatureExtractor\n- __call__\n\n## Speech2TextProcessor\n\n[[autodoc]] Speech2TextProcessor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n\n<frameworkcontent>\n<pt>\n\n## Speech2TextModel\n\n[[autodoc]] Speech2TextModel\n- forward\n\n## Speech2TextForConditionalGeneration\n\n[[autodoc]] Speech2TextForConditionalGeneration\n- forward\n\n</pt>\n<tf>\n\n## TFSpeech2TextModel\n\n[[autodoc]] TFSpeech2TextModel\n- call\n\n## TFSpeech2TextForConditionalGeneration\n\n[[autodoc]] TFSpeech2TextForConditionalGeneration\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 Kyutai and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Helium\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "Helium was proposed in [Announcing Helium-1 Preview](https://kyutai.org/2025/01/13/helium.html) by the Kyutai Team.\n\n\nHelium-1 preview is a lightweight language model with 2B parameters, targeting edge and mobile devices.\nIt supports the following languages: English, French, German, Italian, Portuguese, Spanish.\n\n- **Developed by:** Kyutai\n- **Model type:** Large Language Model\n- **Language(s) (NLP):** English, French, German, Italian, Portuguese, Spanish\n- **License:** CC-BY 4.0\n\n\n\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\nThe model was evaluated on MMLU, TriviaQA, NaturalQuestions, ARC Easy & Challenge, Open Book QA, Common Sense QA,\nPhysical Interaction QA, Social Interaction QA, HellaSwag, WinoGrande, Multilingual Knowledge QA, FLORES 200.\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\nWe report accuracy on MMLU, ARC, OBQA, CSQA, PIQA, SIQA, HellaSwag, WinoGrande.\nWe report exact match on TriviaQA, NQ and MKQA.\nWe report BLEU on FLORES.\n\n### English Results",
  "| Benchmark | Helium-1 Preview | HF SmolLM2 (1.7B) | Gemma-2 (2.6B) | Llama-3.2 (3B) | Qwen2.5 (1.5B) |\n|--------------|--------|--------|--------|--------|--------|\n| | | | | | |\n| MMLU | 51.2 | 50.4 | 53.1 | 56.6 | 61.0 |\n| NQ   | 17.3 | 15.1 | 17.7 | 22.0 | 13.1 |\n| TQA  | 47.9 | 45.4 | 49.9 | 53.6 | 35.9 |\n| ARC E | 80.9 | 81.8 | 81.1 | 84.6 | 89.7 |\n| ARC C | 62.7 | 64.7 | 66.0 | 69.0 | 77.2 |\n| OBQA | 63.8 | 61.4 | 64.6 | 68.4 | 73.8 |\n| CSQA | 65.6 | 59.0 | 64.4 | 65.4 | 72.4 |\n| PIQA | 77.4 | 77.7 | 79.8 | 78.9 | 76.0 |\n| SIQA | 64.4 | 57.5 | 61.9 | 63.8 | 68.7 |\n| HS | 69.7 | 73.2 | 74.7 | 76.9 | 67.5 |\n| WG | 66.5 | 65.6 | 71.2 | 72.0 | 64.8 |\n| | | | | | |\n| Average | 60.7 | 59.3 | 62.2 | 64.7 | 63.6 |\n\n#### Multilingual Results\n\n| Language | Benchmark | Helium-1 Preview | HF SmolLM2 (1.7B) | Gemma-2 (2.6B) | Llama-3.2 (3B) | Qwen2.5 (1.5B) |\n|-----|--------------|--------|--------|--------|--------|--------|\n| | | | | | | |\n|German| MMLU | 45.6 | 35.3 | 45.0 | 47.5 | 49.5 |\n|| ARC C | 56.7 | 38.4 | 54.7 | 58.3 | 60.2 |\n|| HS | 53.5 | 33.9 | 53.4 | 53.7 | 42.8 |\n|| MKQA | 16.1 | 7.1 | 18.9 | 20.2 | 10.4 |\n| | | | | | | |",
  "|Spanish| MMLU | 46.5 | 38.9 | 46.2 | 49.6 | 52.8 |\n|| ARC C | 58.3 | 43.2 | 58.8 | 60.0 | 68.1 |\n|| HS | 58.6 | 40.8 | 60.5 | 61.1 | 51.4 |\n|| MKQA | 16.0 | 7.9 | 18.5 | 20.6 | 10.6 |\n\n\n## Technical Specifications\n\n### Model Architecture and Objective\n\n| Hyperparameter | Value |\n|--------------|--------|\n| Layers | 24 |\n| Heads  | 20 |\n| Model dimension | 2560 |\n| MLP dimension | 7040 |\n| Context size | 4096 |\n| Theta RoPE | 100,000 |\n\nTips:\n\n- This model was contributed by [Laurent Mazare](https://huggingface.co/lmz)\n\n\n## Usage tips\n\n`Helium` can be found on the [Huggingface Hub](https://huggingface.co/models?other=helium)\n\nIn the following, we demonstrate how to use `helium-1-preview` for the inference.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"kyutai/helium-1-preview-2b\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"kyutai/helium-1-preview-2b\")\n\n>>> prompt = \"Give me a short introduction to large language model.\"\n\n>>> model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)",
  ">>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n\n>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n\n>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## HeliumConfig\n\n[[autodoc]] HeliumConfig\n\n## HeliumModel\n\n[[autodoc]] HeliumModel\n- forward\n\n## HeliumForCausalLM\n\n[[autodoc]] HeliumForCausalLM\n- forward\n\n## HeliumForSequenceClassification\n\n[[autodoc]] HeliumForSequenceClassification\n- forward\n\n## HeliumForTokenClassification\n\n[[autodoc]] HeliumForTokenClassification\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Autoformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.",
  "This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\n\nThe abstract from the paper is the following:",
  "*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*",
  "This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).\nThe original code can be found [here](https://github.com/thuml/Autoformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)\n\n## AutoformerConfig\n\n[[autodoc]] AutoformerConfig\n\n## AutoformerModel\n\n[[autodoc]] AutoformerModel\n- forward\n\n## AutoformerForPrediction\n\n[[autodoc]] AutoformerForPrediction\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLIPSeg\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke",
  "and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero-shot and one-shot image segmentation.\n\nThe abstract from the paper is the following:\n\n*Image segmentation is usually addressed by training a\nmodel for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive\nas it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system\nthat can generate image segmentations based on arbitrary\nprompts at test time. A prompt can be either a text or an\nimage. This approach enables us to create a unified model\n(trained once) for three common segmentation tasks, which\ncome with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.\nWe build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense\nprediction. After training on an extended version of the\nPhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on",
  "an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail.\nThis novel hybrid input allows for dynamic adaptation not\nonly to the three segmentation tasks mentioned above, but\nto any binary segmentation task where a text or image query\ncan be formulated. Finally, we find our system to adapt well\nto generalized queries involving affordances or properties*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text",
  "(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-segmentation\"/>\n\n- A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb).\n\n## CLIPSegConfig\n\n[[autodoc]] CLIPSegConfig\n- from_text_vision_configs\n\n## CLIPSegTextConfig\n\n[[autodoc]] CLIPSegTextConfig\n\n## CLIPSegVisionConfig\n\n[[autodoc]] CLIPSegVisionConfig\n\n## CLIPSegProcessor\n\n[[autodoc]] CLIPSegProcessor\n\n## CLIPSegModel\n\n[[autodoc]] CLIPSegModel\n- forward\n- get_text_features\n- get_image_features\n\n## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n- forward",
  "## CLIPSegVisionModel\n\n[[autodoc]] CLIPSegVisionModel\n- forward\n\n## CLIPSegForImageSegmentation\n\n[[autodoc]] CLIPSegForImageSegmentation\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Conditional DETR\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Conditional DETR model was proposed in [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang. Conditional DETR presents a conditional cross-attention mechanism for fast DETR training. Conditional DETR converges 6.7× to 10× faster than DETR.\n\nThe abstract from the paper is the following:",
  "*The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box regression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show that conditional DETR converges 6.7× faster for the backbones R50 and R101 and 10× faster for stronger backbones DC5-R50 and DC5-R101. Code is available at https://github.com/Atten4Vis/ConditionalDETR.*",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/conditional_detr_curve.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Conditional DETR shows much faster convergence compared to the original DETR. Taken from the <a href=\"https://arxiv.org/abs/2108.06152\">original paper</a>.</small>\n\nThis model was contributed by [DepuMeng](https://huggingface.co/DepuMeng). The original code can be found [here](https://github.com/Atten4Vis/ConditionalDETR).\n\n## Resources\n\n- Scripts for finetuning [`ConditionalDetrForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n- See also: [Object detection task guide](../tasks/object_detection).\n\n## ConditionalDetrConfig\n\n[[autodoc]] ConditionalDetrConfig\n\n## ConditionalDetrImageProcessor\n\n[[autodoc]] ConditionalDetrImageProcessor\n- preprocess\n- post_process_object_detection\n- post_process_instance_segmentation\n- post_process_semantic_segmentation\n- post_process_panoptic_segmentation\n\n## ConditionalDetrFeatureExtractor",
  "[[autodoc]] ConditionalDetrFeatureExtractor\n- __call__\n- post_process_object_detection\n- post_process_instance_segmentation\n- post_process_semantic_segmentation\n- post_process_panoptic_segmentation\n\n## ConditionalDetrModel\n\n[[autodoc]] ConditionalDetrModel\n- forward\n\n## ConditionalDetrForObjectDetection\n\n[[autodoc]] ConditionalDetrForObjectDetection\n- forward\n\n## ConditionalDetrForSegmentation\n\n[[autodoc]] ConditionalDetrForSegmentation\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# VisualBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe VisualBERT model was proposed in [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.",
  "VisualBERT is a neural network trained on a variety of (image, text) pairs.\n\nThe abstract from the paper is the following:\n\n*We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks.\nVisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an\nassociated input image with self-attention. We further propose two visually-grounded language model objectives for\npre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2,\nand Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly\nsimpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any\nexplicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between\nverbs and image regions corresponding to their arguments.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/uclanlp/visualbert).\n\n## Usage tips",
  "1. Most of the checkpoints provided work with the [`VisualBertForPreTraining`] configuration. Other\ncheckpoints provided are the fine-tuned checkpoints for down-stream tasks - VQA ('visualbert-vqa'), VCR\n('visualbert-vcr'), NLVR2 ('visualbert-nlvr2'). Hence, if you are not working on these downstream tasks, it is\nrecommended that you use the pretrained checkpoints.\n\n2. For the VCR task, the authors use a fine-tuned detector for generating visual embeddings, for all the checkpoints.\nWe do not provide the detector and its weights as a part of the package, but it will be available in the research\nprojects, and the states can be loaded directly into the detector provided.\n\nVisualBERT is a multi-modal vision and language model. It can be used for visual question answering, multiple choice,\nvisual reasoning and region-to-phrase correspondence tasks. VisualBERT uses a BERT-like transformer to prepare\nembeddings for image-text pairs. Both the text and visual features are then projected to a latent space with identical\ndimension.\n\nTo feed images to the model, each image is passed through a pre-trained object detector and the regions and the",
  "bounding boxes are extracted. The authors use the features generated after passing these regions through a pre-trained\nCNN like ResNet as visual embeddings. They also add absolute position embeddings, and feed the resulting sequence of\nvectors to a standard BERT model. The text input is concatenated in the front of the visual embeddings in the embedding\nlayer, and is expected to be bound by [CLS] and a [SEP] tokens, as in BERT. The segment IDs must also be set\nappropriately for the textual and visual parts.\n\nThe [`BertTokenizer`] is used to encode the text. A custom detector/image processor must be used\nto get the visual embeddings. The following example notebooks show how to use VisualBERT with Detectron-like models:\n\n- [VisualBERT VQA demo notebook](https://github.com/huggingface/transformers-research-projects/tree/main/visual_bert) : This notebook\ncontains an example on VisualBERT VQA.\n\n- [Generate Embeddings for VisualBERT (Colab Notebook)](https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing) : This notebook contains\nan example on how to generate visual embeddings.",
  "The following example shows how to get the last hidden state using [`VisualBertModel`]:\n\n```python\n>>> import torch\n>>> from transformers import BertTokenizer, VisualBertModel\n\n>>> model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\n>>> inputs = tokenizer(\"What is the man eating?\", return_tensors=\"pt\")\n>>> # this is a custom function that returns the visual embeddings given the image path\n>>> visual_embeds = get_visual_embeddings(image_path)\n\n>>> visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n>>> visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n>>> inputs.update(\n...     {\n...         \"visual_embeds\": visual_embeds,\n...         \"visual_token_type_ids\": visual_token_type_ids,\n...         \"visual_attention_mask\": visual_attention_mask,\n...     }\n... )\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n```\n\n## VisualBertConfig\n\n[[autodoc]] VisualBertConfig\n\n## VisualBertModel\n\n[[autodoc]] VisualBertModel\n- forward\n\n## VisualBertForPreTraining\n\n[[autodoc]] VisualBertForPreTraining",
  "- forward\n\n## VisualBertForQuestionAnswering\n\n[[autodoc]] VisualBertForQuestionAnswering\n- forward\n\n## VisualBertForMultipleChoice\n\n[[autodoc]] VisualBertForMultipleChoice\n- forward\n\n## VisualBertForVisualReasoning\n\n[[autodoc]] VisualBertForVisualReasoning\n- forward\n\n## VisualBertForRegionToPhraseAlignment\n\n[[autodoc]] VisualBertForRegionToPhraseAlignment\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BigBirdPegasus\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\nZaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,",
  "Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\nbased transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse\nattention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\nhas been shown that applying sparse, global, and random attention approximates full attention, while being\ncomputationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\nBigBird has shown improved performance on various long document NLP tasks, such as question answering and\nsummarization, compared to BERT or RoBERTa.\n\nThe abstract from the paper is the following:\n\n*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.\nUnfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\nlength due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that",
  "reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and\nis Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\ntheoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\nsequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\n8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,\nBigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\npropose novel applications to genomics data.*\n\nThe original code can be found [here](https://github.com/google-research/bigbird).\n\n## Usage tips\n\n- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\n- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\n**original_full** is advised as there is no benefit in using **block_sparse** attention.",
  "- The code currently uses window size of 3 blocks and 2 global blocks.\n- Sequence length must be divisible by block size.\n- Current implementation supports only **ITC**.\n- Current implementation doesn't support **num_random_blocks = 0**.\n- BigBirdPegasus uses the [PegasusTokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/pegasus/tokenization_pegasus.py).\n- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## BigBirdPegasusConfig\n\n[[autodoc]] BigBirdPegasusConfig\n- all\n\n## BigBirdPegasusModel\n\n[[autodoc]] BigBirdPegasusModel\n- forward\n\n## BigBirdPegasusForConditionalGeneration\n\n[[autodoc]] BigBirdPegasusForConditionalGeneration\n- forward\n\n## BigBirdPegasusForSequenceClassification\n\n[[autodoc]] BigBirdPegasusForSequenceClassification\n- forward",
  "## BigBirdPegasusForQuestionAnswering\n\n[[autodoc]] BigBirdPegasusForQuestionAnswering\n- forward\n\n## BigBirdPegasusForCausalLM\n\n[[autodoc]] BigBirdPegasusForCausalLM\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# EfficientNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)",
  "by Mingxing Tan and Quoc V. Le. EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models.\n\nThe abstract from the paper is the following:\n\n*Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.",
  "To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.*\n\nThis model was contributed by [adirik](https://huggingface.co/adirik).\nThe original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet).\n\n\n## EfficientNetConfig\n\n[[autodoc]] EfficientNetConfig\n\n## EfficientNetImageProcessor\n\n[[autodoc]] EfficientNetImageProcessor\n- preprocess\n\n## EfficientNetModel\n\n[[autodoc]] EfficientNetModel\n- forward\n\n## EfficientNetForImageClassification\n\n[[autodoc]] EfficientNetForImageClassification\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FLAN-UL2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nFlan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the [UL2](ul2) model released earlier last year.\nIt was fine tuned using the \"Flan\" prompt tuning and dataset collection. Similar to `Flan-T5`,  one can directly use FLAN-UL2 weights without finetuning the model:\n\nAccording to the original blog here are the notable improvements:\n\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large.\n- The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning.\n- The original UL2 model also had mode switch tokens that was rather mandatory to get good performance. However, they were a little cumbersome as this requires often some changes during inference or finetuning. In this update/change, we continue training UL2 20B for an additional 100k steps (with small batch) to forget “mode tokens” before applying Flan instruction tuning. This Flan-UL2 checkpoint does not require mode tokens anymore.\nGoogle has released the following variants:",
  "The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints).\n\n\n## Running on low resource devices\n\nThe model is pretty heavy (~40GB in half precision) so if you just want to run the model, make sure you load your model in 8bit, and use `device_map=\"auto\"` to make sure  you don't have any OOM issue!\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", load_in_8bit=True, device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n\n>>> inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['In a large skillet, brown the ground beef and onion over medium heat. Add the garlic']\n```\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for API reference, tips, code examples and notebooks.\n\n</Tip>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\nLicense. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\nspecific language governing permissions and limitations under the License. -->\n\n# Nougat\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe Nougat model was proposed in [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) by\nLukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. Nougat uses the same architecture as [Donut](donut), meaning an image Transformer\nencoder and an autoregressive text Transformer decoder to translate scientific PDFs to markdown, enabling easier access to them.\n\nThe abstract from the paper is the following:",
  "*Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/nougat_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Nougat high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2308.13418\">original paper</a>. </small>",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n[here](https://github.com/facebookresearch/nougat).\n\n## Usage tips\n\n- The quickest way to get started with Nougat is by checking the [tutorial\nnotebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Nougat), which show how to use the model\nat inference time as well as fine-tuning on custom data.\n- Nougat is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework. The model is identical to [Donut](donut) in terms of architecture.\n\n## Inference\n\nNougat's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.\n\nThe [`NougatImageProcessor`] class is responsible for preprocessing the input image and\n[`NougatTokenizerFast`] decodes the generated target tokens to the target string. The\n[`NougatProcessor`] wraps [`NougatImageProcessor`] and [`NougatTokenizerFast`] classes\ninto a single instance to both extract the input features and decode the predicted token ids.\n\n- Step-by-step PDF transcription\n\n```py",
  ">>> from huggingface_hub import hf_hub_download\n>>> import re\n>>> from PIL import Image\n\n>>> from transformers import NougatProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE_RESULT\n\n>>> # prepare PDF image for the model\n>>> filepath = hf_hub_download(repo_id=\"hf-internal-testing/fixtures_docvqa\", filename=\"nougat_paper.png\", repo_type=\"dataset\")\n>>> image = Image.open(filepath)\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> # generate transcription (here we only generate 30 tokens)\n>>> outputs = model.generate(\n...     pixel_values.to(device),\n...     min_length=1,\n...     max_new_tokens=30,\n...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n... )\n\n>>> sequence = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n>>> sequence = processor.post_process_generation(sequence, fix_markdown=False)",
  ">>> # note: we're using repr here such for the sake of printing the \\n characters, feel free to just print the sequence\n>>> print(repr(sequence))\n'\\n\\n# Nougat: Neural Optical Understanding for Academic Documents\\n\\n Lukas Blecher\\n\\nCorrespondence to: lblecher@'\n```\n\nSee the [model hub](https://huggingface.co/models?filter=nougat) to look for Nougat checkpoints.\n\n<Tip>\n\nThe model is identical to [Donut](donut) in terms of architecture.\n\n</Tip>\n\n## NougatImageProcessor\n\n[[autodoc]] NougatImageProcessor\n- preprocess\n\n## NougatTokenizerFast\n\n[[autodoc]] NougatTokenizerFast\n\n## NougatProcessor\n\n[[autodoc]] NougatProcessor\n- __call__\n- from_pretrained\n- save_pretrained\n- batch_decode\n- decode\n- post_process_generation",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LLaVa\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. In other words, it is an multi-modal version of LLMs fine-tuned for chat / instructions.\n\nThe LLaVa model was proposed in [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744) by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.\n\nThe abstract from the paper is the following:",
  "*Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> LLaVa architecture. Taken from the <a href=\"https://arxiv.org/abs/2304.08485\">original paper.</a> </small>\n\nThis model was contributed by [ArthurZ](https://huggingface.co/ArthurZ) and [ybelkada](https://huggingface.co/ybelkada).",
  "The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main/llava).\n\n## Usage tips\n\n- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n\n- Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n\n\n> [!NOTE]\n> LLaVA models after release v4.46 will raise warnings about adding `processor.patch_size = {{patch_size}}`, `processor.num_additional_image_tokens = {{num_additional_image_tokens}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you.",
  "Adding these attributes means that LLaVA will try to infer the number of image tokens required per image and expand the text with as many `<image>` placeholders as there will be tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\nThe attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n\n\n### Formatting Prompts with Chat Templates\n\nEach **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.\n\n**Important:**\n- You must construct a conversation history — passing a plain string won't work.\n- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.\n- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.\n\n\nHere’s an example of how to structure your input.",
  "We will use [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n\n\n```python\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What’s shown in this image?\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n},\n{\n\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe the image in more details.\"},\n],\n},\n]\n\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\n# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images\nprint(text_prompt)\n>>> \"USER: <image>\\n<What’s shown in this image? ASSISTANT: This image shows a red stop sign.</s>USER: Describe the image in more details. ASSISTANT:\"\n```\n\n- If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by each llava checkpoint:",
  "[llava-interleave models](https://huggingface.co/collections/llava-hf/llava-interleave-668e19a97da0036aad4a2f19) requires the following format:\n```bash\n\"<|im_start|>user <image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\"\n```\n\nFor multiple turns conversation:\n\n```bash\n\"<|im_start|>user <image>\\n<prompt1><|im_end|><|im_start|>assistant <answer1><|im_end|><|im_start|>user <image>\\n<prompt1><|im_end|><|im_start|>assistant \"\n```\n\n[llava-1.5 models](https://huggingface.co/collections/llava-hf/llava-15-65f762d5b6941db5c2ba07e0) requires the following format:\n```bash\n\"USER: <image>\\n<prompt> ASSISTANT:\"\n```\n\nFor multiple turns conversation:\n\n```bash\n\"USER: <image>\\n<prompt1> ASSISTANT: <answer1></s>USER: <prompt2> ASSISTANT: <answer2></s>USER: <prompt3> ASSISTANT:\"\n```\n\n🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n\n\n## Usage examples\n\n### Single input inference\n\n\n```python\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\n# Load the model in half-precision",
  "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\n\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\n\n# Generate\ngenerate_ids = model.generate(**inputs, max_new_tokens=30)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True)\n```\n\n\n### Batched inference\n\nLLaVa also supports batched inference. Here is how you can do it:\n\n```python\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\n# Load the model in half-precision\nmodel = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n\n\n# Prepare a batch of two prompts\nconversation_1 = [\n{\n\"role\": \"user\",",
  "\"content\": [\n{\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\n\nconversation_2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\n\ninputs = processor.apply_chat_template(\n[conversation_1, conversation_2],\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\npadding=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\n\n\n# Generate\ngenerate_ids = model.generate(**inputs, max_new_tokens=30)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True)\n```\n\n\n## Note regarding reproducing original implementation\n\nIn order to match the logits of the [original implementation](https://github.com/haotian-liu/LLaVA/tree/main), one needs to additionally specify `do_pad=True` when instantiating `LLavaImageProcessor`:\n\n```python\nfrom transformers import LLavaImageProcessor\n\nimage_processor = LLavaImageProcessor.from_pretrained(\"https://huggingface.co/llava-hf/llava-1.5-7b-hf\", do_pad=True)\n```\n\n### Using Flash Attention 2",
  "Flash Attention 2 is an even faster, optimized version of the previous optimization, please refer to the [Flash Attention 2 section of performance docs](https://huggingface.co/docs/transformers/perf_infer_gpu_one).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BEiT.\n\n<PipelineTag pipeline=\"image-to-text\"/>\n\n- A [Google Colab demo](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing) on how to run Llava on a free-tier Google colab instance leveraging 4-bit inference.\n- A [similar notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb) showcasing batched inference. 🌎\n\n\n## LlavaConfig\n\n[[autodoc]] LlavaConfig\n\n## LlavaImageProcessor\n\n[[autodoc]] LlavaImageProcessor\n- preprocess\n\n## LlavaImageProcessorFast\n\n[[autodoc]] LlavaImageProcessorFast\n- preprocess\n\n## LlavaProcessor\n\n[[autodoc]] LlavaProcessor\n\n## LlavaForConditionalGeneration\n\n[[autodoc]] LlavaForConditionalGeneration\n- forward",
  "<!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MegatronGPT2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe MegatronGPT2 model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\nParallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper and Bryan Catanzaro.\n\nThe abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in\nNatural Language Processing applications. However, very large models can be quite difficult to train due to memory\nconstraints. In this work, we present our techniques for training very large transformer models and implement a simple,\nefficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our\napproach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We\nillustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain",
  "15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline\nthat sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance\nthe state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9\nbillion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in\nBERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we\nachieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA\naccuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy\nof 89.4%).*\n\nThis model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM).\nThat repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, it",
  "contains a hybrid model parallel approach using \"tensor parallel\" and \"pipeline parallel\" techniques.\n\n## Usage tips\n\nWe have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m) checkpoints\nfor use to evaluate or finetuning downstream tasks.\n\nTo access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)\nRegistry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).\n\nAlternatively, you can directly download the checkpoints using:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O\nmegatron_gpt2_345m_v0_0.zip\n```\n\nOnce you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will easily\nbe loaded by Hugging Face Transformers GPT2 implementation.\n\nThe following command allows you to do the conversion. We assume that the folder `models/megatron_gpt2` contains\n`megatron_gpt2_345m_v0_0.zip` and that the command is run from that folder:\n\n```bash",
  "python3 $PATH_TO_TRANSFORMERS/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip\n```\n\n<Tip>\n\nMegatronGPT2 architecture is the same as OpenAI GPT-2 . Refer to [GPT-2 documentation](gpt2) for information on\nconfiguration classes and their parameters.\n\n</Tip>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nCopyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n-->\n\n# Nemotron\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n### License",
  "The use of this model is governed by the [NVIDIA AI Foundation Models Community License Agreement](https://developer.nvidia.com/downloads/nv-ai-foundation-models-license).\n\n### Description\n\nNemotron-4 is a family of enterprise ready generative text models compatible with [NVIDIA NeMo Framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/).\n\nNVIDIA NeMo is an end-to-end, cloud-native platform to build, customize, and deploy generative AI models anywhere. It includes training and inferencing frameworks, guardrailing toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI. To get access to NeMo Framework, please sign up at [this link](https://developer.nvidia.com/nemo-framework/join).\n\n### References\n\n[Announcement Blog](https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/)\n\n### Model Architecture\n\n**Architecture Type:** Transformer\n\n**Network Architecture:** Transformer Decoder (auto-regressive language model).\n\n## Minitron\n\n### Minitron 4B Base",
  "Minitron is a family of small language models (SLMs) obtained by pruning NVIDIA's [Nemotron-4 15B](https://arxiv.org/abs/2402.16819) model. We prune model embedding size, attention heads, and MLP intermediate dimension, following which, we perform continued training with distillation to arrive at the final models.\n\nDeriving the Minitron 8B and 4B models from the base 15B model using our approach requires up to **40x fewer training tokens** per model compared to training from scratch; this results in **compute cost savings of 1.8x** for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. Please refer to our [arXiv paper](https://arxiv.org/abs/2407.14679) for more details.\n\nMinitron models are for research and development only.\n\n### HuggingFace Quickstart\n\nThe following code provides an example of how to load the Minitron-4B model and use it to perform text generation.\n\n```python\nimport torch",
  "from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the tokenizer and model\nmodel_path = 'nvidia/Minitron-4B-Base'\ntokenizer  = AutoTokenizer.from_pretrained(model_path)\n\ndevice = 'cuda'\ndtype  = torch.bfloat16\nmodel  = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)\n\n# Prepare the input text\nprompt = 'Complete the paragraph: our solar system is'\ninputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n\n# Generate the output\noutputs = model.generate(inputs, max_length=20)\n\n# Decode and print the output\noutput_text = tokenizer.decode(outputs[0])\nprint(output_text)\n```\n\n### License\n\nMinitron is released under the [NVIDIA Open Model License Agreement](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\n\n### Evaluation Results\n\n*5-shot performance.* Language Understanding evaluated using [Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300):\n\n| Average |\n| :---- |\n| 58.6 |\n\n*Zero-shot performance.* Evaluated using select datasets from the [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) with additions:",
  "| HellaSwag | Winogrande | GSM8K| ARC-C | XLSum |\n| :------------- | :------------- | :------------- | :------------- | :------------- |\n| 75.0 | 74.0 | 24.1  | 50.9 | 29.5\n\n\n*Code generation performance*. Evaluated using [HumanEval](https://github.com/openai/human-eval):\n\n| p@1, 0-Shot |\n| :------------- |\n| 23.3 |\n\nPlease refer to our [paper](https://arxiv.org/abs/2407.14679) for the full set of results.\n\n### Citation\n\nIf you find our work helpful, please consider citing our paper:\n```\n@article{minitron2024,\ntitle={Compact Language Models via Pruning and Knowledge Distillation},\nauthor={Saurav Muralidharan and Sharath Turuvekere Sreenivas and Raviraj Joshi and Marcin Chochowski and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro and Jan Kautz and Pavlo Molchanov},\njournal={arXiv preprint arXiv:2407.14679},\nyear={2024},\nurl={https://arxiv.org/abs/2407.14679},\n}\n```\n\n## NemotronConfig\n\n[[autodoc]] NemotronConfig\n\n\n## NemotronModel\n\n[[autodoc]] NemotronModel\n- forward\n\n\n## NemotronForCausalLM\n\n[[autodoc]] NemotronForCausalLM\n- forward\n\n## NemotronForSequenceClassification\n\n[[autodoc]] NemotronForSequenceClassification\n- forward\n\n\n## NemotronForQuestionAnswering",
  "[[autodoc]] NemotronForQuestionAnswering\n- forward\n\n\n## NemotronForTokenClassification\n\n[[autodoc]] NemotronForTokenClassification\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe OPT model was proposed in [Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068) by Meta AI.\nOPT is a series of open-sourced large causal language models which perform similar in performance to GPT3.\n\nThe abstract from the paper is the following:",
  "*Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.*\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Younes Belkada](https://huggingface.co/ybelkada), and [Patrick Von Platen](https://huggingface.co/patrickvonplaten).\nThe original code can be found [here](https://github.com/facebookresearch/metaseq).\n\nTips:\n- OPT has the same architecture as [`BartDecoder`].",
  "- Contrary to GPT2, OPT adds the EOS token `</s>` to the beginning of every prompt.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with OPT. If you're\ninterested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.\nThe resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-generation\" />\n\n- A notebook on [fine-tuning OPT with PEFT, bitsandbytes, and Transformers](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing). 🌎\n- A blog post on [decoding strategies with OPT](https://huggingface.co/blog/introducing-csearch#62-example-two---opt).\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the 🤗 Hugging Face Course.",
  "- [`OPTForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFOPTForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxOPTForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling).\n\n<PipelineTag pipeline=\"text-classification\" />\n\n- [Text classification task guide](sequence_classification.md)",
  "- [`OPTForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).\n\n<PipelineTag pipeline=\"question-answering\" />\n\n- [`OPTForQuestionAnswering`] is supported by this [question answering example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter\nof the 🤗 Hugging Face Course.\n\n⚡️ Inference\n\n- A blog post on [How 🤗 Accelerate runs very large models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models) with OPT.\n\n\n## Combining OPT and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```",
  "Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import OPTForCausalLM, GPT2Tokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n>>> tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n\n>>> prompt = (\"A chat between a curious human and the Statue of Liberty.\\n\\nHuman: What is your name?\\nStatue: I am the \"\n\"Statue of Liberty.\\nHuman: Where do you live?\\nStatue: New York City.\\nHuman: How long have you lived \"\n\"there?\")\n\n>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n>>> model.to(device)\n\n>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n>>> tokenizer.batch_decode(generated_ids)[0]",
  "'</s>A chat between a curious human and the Statue of Liberty.\\n\\nHuman: What is your name?\\nStatue: I am the Statue of Liberty.\\nHuman: Where do you live?\\nStatue: New York City.\\nHuman: How long have you lived there?\\nStatue: I have lived here for about a year.\\nHuman: What is your favorite place to eat?\\nStatue: I love'\n```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two different sequence lengths.\n\n<div style=\"text-align: center\">\n<img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\">\n</div>\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `facebook/opt-350m` checkpoint and the Flash Attention 2 version of the model using two different sequence lengths.\n\n<div style=\"text-align: center\">\n<img src=\"https://user-images.githubusercontent.com/49240599/281101682-d1144e90-0dbc-46f4-8fc8-c6206cb793c9.png\">\n</div>\n\n\n### Using Scaled Dot Product Attention (SDPA)",
  "PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```python\nfrom transformers import OPTForCausalLM\nmodel = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (L40S-45GB, PyTorch 2.4.0, OS Debian GNU/Linux 11) using `float16` with\n[facebook/opt-350m](https://huggingface.co/facebook/opt-350m), we saw the",
  "following speedups during training and inference.\n\n### Training\n\n|    batch_size |    seq_len |  Time per batch (eager - s)   |    Time per batch (sdpa - s) |  Speedup (%)   |  Eager peak mem (MB)   |    sdpa peak mem (MB) |  Mem saving (%)   |\n|--------------:|-----------:|:------------------------------|-----------------------------:|:---------------|:-----------------------|----------------------:|:------------------|\n|             1 |        128 | 0.047                         |                        0.037 | 26.360         | 1474.611               |               1474.32 | 0.019             |\n|             1 |        256 | 0.046                         |                        0.037 | 24.335         | 1498.541               |               1499.49 | -0.063            |\n|             1 |        512 | 0.046                         |                        0.037 | 24.959         | 1973.544               |               1551.35 | 27.215            |\n|             1 |       1024 | 0.062                         |                        0.038 | 65.135         | 4867.113               |               1698.35 | 186.578           |",
  "|             1 |       2048 | 0.230                         |                        0.039 | 483.933        | 15662.224              |               2715.75 | 476.718           |\n|             2 |        128 | 0.045                         |                        0.037 | 20.455         | 1498.164               |               1499.49 | -0.089            |\n|             2 |        256 | 0.046                         |                        0.037 | 24.027         | 1569.367               |               1551.35 | 1.161             |\n|             2 |        512 | 0.045                         |                        0.037 | 20.965         | 3257.074               |               1698.35 | 91.778            |\n|             2 |       1024 | 0.122                         |                        0.038 | 225.958        | 9054.405               |               2715.75 | 233.403           |\n|             2 |       2048 | 0.464                         |                        0.067 | 593.646        | 30572.058              |               4750.55 | 543.548           |",
  "|             4 |        128 | 0.045                         |                        0.037 | 21.918         | 1549.448               |               1551.35 | -0.123            |\n|             4 |        256 | 0.044                         |                        0.038 | 18.084         | 2451.768               |               1698.35 | 44.361            |\n|             4 |        512 | 0.069                         |                        0.037 | 84.421         | 5833.180               |               2715.75 | 114.791           |\n|             4 |       1024 | 0.262                         |                        0.062 | 319.475        | 17427.842              |               4750.55 | 266.860           |\n|             4 |       2048 | OOM                           |                        0.062 | Eager OOM      | OOM                    |               4750.55 | Eager OOM         |\n|             8 |        128 | 0.044                         |                        0.037 | 18.436         | 2049.115               |               1697.78 | 20.694            |",
  "|             8 |        256 | 0.048                         |                        0.036 | 32.887         | 4222.567               |               2715.75 | 55.484            |\n|             8 |        512 | 0.153                         |                        0.06  | 154.862        | 10985.391              |               4750.55 | 131.245           |\n|             8 |       1024 | 0.526                         |                        0.122 | 330.697        | 34175.763              |               8821.18 | 287.428           |\n|             8 |       2048 | OOM                           |                        0.122 | Eager OOM      | OOM                    |               8821.18 | Eager OOM         |\n\n### Inference\n\n|    batch_size |    seq_len |    Per token latency eager (ms) |    Per token latency SDPA (ms) |    Speedup (%) |    Mem eager (MB) |    Mem BT (MB) |    Mem saved (%) |\n|--------------:|-----------:|--------------------------------:|-------------------------------:|---------------:|------------------:|---------------:|-----------------:|",
  "|             1 |        128 |                          11.634 |                          8.647 |         34.546 |           717.676 |        717.674 |            0     |\n|             1 |        256 |                          11.593 |                          8.86  |         30.851 |           742.852 |        742.845 |            0.001 |\n|             1 |        512 |                          11.515 |                          8.816 |         30.614 |           798.232 |        799.593 |           -0.17  |\n|             1 |       1024 |                          11.556 |                          8.915 |         29.628 |           917.265 |        895.538 |            2.426 |\n|             2 |        128 |                          12.724 |                         11.002 |         15.659 |           762.434 |        762.431 |            0     |\n|             2 |        256 |                          12.704 |                         11.063 |         14.83  |           816.809 |        816.733 |            0.009 |\n|             2 |        512 |                          12.757 |                         10.947 |         16.535 |           917.383 |        918.339 |           -0.104 |",
  "|             2 |       1024 |                          13.018 |                         11.018 |         18.147 |          1162.65  |       1114.81  |            4.291 |\n|             4 |        128 |                          12.739 |                         10.959 |         16.243 |           856.335 |        856.483 |           -0.017 |\n|             4 |        256 |                          12.718 |                         10.837 |         17.355 |           957.298 |        957.674 |           -0.039 |\n|             4 |        512 |                          12.813 |                         10.822 |         18.393 |          1158.44  |       1158.45  |           -0.001 |\n|             4 |       1024 |                          13.416 |                         11.06  |         21.301 |          1653.42  |       1557.19  |            6.18  |\n|             8 |        128 |                          12.763 |                         10.891 |         17.193 |          1036.13  |       1036.51  |           -0.036 |\n|             8 |        256 |                          12.89  |                         11.104 |         16.085 |          1236.98  |       1236.87  |            0.01  |",
  "|             8 |        512 |                          13.327 |                         10.939 |         21.836 |          1642.29  |       1641.78  |            0.031 |\n|             8 |       1024 |                          15.181 |                         11.175 |         35.848 |          2634.98  |       2443.35  |            7.843 |\n\n## OPTConfig\n\n[[autodoc]] OPTConfig\n\n<frameworkcontent>\n<pt>\n\n## OPTModel\n\n[[autodoc]] OPTModel\n- forward\n\n## OPTForCausalLM\n\n[[autodoc]] OPTForCausalLM\n- forward\n\n## OPTForSequenceClassification\n\n[[autodoc]] OPTForSequenceClassification\n- forward\n\n## OPTForQuestionAnswering\n\n[[autodoc]] OPTForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFOPTModel\n\n[[autodoc]] TFOPTModel\n- call\n\n## TFOPTForCausalLM\n\n[[autodoc]] TFOPTForCausalLM\n- call\n\n</tf>\n<jax>\n\n## FlaxOPTModel\n\n[[autodoc]] FlaxOPTModel\n- __call__\n\n## FlaxOPTForCausalLM\n\n[[autodoc]] FlaxOPTForCausalLM\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# T5v1.1\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC",
  "\">\n</div>\n\n## Overview\n\nT5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)\nrepository by Colin Raffel et al. It's an improved version of the original T5 model.\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511).\n\n## Usage tips\n\nOne can directly plug in the weights of T5v1.1 into a T5 model, like so:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-base\")\n```\n\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\n\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://arxiv.org/abs/2002.05202).\n\n- Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.\n\n- Pre-trained on C4 only without mixing in the downstream tasks.",
  "- No parameter sharing between the embedding and classifier layer.\n\n- \"xl\" and \"xxl\" replace \"3B\" and \"11B\". The model shapes are a bit different - larger `d_model` and smaller\n`num_heads` and `d_ff`.\n\nNote: T5 Version 1.1 was only pre-trained on [C4](https://huggingface.co/datasets/c4) excluding any supervised\ntraining. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5\nmodel. Since t5v1.1 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\nGoogle has released the following variants:\n\n- [google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)\n\n- [google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)\n\n- [google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)\n\n- [google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)\n\n- [google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl).\n\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for all API reference, tips, code examples and notebooks.\n\n</Tip>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ViTMAE\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">",
  "<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe ViTMAE model was proposed in [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377v2) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\nPiotr Dollár, Ross Girshick. The paper shows that, by pre-training a Vision Transformer (ViT) to reconstruct pixel values for masked patches, one can get results after\nfine-tuning that outperform supervised pre-training.\n\nThe abstract from the paper is the following:\n\n*This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates\nonly on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask",
  "tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs\nenables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity\nmodels that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream\ntasks outperforms supervised pre-training and shows promising scaling behavior.*\n\n<img src=\"https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> MAE architecture. Taken from the <a href=\"https://arxiv.org/abs/2111.06377\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow version of the model was contributed by [sayakpaul](https://github.com/sayakpaul) and\n[ariG23498](https://github.com/ariG23498) (equal contribution). The original code can be found [here](https://github.com/facebookresearch/mae).\n\n## Usage tips",
  "- MAE (masked auto encoding) is a method for self-supervised pre-training of Vision Transformers (ViTs). The pre-training objective is relatively simple:\nby masking a large portion (75%) of the image patches, the model must reconstruct raw pixel values. One can use [`ViTMAEForPreTraining`] for this purpose.\n- After pre-training, one \"throws away\" the decoder used to reconstruct pixels, and one uses the encoder for fine-tuning/linear probing. This means that after\nfine-tuning, one can directly plug in the weights into a [`ViTForImageClassification`].\n- One can use [`ViTImageProcessor`] to prepare images for the model. See the code examples for more info.\n- Note that the encoder of MAE is only used to encode the visual patches. The encoded patches are then concatenated with mask tokens, which the decoder (which also\nconsists of Transformer blocks) takes as input. Each mask token is a shared, learned vector that indicates the presence of a missing patch to be predicted. Fixed\nsin/cos position embeddings are added both to the input of the encoder and the decoder.",
  "- For a visual understanding of how MAEs work you can check out this [post](https://keras.io/examples/vision/masked_image_modeling/).\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\nencompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import ViTMAEModel\nmodel = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n...\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).",
  "On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `facebook/vit-mae-base` model, we saw the following speedups during inference.\n\n|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n|            1 |                                        11 |                                         6 |                      1.83 |\n|            2 |                                         8 |                                         6 |                      1.33 |\n|            4 |                                         8 |                                         6 |                      1.33 |\n|            8 |                                         8 |                                         6 |                      1.33 |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViTMAE.",
  "- [`ViTMAEForPreTraining`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining), allowing you to pre-train the model from scratch/further pre-train the model on custom data.\n- A notebook that illustrates how to visualize reconstructed pixel values with [`ViTMAEForPreTraining`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ViTMAEConfig\n\n[[autodoc]] ViTMAEConfig\n\n<frameworkcontent>\n<pt>\n\n## ViTMAEModel\n\n[[autodoc]] ViTMAEModel\n- forward\n\n## ViTMAEForPreTraining\n\n[[autodoc]] transformers.ViTMAEForPreTraining\n- forward\n\n</pt>\n<tf>\n\n## TFViTMAEModel\n\n[[autodoc]] TFViTMAEModel\n- call\n\n## TFViTMAEForPreTraining\n\n[[autodoc]] transformers.TFViTMAEForPreTraining\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# I-BERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney and Kurt Keutzer. It's a quantized version of RoBERTa running\ninference up to four times faster.",
  "The abstract from the paper is the following:\n\n*Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language\nProcessing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive for\nefficient inference at the edge, and even at the data center. While quantization can be a viable solution for this,\nprevious work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot\nefficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM\nprocessors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes\nthe entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for\nnonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT\ninference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using\nRoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to",
  "the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4 - 4.0x for\nINT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has\nbeen open-sourced.*\n\nThis model was contributed by [kssteven](https://huggingface.co/kssteven). The original code can be found [here](https://github.com/kssteven418/I-BERT).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/masked_language_modeling)\n\n## IBertConfig\n\n[[autodoc]] IBertConfig\n\n## IBertModel\n\n[[autodoc]] IBertModel\n- forward\n\n## IBertForMaskedLM\n\n[[autodoc]] IBertForMaskedLM\n- forward\n\n## IBertForSequenceClassification\n\n[[autodoc]] IBertForSequenceClassification\n- forward\n\n## IBertForMultipleChoice\n\n[[autodoc]] IBertForMultipleChoice\n- forward\n\n## IBertForTokenClassification\n\n[[autodoc]] IBertForTokenClassification\n- forward\n\n## IBertForQuestionAnswering",
  "[[autodoc]] IBertForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Decision Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Decision Transformer model was proposed in [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)",
  "by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n\nThe abstract from the paper is the following:\n\n*We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem.\nThis allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances\nin language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that\ncasts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or\ncompute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked\nTransformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our\nDecision Transformer model can generate future actions that achieve the desired return. Despite its simplicity,\nDecision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on\nAtari, OpenAI Gym, and Key-to-Door tasks.*\n\nThis version of the model is for tasks where the state is a vector.",
  "This model was contributed by [edbeeching](https://huggingface.co/edbeeching). The original code can be found [here](https://github.com/kzl/decision-transformer).\n\n## DecisionTransformerConfig\n\n[[autodoc]] DecisionTransformerConfig\n\n\n## DecisionTransformerGPT2Model\n\n[[autodoc]] DecisionTransformerGPT2Model\n- forward\n\n## DecisionTransformerModel\n\n[[autodoc]] DecisionTransformerModel\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Pegasus\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe Pegasus model was proposed in [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019.\n\nAccording to the abstract,\n\n- Pegasus' pretraining task is intentionally similar to summarization: important sentences are removed/masked from an\ninput document and are generated together as one output sequence from the remaining sentences, similar to an\nextractive summary.\n- Pegasus achieves SOTA summarization performance on all 12 downstream tasks, as measured by ROUGE and human eval.\n\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer). The Authors' code can be found [here](https://github.com/google-research/pegasus).\n\n## Usage tips\n\n- Sequence-to-sequence model with the same encoder-decoder model architecture as BART. Pegasus is pre-trained jointly on two self-supervised objective functions: Masked Language Modeling (MLM) and a novel summarization specific pretraining objective, called Gap Sentence Generation (GSG).",
  "* MLM: encoder input tokens are randomly replaced by a mask tokens and have to be predicted by the encoder (like in BERT)\n* GSG: whole encoder input sentences are replaced by a second mask token and fed to the decoder, but which has a causal mask to hide the future words like a regular auto-regressive transformer decoder.\n\n- FP16 is not supported (help/ideas on this appreciated!).\n- The adafactor optimizer is recommended for pegasus fine-tuning.\n\n\n## Checkpoints\n\nAll the [checkpoints](https://huggingface.co/models?search=pegasus) are fine-tuned for summarization, besides\n*pegasus-large*, whence the other checkpoints are fine-tuned:\n\n- Each checkpoint is 2.2 GB on disk and 568M parameters.\n- FP16 is not supported (help/ideas on this appreciated!).\n- Summarizing xsum in fp32 takes about 400ms/sample, with default parameters on a v100 GPU.\n- Full replication results and correctly pre-processed data can be found in this [Issue](https://github.com/huggingface/transformers/issues/6844#issue-689259666).\n- [Distilled checkpoints](https://huggingface.co/models?search=distill-pegasus) are described in this [paper](https://arxiv.org/abs/2010.13002).\n\n## Implementation Notes",
  "- All models are transformer encoder-decoders with 16 layers in each component.\n- The implementation is completely inherited from [`BartForConditionalGeneration`]\n- Some key configuration differences:\n- static, sinusoidal position embeddings\n- the model starts generating with pad_token_id (which has 0 token_embedding) as the prefix.\n- more beams are used (`num_beams=8`)\n- All pretrained pegasus checkpoints are the same besides three attributes: `tokenizer.model_max_length` (maximum\ninput size), `max_length` (the maximum number of tokens to generate) and `length_penalty`.\n- The code to convert checkpoints trained in the author's [repo](https://github.com/google-research/pegasus) can be\nfound in `convert_pegasus_tf_to_pytorch.py`.\n\n## Usage Example\n\n```python\n>>> from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n>>> import torch\n\n>>> src_text = [\n...     \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n... ]",
  "... model_name = \"google/pegasus-xsum\"\n... device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n... tokenizer = PegasusTokenizer.from_pretrained(model_name)\n... model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n... batch = tokenizer(src_text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n... translated = model.generate(**batch)\n... tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n... assert (\n...     tgt_text[0]\n...     == \"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"\n... )\n```\n\n## Resources\n\n- [Script](https://github.com/huggingface/transformers-research-projects/tree/main/seq2seq-distillation/finetune_pegasus_xsum.sh) to fine-tune pegasus\non the XSUM dataset. Data download instructions at [examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md).\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## PegasusConfig\n\n[[autodoc]] PegasusConfig",
  "## PegasusTokenizer\n\nwarning: `add_tokens` does not work at the moment.\n\n[[autodoc]] PegasusTokenizer\n\n## PegasusTokenizerFast\n\n[[autodoc]] PegasusTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## PegasusModel\n\n[[autodoc]] PegasusModel\n- forward\n\n## PegasusForConditionalGeneration\n\n[[autodoc]] PegasusForConditionalGeneration\n- forward\n\n## PegasusForCausalLM\n\n[[autodoc]] PegasusForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFPegasusModel\n\n[[autodoc]] TFPegasusModel\n- call\n\n## TFPegasusForConditionalGeneration\n\n[[autodoc]] TFPegasusForConditionalGeneration\n- call\n\n</tf>\n<jax>\n\n## FlaxPegasusModel\n\n[[autodoc]] FlaxPegasusModel\n- __call__\n- encode\n- decode\n\n## FlaxPegasusForConditionalGeneration\n\n[[autodoc]] FlaxPegasusForConditionalGeneration\n- __call__\n- encode\n- decode\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PoolFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The PoolFormer model was proposed in [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418)  by Sea AI Labs. Instead of designing complicated token mixer to achieve SOTA performance, the target of this work is to demonstrate the competence of transformer models largely stem from the general architecture MetaFormer.\n\nThe abstract from the paper is the following:",
  "*Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of \"MetaFormer\", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design.*",
  "The figure below illustrates the architecture of PoolFormer. Taken from the [original paper](https://arxiv.org/abs/2111.11418).\n\n<img width=\"600\" src=\"https://user-images.githubusercontent.com/15921929/142746124-1ab7635d-2536-4a0e-ad43-b4fe2c5a525d.png\"/>\n\nThis model was contributed by [heytanay](https://huggingface.co/heytanay). The original code can be found [here](https://github.com/sail-sg/poolformer).\n\n## Usage tips\n\n- PoolFormer has a hierarchical architecture, where instead of Attention, a simple Average Pooling layer is present. All checkpoints of the model can be found on the [hub](https://huggingface.co/models?other=poolformer).\n- One can use [`PoolFormerImageProcessor`] to prepare images for the model.\n- As most models, PoolFormer comes in different sizes, the details of which can be found in the table below.\n\n| **Model variant** | **Depths**    | **Hidden sizes**    | **Params (M)** | **ImageNet-1k Top 1** |\n| :---------------: | ------------- | ------------------- | :------------: | :-------------------: |\n| s12               | [2, 2, 6, 2]  | [64, 128, 320, 512] | 12             | 77.2                  |",
  "| s24               | [4, 4, 12, 4] | [64, 128, 320, 512] | 21             | 80.3                  |\n| s36               | [6, 6, 18, 6] | [64, 128, 320, 512] | 31             | 81.4                  |\n| m36               | [6, 6, 18, 6] | [96, 192, 384, 768] | 56             | 82.1                  |\n| m48               | [8, 8, 24, 8] | [96, 192, 384, 768] | 73             | 82.5                  |\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with PoolFormer.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`PoolFormerForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
  "## PoolFormerConfig\n\n[[autodoc]] PoolFormerConfig\n\n## PoolFormerFeatureExtractor\n\n[[autodoc]] PoolFormerFeatureExtractor\n- __call__\n\n## PoolFormerImageProcessor\n\n[[autodoc]] PoolFormerImageProcessor\n- preprocess\n\n## PoolFormerModel\n\n[[autodoc]] PoolFormerModel\n- forward\n\n## PoolFormerForImageClassification\n\n[[autodoc]] PoolFormerForImageClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Chameleon\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models\n](https://arxiv.org/abs/2405.09818v1) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n\n\nThe abstract from the paper is the following:\n\n*We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training\napproach from inception, an alignment recipe, and an architectural parameterization tailored for the\nearly-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range\nof tasks, including visual question answering, image captioning, text generation, image generation, and\nlong-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including\nstate-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while",
  "being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image\ngeneration, all in a single model. It also matches or exceeds the performance of much larger models,\nincluding Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal\ngeneration evaluation, where either the prompt or outputs contain mixed sequences of both images and\ntext. Chameleon marks a significant step forward in unified modeling of full multimodal documents*\n\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/chameleon_arch.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Chameleon incorporates a vector quantizer module to transform images into discrete tokens. That also enables image generation using an auto-regressive transformer. Taken from the <a href=\"https://arxiv.org/abs/2405.09818v1\">original paper.</a> </small>\n\nThis model was contributed by [joaogante](https://huggingface.co/joaogante) and [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\nThe original code can be found [here](https://github.com/facebookresearch/chameleon).\n\n\n## Usage tips",
  "- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to set `processor.tokenizer.padding_side = \"left\"` before generating.\n\n- Note that Chameleon was tuned for safety alignment. If the model is refusing to answer, consider asking a more concrete question, instead of an open question.\n\n- Chameleon generates in chat format which means that the generated text will always be the \"assistant's turn\". You can enable a text completion generation by passing `return_for_text_completion=True` when calling the processor.\n\n> [!NOTE]\n> Chameleon implementation in Transformers uses a special image token to indicate where to merge image embeddings. For special image token we didn't add a new one but used one of the reserved tokens: `<reserved08707>`. You have to add `<image>` to your prompt in the place where the image should be embedded for correct generation.\n\n## Usage example\n\n### Single image inference\n\nChameleon is a gated model so make sure to have access and login to Hugging Face Hub using a token.\nHere's how to load the model and perform inference in half-precision (`torch.bfloat16`):\n\n```python",
  "from transformers import ChameleonProcessor, ChameleonForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport requests\n\nprocessor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\nmodel = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n\n# prepare image and text prompt\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"What do you see in this image?<image>\"\n\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n# autoregressively complete prompt\noutput = model.generate(**inputs, max_new_tokens=50)\nprint(processor.decode(output[0], skip_special_tokens=True))\n```\n\n### Multi image inference\n\nChameleon can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). Here is how you can do it:\n\n```python\nfrom transformers import ChameleonProcessor, ChameleonForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport requests",
  "processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n\nmodel = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n\n# Get three different images\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage_stop = Image.open(requests.get(url, stream=True).raw)\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage_cats = Image.open(requests.get(url, stream=True).raw)\n\nurl = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\nimage_snowman = Image.open(requests.get(url, stream=True).raw)\n\n# Prepare a batched prompt, where the first one is a multi-image prompt and the second is not\nprompts = [\n\"What do these images have in common?<image><image>\",\n\"<image>What is shown in this image?\"\n]\n\n# We can simply feed images in the order they have to be used in the text prompt\n# Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\ninputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n\n# Generate",
  "generate_ids = model.generate(**inputs, max_new_tokens=50)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n\n## Model optimization\n\n### Quantization using Bitsandbytes\n\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes` and to have access to a GPU/accelerator that is supported by the library.\n\n<Tip>\n\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit [this link](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend).\n\nWe value your feedback to help identify bugs before the full release! Check out [these docs](https://huggingface.co/docs/bitsandbytes/main/en/non_cuda_backends) for more details and feedback links.\n\n</Tip>\n\nSimply change the snippet above with:\n\n```python",
  "from transformers import ChameleonForConditionalGeneration, BitsAndBytesConfig\n\n# specify how to quantize the model\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", quantization_config=quantization_config, device_map=\"cuda\")\n```\n\n### Use Flash-Attention 2 and SDPA to further speed-up generation\n\nThe models supports both, Flash-Attention 2 and PyTorch's [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) which can be enables for optimization. SDPA is the default options when you load the model, If you want to switch for Flash Attention 2, first make sure to install flash-attn. Refer to the [original repository](https://github.com/Dao-AILab/flash-attention) regarding that package installation. Simply change the snippet above with:\n\n```python\nfrom transformers import ChameleonForConditionalGeneration\n\nmodel_id = \"facebook/chameleon-7b\"\nmodel = ChameleonForConditionalGeneration.from_pretrained(\nmodel_id,",
  "torch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nattn_implementation=\"flash_attention_2\"\n).to(0)\n```\n\n## ChameleonConfig\n\n[[autodoc]] ChameleonConfig\n\n## ChameleonVQVAEConfig\n\n[[autodoc]] ChameleonVQVAEConfig\n\n## ChameleonProcessor\n\n[[autodoc]] ChameleonProcessor\n\n## ChameleonImageProcessor\n\n[[autodoc]] ChameleonImageProcessor\n- preprocess\n\n## ChameleonVQVAE\n\n[[autodoc]] ChameleonVQVAE\n- forward\n\n## ChameleonModel\n\n[[autodoc]] ChameleonModel\n- forward\n\n## ChameleonForConditionalGeneration\n\n[[autodoc]] ChameleonForConditionalGeneration\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# YOSO\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe YOSO model was proposed in [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714)",
  "by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. YOSO approximates standard softmax self-attention\nvia a Bernoulli sampling scheme based on Locality Sensitive Hashing (LSH). In principle, all the Bernoulli random variables can be sampled with\na single hash.\n\nThe abstract from the paper is the following:\n\n*Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is\nthe self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically\non the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling\nattention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear.\nWe bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random\nvariables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant).",
  "This leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of\nLSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence\nlength where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark,\nfor evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable\nspeed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at this https URL*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/YOSO).\n\n## Usage tips\n\n- The YOSO attention algorithm is implemented through custom CUDA kernels, functions written in CUDA C++ that can be executed multiple times\nin parallel on a GPU.\n- The kernels provide a `fast_hash` function, which approximates the random projections of the queries and keys using the Fast Hadamard Transform. Using these",
  "hash codes, the `lsh_cumulation` function approximates self-attention via LSH-based Bernoulli sampling.\n- To use the custom kernels, the user should set `config.use_expectation = False`. To ensure that the kernels are compiled successfully,\nthe user must install the correct version of PyTorch and cudatoolkit. By default, `config.use_expectation = True`, which uses YOSO-E and\ndoes not require compiling CUDA kernels.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yoso_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> YOSO Attention Algorithm. Taken from the <a href=\"https://arxiv.org/abs/2111.09714\">original paper</a>.</small>\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## YosoConfig\n\n[[autodoc]] YosoConfig\n\n## YosoModel\n\n[[autodoc]] YosoModel\n- forward\n\n## YosoForMaskedLM\n\n[[autodoc]] YosoForMaskedLM\n- forward",
  "## YosoForSequenceClassification\n\n[[autodoc]] YosoForSequenceClassification\n- forward\n\n## YosoForMultipleChoice\n\n[[autodoc]] YosoForMultipleChoice\n- forward\n\n## YosoForTokenClassification\n\n[[autodoc]] YosoForTokenClassification\n- forward\n\n## YosoForQuestionAnswering\n\n[[autodoc]] YosoForQuestionAnswering\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Trajectory Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, so we won't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.",
  "You can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n</Tip>\n\n## Overview\n\nThe Trajectory Transformer model was proposed in [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)  by Michael Janner, Qiyang Li, Sergey Levine.\n\nThe abstract from the paper is the following:\n\n*Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models,\nleveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence\nmodeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards.\nViewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well\nin other domains, such as natural-language processing, can also provide effective solutions to the RL problem.\nTo this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture\nto model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence",
  "modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common\nin offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction,\nimitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with\nexisting model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.*\n\nThis model was contributed by [CarlCochet](https://huggingface.co/CarlCochet). The original code can be found [here](https://github.com/jannerm/trajectory-transformer).\n\n## Usage tips\n\nThis Transformer is used for deep reinforcement learning. To use it, you need to create sequences from\nactions, states and rewards from all previous timesteps. This model will treat all these elements together\nas one big sequence (a trajectory).\n\n## TrajectoryTransformerConfig\n\n[[autodoc]] TrajectoryTransformerConfig\n\n## TrajectoryTransformerModel\n\n[[autodoc]] TrajectoryTransformerModel\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# StableLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "`StableLM 3B 4E1T` was proposed in [`StableLM 3B 4E1T`: Technical Report](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) by Stability AI and is the first model in a series of multi-epoch pre-trained language models.\n\n### Model Details\n\n`StableLM 3B 4E1T` is a decoder-only base language model pre-trained on 1 trillion tokens of diverse English and code datasets for four epochs.\nThe model architecture is transformer-based with partial Rotary Position Embeddings, SwiGLU activation, LayerNorm, etc.\n\nWe also provide `StableLM Zephyr 3B`, an instruction fine-tuned version of the model that can be used for chat-based applications.\n\n### Usage Tips\n\n- The architecture is similar to LLaMA but with RoPE applied to 25% of head embedding dimensions, LayerNorm instead of RMSNorm, and optional QKV bias terms.\n- `StableLM 3B 4E1T`-based models uses the same tokenizer as [`GPTNeoXTokenizerFast`].\n\n`StableLM 3B 4E1T` and `StableLM Zephyr 3B` can be found on the [Huggingface Hub](https://huggingface.co/stabilityai)",
  "The following code snippet demonstrates how to use `StableLM 3B 4E1T` for inference:\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> set_seed(0)\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\n>>> model.to(device)  # doctest: +IGNORE_RESULT\n\n>>> model_inputs = tokenizer(\"The weather is always wonderful in\", return_tensors=\"pt\").to(model.device)\n\n>>> generated_ids = model.generate(**model_inputs, max_length=32, do_sample=True)\n>>> responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n>>> responses\n['The weather is always wonderful in Costa Rica, which makes it a prime destination for retirees. That’s where the Pensionado program comes in, offering']\n```\n\n## Combining StableLM and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention v2.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```",
  "Also make sure that your hardware is compatible with Flash-Attention 2. Read more about it in the official documentation of the [`flash-attn`](https://github.com/Dao-AILab/flash-attention) repository. Note: you must load your model in half-precision (e.g. `torch.bfloat16`).\n\nNow, to run the model with Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> set_seed(0)\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")  # doctest: +SKIP\n>>> model.to(device)  # doctest: +SKIP\n\n>>> model_inputs = tokenizer(\"The weather is always wonderful in\", return_tensors=\"pt\").to(model.device)\n\n>>> generated_ids = model.generate(**model_inputs, max_length=32, do_sample=True)  # doctest: +SKIP\n>>> responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)  # doctest: +SKIP\n>>> responses  # doctest: +SKIP",
  "['The weather is always wonderful in Costa Rica, which makes it a prime destination for retirees. That’s where the Pensionado program comes in, offering']\n```\n\n\n## StableLmConfig\n\n[[autodoc]] StableLmConfig\n\n## StableLmModel\n\n[[autodoc]] StableLmModel\n- forward\n\n## StableLmForCausalLM\n\n[[autodoc]] StableLmForCausalLM\n- forward\n\n## StableLmForSequenceClassification\n\n[[autodoc]] StableLmForSequenceClassification\n- forward\n\n## StableLmForTokenClassification\n\n[[autodoc]] StableLmForTokenClassification\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DiffLlama\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The DiffLlama model was proposed in [Differential Transformer](https://arxiv.org/abs/2410.05258) by Kazuma Matsumoto and .\nThis model is combine Llama model and Differential Transformer's Attention.\n\nThe abstract from the paper is the following:",
  "*Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.*",
  "### Usage tips\nThe hyperparameters of this model is the same as Llama model.\n\n\n## DiffLlamaConfig\n\n[[autodoc]] DiffLlamaConfig\n\n## DiffLlamaModel\n\n[[autodoc]] DiffLlamaModel\n- forward\n\n## DiffLlamaForCausalLM\n\n[[autodoc]] DiffLlamaForCausalLM\n- forward\n\n## DiffLlamaForSequenceClassification\n\n[[autodoc]] DiffLlamaForSequenceClassification\n- forward\n\n## DiffLlamaForQuestionAnswering\n\n[[autodoc]] DiffLlamaForQuestionAnswering\n- forward\n\n## DiffLlamaForTokenClassification\n\n[[autodoc]] DiffLlamaForTokenClassification\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BERTweet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe BERTweet model was proposed in [BERTweet: A pre-trained language model for English Tweets](https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf) by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen.\n\nThe abstract from the paper is the following:\n\n*We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having\nthe same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et\nal., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,\n2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:\nPart-of-speech tagging, Named-entity recognition and text classification.*\n\nThis model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/BERTweet).\n\n## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n\n>>> # For transformers v4.x+:",
  ">>> tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n\n>>> # For transformers v3.x:\n>>> # tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n\n>>> # INPUT TWEET IS ALREADY NORMALIZED!\n>>> line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n\n>>> input_ids = torch.tensor([tokenizer.encode(line)])\n\n>>> with torch.no_grad():\n...     features = bertweet(input_ids)  # Models outputs are now tuples\n\n>>> # With TensorFlow 2.0+:\n>>> # from transformers import TFAutoModel\n>>> # bertweet = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")\n```\n\n<Tip>\n\nThis implementation is the same as BERT, except for tokenization method. Refer to [BERT documentation](bert) for\nAPI reference information.\n\n</Tip>\n\n## BertweetTokenizer\n\n[[autodoc]] BertweetTokenizer",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ModernBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The ModernBERT model was proposed in [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/abs/2412.13663) by Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Galalgher, Raja Bisas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Grifin Adams, Jeremy Howard and Iacopo Poli.\n\nIt is a refresh of the traditional encoder architecture, as used in previous models such as [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) and [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta).\n\nIt builds on BERT and implements many modern architectural improvements which have been developed since its original release, such as:\n- [Rotary Positional Embeddings](https://huggingface.co/blog/designing-positional-encoding) to support sequences of up to 8192 tokens.\n- [Unpadding](https://arxiv.org/abs/2208.08124) to ensure no compute is wasted on padding tokens, speeding up processing time for batches with mixed-length sequences.",
  "- [GeGLU](https://arxiv.org/abs/2002.05202) Replacing the original MLP layers with GeGLU layers, shown to improve performance.\n- [Alternating Attention](https://arxiv.org/abs/2004.05150v2) where most attention layers employ a sliding window of 128 tokens, with Global Attention only used every 3 layers.\n- [Flash Attention](https://github.com/Dao-AILab/flash-attention) to speed up processing.\n- A model designed following recent [The Case for Co-Designing Model Architectures with Hardware](https://arxiv.org/abs/2401.14489), ensuring maximum efficiency across inference GPUs.\n- Modern training data scales (2 trillion tokens) and mixtures (including code ande math data)\n\nThe abstract from the paper is the following:",
  "*Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.*\n\nThe original code can be found [here](https://github.com/answerdotai/modernbert).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ModernBert.\n\n<PipelineTag pipeline=\"text-classification\"/>",
  "- A notebook on how to [finetune for General Language Understanding Evaluation (GLUE) with Transformers](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/finetune_modernbert_on_glue.ipynb), also available as a Google Colab [notebook](https://colab.research.google.com/github/AnswerDotAI/ModernBERT/blob/main/examples/finetune_modernbert_on_glue.ipynb). 🌎\n\n<PipelineTag pipeline=\"sentence-similarity\"/>\n\n- A script on how to [finetune for text similarity or information retrieval with Sentence Transformers](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/train_st.py). 🌎\n- A script on how to [finetune for information retrieval with PyLate](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/train_pylate.py). 🌎\n\n<PipelineTag pipeline=\"fill-mask\"/>\n\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n\n## ModernBertConfig\n\n[[autodoc]] ModernBertConfig\n\n<frameworkcontent>\n<pt>\n\n## ModernBertModel\n\n[[autodoc]] ModernBertModel\n- forward\n\n## ModernBertForMaskedLM\n\n[[autodoc]] ModernBertForMaskedLM\n- forward\n\n## ModernBertForSequenceClassification\n\n[[autodoc]] ModernBertForSequenceClassification\n- forward",
  "## ModernBertForTokenClassification\n\n[[autodoc]] ModernBertForTokenClassification\n- forward\n\n</pt>\n</frameworkcontent>",
  "<!--Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BridgeTower\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The BridgeTower model was proposed in [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The goal of this model is to build a\nbridge between each uni-modal encoder and the cross-modal encoder to enable comprehensive and detailed interaction at each layer of the cross-modal encoder thus achieving remarkable performance on various downstream tasks with almost negligible additional performance and computational costs.\n\nThis paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference.\n\nThe abstract from the paper is the following:\n\n*Vision-Language (VL) models with the TWO-TOWER architecture have dominated visual-language representation learning in recent years.\nCurrent VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder.",
  "Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BRIDGETOWER, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the crossmodal encoder.\nThis enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BRIDGETOWER achieves state-of-the-art performance on various downstream vision-language tasks.\nIn particular, on the VQAv2 test-std set, BRIDGETOWER achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BRIDGETOWER achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/bridgetower_architecture%20.jpg\"",
  "alt=\"drawing\" width=\"600\"/>\n\n<small> BridgeTower architecture. Taken from the <a href=\"https://arxiv.org/abs/2206.08657\">original paper.</a> </small>\n\nThis model was contributed by [Anahita Bhiwandiwalla](https://huggingface.co/anahita-b), [Tiep Le](https://huggingface.co/Tile) and [Shaoyen Tseng](https://huggingface.co/shaoyent). The original code can be found [here](https://github.com/microsoft/BridgeTower).\n\n## Usage tips and examples\n\nBridgeTower consists of a visual encoder, a textual encoder and cross-modal encoder with multiple lightweight bridge layers.\nThe goal of this approach was to build a bridge between each uni-modal encoder and the cross-modal encoder to enable comprehensive and detailed interaction at each layer of the cross-modal encoder.\nIn principle, one can apply any visual, textual or cross-modal encoder in the proposed architecture.\n\nThe [`BridgeTowerProcessor`] wraps [`RobertaTokenizer`] and [`BridgeTowerImageProcessor`] into a single instance to both\nencode the text and prepare the images respectively.\n\nThe following example shows how to run contrastive learning using [`BridgeTowerProcessor`] and [`BridgeTowerForContrastiveLearning`].\n```python",
  ">>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n>>> model = BridgeTowerForContrastiveLearning.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs\n```\n\nThe following example shows how to run image-text retrieval using [`BridgeTowerProcessor`] and [`BridgeTowerForImageAndTextRetrieval`].\n```python\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)",
  ">>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n>>> model = BridgeTowerForImageAndTextRetrieval.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs.logits[0, 1].item()\n```\n\nThe following example shows how to run masked language modeling using [`BridgeTowerProcessor`] and [`BridgeTowerForMaskedLM`].\n\n```python\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000360943.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n>>> text = \"a <mask> looking out of the window\"\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n>>> model = BridgeTowerForMaskedLM.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n\n>>> # prepare inputs",
  ">>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())\n\n>>> print(results)\n.a cat looking out of the window.\n```\n\nTips:\n\n- This implementation of BridgeTower uses [`RobertaTokenizer`] to generate text embeddings and OpenAI's CLIP/ViT model to compute visual embeddings.\n- Checkpoints for pre-trained [bridgeTower-base](https://huggingface.co/BridgeTower/bridgetower-base) and [bridgetower masked language modeling and image text matching](https://huggingface.co/BridgeTower/bridgetower-base-itm-mlm) are released.\n- Please refer to [Table 5](https://arxiv.org/pdf/2206.08657.pdf) for BridgeTower's performance on Image Retrieval and other down stream tasks.\n- The PyTorch version of this model is only available in torch 1.10 and higher.\n\n\n## BridgeTowerConfig\n\n[[autodoc]] BridgeTowerConfig\n\n## BridgeTowerTextConfig\n\n[[autodoc]] BridgeTowerTextConfig\n\n## BridgeTowerVisionConfig\n\n[[autodoc]] BridgeTowerVisionConfig\n\n## BridgeTowerImageProcessor\n\n[[autodoc]] BridgeTowerImageProcessor\n- preprocess\n\n## BridgeTowerProcessor\n\n[[autodoc]] BridgeTowerProcessor",
  "- __call__\n\n## BridgeTowerModel\n\n[[autodoc]] BridgeTowerModel\n- forward\n\n## BridgeTowerForContrastiveLearning\n\n[[autodoc]] BridgeTowerForContrastiveLearning\n- forward\n\n## BridgeTowerForMaskedLM\n\n[[autodoc]] BridgeTowerForMaskedLM\n- forward\n\n## BridgeTowerForImageAndTextRetrieval\n\n[[autodoc]] BridgeTowerForImageAndTextRetrieval\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BART\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Bart model was proposed in [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019.\n\nAccording to the abstract,\n\n- Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a\nleft-to-right decoder (like GPT).\n- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme,\nwhere spans of text are replaced with a single mask token.\n- BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new",
  "state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains\nof up to 6 ROUGE.\n\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer). The authors' code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/bart).\n\n## Usage tips:\n\n- BART is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.\n- Sequence-to-sequence model with an encoder and a decoder. Encoder is fed a corrupted version of the tokens, decoder is fed the original tokens (but has a mask to hide the future words like a regular transformers decoder). A composition of the following transformations are applied on the pretraining tasks for the encoder:\n\n* mask random tokens (like in BERT)\n* delete random tokens\n* mask a span of k tokens with a single mask token (a span of 0 tokens is an insertion of a mask token)\n* permute sentences\n* rotate the document to make it start at a specific token\n\n## Implementation Notes\n\n- Bart doesn't use `token_type_ids` for sequence classification. Use [`BartTokenizer`] or",
  "[`~BartTokenizer.encode`] to get the proper splitting.\n- The forward pass of [`BartModel`] will create the `decoder_input_ids` if they are not passed.\nThis is different than some other modeling APIs. A typical use case of this feature is mask filling.\n- Model predictions are intended to be identical to the original implementation when\n`forced_bos_token_id=0`. This only works, however, if the string you pass to\n[`fairseq.encode`] starts with a space.\n- [`~generation.GenerationMixin.generate`] should be used for conditional generation tasks like\nsummarization, see the example in that docstrings.\n- Models that load the *facebook/bart-large-cnn* weights will not have a `mask_token_id`, or be able to perform\nmask-filling tasks.\n\n## Mask Filling\n\nThe `facebook/bart-base` and `facebook/bart-large` checkpoints can be used to fill multi-token masks.\n\n```python\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\ntok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\nexample_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"",
  "batch = tok(example_english_phrase, return_tensors=\"pt\")\ngenerated_ids = model.generate(batch[\"input_ids\"])\nassert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n\"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n]\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BART. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"summarization\"/>\n\n- A blog post on [Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq).\n- A notebook on how to [finetune BART for summarization with fastai using blurr](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb). 🌎",
  "- A notebook on how to [finetune BART for summarization in two languages with Trainer class](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb). 🌎\n- [`BartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb).\n- [`TFBartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n- [`FlaxBartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization).\n- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object can be found in this [forum discussion](https://discuss.huggingface.co/t/train-bart-for-conditional-generation-e-g-summarization/1904)",
  "- [Summarization](https://huggingface.co/course/chapter7/5?fw=pt#summarization) chapter of the 🤗 Hugging Face course.\n- [Summarization task guide](../tasks/summarization)\n\n<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`BartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFBartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxBartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).",
  "- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n<PipelineTag pipeline=\"translation\"/>\n\n- A notebook on how to [finetune mBART using Seq2SeqTrainer for Hindi to English translation](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb). 🌎\n- [`BartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb).\n- [`TFBartForConditionalGeneration`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n- [Translation task guide](../tasks/translation)\n\nSee also:\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)",
  "- [Causal language modeling task guide](../tasks/language_modeling)\n- [Distilled checkpoints](https://huggingface.co/models?search=distilbart) are described in this [paper](https://arxiv.org/abs/2010.13002).\n\n## BartConfig\n\n[[autodoc]] BartConfig\n- all\n\n## BartTokenizer\n\n[[autodoc]] BartTokenizer\n- all\n\n## BartTokenizerFast\n\n[[autodoc]] BartTokenizerFast\n- all\n\n\n<frameworkcontent>\n<pt>\n\n## BartModel\n\n[[autodoc]] BartModel\n- forward\n\n## BartForConditionalGeneration\n\n[[autodoc]] BartForConditionalGeneration\n- forward\n\n## BartForSequenceClassification\n\n[[autodoc]] BartForSequenceClassification\n- forward\n\n## BartForQuestionAnswering\n\n[[autodoc]] BartForQuestionAnswering\n- forward\n\n## BartForCausalLM\n\n[[autodoc]] BartForCausalLM\n- forward\n\n</pt>\n<tf>\n\n## TFBartModel\n\n[[autodoc]] TFBartModel\n- call\n\n## TFBartForConditionalGeneration\n\n[[autodoc]] TFBartForConditionalGeneration\n- call\n\n## TFBartForSequenceClassification\n\n[[autodoc]] TFBartForSequenceClassification\n- call\n\n</tf>\n<jax>\n\n## FlaxBartModel\n\n[[autodoc]] FlaxBartModel\n- __call__\n- encode\n- decode\n\n## FlaxBartForConditionalGeneration\n\n[[autodoc]] FlaxBartForConditionalGeneration\n- __call__\n- encode\n- decode",
  "## FlaxBartForSequenceClassification\n\n[[autodoc]] FlaxBartForSequenceClassification\n- __call__\n- encode\n- decode\n\n## FlaxBartForQuestionAnswering\n\n[[autodoc]] FlaxBartForQuestionAnswering\n- __call__\n- encode\n- decode\n\n## FlaxBartForCausalLM\n\n[[autodoc]] FlaxBartForCausalLM\n- __call__\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DAC\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\n\nThe DAC model was proposed in [Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN](https://arxiv.org/abs/2306.06546) by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar.",
  "The Descript Audio Codec (DAC) model is a powerful tool for compressing audio data, making it highly efficient for storage and transmission. By compressing 44.1 KHz audio into tokens at just 8kbps bandwidth, the DAC model enables high-quality audio processing while significantly reducing the data footprint. This is particularly useful in scenarios where bandwidth is limited or storage space is at a premium, such as in streaming applications, remote conferencing, and archiving large audio datasets.\n\nThe abstract from the paper is the following:",
  "*Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.*\n\nThis model was contributed by [Kamil Akesbi](https://huggingface.co/kamilakesbi).",
  "The original code can be found [here](https://github.com/descriptinc/descript-audio-codec/tree/main?tab=readme-ov-file).\n\n\n## Model structure\n\nThe Descript Audio Codec (DAC) model is structured into three distinct stages:\n\n1. Encoder Model: This stage compresses the input audio, reducing its size while retaining essential information.\n2. Residual Vector Quantizer (RVQ) Model: Working in tandem with the encoder, this model quantizes the latent codes of the audio, refining the compression and ensuring high-quality reconstruction.\n3. Decoder Model: This final stage reconstructs the audio from its compressed form, restoring it to a state that closely resembles the original input.\n\n## Usage example\n\nHere is a quick example of how to encode and decode an audio using this model:\n\n```python\n>>> from datasets import load_dataset, Audio\n>>> from transformers import DacModel, AutoProcessor\n>>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> model = DacModel.from_pretrained(\"descript/dac_16khz\")\n>>> processor = AutoProcessor.from_pretrained(\"descript/dac_16khz\")",
  ">>> librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n>>> audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n>>> inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n\n>>> encoder_outputs = model.encode(inputs[\"input_values\"])\n>>> # Get the intermediate audio codes\n>>> audio_codes = encoder_outputs.audio_codes\n>>> # Reconstruct the audio from its quantized representation\n>>> audio_values = model.decode(encoder_outputs.quantized_representation)\n>>> # or the equivalent with a forward pass\n>>> audio_values = model(inputs[\"input_values\"]).audio_values\n```\n\n## DacConfig\n\n[[autodoc]] DacConfig\n\n## DacFeatureExtractor\n\n[[autodoc]] DacFeatureExtractor\n- __call__\n\n## DacModel\n\n[[autodoc]] DacModel\n- decode\n- encode\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# TAPEX\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n</Tip>\n\n## Overview\n\nThe TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu,\nBei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after\nwhich it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.\n\nTAPEX has been fine-tuned on several datasets:\n- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n- [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\n- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)\n- [TabFact](https://tabfact.github.io/) (by USCB NLP Lab).",
  "The abstract from the paper is the following:\n\n*Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is\nstill a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we\npropose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically\nsynthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL\nexecutor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that\nTAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements\non the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy",
  "to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs\nand to achieve new state-of-the-art results on various downstream tasks.*\n\n## Usage tips\n\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPEX into a BART model.\n- TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned on WTQ, SQA, WikiSQL and TabFact.\n- Sentences + tables are presented to the model as `sentence + \" \" + linearized table`. The linearized table has the following format:\n`col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ...`.\n- TAPEX has its own tokenizer, that allows to prepare all data for the model easily. One can pass Pandas DataFrames and strings to the tokenizer,\nand it will automatically create the `input_ids` and `attention_mask` (as shown in the usage examples below).\n\n### Usage: inference\n\nBelow, we illustrate how to use TAPEX for table question answering. As one can see, one can directly plug in the weights of TAPEX into a BART model.",
  "We use the [Auto API](auto), which will automatically instantiate the appropriate tokenizer ([`TapexTokenizer`]) and model ([`BartForConditionalGeneration`]) for us,\nbased on the configuration file of the checkpoint on the hub.\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import pandas as pd\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n\n>>> # prepare table + question\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> table = pd.DataFrame.from_dict(data)\n>>> question = \"how many movies does Leonardo Di Caprio have?\"\n\n>>> encoding = tokenizer(table, question, return_tensors=\"pt\")\n\n>>> # let the model generate an answer autoregressively\n>>> outputs = model.generate(**encoding)\n\n>>> # decode back to text\n>>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n>>> print(predicted_answer)\n53\n```",
  "Note that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of different tables/questions, or a batch of a single table\nand multiple questions, or a batch of a single query and multiple tables. Let's illustrate this:\n\n```python\n>>> # prepare table + question\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> table = pd.DataFrame.from_dict(data)\n>>> questions = [\n...     \"how many movies does Leonardo Di Caprio have?\",\n...     \"which actor has 69 movies?\",\n...     \"what's the first name of the actor who has 87 movies?\",\n... ]\n>>> encoding = tokenizer(table, questions, padding=True, return_tensors=\"pt\")\n\n>>> # let the model generate an answer autoregressively\n>>> outputs = model.generate(**encoding)\n\n>>> # decode back to text\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n[' 53', ' george clooney', ' brad pitt']\n```\n\nIn case one wants to do table verification (i.e. the task of determining whether a given sentence is supported or refuted by the contents",
  "of a table), one can instantiate a [`BartForSequenceClassification`] model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important\nbenchmark for table fact checking (it achieves 84% accuracy). The code example below again leverages the [Auto API](auto).\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n\n>>> # prepare table + sentence\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> table = pd.DataFrame.from_dict(data)\n>>> sentence = \"George Clooney has 30 movies\"\n\n>>> encoding = tokenizer(table, sentence, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> # print prediction\n>>> predicted_class_idx = outputs.logits[0].argmax(dim=0).item()\n>>> print(model.config.id2label[predicted_class_idx])\nRefused\n```\n\n<Tip>\n\nTAPEX architecture is the same as BART, except for tokenization. Refer to [BART documentation](bart) for information on",
  "configuration classes and their parameters. TAPEX-specific tokenizer is documented below.\n\n</Tip>\n\n## TapexTokenizer\n\n[[autodoc]] TapexTokenizer\n- __call__\n- save_vocabulary",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# EfficientFormer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.",
  "If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe EfficientFormer model was proposed in [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\nby Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.  EfficientFormer proposes a\ndimension-consistent pure transformer that can be run on mobile devices for dense prediction tasks like image classification, object\ndetection and semantic segmentation.\n\nThe abstract from the paper is the following:\n\n*Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks.\nHowever, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally\ntimes slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly",
  "challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation\ncomplexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still\nunsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance?\nTo answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs.\nThen we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm.\nFinally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer.\nExtensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices.\nOur fastest model, EfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on\niPhone 12 (compiled with CoreML), which { runs as fast as MobileNetV2×1.4 (1.6 ms, 74.7% top-1),} and our largest model,\nEfficientFormer-L7, obtains 83.3% accuracy with only 7.0 ms latency. Our work proves that properly designed transformers can",
  "reach extremely low latency on mobile devices while maintaining high performance.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03) and [Bearnardd](https://huggingface.co/Bearnardd).\nThe original code can be found [here](https://github.com/snap-research/EfficientFormer). The TensorFlow version of this model was added by [D-Roberts](https://huggingface.co/D-Roberts).\n\n## Documentation resources\n\n- [Image classification task guide](../tasks/image_classification)\n\n## EfficientFormerConfig\n\n[[autodoc]] EfficientFormerConfig\n\n## EfficientFormerImageProcessor\n\n[[autodoc]] EfficientFormerImageProcessor\n- preprocess\n\n<frameworkcontent>\n<pt>\n\n## EfficientFormerModel\n\n[[autodoc]] EfficientFormerModel\n- forward\n\n## EfficientFormerForImageClassification\n\n[[autodoc]] EfficientFormerForImageClassification\n- forward\n\n## EfficientFormerForImageClassificationWithTeacher\n\n[[autodoc]] EfficientFormerForImageClassificationWithTeacher\n- forward\n\n</pt>\n<tf>\n\n## TFEfficientFormerModel\n\n[[autodoc]] TFEfficientFormerModel\n- call\n\n## TFEfficientFormerForImageClassification\n\n[[autodoc]] TFEfficientFormerForImageClassification\n- call",
  "## TFEfficientFormerForImageClassificationWithTeacher\n\n[[autodoc]] TFEfficientFormerForImageClassificationWithTeacher\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MADLAD-400\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nMADLAD-400 models were released in the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](MADLAD-400: A Multilingual And Document-Level Large Audited Dataset).\n\nThe abstract from the paper is the following:\n\n*We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss\nthe limitations revealed by self-auditing MADLAD-400, and the role data auditing\nhad in the dataset creation process. We then train and release a 10.7B-parameter\nmultilingual machine translation model on 250 billion tokens covering over 450\nlanguages using publicly available data, and find that it is competitive with models\nthat are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot\ntranslation. We make the baseline models 1\navailable to the research community.*\n\nThis model was added by [Juarez Bochi](https://huggingface.co/jbochi). The original checkpoints can be found [here](https://github.com/google-research/google-research/tree/master/madlad_400).",
  "This is a machine translation model that supports many low-resource languages, and that is competitive with models that are significantly larger.\n\nOne can directly use MADLAD-400 weights without finetuning the model:\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/madlad400-3b-mt\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/madlad400-3b-mt\")\n\n>>> inputs = tokenizer(\"<2pt> I love pizza!\", return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Eu amo pizza!']\n```\n\nGoogle has released the following variants:\n\n- [google/madlad400-3b-mt](https://huggingface.co/google/madlad400-3b-mt)\n\n- [google/madlad400-7b-mt](https://huggingface.co/google/madlad400-7b-mt)\n\n- [google/madlad400-7b-mt-bt](https://huggingface.co/google/madlad400-7b-mt-bt)\n\n- [google/madlad400-10b-mt](https://huggingface.co/google/madlad400-10b-mt)\n\nThe original checkpoints can be found [here](https://github.com/google-research/google-research/tree/master/madlad_400).\n\n<Tip>",
  "Refer to [T5's documentation page](t5) for all API references, code examples, and notebooks. For more details regarding training and evaluation of the MADLAD-400, refer to the model card.\n\n</Tip>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mamba\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Mamba model was proposed in [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) by Albert Gu and Tri Dao.",
  "This model is a new paradigm architecture based on `state-space-models`. You can read more about the intuition behind these [here](https://srush.github.io/annotated-s4/).\n\nThe abstract from the paper is the following:",
  "*Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.*",
  "Tips:\n\n- Mamba is a new `state space model` architecture that rivals the classic Transformers. It is based on the line of progress on structured state space models, with an efficient hardware-aware design and implementation in the spirit of [FlashAttention](https://github.com/Dao-AILab/flash-attention).\n- Mamba stacks `mixer` layers, which are the equivalent of `Attention` layers. The core logic of `mamba` is held in the `MambaMixer` class.\n- Two implementations cohabit: one is optimized and uses fast cuda kernels, while the other one is naive but can run on any device!\n- The current implementation leverages the original cuda kernels: the equivalent of flash attention for Mamba are hosted in the [`mamba-ssm`](https://github.com/state-spaces/mamba) and the [`causal_conv1d`](https://github.com/Dao-AILab/causal-conv1d) repositories. Make sure to install them if your hardware supports them!\n- Contributions to make the naive path faster are welcome 🤗\n\nThis model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/state-spaces/mamba).\n\n# Usage\n\n### A simple generation example:\n```python",
  "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\nmodel = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\n### Peft finetuning\nThe slow version is not very stable for training, and the fast one needs `float32`!\n\n```python\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nmodel_id = \"state-spaces/mamba-130m-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ndataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\ntraining_args = TrainingArguments(\noutput_dir=\"./results\",\nnum_train_epochs=3,\nper_device_train_batch_size=4,\nlogging_dir='./logs',\nlogging_steps=10,\nlearning_rate=2e-3\n)\nlora_config =  LoraConfig(\nr=8,\ntarget_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\ntask_type=\"CAUSAL_LM\",",
  "bias=\"none\"\n)\ntrainer = SFTTrainer(\nmodel=model,\nprocessing_class=tokenizer,\nargs=training_args,\npeft_config=lora_config,\ntrain_dataset=dataset,\ndataset_text_field=\"quote\",\n)\ntrainer.train()\n```\n\n## MambaConfig\n\n[[autodoc]] MambaConfig\n\n## MambaModel\n\n[[autodoc]] MambaModel\n- forward\n\n## MambaLMHeadModel\n\n[[autodoc]] MambaForCausalLM\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Convolutional Vision Transformer (CvT)\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer (ViT)](vit) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs.\n\nThe abstract from the paper is the following:\n\n*We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT)\nin performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through\ntwo primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs)\nto the ViT architecture (\\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\\ie dynamic attention,",
  "global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition,\nperformance gains are maintained when pretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on\nImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\\% on the ImageNet-1k val set. Finally, our results show that the positional encoding,\na crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks.*\n\nThis model was contributed by [anugunj](https://huggingface.co/anugunj). The original code can be found [here](https://github.com/microsoft/CvT).\n\n## Usage tips\n\n- CvT models are regular Vision Transformers, but trained with convolutions. They outperform the [original model (ViT)](vit) when fine-tuned on ImageNet-1K and CIFAR-100.",
  "- You can check out demo notebooks regarding inference as well as fine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer) (you can just replace [`ViTFeatureExtractor`] by [`AutoImageProcessor`] and [`ViTForImageClassification`] by [`CvtForImageClassification`]).\n- The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/) (a collection of 14 million images and 22k classes) only, (2) also fine-tuned on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/) (also referred to as ILSVRC 2012, a collection of 1.3 million\nimages and 1,000 classes).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with CvT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`CvtForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).",
  "- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## CvtConfig\n\n[[autodoc]] CvtConfig\n\n<frameworkcontent>\n<pt>\n\n## CvtModel\n\n[[autodoc]] CvtModel\n- forward\n\n## CvtForImageClassification\n\n[[autodoc]] CvtForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFCvtModel\n\n[[autodoc]] TFCvtModel\n- call\n\n## TFCvtForImageClassification\n\n[[autodoc]] TFCvtForImageClassification\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# DINOv2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DINOv2 model was proposed in [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by\nMaxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.\nDINOv2 is an upgrade of [DINO](https://arxiv.org/abs/2104.14294), a self-supervised method applied on [Vision Transformers](vit). This method enables all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning.\n\nThe abstract from the paper is the following:",
  "*The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.*",
  "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/facebookresearch/dinov2).\n\n## Usage tips\n\nThe model can be traced using `torch.jit.trace` which leverages JIT compilation to optimize the model making it faster to run. Note this still produces some mis-matched elements and the difference between the original model and the traced model is of the order of 1e-4.\n\n```python\nimport torch\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\nmodel = AutoModel.from_pretrained('facebook/dinov2-base')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs[0]\n\n# We have to force return_dict=False for tracing\nmodel.config.return_dict = False\n\nwith torch.no_grad():\ntraced_model = torch.jit.trace(model, [inputs.pixel_values])\ntraced_outputs = traced_model(inputs.pixel_values)\n\nprint((last_hidden_states - traced_outputs[0]).abs().max())",
  "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DINOv2.\n\n- Demo notebooks for DINOv2 can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DINOv2). 🌎\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`Dinov2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## Dinov2Config\n\n[[autodoc]] Dinov2Config\n\n<frameworkcontent>\n<pt>\n\n## Dinov2Model\n\n[[autodoc]] Dinov2Model\n- forward\n\n## Dinov2ForImageClassification\n\n[[autodoc]] Dinov2ForImageClassification\n- forward\n\n</pt>\n<jax>\n\n## FlaxDinov2Model\n\n[[autodoc]] FlaxDinov2Model\n- __call__",
  "## FlaxDinov2ForImageClassification\n\n[[autodoc]] FlaxDinov2ForImageClassification\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# UnivNet\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe UnivNet model was proposed in [UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://arxiv.org/abs/2106.07889) by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kin, and Juntae Kim.",
  "The UnivNet model is a generative adversarial network (GAN) trained to synthesize high fidelity speech waveforms. The UnivNet model shared in `transformers` is the *generator*, which maps a conditioning log-mel spectrogram and optional noise sequence to a speech waveform (e.g. a vocoder). Only the generator is required for inference. The *discriminator* used to train the `generator` is not implemented.\n\nThe abstract from the paper is the following:",
  "*Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.*",
  "Tips:\n\n- The `noise_sequence` argument for [`UnivNetModel.forward`] should be standard Gaussian noise (such as from `torch.randn`) of shape `([batch_size], noise_length, model.config.model_in_channels)`, where `noise_length` should match the length dimension (dimension 1) of the `input_features` argument. If not supplied, it will be randomly generated; a `torch.Generator` can be supplied to the `generator` argument so that the forward pass can be reproduced. (Note that [`UnivNetFeatureExtractor`] will return generated noise by default, so it shouldn't be necessary to generate `noise_sequence` manually.)\n- Padding added by [`UnivNetFeatureExtractor`] can be removed from the [`UnivNetModel`] output through the [`UnivNetFeatureExtractor.batch_decode`] method, as shown in the usage example below.\n- Padding the end of each waveform with silence can reduce artifacts at the end of the generated audio sample. This can be done by supplying `pad_end = True` to [`UnivNetFeatureExtractor.__call__`]. See [this issue](https://github.com/seungwonpark/melgan/issues/8) for more details.\n\nUsage Example:\n\n```python\nimport torch\nfrom scipy.io.wavfile import write",
  "from datasets import Audio, load_dataset\n\nfrom transformers import UnivNetFeatureExtractor, UnivNetModel\n\nmodel_id_or_path = \"dg845/univnet-dev\"\nmodel = UnivNetModel.from_pretrained(model_id_or_path)\nfeature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)\n\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# Resample the audio to the model and feature extractor's sampling rate.\nds = ds.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n# Pad the end of the converted waveforms to reduce artifacts at the end of the output audio samples.\ninputs = feature_extractor(\nds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], pad_end=True, return_tensors=\"pt\"\n)\n\nwith torch.no_grad():\naudio = model(**inputs)\n\n# Remove the extra padding at the end of the output.\naudio = feature_extractor.batch_decode(**audio)[0]\n# Convert to wav file\nwrite(\"sample_audio.wav\", feature_extractor.sampling_rate, audio)\n```\n\nThis model was contributed by [dg845](https://huggingface.co/dg845).",
  "To the best of my knowledge, there is no official code release, but an unofficial implementation can be found at [maum-ai/univnet](https://github.com/maum-ai/univnet) with pretrained checkpoints [here](https://github.com/maum-ai/univnet#pre-trained-model).\n\n\n## UnivNetConfig\n\n[[autodoc]] UnivNetConfig\n\n## UnivNetFeatureExtractor\n\n[[autodoc]] UnivNetFeatureExtractor\n- __call__\n\n## UnivNetModel\n\n[[autodoc]] UnivNetModel\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n# Jukebox\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.",
  "You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n</Tip>\n\n## Overview\n\nThe Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf)\nby Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,\nIlya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditioned on\nan artist, genres and lyrics.\n\nThe abstract from the paper is the following:\n\n*We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.*",
  "As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They follow the architecture described in [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509), modified to support longer context length.\nFirst, a autoencoder is used to encode the text lyrics. Next, the first (also called `top_prior`) prior attends to the last hidden states extracted from the lyrics encoder. The priors are linked to the previous priors respectively via an `AudioConditioner` module. The`AudioConditioner` upsamples the outputs of the previous prior to raw tokens at a certain audio frame per second resolution.\nThe metadata such as *artist, genre and timing* are passed to each prior, in the form of a start token and positional embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio.\n\n![JukeboxModel](https://gist.githubusercontent.com/ArthurZucker/92c1acaae62ebf1b6a951710bdd8b6af/raw/c9c517bf4eff61393f6c7dec9366ef02bdd059a3/jukebox.svg)\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).",
  "The original code can be found [here](https://github.com/openai/jukebox).\n\n## Usage tips\n\n- This model only supports inference. This is for a few reasons, mostly because it requires a crazy amount of memory to train. Feel free to open a PR and add what's missing to have a full integration with the hugging face trainer!\n- This model is very slow, and takes 8h to generate a minute long audio using the 5b top prior on a V100 GPU. In order automaticallay handle the device on which the model should execute, use `accelerate`.\n- Contrary to the paper, the order of the priors goes from `0` to `1` as it felt more intuitive : we sample starting from `0`.\n- Primed sampling (conditioning the sampling on raw audio) requires more memory than ancestral sampling and should be used with `fp16` set to `True`.\n\nThis model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).\nThe original code can be found [here](https://github.com/openai/jukebox).\n\n## JukeboxConfig\n\n[[autodoc]] JukeboxConfig\n\n## JukeboxPriorConfig\n\n[[autodoc]] JukeboxPriorConfig\n\n## JukeboxVQVAEConfig\n\n[[autodoc]] JukeboxVQVAEConfig\n\n## JukeboxTokenizer\n\n[[autodoc]] JukeboxTokenizer\n- save_vocabulary\n\n## JukeboxModel",
  "[[autodoc]] JukeboxModel\n- ancestral_sample\n- primed_sample\n- continue_sample\n- upsample\n- _sample\n\n## JukeboxPrior\n\n[[autodoc]] JukeboxPrior\n- sample\n- forward\n\n## JukeboxVQVAE\n\n[[autodoc]] JukeboxVQVAE\n- forward\n- encode\n- decode",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MusicGen\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The MusicGen model was proposed in the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)\nby Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.\n\nMusicGen is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned\non text descriptions or audio prompts. The text descriptions are passed through a frozen text encoder model to obtain a\nsequence of hidden-state representations. MusicGen is then trained to predict discrete audio tokens, or *audio codes*,\nconditioned on these hidden-states. These audio tokens are then decoded using an audio compression model, such as EnCodec,\nto recover the audio waveform.\n\nThrough an efficient token interleaving pattern, MusicGen does not require a self-supervised semantic representation of\nthe text/audio prompts, thus eliminating the need to cascade multiple models to predict a set of codebooks (e.g.\nhierarchically or upsampling). Instead, it is able to generate all the codebooks in a single forward pass.\n\nThe abstract from the paper is the following:",
  "*We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates\nover several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised\nof a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for\ncascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen\ncan generate high-quality samples, while being conditioned on textual description or melodic features, allowing better\ncontrols over the generated output. We conduct extensive empirical evaluation, considering both automatic and human\nstudies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark.\nThrough ablation studies, we shed light over the importance of each of the components comprising MusicGen.*\n\nThis model was contributed by [sanchit-gandhi](https://huggingface.co/sanchit-gandhi). The original code can be found\n[here](https://github.com/facebookresearch/audiocraft). The pre-trained checkpoints can be found on the",
  "[Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen-).\n\n## Usage tips\n\n- After downloading the original checkpoints from [here](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models) , you can convert them using the **conversion script** available at\n`src/transformers/models/musicgen/convert_musicgen_transformers.py` with the following command:\n\n```bash\npython src/transformers/models/musicgen/convert_musicgen_transformers.py \\\n--checkpoint small --pytorch_dump_folder /output/path --safe_serialization\n```\n\n## Generation\n\nMusicGen is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly\nbetter results than greedy, thus we encourage sampling mode to be used where possible. Sampling is enabled by default,\nand can be explicitly specified by setting `do_sample=True` in the call to [`MusicgenForConditionalGeneration.generate`],\nor by overriding the model's generation config (see below).\n\nGeneration is limited by the sinusoidal positional embeddings to 30 second inputs. Meaning, MusicGen cannot generate more",
  "than 30 seconds of audio (1503 tokens), and input audio passed by Audio-Prompted Generation contributes to this limit so,\ngiven an input of 20 seconds of audio, MusicGen cannot generate more than 10 seconds of additional audio.\n\nTransformers supports both mono (1-channel) and stereo (2-channel) variants of MusicGen. The mono channel versions\ngenerate a single set of codebooks. The stereo versions generate 2 sets of codebooks, 1 for each channel (left/right),\nand each set of codebooks is decoded independently through the audio compression model. The audio streams for each\nchannel are combined to give the final stereo output.\n\n### Unconditional Generation\n\nThe inputs for unconditional (or 'null') generation can be obtained through the method\n[`MusicgenForConditionalGeneration.get_unconditional_inputs`]:\n\n```python\n>>> from transformers import MusicgenForConditionalGeneration\n\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n>>> unconditional_inputs = model.get_unconditional_inputs(num_samples=1)\n\n>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)\n```",
  "The audio outputs are a three-dimensional Torch tensor of shape `(batch_size, num_channels, sequence_length)`. To listen\nto the generated audio samples, you can either play them in an ipynb notebook:\n\n```python\nfrom IPython.display import Audio\n\nsampling_rate = model.config.audio_encoder.sampling_rate\nAudio(audio_values[0].numpy(), rate=sampling_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\n\n```python\n>>> import scipy\n\n>>> sampling_rate = model.config.audio_encoder.sampling_rate\n>>> scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())\n```\n\n### Text-Conditional Generation\n\nThe model can generate an audio sample conditioned on a text prompt through use of the [`MusicgenProcessor`] to pre-process\nthe inputs:\n\n```python\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> inputs = processor(\n...     text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],",
  "...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\nThe `guidance_scale` is used in classifier free guidance (CFG), setting the weighting between the conditional logits\n(which are predicted from the text prompts) and the unconditional logits (which are predicted from an unconditional or\n'null' prompt). Higher guidance scale encourages the model to generate samples that are more closely linked to the input\nprompt, usually at the expense of poorer audio quality. CFG is enabled by setting `guidance_scale > 1`. For best results,\nuse `guidance_scale=3` (default).\n\n### Audio-Prompted Generation\n\nThe same [`MusicgenProcessor`] can be used to pre-process an audio prompt that is used for audio continuation. In the\nfollowing example, we load an audio file using the 🤗 Datasets library, which can be pip installed through the command\nbelow:\n\n```bash\npip install --upgrade pip\npip install datasets[audio]\n```\n\n```python\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n>>> from datasets import load_dataset",
  ">>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> dataset = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\", streaming=True)\n>>> sample = next(iter(dataset))[\"audio\"]\n\n>>> # take the first half of the audio sample\n>>> sample[\"array\"] = sample[\"array\"][: len(sample[\"array\"]) // 2]\n\n>>> inputs = processor(\n...     audio=sample[\"array\"],\n...     sampling_rate=sample[\"sampling_rate\"],\n...     text=[\"80s blues track with groovy saxophone\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\nFor batched audio-prompted generation, the generated `audio_values` can be post-processed to remove padding by using the\n[`MusicgenProcessor`] class:\n\n```python\n>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n>>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")",
  ">>> dataset = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\", streaming=True)\n>>> sample = next(iter(dataset))[\"audio\"]\n\n>>> # take the first quarter of the audio sample\n>>> sample_1 = sample[\"array\"][: len(sample[\"array\"]) // 4]\n\n>>> # take the first half of the audio sample\n>>> sample_2 = sample[\"array\"][: len(sample[\"array\"]) // 2]\n\n>>> inputs = processor(\n...     audio=[sample_1, sample_2],\n...     sampling_rate=sample[\"sampling_rate\"],\n...     text=[\"80s blues track with groovy saxophone\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n\n>>> # post-process to remove padding from the batched audio\n>>> audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)\n```\n\n### Generation Configuration\n\nThe default parameters that control the generation process, such as sampling, guidance scale and number of generated\ntokens, can be found in the model's generation config, and updated as desired:\n\n```python\n>>> from transformers import MusicgenForConditionalGeneration",
  ">>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n\n>>> # inspect the default generation config\n>>> model.generation_config\n\n>>> # increase the guidance scale to 4.0\n>>> model.generation_config.guidance_scale = 4.0\n\n>>> # decrease the max length to 256 tokens\n>>> model.generation_config.max_length = 256\n```\n\nNote that any arguments passed to the generate method will **supersede** those in the generation config, so setting\n`do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the\ngeneration config.\n\n## Model Structure\n\nThe MusicGen model can be de-composed into three distinct stages:\n1. Text encoder: maps the text inputs to a sequence of hidden-state representations. The pre-trained MusicGen models use a frozen text encoder from either T5 or Flan-T5\n2. MusicGen decoder: a language model (LM) that auto-regressively generates audio tokens (or codes) conditional on the encoder hidden-state representations\n3. Audio encoder/decoder: used to encode an audio prompt to use as prompt tokens, and recover the audio waveform from the audio tokens predicted by the decoder",
  "Thus, the MusicGen model can either be used as a standalone decoder model, corresponding to the class [`MusicgenForCausalLM`],\nor as a composite model that includes the text encoder and audio encoder/decoder, corresponding to the class\n[`MusicgenForConditionalGeneration`]. If only the decoder needs to be loaded from the pre-trained checkpoint, it can be loaded by first\nspecifying the correct config, or be accessed through the `.decoder` attribute of the composite model:\n\n```python\n>>> from transformers import AutoConfig, MusicgenForCausalLM, MusicgenForConditionalGeneration\n\n>>> # Option 1: get decoder config and pass to `.from_pretrained`\n>>> decoder_config = AutoConfig.from_pretrained(\"facebook/musicgen-small\").decoder\n>>> decoder = MusicgenForCausalLM.from_pretrained(\"facebook/musicgen-small\", **decoder_config)\n\n>>> # Option 2: load the entire composite model, but only return the decoder\n>>> decoder = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").decoder\n```\n\nSince the text encoder and audio encoder/decoder models are frozen during training, the MusicGen decoder [`MusicgenForCausalLM`]",
  "can be trained standalone on a dataset of encoder hidden-states and audio codes. For inference, the trained decoder can\nbe combined with the frozen text encoder and audio encoder/decoders to recover the composite [`MusicgenForConditionalGeneration`]\nmodel.\n\nTips:\n* MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you use a compatible version of the Encodec model.\n* Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenForConditionalGeneration.generate`]\n\n## MusicgenDecoderConfig\n\n[[autodoc]] MusicgenDecoderConfig\n\n## MusicgenConfig\n\n[[autodoc]] MusicgenConfig\n\n## MusicgenProcessor\n\n[[autodoc]] MusicgenProcessor\n\n## MusicgenModel\n\n[[autodoc]] MusicgenModel\n- forward\n\n## MusicgenForCausalLM\n\n[[autodoc]] MusicgenForCausalLM\n- forward\n\n## MusicgenForConditionalGeneration\n\n[[autodoc]] MusicgenForConditionalGeneration\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Swin Transformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The Swin Transformer was proposed in [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\nby Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n\nThe abstract from the paper is the following:\n\n*This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone\nfor computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains,\nsuch as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text.\nTo address these differences, we propose a hierarchical Transformer whose representation is computed with \\bold{S}hifted\n\\bold{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping\nlocal windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at\nvarious scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it",
  "compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense\nprediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation\n(53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and\n+2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones.\nThe hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/swin_transformer_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Swin Transformer architecture. Taken from the <a href=\"https://arxiv.org/abs/2102.03334\">original paper</a>.</small>\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The Tensorflow version of this model was contributed by [amyeroberts](https://huggingface.co/amyeroberts). The original code can be found [here](https://github.com/microsoft/Swin-Transformer).\n\n## Usage tips",
  "- Swin pads the inputs supporting any input height and width (if divisible by `32`).\n- Swin can be used as a *backbone*. When `output_hidden_states = True`, it will output both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states` have a shape of `(batch, num_channels, height, width)` rather than `(batch_size, sequence_length, num_channels)`.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`SwinForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nBesides that:\n\n- [`SwinForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## SwinConfig\n\n[[autodoc]] SwinConfig\n\n<frameworkcontent>\n<pt>\n\n## SwinModel\n\n[[autodoc]] SwinModel\n- forward\n\n## SwinForMaskedImageModeling\n\n[[autodoc]] SwinForMaskedImageModeling\n- forward\n\n## SwinForImageClassification\n\n[[autodoc]] transformers.SwinForImageClassification\n- forward\n\n</pt>\n<tf>\n\n## TFSwinModel\n\n[[autodoc]] TFSwinModel\n- call\n\n## TFSwinForMaskedImageModeling\n\n[[autodoc]] TFSwinForMaskedImageModeling\n- call\n\n## TFSwinForImageClassification\n\n[[autodoc]] transformers.TFSwinForImageClassification\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Prompt Depth Anything\n\n## Overview\n\nThe Prompt Depth Anything model was introduced in [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://arxiv.org/abs/2412.14015) by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang.\n\n\nThe abstract from the paper is as follows:",
  "*Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. Our approach sets new state-of-the-arts on the ARKitScenes and ScanNet++ datasets and benefits downstream applications, including 3D reconstruction and generalized robotic grasping.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/prompt_depth_anything_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>",
  "<small> Prompt Depth Anything overview. Taken from the <a href=\"https://arxiv.org/pdf/2412.14015\">original paper</a>.</small>\n\n## Usage example\n\nThe Transformers library allows you to use the model with just a few lines of code:\n\n```python\n>>> import torch\n>>> import requests\n>>> import numpy as np\n\n>>> from PIL import Image\n>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n\n>>> url = \"https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/image.jpg?raw=true\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/prompt-depth-anything-vits-hf\")\n>>> model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/prompt-depth-anything-vits-hf\")\n\n>>> prompt_depth_url = \"https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/arkit_depth.png?raw=true\"\n>>> prompt_depth = Image.open(requests.get(prompt_depth_url, stream=True).raw)\n>>> # the prompt depth can be None, and the model will output a monocular relative depth.\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\", prompt_depth=prompt_depth)",
  ">>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # interpolate to original size\n>>> post_processed_output = image_processor.post_process_depth_estimation(\n...     outputs,\n...     target_sizes=[(image.height, image.width)],\n... )\n\n>>> # visualize the prediction\n>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n>>> depth = predicted_depth * 1000\n>>> depth = depth.detach().cpu().numpy()\n>>> depth = Image.fromarray(depth.astype(\"uint16\")) # mm\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Prompt Depth Anything.\n\n- [Prompt Depth Anything Demo](https://huggingface.co/spaces/depth-anything/PromptDA)\n- [Prompt Depth Anything Interactive Results](https://promptda.github.io/interactive.html)\n\nIf you are interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## PromptDepthAnythingConfig\n\n[[autodoc]] PromptDepthAnythingConfig\n\n## PromptDepthAnythingForDepthEstimation\n\n[[autodoc]] PromptDepthAnythingForDepthEstimation",
  "- forward\n\n## PromptDepthAnythingImageProcessor\n\n[[autodoc]] PromptDepthAnythingImageProcessor\n- preprocess\n- post_process_depth_estimation",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Perceiver\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe Perceiver IO model was proposed in [Perceiver IO: A General Architecture for Structured Inputs &\nOutputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,",
  "Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M.\nBotvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.\n\nPerceiver IO is a generalization of [Perceiver](https://arxiv.org/abs/2103.03206) to handle arbitrary outputs in\naddition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to\nclassification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with audio.\nThis is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO is\nlinear in the input and output size and the bulk of the processing occurs in the latent space, allowing us to process\ninputs and outputs that are much larger than can be handled by standard Transformers. This means, for example,\nPerceiver IO can do BERT-style masked language modeling directly using bytes instead of tokenized inputs.\n\nThe abstract from the paper is the following:\n\n*The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point",
  "clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of\ninputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without\nsacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales\nlinearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural language and visual understanding,\nStarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT\nbaseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art\nperformance on Sintel optical flow estimation.*\n\nHere's a TLDR explaining how Perceiver works:\n\nThe main problem with the self-attention mechanism of the Transformer is that the time and memory requirements scale",
  "quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of 512\ntokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set\nof latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don't\ndepend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are\nrandomly initialized, after which they are trained end-to-end using backpropagation.\n\nInternally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch_size, num_latents,\nd_latents)`. One must provide `inputs` (which could be text, images, audio, you name it!) to the model, which it will\nuse to perform cross-attention with the latents. The output of the Perceiver encoder is a tensor of the same shape. One\ncan then, similar to BERT, convert the last hidden states of the latents to classification logits by averaging along\nthe sequence dimension, and placing a linear layer on top of that to project the `d_latents` to `num_labels`.",
  "This was the idea of the original Perceiver paper. However, it could only output classification logits. In a follow-up\nwork, PerceiverIO, they generalized it to let the model also produce outputs of arbitrary size. How, you might ask? The\nidea is actually relatively simple: one defines outputs of an arbitrary size, and then applies cross-attention with the\nlast hidden states of the latents, using the outputs as queries, and the latents as keys and values.\n\nSo let's say one wants to perform masked language modeling (BERT-style) with the Perceiver. As the Perceiver's input\nlength will not have an impact on the computation time of the self-attention layers, one can provide raw bytes,\nproviding `inputs` of length 2048 to the model. If one now masks out certain of these 2048 tokens, one can define the\n`outputs` as being of shape: `(batch_size, 2048, 768)`. Next, one performs cross-attention with the final hidden states\nof the latents to update the `outputs` tensor. After cross-attention, one still has a tensor of shape `(batch_size,\n2048, 768)`. One can then place a regular language modeling head on top, to project the last dimension to the",
  "vocabulary size of the model, i.e. creating logits of shape `(batch_size, 2048, 262)` (as Perceiver uses a vocabulary\nsize of 262 byte IDs).\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perceiver_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> Perceiver IO architecture. Taken from the <a href=\"https://arxiv.org/abs/2105.15203\">original paper</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n[here](https://github.com/deepmind/deepmind-research/tree/master/perceiver).\n\n<Tip warning={true}>\n\nPerceiver does **not** work with `torch.nn.DataParallel` due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)\n\n</Tip>\n\n## Resources\n\n- The quickest way to get started with the Perceiver is by checking the [tutorial\nnotebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Perceiver).\n- Refer to the [blog post](https://huggingface.co/blog/perceiver) if you want to fully understand how the model works and\nis implemented in the library. Note that the models available in the library only showcase some examples of what you can do",
  "with the Perceiver. There are many more use cases, including question answering, named-entity recognition, object detection,\naudio classification, video classification, etc.\n- [Text classification task guide](../tasks/sequence_classification)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Image classification task guide](../tasks/image_classification)\n\n## Perceiver specific outputs\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverModelOutput\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverDecoderOutput\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverClassifierOutput\n\n## PerceiverConfig\n\n[[autodoc]] PerceiverConfig\n\n## PerceiverTokenizer\n\n[[autodoc]] PerceiverTokenizer\n- __call__\n\n## PerceiverFeatureExtractor\n\n[[autodoc]] PerceiverFeatureExtractor\n- __call__\n\n## PerceiverImageProcessor\n\n[[autodoc]] PerceiverImageProcessor\n- preprocess\n\n## PerceiverTextPreprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverTextPreprocessor\n\n## PerceiverImagePreprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverImagePreprocessor",
  "## PerceiverOneHotPreprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor\n\n## PerceiverAudioPreprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor\n\n## PerceiverMultimodalPreprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor\n\n## PerceiverProjectionDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverProjectionDecoder\n\n## PerceiverBasicDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverBasicDecoder\n\n## PerceiverClassificationDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverClassificationDecoder\n\n## PerceiverOpticalFlowDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverOpticalFlowDecoder\n\n## PerceiverBasicVideoAutoencodingDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverBasicVideoAutoencodingDecoder\n\n## PerceiverMultimodalDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder\n\n## PerceiverProjectionPostprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor\n\n## PerceiverAudioPostprocessor",
  "[[autodoc]] models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor\n\n## PerceiverClassificationPostprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor\n\n## PerceiverMultimodalPostprocessor\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor\n\n## PerceiverModel\n\n[[autodoc]] PerceiverModel\n- forward\n\n## PerceiverForMaskedLM\n\n[[autodoc]] PerceiverForMaskedLM\n- forward\n\n## PerceiverForSequenceClassification\n\n[[autodoc]] PerceiverForSequenceClassification\n- forward\n\n## PerceiverForImageClassificationLearned\n\n[[autodoc]] PerceiverForImageClassificationLearned\n- forward\n\n## PerceiverForImageClassificationFourier\n\n[[autodoc]] PerceiverForImageClassificationFourier\n- forward\n\n## PerceiverForImageClassificationConvProcessing\n\n[[autodoc]] PerceiverForImageClassificationConvProcessing\n- forward\n\n## PerceiverForOpticalFlow\n\n[[autodoc]] PerceiverForOpticalFlow\n- forward\n\n## PerceiverForMultimodalAutoencoding\n\n[[autodoc]] PerceiverForMultimodalAutoencoding\n- forward",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# X-MOD\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.",
  "X-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (_language adapters_) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.\n\nThe abstract from the paper is the following:",
  "*Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-MOD) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.*\n\nThis model was contributed by [jvamvas](https://huggingface.co/jvamvas).",
  "The original code can be found [here](https://github.com/facebookresearch/fairseq/tree/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/models/xmod) and the original documentation is found [here](https://github.com/facebookresearch/fairseq/tree/58cc6cca18f15e6d56e3f60c959fe4f878960a60/examples/xmod).\n\n## Usage tips\n\nTips:\n- X-MOD is similar to [XLM-R](xlm-roberta), but a difference is that the input language needs to be specified so that the correct language adapter can be activated.\n- The main models – base and large – have adapters for 81 languages.\n\n## Adapter Usage\n\n### Input language\n\nThere are two ways to specify the input language:\n1. By setting a default language before using the model:\n\n```python\nfrom transformers import XmodModel\n\nmodel = XmodModel.from_pretrained(\"facebook/xmod-base\")\nmodel.set_default_language(\"en_XX\")\n```\n\n2. By explicitly passing the index of the language adapter for each sample:\n\n```python\nimport torch\n\ninput_ids = torch.tensor(\n[\n[0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2],\n[0, 1310, 49083, 443, 269, 71, 5486, 165, 60429, 660, 23, 2],\n]\n)\nlang_ids = torch.LongTensor(\n[\n0,  # en_XX\n8,  # de_DE\n]\n)",
  "output = model(input_ids, lang_ids=lang_ids)\n```\n\n### Fine-tuning\nThe paper recommends that the embedding layer and the language adapters are frozen during fine-tuning. A method for doing this is provided:\n\n```python\nmodel.freeze_embeddings_and_language_adapters()\n# Fine-tune the model ...\n```\n\n### Cross-lingual transfer\nAfter fine-tuning, zero-shot cross-lingual transfer can be tested by activating the language adapter of the target language:\n\n```python\nmodel.set_default_language(\"de_DE\")\n# Evaluate the model on German examples ...\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## XmodConfig\n\n[[autodoc]] XmodConfig\n\n## XmodModel\n\n[[autodoc]] XmodModel\n- forward\n\n## XmodForCausalLM\n\n[[autodoc]] XmodForCausalLM\n- forward\n\n## XmodForMaskedLM\n\n[[autodoc]] XmodForMaskedLM\n- forward\n\n## XmodForSequenceClassification",
  "[[autodoc]] XmodForSequenceClassification\n- forward\n\n## XmodForMultipleChoice\n\n[[autodoc]] XmodForMultipleChoice\n- forward\n\n## XmodForTokenClassification\n\n[[autodoc]] XmodForTokenClassification\n- forward\n\n## XmodForQuestionAnswering\n\n[[autodoc]] XmodForQuestionAnswering\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DistilBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a\ndistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a\nsmall, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than\n*google-bert/bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language\nunderstanding benchmark.\n\nThe abstract from the paper is the following:\n\n*As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),\noperating these large models in on-the-edge and/or under constrained computational training or inference budgets",
  "remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation\nmodel, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger\ncounterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage\nknowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by\n40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive\nbiases learned by larger models during pretraining, we introduce a triple loss combining language modeling,\ndistillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we\ndemonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device\nstudy.*\n\nThis model was contributed by [victorsanh](https://huggingface.co/victorsanh). This model jax version was",
  "contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/huggingface/transformers-research-projects/tree/main/distillation).\n\n## Usage tips\n\n- DistilBERT doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. Just\nseparate your segments with the separation token `tokenizer.sep_token` (or `[SEP]`).\n- DistilBERT doesn't have options to select the input positions (`position_ids` input). This could be added if\nnecessary though, just let us know if you need this option.\n- Same as BERT but smaller. Trained by distillation of the pretrained BERT model, meaning it’s been trained to predict the same probabilities as the larger model. The actual objective is a combination of:\n\n* finding the same probabilities as the teacher model\n* predicting the masked tokens correctly (but no next-sentence objective)\n* a cosine similarity between the hidden states of the student and the teacher model\n\n### Using Scaled Dot Product Attention (SDPA)\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function",
  "encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\nor the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\npage for more information.\n\nSDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n\n```\nfrom transformers import DistilBertModel\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n```\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\nOn a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and the `distilbert-base-uncased` model with\na MaskedLM head, we saw the following speedups during training and inference.\n\n#### Training",
  "| num_training_steps | batch_size | seq_len | is cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | sdpa peak mem (MB) | Mem saving (%) |\n|--------------------|------------|---------|---------|----------------------------|---------------------------|-------------|---------------------|--------------------|----------------|\n| 100                | 1          | 128     | False   | 0.010                      | 0.008                     | 28.870      | 397.038             | 399.629            | -0.649         |\n| 100                | 1          | 256     | False   | 0.011                      | 0.009                     | 20.681      | 412.505             | 412.606            | -0.025         |\n| 100                | 2          | 128     | False   | 0.011                      | 0.009                     | 23.741      | 412.213             | 412.606            | -0.095         |\n| 100                | 2          | 256     | False   | 0.015                      | 0.013                     | 16.502      | 427.491             | 425.787            | 0.400          |",
  "| 100                | 4          | 128     | False   | 0.015                      | 0.013                     | 13.828      | 427.491             | 425.787            | 0.400          |\n| 100                | 4          | 256     | False   | 0.025                      | 0.022                     | 12.882      | 594.156             | 502.745            | 18.182         |\n| 100                | 8          | 128     | False   | 0.023                      | 0.022                     | 8.010       | 545.922             | 502.745            | 8.588          |\n| 100                | 8          | 256     | False   | 0.046                      | 0.041                     | 12.763      | 983.450             | 798.480            | 23.165         |\n\n#### Inference\n\n| num_batches | batch_size | seq_len | is cuda | is half | use mask | Per token latency eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem eager (MB) | Mem BT (MB) | Mem saved (%) |\n|-------------|------------|---------|---------|---------|----------|-----------------------------|-----------------------------|-------------|----------------|--------------|---------------|",
  "| 50          | 2          | 64      | True    | True    | True     | 0.032                       | 0.025                       | 28.192      | 154.532        | 155.531      | -0.642        |\n| 50          | 2          | 128     | True    | True    | True     | 0.033                       | 0.025                       | 32.636      | 157.286        | 157.482      | -0.125        |\n| 50          | 4          | 64      | True    | True    | True     | 0.032                       | 0.026                       | 24.783      | 157.023        | 157.449      | -0.271        |\n| 50          | 4          | 128     | True    | True    | True     | 0.034                       | 0.028                       | 19.299      | 162.794        | 162.269      | 0.323         |\n| 50          | 8          | 64      | True    | True    | True     | 0.035                       | 0.028                       | 25.105      | 160.958        | 162.204      | -0.768        |\n| 50          | 8          | 128     | True    | True    | True     | 0.052                       | 0.046                       | 12.375      | 173.155        | 171.844      | 0.763         |",
  "| 50          | 16         | 64      | True    | True    | True     | 0.051                       | 0.045                       | 12.882      | 172.106        | 171.713      | 0.229         |\n| 50          | 16         | 128     | True    | True    | True     | 0.096                       | 0.081                       | 18.524      | 191.257        | 191.517      | -0.136        |\n\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DistilBERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python) with DistilBERT.\n- A blog post on how to [train DistilBERT with Blurr for sequence classification](https://huggingface.co/blog/fastai).\n- A blog post on how to use [Ray to tune DistilBERT hyperparameters](https://huggingface.co/blog/ray-tune).",
  "- A blog post on how to [train DistilBERT with Hugging Face and Amazon SageMaker](https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face).\n- A notebook on how to [finetune DistilBERT for multi-label classification](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb). 🌎\n- A notebook on how to [finetune DistilBERT for multiclass classification with PyTorch](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb). 🌎\n- A notebook on how to [finetune DistilBERT for text classification in TensorFlow](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb). 🌎\n- [`DistilBertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).",
  "- [`TFDistilBertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n- [`FlaxDistilBertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n- [Text classification task guide](../tasks/sequence_classification)\n\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- [`DistilBertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).",
  "- [`TFDistilBertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n- [`FlaxDistilBertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).\n- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Token classification task guide](../tasks/token_classification)\n\n\n<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`DistilBertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).",
  "- [`TFDistilBertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxDistilBertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- [`DistilBertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).",
  "- [`TFDistilBertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxDistilBertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n**Multiple choice**\n- [`DistilBertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).",
  "- [`TFDistilBertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n⚗️ Optimization\n\n- A blog post on how to [quantize DistilBERT with 🤗 Optimum and Intel](https://huggingface.co/blog/intel).\n- A blog post on how [Optimizing Transformers for GPUs with 🤗 Optimum](https://www.philschmid.de/optimizing-transformers-with-optimum-gpu).\n- A blog post on [Optimizing Transformers with Hugging Face Optimum](https://www.philschmid.de/optimizing-transformers-with-optimum).\n\n⚡️ Inference\n\n- A blog post on how to [Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker) with DistilBERT.\n- A blog post on [Serverless Inference with Hugging Face's Transformers, DistilBERT and Amazon SageMaker](https://www.philschmid.de/sagemaker-serverless-huggingface-distilbert).\n\n🚀 Deploy",
  "- A blog post on how to [deploy DistilBERT on Google Cloud](https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds).\n- A blog post on how to [deploy DistilBERT with Amazon SageMaker](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker).\n- A blog post on how to [Deploy BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker).\n\n\n## Combining DistilBERT and Flash Attention 2\n\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16`)\n\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n\n```python\n>>> import torch\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> device = \"cuda\" # the device to load the model onto",
  ">>> tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n>>> model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n\n>>> text = \"Replace me by any text you'd like.\"\n\n>>> encoded_input = tokenizer(text, return_tensors='pt').to(device)\n>>> model.to(device)\n\n>>> output = model(**encoded_input)\n```\n\n\n## DistilBertConfig\n\n[[autodoc]] DistilBertConfig\n\n## DistilBertTokenizer\n\n[[autodoc]] DistilBertTokenizer\n\n## DistilBertTokenizerFast\n\n[[autodoc]] DistilBertTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## DistilBertModel\n\n[[autodoc]] DistilBertModel\n- forward\n\n## DistilBertForMaskedLM\n\n[[autodoc]] DistilBertForMaskedLM\n- forward\n\n## DistilBertForSequenceClassification\n\n[[autodoc]] DistilBertForSequenceClassification\n- forward\n\n## DistilBertForMultipleChoice\n\n[[autodoc]] DistilBertForMultipleChoice\n- forward\n\n## DistilBertForTokenClassification\n\n[[autodoc]] DistilBertForTokenClassification\n- forward\n\n## DistilBertForQuestionAnswering\n\n[[autodoc]] DistilBertForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFDistilBertModel\n\n[[autodoc]] TFDistilBertModel\n- call",
  "## TFDistilBertForMaskedLM\n\n[[autodoc]] TFDistilBertForMaskedLM\n- call\n\n## TFDistilBertForSequenceClassification\n\n[[autodoc]] TFDistilBertForSequenceClassification\n- call\n\n## TFDistilBertForMultipleChoice\n\n[[autodoc]] TFDistilBertForMultipleChoice\n- call\n\n## TFDistilBertForTokenClassification\n\n[[autodoc]] TFDistilBertForTokenClassification\n- call\n\n## TFDistilBertForQuestionAnswering\n\n[[autodoc]] TFDistilBertForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxDistilBertModel\n\n[[autodoc]] FlaxDistilBertModel\n- __call__\n\n## FlaxDistilBertForMaskedLM\n\n[[autodoc]] FlaxDistilBertForMaskedLM\n- __call__\n\n## FlaxDistilBertForSequenceClassification\n\n[[autodoc]] FlaxDistilBertForSequenceClassification\n- __call__\n\n## FlaxDistilBertForMultipleChoice\n\n[[autodoc]] FlaxDistilBertForMultipleChoice\n- __call__\n\n## FlaxDistilBertForTokenClassification\n\n[[autodoc]] FlaxDistilBertForTokenClassification\n- __call__\n\n## FlaxDistilBertForQuestionAnswering\n\n[[autodoc]] FlaxDistilBertForQuestionAnswering\n- __call__\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# OpenAI GPT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAMAAAANxBKoAAAC7lBMVEUAAADg5vYHPVgAoJH+/v76+v39/f9JbLP///9+AIgAnY3///+mcqzt8fXy9fgkXa3Ax9709fr+///9/f8qXq49qp5AaLGMwrv8/P0eW60VWawxYq8yqJzG2dytt9Wyu9elzci519Lf3O3S2efY3OrY0+Xp7PT///////+dqNCexMc6Z7AGpJeGvbenstPZ5ejQ1OfJzOLa7ejh4+/r8fT29vpccbklWK8PVa0AS6ghW63O498vYa+lsdKz1NDRt9Kw1c672tbD3tnAxt7R6OHp5vDe7OrDyuDn6vLl6/EAQKak0MgATakkppo3ZK/Bz9y8w9yzu9jey97axdvHzeG21NHH4trTwthKZrVGZLSUSpuPQJiGAI+GAI8SWKydycLL4d7f2OTi1+S9xNzL0ePT6OLGzeEAo5U0qJw/aLEAo5JFa7JBabEAp5Y4qZ2QxLyKmsm3kL2xoMOehrRNb7RIbbOZgrGre68AUqwAqZqNN5aKJ5N/lMq+qsd8kMa4pcWzh7muhLMEV69juq2kbKqgUaOTR5uMMZWLLZSGAI5VAIdEAH+ovNDHuNCnxcy3qcaYx8K8msGplrx+wLahjbYdXrV6vbMvYK9DrZ8QrZ8tqJuFms+Sos6sw8ecy8RffsNVeMCvmb43aLltv7Q4Y7EZWK4QWa1gt6meZKUdr6GOAZVeA4xPAISyveLUwtivxtKTpNJ2jcqfvcltiMiwwcfAoMVxhL+Kx7xjdrqTe60tsaNQs6KaRKACrJ6UTZwkqpqTL5pkHY4AloSgsd2ptNXPvNOOncuxxsqFl8lmg8apt8FJcr9EbryGxLqlkrkrY7dRa7ZGZLQ5t6iXUZ6PPpgVpZeJCJFKAIGareTa0+KJod3H0deY2M+esM25usmYu8d2zsJOdcBVvrCLbqcAOaaHaKQAMaScWqKBXqCXMJ2RHpiLF5NmJZAdAHN2kta11dKu1M+DkcZLdb+Mcql3TppyRJdzQ5ZtNZNlIY+DF4+voCOQAAAAZ3RSTlMABAT+MEEJ/RH+/TP+Zlv+pUo6Ifz8+fco/fz6+evr39S9nJmOilQaF/7+/f38+smmoYp6b1T+/v7++vj189zU0tDJxsGzsrKSfv34+Pf27dDOysG9t6+n/vv6+vr59uzr1tG+tZ6Qg9Ym3QAABR5JREFUSMeNlVVUG1EQhpcuxEspXqS0SKEtxQp1d3d332STTRpIQhIISQgJhODu7lAoDoUCpe7u7u7+1puGpqnCPOyZvffbOXPm/PsP9JfQgyCC+tmTABTOcbxDz/heENS7/1F+9nhvkHePG0wNDLbGWwdXL+rbLWvpmZHXD8+gMfBjTh+aSe6Gnn7lwQIOTR0c8wfX3PWgv7avbdKwf/ZoBp1Gp/PvuvXW3vw5ib7emnTW4OR+3D4jB9vjNJ/7gNvfWWeH/TO/JyYrsiKCRjVEZA3UB+96kON+DxOQ/NLE8PE5iUYgIXjFnCOlxEQMaSGVxjg4gxOnEycGz8bptuNjVx08LscIgrzH3umcn+KKtiBIyvzOO2O99aAdR8cF19oZalnCtvREUw79tCd5sow1g1UKM6kXqUx4T8wsi3sTjJ3yzDmmhenLXLpo8u45eG5y4Vvbk6kkC4LLtJMowkSQxmk4ggVJEG+7c6QpHT8vvW9X7/o7+3ELmiJi2mEzZJiz8cT6TBlanBk70cB5GGIGC1gRDdZ00yADLW1FL6gqhtvNXNG5S9gdSrk4M1qu7JAsmYshzDS4peoMrU/gT7qQdqYGZaYhxZmVbGJAm/CS/HloWyhRUlknQ9KYcExTwS80d3VNOxUZJpITYyspl0LbhArhpZCD9cRWEQuhYkNGMHToQ/2Cs6swJlb39CsllxdXX6IUKh/H5jbnSsPKjgmoaFQ1f8wRLR0UnGE/RcDEjj2jXG1WVTwUs8+zxfcrVO+vSsuOpVKxCfYZiQ0/aPKuxQbQ8lIz+DClxC8u+snlcJ7Yr1z1JPqUH0V+GDXbOwAib931Y4Imaq0NTIXPXY+N5L18GJ37SVWu+hwXff8l72Ds9XuwYIBaXPq6Shm4l+Vl/5QiOlV+uTk6YR9PxKsI9xNJny31ygK1e+nIRC1N97EGkFPI+jCpiHe5PCEy7oWqWSwRrpOvhFzcbTWMbm3ZJAOn1rUKpYIt/lDhW/5RHHteeWFN60qo98YJuoq1nK3uW5AabyspC1BcIEpOhft+SZAShYoLSvnmSfnYADUERP5jJn2h5XtsgCRuhYQqAvwTwn33+YWEKUI72HX5AtfSAZDe8F2DtPPm77afhl0EkthzuCQU0BWApgQIH9+KB0JhopMM7bJrdTRoleM2JAVNMyPF+wdoaz+XJpGoVAQ7WXUkcV7gT3oUZyi/ISIJAVKhgNp+4b4veCFhYVJw4locdSjZCp9cPUhLF9EZ3KKzURepMEtCDPP3VcWFx4UIiZIklIpFNfHpdEafIF2aRmOcrUmjohbT2WUllbmRvgfbythbQO3222fpDJoufaQPncYYuqoGtUEsCJZL6/3PR5b4syeSjZMQG/T2maGANlXT2v8S4AULWaUkCxfLyW8iW4kdka+nEMjxpL2NCwsYNBp+Q61PF43zyDg9Bm9+3NNySn78jMZUUkumqE4Gp7JmFOdP1vc8PpRrzj9+wPinCy8K1PiJ4aYbnTYpCCbDkBSbzhu2QJ1Gd82t8jI8TH51+OzvXoWbnXUOBkNW+0mWFwGcGOUVpU81/n3TOHb5oMt2FgYGjzau0Nif0Ss7Q3XB33hjjQHjHA5E5aOyIQc8CBrLdQSs3j92VG+3nNEjbkbdbBr9zm04ruvw37vh0QKOdeGIkckc80fX3KH/h7PT4BOjgCty8VZ5ux1MoO5Cf5naca2LAsEgehI+drX8o/0Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nOpenAI GPT model was proposed in [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\nby Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. It's a causal (unidirectional) transformer\npre-trained using language modeling on a large corpus with long range dependencies, the Toronto Book Corpus.\n\nThe abstract from the paper is the following:\n\n*Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering,\nsemantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant,\nlabeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to\nperform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a",
  "language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In\ncontrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve\neffective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our\napproach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms\ndiscriminatively trained models that use architectures specifically crafted for each task, significantly improving upon\nthe state of the art in 9 out of the 12 tasks studied.*\n\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face\nshowcasing the generative capabilities of several models. GPT is one of them.\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/openai/finetune-transformer-lm).\n\n## Usage tips\n\n- GPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left.",
  "- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\ntoken in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\nobserved in the *run_generation.py* example script.\n\n\nNote:\n\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you will need to install `ftfy`\nand `SpaCy`:\n\n```bash\npip install spacy ftfy==4.4.3\npython -m spacy download en\n```\n\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\nusing BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most usage, don't worry).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with OpenAI GPT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>",
  "- A blog post on [outperforming OpenAI GPT-3 with SetFit for text-classification](https://www.philschmid.de/getting-started-setfit).\n- See also: [Text classification task guide](../tasks/sequence_classification)\n\n<PipelineTag pipeline=\"text-generation\"/>\n\n- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).\n- A blog on [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) with GPT-2.\n- A blog on [Training CodeParrot 🦜 from Scratch](https://huggingface.co/blog/codeparrot), a large GPT-2 model.\n- A blog on [Faster Text Generation with TensorFlow and XLA](https://huggingface.co/blog/tf-xla-generate) with GPT-2.\n- A blog on [How to train a Language Model with Megatron-LM](https://huggingface.co/blog/megatron-training) with a GPT-2 model.\n- A notebook on how to [finetune GPT2 to generate lyrics in the style of your favorite artist](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb). 🌎",
  "- A notebook on how to [finetune GPT2 to generate tweets in the style of your favorite Twitter user](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb). 🌎\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the 🤗 Hugging Face Course.\n- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFOpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).",
  "- See also: [Causal language modeling task guide](../tasks/language_modeling)\n\n<PipelineTag pipeline=\"token-classification\"/>\n\n- A course material on [Byte-Pair Encoding tokenization](https://huggingface.co/course/en/chapter6/5).\n\n## OpenAIGPTConfig\n\n[[autodoc]] OpenAIGPTConfig\n\n## OpenAIGPTTokenizer\n\n[[autodoc]] OpenAIGPTTokenizer\n- save_vocabulary\n\n## OpenAIGPTTokenizerFast\n\n[[autodoc]] OpenAIGPTTokenizerFast\n\n## OpenAI specific outputs\n\n[[autodoc]] models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput\n\n[[autodoc]] models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput\n\n<frameworkcontent>\n<pt>\n\n## OpenAIGPTModel\n\n[[autodoc]] OpenAIGPTModel\n- forward\n\n## OpenAIGPTLMHeadModel\n\n[[autodoc]] OpenAIGPTLMHeadModel\n- forward\n\n## OpenAIGPTDoubleHeadsModel\n\n[[autodoc]] OpenAIGPTDoubleHeadsModel\n- forward\n\n## OpenAIGPTForSequenceClassification\n\n[[autodoc]] OpenAIGPTForSequenceClassification\n- forward\n\n</pt>\n<tf>\n\n## TFOpenAIGPTModel\n\n[[autodoc]] TFOpenAIGPTModel\n- call\n\n## TFOpenAIGPTLMHeadModel\n\n[[autodoc]] TFOpenAIGPTLMHeadModel\n- call\n\n## TFOpenAIGPTDoubleHeadsModel\n\n[[autodoc]] TFOpenAIGPTDoubleHeadsModel\n- call\n\n## TFOpenAIGPTForSequenceClassification",
  "[[autodoc]] TFOpenAIGPTForSequenceClassification\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LeViT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The LeViT model was proposed in [LeViT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze. LeViT improves the [Vision Transformer (ViT)](vit) in performance and efficiency by a few architectural differences such as activation maps with decreasing resolutions in Transformers and the introduction of an attention bias to integrate positional information.\n\nThe abstract from the paper is the following:\n\n*We design a family of image classification architectures that optimize the trade-off between accuracy\nand efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures,\nwhich are competitive on highly parallel processing hardware. We revisit principles from the extensive\nliterature on convolutional neural networks to apply them to transformers, in particular activation maps\nwith decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information\nin vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification.",
  "We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of\napplication scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable\nto most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect\nto the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. *\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/levit_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> LeViT Architecture. Taken from the <a href=\"https://arxiv.org/abs/2104.01136\">original paper</a>.</small>\n\nThis model was contributed by [anugunj](https://huggingface.co/anugunj). The original code can be found [here](https://github.com/facebookresearch/LeViT).\n\n## Usage tips",
  "- Compared to ViT, LeViT models use an additional distillation head to effectively learn from a teacher (which, in the LeViT paper, is a ResNet like-model). The distillation head is learned through backpropagation under supervision of a ResNet like-model. They also draw inspiration from convolution neural networks to use activation maps with decreasing resolutions to increase the efficiency.\n- There are 2 ways to fine-tune distilled models, either (1) in a classic way, by only placing a prediction head on top\nof the final hidden state and not using the distillation head, or (2) by placing both a prediction head and distillation\nhead on top of the final hidden state. In that case, the prediction head is trained using regular cross-entropy between\nthe prediction of the head and the ground-truth label, while the distillation prediction head is trained using hard distillation\n(cross-entropy between the prediction of the distillation head and the label predicted by the teacher). At inference time,\none takes the average prediction between both heads as final prediction. (2) is also called \"fine-tuning with distillation\",",
  "because one relies on a teacher that has already been fine-tuned on the downstream dataset. In terms of models, (1) corresponds\nto [`LevitForImageClassification`] and (2) corresponds to [`LevitForImageClassificationWithTeacher`].\n- All released checkpoints were pre-trained and fine-tuned on  [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)\n(also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). only. No external data was used. This is in\ncontrast with the original ViT model, which used external data like the JFT-300M dataset/Imagenet-21k for\npre-training.\n- The authors of LeViT released 5 trained LeViT models, which you can directly plug into [`LevitModel`] or [`LevitForImageClassification`].\nTechniques like data augmentation, optimization, and regularization were used in order to simulate training on a much larger dataset\n(while only using ImageNet-1k for pre-training). The 5 variants available are (all trained on images of size 224x224):\n*facebook/levit-128S*, *facebook/levit-128*, *facebook/levit-192*, *facebook/levit-256* and\n*facebook/levit-384*. Note that one should use [`LevitImageProcessor`] in order to",
  "prepare images for the model.\n- [`LevitForImageClassificationWithTeacher`] currently supports only inference and not training or fine-tuning.\n- You can check out demo notebooks regarding inference as well as fine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)\n(you can just replace [`ViTFeatureExtractor`] by [`LevitImageProcessor`] and [`ViTForImageClassification`] by [`LevitForImageClassification`] or [`LevitForImageClassificationWithTeacher`]).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LeViT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`LevitForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## LevitConfig\n\n[[autodoc]] LevitConfig\n\n## LevitFeatureExtractor\n\n[[autodoc]] LevitFeatureExtractor\n- __call__\n\n## LevitImageProcessor\n\n[[autodoc]] LevitImageProcessor\n- preprocess\n\n## LevitModel\n\n[[autodoc]] LevitModel\n- forward\n\n## LevitForImageClassification\n\n[[autodoc]] LevitForImageClassification\n- forward\n\n## LevitForImageClassificationWithTeacher\n\n[[autodoc]] LevitForImageClassificationWithTeacher\n- forward",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MobileNet V2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe MobileNet model was proposed in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n\nThe abstract from the paper is the following:",
  "*In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.*",
  "*The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters.*",
  "This model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and weights can be found [here for the main model](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) and [here for DeepLabV3+](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\n## Usage tips\n\n- The checkpoints are named **mobilenet\\_v2\\_*depth*\\_*size***, for example **mobilenet\\_v2\\_1.0\\_224**, where **1.0** is the depth multiplier (sometimes also referred to as \"alpha\" or the width multiplier) and **224** is the resolution of the input images the model was trained on.\n\n- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32.\n\n- One can use [`MobileNetV2ImageProcessor`] to prepare images for the model.\n\n- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0).",
  "- The segmentation model uses a [DeepLabV3+](https://arxiv.org/abs/1802.02611) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n\n- The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV2Config`] with `tf_padding = False`.\n\nUnsupported features:\n\n- The [`MobileNetV2Model`] outputs a globally pooled version of the last hidden state. In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this.\n\n- The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \"FakeQuantization\" operations to unquantize the weights.",
  "- It's common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output_hidden_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers.\n\n- The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with MobileNetV2.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`MobileNetV2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\n**Semantic segmentation**\n- [Semantic segmentation task guide](../tasks/semantic_segmentation)",
  "If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## MobileNetV2Config\n\n[[autodoc]] MobileNetV2Config\n\n## MobileNetV2FeatureExtractor\n\n[[autodoc]] MobileNetV2FeatureExtractor\n- preprocess\n- post_process_semantic_segmentation\n\n## MobileNetV2ImageProcessor\n\n[[autodoc]] MobileNetV2ImageProcessor\n- preprocess\n- post_process_semantic_segmentation\n\n## MobileNetV2Model\n\n[[autodoc]] MobileNetV2Model\n- forward\n\n## MobileNetV2ForImageClassification\n\n[[autodoc]] MobileNetV2ForImageClassification\n- forward\n\n## MobileNetV2ForSemanticSegmentation\n\n[[autodoc]] MobileNetV2ForSemanticSegmentation\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n:warning: Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MusicGen Melody\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The MusicGen Melody model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.\n\nMusicGen Melody is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned on text descriptions or audio prompts. The text descriptions are passed through a frozen text encoder model to obtain a sequence of hidden-state representations. MusicGen is then trained to predict discrete audio tokens, or *audio codes*, conditioned on these hidden-states. These audio tokens are then decoded using an audio compression model, such as EnCodec, to recover the audio waveform.\n\nThrough an efficient token interleaving pattern, MusicGen does not require a self-supervised semantic representation of the text/audio prompts, thus eliminating the need to cascade multiple models to predict a set of codebooks (e.g. hierarchically or upsampling). Instead, it is able to generate all the codebooks in a single forward pass.\n\nThe abstract from the paper is the following:",
  "*We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen.*",
  "This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/audiocraft). The pre-trained checkpoints can be found on the [Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen).\n\n\n## Difference with [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)\n\nThere are two key differences with MusicGen:\n1. The audio prompt is used here as a conditional signal for the generated audio sample, whereas it's used for audio continuation in [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen).\n2. Conditional text and audio signals are concatenated to the decoder's hidden states instead of being used as a cross-attention signal, as in MusicGen.\n\n## Generation",
  "MusicGen Melody is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly better results than greedy, thus we encourage sampling mode to be used where possible. Sampling is enabled by default, and can be explicitly specified by setting `do_sample=True` in the call to [`MusicgenMelodyForConditionalGeneration.generate`], or by overriding the model's generation config (see below).\n\nTransformers supports both mono (1-channel) and stereo (2-channel) variants of MusicGen Melody. The mono channel versions generate a single set of codebooks. The stereo versions generate 2 sets of codebooks, 1 for each channel (left/right), and each set of codebooks is decoded independently through the audio compression model. The audio streams for each channel are combined to give the final stereo output.\n\n\n#### Audio Conditional Generation\n\nThe model can generate an audio sample conditioned on a text and an audio prompt through use of the [`MusicgenMelodyProcessor`] to pre-process the inputs.\n\nIn the following examples, we load an audio file using the 🤗 Datasets library, which can be pip installed through the command below:\n\n```\npip install --upgrade pip",
  "pip install datasets[audio]\n```\n\nThe audio file we are about to use is loaded as follows:\n```python\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\", streaming=True)\n>>> sample = next(iter(dataset))[\"audio\"]\n```\n\nThe audio prompt should ideally be free of the low-frequency signals usually produced by instruments such as drums and bass. The [Demucs](https://github.com/adefossez/demucs/tree/main) model can be used to separate vocals and other signals from the drums and bass components.\n\nIf you wish to use Demucs, you first need to follow the installation steps [here](https://github.com/adefossez/demucs/tree/main?tab=readme-ov-file#for-musicians) before using the following snippet:\n\n```python\nfrom demucs import pretrained\nfrom demucs.apply import apply_model\nfrom demucs.audio import convert_audio\nimport torch\n\n\nwav = torch.tensor(sample[\"array\"]).to(torch.float32)\n\ndemucs = pretrained.get_model('htdemucs')\n\nwav = convert_audio(wav[None], sample[\"sampling_rate\"], demucs.samplerate, demucs.audio_channels)\nwav = apply_model(demucs, wav[None])\n```\n\nYou can then use the following snippet to generate music:\n\n```python",
  ">>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-melody\")\n>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n\n>>> inputs = processor(\n...     audio=wav,\n...     sampling_rate=demucs.samplerate,\n...     text=[\"80s blues track with groovy saxophone\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\nYou can also pass the audio signal directly without using Demucs, although the quality of the generation will probably be degraded:\n\n```python\n>>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-melody\")\n>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n\n>>> inputs = processor(\n...     audio=sample[\"array\"],\n...     sampling_rate=sample[\"sampling_rate\"],\n...     text=[\"80s blues track with groovy saxophone\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )",
  ">>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\nThe audio outputs are a three-dimensional Torch tensor of shape `(batch_size, num_channels, sequence_length)`. To listen to the generated audio samples, you can either play them in an ipynb notebook:\n\n```python\nfrom IPython.display import Audio\n\nsampling_rate = model.config.audio_encoder.sampling_rate\nAudio(audio_values[0].numpy(), rate=sampling_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `soundfile`:\n\n```python\n>>> import soundfile as sf\n\n>>> sampling_rate = model.config.audio_encoder.sampling_rate\n>>> sf.write(\"musicgen_out.wav\", audio_values[0].T.numpy(), sampling_rate)\n```\n\n\n### Text-only Conditional Generation\n\nThe same [`MusicgenMelodyProcessor`] can be used to pre-process a text-only prompt.\n\n```python\n>>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-melody\")\n>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n\n>>> inputs = processor(",
  "...     text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\nThe `guidance_scale` is used in classifier free guidance (CFG), setting the weighting between the conditional logits (which are predicted from the text prompts) and the unconditional logits (which are predicted from an unconditional or 'null' prompt). Higher guidance scale encourages the model to generate samples that are more closely linked to the input prompt, usually at the expense of poorer audio quality. CFG is enabled by setting `guidance_scale > 1`. For best results, use `guidance_scale=3` (default).\n\n\nYou can also generate in batch:\n\n```python\n>>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-melody\")\n>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n\n>>> # take the first quarter of the audio sample",
  ">>> sample_1 = sample[\"array\"][: len(sample[\"array\"]) // 4]\n\n>>> # take the first half of the audio sample\n>>> sample_2 = sample[\"array\"][: len(sample[\"array\"]) // 2]\n\n>>> inputs = processor(\n...     audio=[sample_1, sample_2],\n...     sampling_rate=sample[\"sampling_rate\"],\n...     text=[\"80s blues track with groovy saxophone\", \"90s rock song with loud guitars and heavy drums\"],\n...     padding=True,\n...     return_tensors=\"pt\",\n... )\n>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n```\n\n### Unconditional Generation\n\nThe inputs for unconditional (or 'null') generation can be obtained through the method [`MusicgenMelodyProcessor.get_unconditional_inputs`]:\n\n```python\n>>> from transformers import MusicgenMelodyForConditionalGeneration, MusicgenMelodyProcessor\n\n>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n>>> unconditional_inputs = MusicgenMelodyProcessor.from_pretrained(\"facebook/musicgen-melody\").get_unconditional_inputs(num_samples=1)\n\n>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)\n```\n\n### Generation Configuration",
  "The default parameters that control the generation process, such as sampling, guidance scale and number of generated tokens, can be found in the model's generation config, and updated as desired:\n\n```python\n>>> from transformers import MusicgenMelodyForConditionalGeneration\n\n>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\")\n\n>>> # inspect the default generation config\n>>> model.generation_config\n\n>>> # increase the guidance scale to 4.0\n>>> model.generation_config.guidance_scale = 4.0\n\n>>> # decrease the max length to 256 tokens\n>>> model.generation_config.max_length = 256\n```\n\nNote that any arguments passed to the generate method will **supersede** those in the generation config, so setting `do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the generation config.\n\n## Model Structure\n\nThe MusicGen model can be de-composed into three distinct stages:\n1. Text encoder: maps the text inputs to a sequence of hidden-state representations. The pre-trained MusicGen models use a frozen text encoder from either T5 or Flan-T5.",
  "2. MusicGen Melody decoder: a language model (LM) that auto-regressively generates audio tokens (or codes) conditional on the encoder hidden-state representations\n3. Audio decoder: used to recover the audio waveform from the audio tokens predicted by the decoder.\n\nThus, the MusicGen model can either be used as a standalone decoder model, corresponding to the class [`MusicgenMelodyForCausalLM`], or as a composite model that includes the text encoder and audio encoder, corresponding to the class [`MusicgenMelodyForConditionalGeneration`]. If only the decoder needs to be loaded from the pre-trained checkpoint, it can be loaded by first specifying the correct config, or be accessed through the `.decoder` attribute of the composite model:\n\n```python\n>>> from transformers import AutoConfig, MusicgenMelodyForCausalLM, MusicgenMelodyForConditionalGeneration\n\n>>> # Option 1: get decoder config and pass to `.from_pretrained`\n>>> decoder_config = AutoConfig.from_pretrained(\"facebook/musicgen-melody\").decoder\n>>> decoder = MusicgenMelodyForCausalLM.from_pretrained(\"facebook/musicgen-melody\", **decoder_config.to_dict())",
  ">>> # Option 2: load the entire composite model, but only return the decoder\n>>> decoder = MusicgenMelodyForConditionalGeneration.from_pretrained(\"facebook/musicgen-melody\").decoder\n```\n\nSince the text encoder and audio encoder models are frozen during training, the MusicGen decoder [`MusicgenMelodyForCausalLM`] can be trained standalone on a dataset of encoder hidden-states and audio codes. For inference, the trained decoder can be combined with the frozen text encoder and audio encoder to recover the composite [`MusicgenMelodyForConditionalGeneration`] model.\n\n## Checkpoint Conversion\n\n- After downloading the original checkpoints from [here](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models), you can convert them using the **conversion script** available at `src/transformers/models/musicgen_melody/convert_musicgen_melody_transformers.py` with the following command:\n\n```bash\npython src/transformers/models/musicgen_melody/convert_musicgen_melody_transformers.py \\\n--checkpoint=\"facebook/musicgen-melody\" --pytorch_dump_folder /output/path\n```\n\nTips:",
  "* MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you use a compatible version of the Encodec model.\n* Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenMelodyForConditionalGeneration.generate`]\n\n\n## MusicgenMelodyDecoderConfig\n\n[[autodoc]] MusicgenMelodyDecoderConfig\n\n## MusicgenMelodyProcessor\n\n[[autodoc]] MusicgenMelodyProcessor\n- get_unconditional_inputs\n\n## MusicgenMelodyFeatureExtractor\n\n[[autodoc]] MusicgenMelodyFeatureExtractor\n\n## MusicgenMelodyConfig\n\n[[autodoc]] MusicgenMelodyConfig\n\n## MusicgenMelodyModel\n\n[[autodoc]] MusicgenMelodyModel\n- forward\n\n## MusicgenMelodyForCausalLM\n\n[[autodoc]] MusicgenMelodyForCausalLM\n- forward\n\n## MusicgenMelodyForConditionalGeneration\n\n[[autodoc]] MusicgenMelodyForConditionalGeneration\n- forward",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPT-J\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n</div>\n\n## Overview\n\nThe GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like\ncausal language model trained on [the Pile](https://pile.eleuther.ai/) dataset.\n\nThis model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\n\n## Usage tips\n\n- To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\nRAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\nRAM to just load the model. To reduce the RAM usage there are a few options. The `torch_dtype` argument can be\nused to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\nwhich could be used to further minimize the RAM usage:\n\n```python\n>>> from transformers import GPTJForCausalLM\n>>> import torch\n\n>>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\n...     \"EleutherAI/gpt-j-6B\",",
  "...     revision=\"float16\",\n...     torch_dtype=torch.float16,\n... ).to(device)\n```\n\n- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much more GPU RAM. Adam\noptimizer for example makes four copies of the model: model, gradients, average and squared average of the gradients.\nSo it would need at least 4x model size GPU memory, even with mixed precision as gradient updates are in fp32. This\nis not including the activations and data batches, which would again require some more GPU RAM. So one should explore\nsolutions such as DeepSpeed, to train/fine-tune the model. Another option is to use the original codebase to\ntrain/fine-tune the model on TPU and then convert the model to Transformers format for inference. Instructions for\nthat could be found [here](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)\n\n- Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer. These extra\ntokens are added for the sake of efficiency on TPUs. To avoid the mismatch between embedding matrix size and vocab",
  "size, the tokenizer for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) contains 143 extra tokens\n`<|extratoken_1|>... <|extratoken_143|>`, so the `vocab_size` of tokenizer also becomes 50400.\n\n## Usage examples\n\nThe [`~generation.GenerationMixin.generate`] method can be used to generate text using GPT-J\nmodel.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```\n\n...or in float16 precision:\n\n```python\n>>> from transformers import GPTJForCausalLM, AutoTokenizer\n>>> import torch",
  ">>> device = \"cuda\"\n>>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n\n>>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n...     \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n...     \"researchers was the fact that the unicorns spoke perfect English.\"\n... )\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n>>> gen_tokens = model.generate(\n...     input_ids,\n...     do_sample=True,\n...     temperature=0.9,\n...     max_length=100,\n... )\n>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GPT-J. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-generation\"/>",
  "- Description of [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B).\n- A blog on how to [Deploy GPT-J 6B for inference using Hugging Face Transformers and Amazon SageMaker](https://huggingface.co/blog/gptj-sagemaker).\n- A blog on how to [Accelerate GPT-J inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/gptj-deepspeed-inference).\n- A blog post introducing [GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/). 🌎\n- A notebook for [GPT-J-6B Inference Demo](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb). 🌎\n- Another notebook demonstrating [Inference with GPT-J-6B](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb).\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the 🤗 Hugging Face Course.",
  "- [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFGPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxGPTJForCausalLM`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb).\n\n**Documentation resources**",
  "- [Text classification task guide](../tasks/sequence_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## GPTJConfig\n\n[[autodoc]] GPTJConfig\n- all\n\n<frameworkcontent>\n<pt>\n\n## GPTJModel\n\n[[autodoc]] GPTJModel\n- forward\n\n## GPTJForCausalLM\n\n[[autodoc]] GPTJForCausalLM\n- forward\n\n## GPTJForSequenceClassification\n\n[[autodoc]] GPTJForSequenceClassification\n- forward\n\n## GPTJForQuestionAnswering\n\n[[autodoc]] GPTJForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFGPTJModel\n\n[[autodoc]] TFGPTJModel\n- call\n\n## TFGPTJForCausalLM\n\n[[autodoc]] TFGPTJForCausalLM\n- call\n\n## TFGPTJForSequenceClassification\n\n[[autodoc]] TFGPTJForSequenceClassification\n- call\n\n## TFGPTJForQuestionAnswering\n\n[[autodoc]] TFGPTJForQuestionAnswering\n- call\n\n</tf>\n<jax>\n\n## FlaxGPTJModel\n\n[[autodoc]] FlaxGPTJModel\n- __call__\n\n## FlaxGPTJForCausalLM\n\n[[autodoc]] FlaxGPTJForCausalLM\n- __call__\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MobileViT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The MobileViT model was proposed in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari. MobileViT introduces a new layer that replaces local processing in convolutions with global processing using transformers.\n\nThe abstract from the paper is the following:",
  "*Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters.*",
  "This model was contributed by [matthijs](https://huggingface.co/Matthijs). The TensorFlow version of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code and weights can be found [here](https://github.com/apple/ml-cvnets).\n\n## Usage tips\n\n- MobileViT is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map. You can follow [this tutorial](https://keras.io/examples/vision/mobilevit) for a lightweight introduction.\n- One can use [`MobileViTImageProcessor`] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).\n- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).",
  "- The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n- As the name suggests MobileViT was designed to be performant and efficient on mobile phones. The TensorFlow versions of the MobileViT models are fully compatible with [TensorFlow Lite](https://www.tensorflow.org/lite).\n\nYou can use the following code to convert a MobileViT checkpoint (be it image classification or semantic segmentation) to generate a\nTensorFlow Lite model:\n\n```py\nfrom transformers import TFMobileViTForImageClassification\nimport tensorflow as tf\n\n\nmodel_ckpt = \"apple/mobilevit-xx-small\"\nmodel = TFMobileViTForImageClassification.from_pretrained(model_ckpt)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [\ntf.lite.OpsSet.TFLITE_BUILTINS,\ntf.lite.OpsSet.SELECT_TF_OPS,\n]\ntflite_model = converter.convert()\ntflite_filename = model_ckpt.split(\"/\")[-1] + \".tflite\"\nwith open(tflite_filename, \"wb\") as f:\nf.write(tflite_model)\n```",
  "The resulting model will be just **about an MB** making it a good fit for mobile applications where resources and network\nbandwidth can be constrained.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with MobileViT.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`MobileViTForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\n**Semantic segmentation**\n- [Semantic segmentation task guide](../tasks/semantic_segmentation)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## MobileViTConfig\n\n[[autodoc]] MobileViTConfig\n\n## MobileViTFeatureExtractor\n\n[[autodoc]] MobileViTFeatureExtractor\n- __call__\n- post_process_semantic_segmentation",
  "## MobileViTImageProcessor\n\n[[autodoc]] MobileViTImageProcessor\n- preprocess\n- post_process_semantic_segmentation\n\n<frameworkcontent>\n<pt>\n\n## MobileViTModel\n\n[[autodoc]] MobileViTModel\n- forward\n\n## MobileViTForImageClassification\n\n[[autodoc]] MobileViTForImageClassification\n- forward\n\n## MobileViTForSemanticSegmentation\n\n[[autodoc]] MobileViTForSemanticSegmentation\n- forward\n\n</pt>\n<tf>\n\n## TFMobileViTModel\n\n[[autodoc]] TFMobileViTModel\n- call\n\n## TFMobileViTForImageClassification\n\n[[autodoc]] TFMobileViTForImageClassification\n- call\n\n## TFMobileViTForSemanticSegmentation\n\n[[autodoc]] TFMobileViTForSemanticSegmentation\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# XLM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview\n\nThe XLM model was proposed in [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by",
  "Guillaume Lample, Alexis Conneau. It's a transformer pretrained using one of the following objectives:\n\n- a causal language modeling (CLM) objective (next token prediction),\n- a masked language modeling (MLM) objective (BERT-like), or\n- a Translation Language Modeling (TLM) object (extension of BERT's MLM to multiple language inputs)\n\nThe abstract from the paper is the following:\n\n*Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding.\nIn this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We\npropose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual\ndata, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain\nstate-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our\napproach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we",
  "obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised\nmachine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the\nprevious best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/facebookresearch/XLM/).\n\n## Usage tips\n\n- XLM has many different checkpoints, which were trained using different objectives: CLM, MLM or TLM. Make sure to\nselect the correct objective for your task (e.g. MLM checkpoints are not suitable for generation).\n- XLM has multilingual checkpoints which leverage a specific `lang` parameter. Check out the [multi-lingual](../multilingual) page for more information.\n- A transformer model trained on several languages. There are three different type of training for this model and the library provides checkpoints for all of them:",
  "* Causal language modeling (CLM) which is the traditional autoregressive training (so this model could be in the previous section as well). One of the languages is selected for each training sample, and the model input is a sentence of 256 tokens, that may span over several documents in one of those languages.\n* Masked language modeling (MLM) which is like RoBERTa. One of the languages is selected for each training sample, and the model input is a sentence of 256 tokens, that may span over several documents in one of those languages, with dynamic masking of the tokens.\n* A combination of MLM and translation language modeling (TLM). This consists of concatenating a sentence in two different languages, with random masking. To predict one of the masked tokens, the model can use both, the surrounding context in language 1 and the context given by language 2.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Causal language modeling task guide](../tasks/language_modeling)",
  "- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## XLMConfig\n\n[[autodoc]] XLMConfig\n\n## XLMTokenizer\n\n[[autodoc]] XLMTokenizer\n- build_inputs_with_special_tokens\n- get_special_tokens_mask\n- create_token_type_ids_from_sequences\n- save_vocabulary\n\n## XLM specific outputs\n\n[[autodoc]] models.xlm.modeling_xlm.XLMForQuestionAnsweringOutput\n\n<frameworkcontent>\n<pt>\n\n## XLMModel\n\n[[autodoc]] XLMModel\n- forward\n\n## XLMWithLMHeadModel\n\n[[autodoc]] XLMWithLMHeadModel\n- forward\n\n## XLMForSequenceClassification\n\n[[autodoc]] XLMForSequenceClassification\n- forward\n\n## XLMForMultipleChoice\n\n[[autodoc]] XLMForMultipleChoice\n- forward\n\n## XLMForTokenClassification\n\n[[autodoc]] XLMForTokenClassification\n- forward\n\n## XLMForQuestionAnsweringSimple\n\n[[autodoc]] XLMForQuestionAnsweringSimple\n- forward\n\n## XLMForQuestionAnswering\n\n[[autodoc]] XLMForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFXLMModel\n\n[[autodoc]] TFXLMModel\n- call\n\n## TFXLMWithLMHeadModel\n\n[[autodoc]] TFXLMWithLMHeadModel\n- call\n\n## TFXLMForSequenceClassification\n\n[[autodoc]] TFXLMForSequenceClassification\n- call\n\n## TFXLMForMultipleChoice",
  "[[autodoc]] TFXLMForMultipleChoice\n- call\n\n## TFXLMForTokenClassification\n\n[[autodoc]] TFXLMForTokenClassification\n- call\n\n## TFXLMForQuestionAnsweringSimple\n\n[[autodoc]] TFXLMForQuestionAnsweringSimple\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LongT5\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}",
  "(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*\n\nThis model was contributed by [stancld](https://huggingface.co/stancld).\nThe original code can be found [here](https://github.com/google-research/longt5).\n\n## Usage tips\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`",
  "tokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.",
  "- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.\n\n```python\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n>>> model = (\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n...     .to(\"cuda\")\n...     .half()\n... )\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\n\n>>> def generate_answers(batch):\n...     inputs_dict = tokenizer(\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n...     )\n...     input_ids = inputs_dict.input_ids.to(\"cuda\")\n...     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n...     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n...     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n...     return batch",
  ">>> result = dataset.map(generate_answer, batched=True, batch_size=2)\n>>> rouge = evaluate.load(\"rouge\")\n>>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\n```\n\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks/summarization)\n\n## LongT5Config\n\n[[autodoc]] LongT5Config\n\n<frameworkcontent>\n<pt>\n\n## LongT5Model\n\n[[autodoc]] LongT5Model\n- forward\n\n## LongT5ForConditionalGeneration\n\n[[autodoc]] LongT5ForConditionalGeneration\n- forward\n\n## LongT5EncoderModel\n\n[[autodoc]] LongT5EncoderModel\n- forward\n\n</pt>\n<jax>\n\n## FlaxLongT5Model\n\n[[autodoc]] FlaxLongT5Model\n- __call__\n- encode\n- decode\n\n## FlaxLongT5ForConditionalGeneration\n\n[[autodoc]] FlaxLongT5ForConditionalGeneration\n- __call__\n- encode\n- decode\n\n</jax>\n</frameworkcontent>",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CPM\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">",
  "<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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",
  "\">\n</div>\n\n## Overview\n\nThe CPM model was proposed in [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,\nYusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,\nDaixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n\nThe abstract from the paper is the following:\n\n*Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3,\nwith 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even\nzero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus\nof GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the\nChinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best",
  "of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained\nlanguage model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation,\ncloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many\nNLP tasks in the settings of few-shot (even zero-shot) learning.*\n\nThis model was contributed by [canwenxu](https://huggingface.co/canwenxu). The original implementation can be found\nhere: https://github.com/TsinghuaAI/CPM-Generate\n\n\n<Tip>\n\nCPM's architecture is the same as GPT-2, except for tokenization method. Refer to [GPT-2 documentation](gpt2) for\nAPI reference information.\n\n</Tip>\n\n\n## CpmTokenizer\n\n[[autodoc]] CpmTokenizer\n\n## CpmTokenizerFast\n\n[[autodoc]] CpmTokenizerFast",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# PatchTST\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview\n\nThe PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.",
  "At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:\n\n![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)\n\nThe abstract from the paper is the following:",
  "*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy.*",
  "This model was contributed by [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong), [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12), [wmgifford](https://huggingface.co/wmgifford), and [kashif](https://huggingface.co/kashif). The original code can be found [here](https://github.com/yuqinie98/PatchTST).\n\n## Usage tips\n\nThe model can also be used for time series classification and time series regression. See the respective [`PatchTSTForClassification`] and [`PatchTSTForRegression`] classes.\n\n## Resources\n\n- A blog post explaining PatchTST in depth can be found [here](https://huggingface.co/blog/patchtst). The blog can also be opened in Google Colab.\n\n## PatchTSTConfig\n\n[[autodoc]] PatchTSTConfig\n\n## PatchTSTModel\n\n[[autodoc]] PatchTSTModel\n- forward\n\n## PatchTSTForPrediction\n\n[[autodoc]] PatchTSTForPrediction\n- forward\n\n## PatchTSTForClassification\n\n[[autodoc]] PatchTSTForClassification\n- forward\n\n## PatchTSTForPretraining\n\n[[autodoc]] PatchTSTForPretraining\n- forward\n\n## PatchTSTForRegression\n\n[[autodoc]] PatchTSTForRegression\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Longformer\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The Longformer model was presented in [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n\nThe abstract from the paper is the following:\n\n*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our\npretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on\nWikiHop and TriviaQA.*",
  "This model was contributed by [beltagy](https://huggingface.co/beltagy). The Authors' code can be found [here](https://github.com/allenai/longformer).\n\n## Usage tips\n\n- Since the Longformer is based on RoBERTa, it doesn't have `token_type_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep_token` (or\n`</s>`).\n- A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information.\n\n## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in",
  "`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.\n\nNote that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.\n\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to",
  "\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training\n\n[`LongformerForMaskedLM`] is trained the exact same way [`RobertaForMaskedLM`] is\ntrained and should be used as follows:\n\n```python\ninput_ids = tokenizer.encode(\"This is a sentence from [MASK] training data\", return_tensors=\"pt\")\nmlm_labels = tokenizer.encode(\"This is a sentence from the training data\", return_tensors=\"pt\")\n\nloss = model(input_ids, labels=input_ids, masked_lm_labels=mlm_labels)[0]\n```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## LongformerConfig\n\n[[autodoc]] LongformerConfig\n\n## LongformerTokenizer",
  "[[autodoc]] LongformerTokenizer\n\n## LongformerTokenizerFast\n\n[[autodoc]] LongformerTokenizerFast\n\n## Longformer specific outputs\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerBaseModelOutput\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerMaskedLMOutput\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerSequenceClassifierOutput\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput\n\n[[autodoc]] models.longformer.modeling_longformer.LongformerTokenClassifierOutput\n\n[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutput\n\n[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling\n\n[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput\n\n[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput\n\n[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput",
  "[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput\n\n[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput\n\n<frameworkcontent>\n<pt>\n\n## LongformerModel\n\n[[autodoc]] LongformerModel\n- forward\n\n## LongformerForMaskedLM\n\n[[autodoc]] LongformerForMaskedLM\n- forward\n\n## LongformerForSequenceClassification\n\n[[autodoc]] LongformerForSequenceClassification\n- forward\n\n## LongformerForMultipleChoice\n\n[[autodoc]] LongformerForMultipleChoice\n- forward\n\n## LongformerForTokenClassification\n\n[[autodoc]] LongformerForTokenClassification\n- forward\n\n## LongformerForQuestionAnswering\n\n[[autodoc]] LongformerForQuestionAnswering\n- forward\n\n</pt>\n<tf>\n\n## TFLongformerModel\n\n[[autodoc]] TFLongformerModel\n- call\n\n## TFLongformerForMaskedLM\n\n[[autodoc]] TFLongformerForMaskedLM\n- call\n\n## TFLongformerForQuestionAnswering\n\n[[autodoc]] TFLongformerForQuestionAnswering\n- call\n\n## TFLongformerForSequenceClassification\n\n[[autodoc]] TFLongformerForSequenceClassification\n- call\n\n## TFLongformerForTokenClassification\n\n[[autodoc]] TFLongformerForTokenClassification\n- call\n\n## TFLongformerForMultipleChoice",
  "[[autodoc]] TFLongformerForMultipleChoice\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GroupViT\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n</div>\n\n## Overview",
  "The GroupViT model was proposed in [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\nInspired by [CLIP](clip), GroupViT is a vision-language model that can perform zero-shot semantic segmentation on any given vocabulary categories.\n\nThe abstract from the paper is the following:",
  "*Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*",
  "This model was contributed by [xvjiarui](https://huggingface.co/xvjiarui). The TensorFlow version was contributed by [ariG23498](https://huggingface.co/ariG23498) with the help of [Yih-Dar SHIEH](https://huggingface.co/ydshieh), [Amy Roberts](https://huggingface.co/amyeroberts), and [Joao Gante](https://huggingface.co/joaogante).\nThe original code can be found [here](https://github.com/NVlabs/GroupViT).\n\n## Usage tips\n\n- You may specify `output_segmentation=True` in the forward of `GroupViTModel` to get the segmentation logits of input texts.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GroupViT.\n\n- The quickest way to get started with GroupViT is by checking the [example notebooks](https://github.com/xvjiarui/GroupViT/blob/main/demo/GroupViT_hf_inference_notebook.ipynb) (which showcase zero-shot segmentation inference).\n- One can also check out the [HuggingFace Spaces demo](https://huggingface.co/spaces/xvjiarui/GroupViT) to play with GroupViT.\n\n## GroupViTConfig\n\n[[autodoc]] GroupViTConfig\n- from_text_vision_configs\n\n## GroupViTTextConfig\n\n[[autodoc]] GroupViTTextConfig\n\n## GroupViTVisionConfig",
  "[[autodoc]] GroupViTVisionConfig\n\n<frameworkcontent>\n<pt>\n\n## GroupViTModel\n\n[[autodoc]] GroupViTModel\n- forward\n- get_text_features\n- get_image_features\n\n## GroupViTTextModel\n\n[[autodoc]] GroupViTTextModel\n- forward\n\n## GroupViTVisionModel\n\n[[autodoc]] GroupViTVisionModel\n- forward\n\n</pt>\n<tf>\n\n## TFGroupViTModel\n\n[[autodoc]] TFGroupViTModel\n- call\n- get_text_features\n- get_image_features\n\n## TFGroupViTTextModel\n\n[[autodoc]] TFGroupViTTextModel\n- call\n\n## TFGroupViTVisionModel\n\n[[autodoc]] TFGroupViTVisionModel\n- call\n\n</tf>\n</frameworkcontent>",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Pix2Struct\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\n\nThe abstract from the paper is the following:",
  "> Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.",
  "Tips:\n\nPix2Struct has been fine tuned on a variety of tasks and datasets, ranging from image captioning, visual question answering (VQA) over different inputs (books, charts, science diagrams), captioning UI components etc. The full list can be found in Table 1 of the paper.\nWe therefore advise you to use these models for the tasks they have been fine tuned on. For instance, if you want to use Pix2Struct for UI captioning, you should use the model fine tuned on the UI dataset. If you want to use Pix2Struct for image captioning, you should use the model fine tuned on the natural images captioning dataset and so on.\n\nIf you want to use the model to perform conditional text captioning, make sure to use the processor with `add_special_tokens=False`.\n\nThis model was contributed by [ybelkada](https://huggingface.co/ybelkada).\nThe original code can be found [here](https://github.com/google-research/pix2struct).\n\n## Resources\n\n- [Fine-tuning Notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)\n- [All models](https://huggingface.co/models?search=pix2struct)\n\n## Pix2StructConfig\n\n[[autodoc]] Pix2StructConfig\n- from_text_vision_configs",
  "## Pix2StructTextConfig\n\n[[autodoc]] Pix2StructTextConfig\n\n## Pix2StructVisionConfig\n\n[[autodoc]] Pix2StructVisionConfig\n\n## Pix2StructProcessor\n\n[[autodoc]] Pix2StructProcessor\n\n## Pix2StructImageProcessor\n\n[[autodoc]] Pix2StructImageProcessor\n- preprocess\n\n## Pix2StructTextModel\n\n[[autodoc]] Pix2StructTextModel\n- forward\n\n## Pix2StructVisionModel\n\n[[autodoc]] Pix2StructVisionModel\n- forward\n\n## Pix2StructForConditionalGeneration\n\n[[autodoc]] Pix2StructForConditionalGeneration\n- forward",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Mamba 2\n\n<div class=\"flex flex-wrap space-x-1\">\n<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n</div>\n\n## Overview",
  "The Mamba2 model was proposed in [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) by Tri Dao and Albert Gu. It is a State Space Model similar to Mamba 1, with better performances in a simplified architecture.\n\n\nThe abstract from the paper is the following:\n\n*While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.*\n\nTips:",
  "This version should support all implementations of Mamba 2, and in particular [Mamba-2 codestral](https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1) from Mistral AI. In particular, mamba 2 codestral was released with a number of `groups` equal to 8, which can be thought intuitively as similar to the number of kv heads in an attention-based model.",
  "This model has two different forward passes, `torch_forward` or `cuda_kernels_forward`. The latter uses the original cuda kernels if they are found in your environment, and is slower on the prefill i.e. requires a \"warmup run\" due to high cpu overhead, see [here](https://github.com/state-spaces/mamba/issues/389#issuecomment-2171755306) and [also here](https://github.com/state-spaces/mamba/issues/355#issuecomment-2147597457). Without compilation, the `torch_forward` implementation is faster by a factor 3 to 4. Further, there are no positional embeddings in this model, but there is an `attention_mask` and a specific logic to mask out hidden states in two places in the case of batched generation, see [here](https://github.com/state-spaces/mamba/issues/66#issuecomment-1863563829) as well. Due to this, in addition to the reimplementation of mamba2 kernels, batched generation and cached generation are expected to have slight discrepancies. Further, the results given by the cuda kernels or the torch forward are expected to be slightly different. The SSM algorithm heavily relies on tensor contractions, which have matmul equivalents but the order of operations is slightly different, making the difference greater at smaller precisions.",
  "Another note, shutdown of hidden states corresponding to padding tokens is done in 2 places and mostly has been tested with left-padding. Right-padding will propagate noise down the line and is not guaranteed to yield satisfactory results. `tokenizer.padding_side = \"left\"` ensures you are using the correct padding side.\n\nThis model was contributed by [Molbap](https://huggingface.co/Molbap), with tremendous help from [Anton Vlasjuk](https://github.com/vasqu).\nThe original code can be found [here](https://github.com/state-spaces/mamba).\n\n\n# Usage\n\n### A simple generation example:\n```python\nfrom transformers import Mamba2Config, Mamba2ForCausalLM, AutoTokenizer\nimport torch\nmodel_id = 'mistralai/Mamba-Codestral-7B-v0.1'\ntokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)\nmodel = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')\ninput_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n\nout = model.generate(input_ids, max_new_tokens=10)\nprint(tokenizer.batch_decode(out))\n```\n\nHere's a draft script for finetuning:\n```python\nfrom trl import SFTTrainer\nfrom peft import LoraConfig",
  "from transformers import AutoTokenizer, Mamba2ForCausalLM, TrainingArguments\nmodel_id = 'mistralai/Mamba-Codestral-7B-v0.1'\ntokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\" #enforce padding side left\n\nmodel = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')\ndataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n# Without CUDA kernels, batch size of 2 occupies one 80GB device\n# but precision can be reduced.\n# Experiments and trials welcome!\ntraining_args = TrainingArguments(\noutput_dir=\"./results\",\nnum_train_epochs=3,\nper_device_train_batch_size=2,\nlogging_dir='./logs',\nlogging_steps=10,\nlearning_rate=2e-3\n)\nlora_config =  LoraConfig(\nr=8,\ntarget_modules=[\"embeddings\", \"in_proj\", \"out_proj\"],\ntask_type=\"CAUSAL_LM\",\nbias=\"none\"\n)\ntrainer = SFTTrainer(\nmodel=model,\ntokenizer=tokenizer,\nargs=training_args,\npeft_config=lora_config,\ntrain_dataset=dataset,\ndataset_text_field=\"quote\",\n)\ntrainer.train()\n```\n\n\n## Mamba2Config\n\n[[autodoc]] Mamba2Config\n\n## Mamba2Model\n\n[[autodoc]] Mamba2Model\n- forward\n\n## Mamba2LMHeadModel",
  "[[autodoc]] Mamba2ForCausalLM\n- forward",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Custom Layers and Utilities\n\nThis page lists all the custom layers used by the library, as well as the utility functions it provides for modeling.\n\nMost of those are only useful if you are studying the code of the models in the library.\n\n\n## Pytorch custom modules\n\n[[autodoc]] pytorch_utils.Conv1D\n\n[[autodoc]] modeling_utils.PoolerStartLogits\n- forward\n\n[[autodoc]] modeling_utils.PoolerEndLogits\n- forward",
  "[[autodoc]] modeling_utils.PoolerAnswerClass\n- forward\n\n[[autodoc]] modeling_utils.SquadHeadOutput\n\n[[autodoc]] modeling_utils.SQuADHead\n- forward\n\n[[autodoc]] modeling_utils.SequenceSummary\n- forward\n\n## PyTorch Helper Functions\n\n[[autodoc]] pytorch_utils.apply_chunking_to_forward\n\n[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n\n[[autodoc]] pytorch_utils.prune_layer\n\n[[autodoc]] pytorch_utils.prune_conv1d_layer\n\n[[autodoc]] pytorch_utils.prune_linear_layer\n\n## TensorFlow custom layers\n\n[[autodoc]] modeling_tf_utils.TFConv1D\n\n[[autodoc]] modeling_tf_utils.TFSequenceSummary\n\n## TensorFlow loss functions\n\n[[autodoc]] modeling_tf_utils.TFCausalLanguageModelingLoss\n\n[[autodoc]] modeling_tf_utils.TFMaskedLanguageModelingLoss\n\n[[autodoc]] modeling_tf_utils.TFMultipleChoiceLoss\n\n[[autodoc]] modeling_tf_utils.TFQuestionAnsweringLoss\n\n[[autodoc]] modeling_tf_utils.TFSequenceClassificationLoss\n\n[[autodoc]] modeling_tf_utils.TFTokenClassificationLoss\n\n## TensorFlow Helper Functions\n\n[[autodoc]] modeling_tf_utils.get_initializer\n\n[[autodoc]] modeling_tf_utils.keras_serializable\n\n[[autodoc]] modeling_tf_utils.shape_list",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# General Utilities\n\nThis page lists all of Transformers general utility functions that are found in the file `utils.py`.\n\nMost of those are only useful if you are studying the general code in the library.\n\n\n## Enums and namedtuples\n\n[[autodoc]] utils.ExplicitEnum\n\n[[autodoc]] utils.PaddingStrategy\n\n[[autodoc]] utils.TensorType\n\n## Special Decorators\n\n[[autodoc]] utils.add_start_docstrings",
  "[[autodoc]] utils.add_start_docstrings_to_model_forward\n\n[[autodoc]] utils.add_end_docstrings\n\n[[autodoc]] utils.add_code_sample_docstrings\n\n[[autodoc]] utils.replace_return_docstrings\n\n## Special Properties\n\n[[autodoc]] utils.cached_property\n\n## Other Utilities\n\n[[autodoc]] utils._LazyModule",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Model debugging toolboxes\n\nThis page lists all the debugging and model adding tools used by the library, as well as the utility functions it provides for it.\n\nMost of those are only useful if you are adding new models in the library.\n\n\n## Model addition debuggers\n\n\n### Model addition debugger - context manager for model adders\n\nThis context manager is a power user tool intended for model adders.",
  "It tracks all forward calls within a model forward and logs a slice of each input and output on a nested Json.\nTo note, this context manager enforces `torch.inference_mode()`.\n\n### Rationale\n\nBecause when porting models to transformers, even from python to python, model adders often have to do a lot of manual operations, involving saving and loading tensors, comparing dtypes, etc. This small tool can hopefully shave off some time.\n\n### Usage\n\nAdd this context manager as follows to debug a model:\n\n```python\nimport torch\nfrom PIL import Image\nimport requests\nfrom transformers import LlavaProcessor, LlavaForConditionalGeneration\ntorch.random.manual_seed(673)\n\n# load pretrained model and processor\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nprocessor = LlavaProcessor.from_pretrained(model_id)\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n\n# create random image input\nrandom_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())\n\n# prompt\nprompt = \"<image>Describe this image.\"\n\n# process inputs\ninputs = processor(text=prompt, images=random_image, return_tensors=\"pt\")\n\n# call forward method (not .generate!)",
  "with model_addition_debugger_context(model, \"optional_path_to_your_output_file.json\"):\noutput = model.forward(**inputs)\n\n```\n\n\n[[autodoc]] model_addition_debugger\n\n[[autodoc]] model_addition_debugger_context",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Utilities for `FeatureExtractors`\n\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as *Short Time Fourier Transform* or *log mel spectrogram*.\n\nMost of those are only useful if you are studying the code of the audio processors in the library.\n\n## Audio Transformations\n\n[[autodoc]] audio_utils.hertz_to_mel",
  "[[autodoc]] audio_utils.mel_to_hertz\n\n[[autodoc]] audio_utils.mel_filter_bank\n\n[[autodoc]] audio_utils.optimal_fft_length\n\n[[autodoc]] audio_utils.window_function\n\n[[autodoc]] audio_utils.spectrogram\n\n[[autodoc]] audio_utils.power_to_db\n\n[[autodoc]] audio_utils.amplitude_to_db",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Utilities for Generation\n\nThis page lists all the utility functions used by [`~generation.GenerationMixin.generate`].\n\n## Generate Outputs\n\nThe output of [`~generation.GenerationMixin.generate`] is an instance of a subclass of\n[`~utils.ModelOutput`]. This output is a data structure containing all the information returned\nby [`~generation.GenerationMixin.generate`], but that can also be used as tuple or dictionary.\n\nHere's an example:",
  "```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n\ninputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\ngeneration_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n```\n\nThe `generation_output` object is a [`~generation.GenerateDecoderOnlyOutput`], as we can\nsee in the documentation of that class below, it means it has the following attributes:\n\n- `sequences`: the generated sequences of tokens\n- `scores` (optional): the prediction scores of the language modelling head, for each generation step\n- `hidden_states` (optional): the hidden states of the model, for each generation step\n- `attentions` (optional): the attention weights of the model, for each generation step\n\nHere we have the `scores` since we passed along `output_scores=True`, but we don't have `hidden_states` and\n`attentions` because we didn't pass `output_hidden_states=True` or `output_attentions=True`.\n\nYou can access each attribute as you would usually do, and if that attribute has not been returned by the model, you",
  "will get `None`. Here for instance `generation_output.scores` are all the generated prediction scores of the\nlanguage modeling head, and `generation_output.attentions` is `None`.\n\nWhen using our `generation_output` object as a tuple, it only keeps the attributes that don't have `None` values.\nHere, for instance, it has two elements, `loss` then `logits`, so\n\n```python\ngeneration_output[:2]\n```\n\nwill return the tuple `(generation_output.sequences, generation_output.scores)` for instance.\n\nWhen using our `generation_output` object as a dictionary, it only keeps the attributes that don't have `None`\nvalues. Here, for instance, it has two keys that are `sequences` and `scores`.\n\nWe document here all output types.\n\n\n### PyTorch\n\n[[autodoc]] generation.GenerateDecoderOnlyOutput\n\n[[autodoc]] generation.GenerateEncoderDecoderOutput\n\n[[autodoc]] generation.GenerateBeamDecoderOnlyOutput\n\n[[autodoc]] generation.GenerateBeamEncoderDecoderOutput\n\n### TensorFlow\n\n[[autodoc]] generation.TFGreedySearchEncoderDecoderOutput\n\n[[autodoc]] generation.TFGreedySearchDecoderOnlyOutput\n\n[[autodoc]] generation.TFSampleEncoderDecoderOutput\n\n[[autodoc]] generation.TFSampleDecoderOnlyOutput",
  "[[autodoc]] generation.TFBeamSearchEncoderDecoderOutput\n\n[[autodoc]] generation.TFBeamSearchDecoderOnlyOutput\n\n[[autodoc]] generation.TFBeamSampleEncoderDecoderOutput\n\n[[autodoc]] generation.TFBeamSampleDecoderOnlyOutput\n\n[[autodoc]] generation.TFContrastiveSearchEncoderDecoderOutput\n\n[[autodoc]] generation.TFContrastiveSearchDecoderOnlyOutput\n\n### FLAX\n\n[[autodoc]] generation.FlaxSampleOutput\n\n[[autodoc]] generation.FlaxGreedySearchOutput\n\n[[autodoc]] generation.FlaxBeamSearchOutput\n\n## LogitsProcessor\n\nA [`LogitsProcessor`] can be used to modify the prediction scores of a language model head for\ngeneration.\n\n### PyTorch\n\n[[autodoc]] AlternatingCodebooksLogitsProcessor\n- __call__\n\n[[autodoc]] ClassifierFreeGuidanceLogitsProcessor\n- __call__\n\n[[autodoc]] EncoderNoRepeatNGramLogitsProcessor\n- __call__\n\n[[autodoc]] EncoderRepetitionPenaltyLogitsProcessor\n- __call__\n\n[[autodoc]] EpsilonLogitsWarper\n- __call__\n\n[[autodoc]] EtaLogitsWarper\n- __call__\n\n[[autodoc]] ExponentialDecayLengthPenalty\n- __call__\n\n[[autodoc]] ForcedBOSTokenLogitsProcessor\n- __call__\n\n[[autodoc]] ForcedEOSTokenLogitsProcessor\n- __call__\n\n[[autodoc]] HammingDiversityLogitsProcessor\n- __call__",
  "[[autodoc]] InfNanRemoveLogitsProcessor\n- __call__\n\n[[autodoc]] LogitNormalization\n- __call__\n\n[[autodoc]] LogitsProcessor\n- __call__\n\n[[autodoc]] LogitsProcessorList\n- __call__\n\n[[autodoc]] MinLengthLogitsProcessor\n- __call__\n\n[[autodoc]] MinNewTokensLengthLogitsProcessor\n- __call__\n\n[[autodoc]] MinPLogitsWarper\n- __call__\n\n[[autodoc]] NoBadWordsLogitsProcessor\n- __call__\n\n[[autodoc]] NoRepeatNGramLogitsProcessor\n- __call__\n\n[[autodoc]] PrefixConstrainedLogitsProcessor\n- __call__\n\n[[autodoc]] RepetitionPenaltyLogitsProcessor\n- __call__\n\n[[autodoc]] SequenceBiasLogitsProcessor\n- __call__\n\n[[autodoc]] SuppressTokensAtBeginLogitsProcessor\n- __call__\n\n[[autodoc]] SuppressTokensLogitsProcessor\n- __call__\n\n[[autodoc]] SynthIDTextWatermarkLogitsProcessor\n- __call__\n\n[[autodoc]] TemperatureLogitsWarper\n- __call__\n\n[[autodoc]] TopKLogitsWarper\n- __call__\n\n[[autodoc]] TopPLogitsWarper\n- __call__\n\n[[autodoc]] TypicalLogitsWarper\n- __call__\n\n[[autodoc]] UnbatchedClassifierFreeGuidanceLogitsProcessor\n- __call__\n\n[[autodoc]] WhisperTimeStampLogitsProcessor\n- __call__\n\n[[autodoc]] WatermarkLogitsProcessor\n- __call__\n\n\n### TensorFlow\n\n[[autodoc]] TFForcedBOSTokenLogitsProcessor\n- __call__",
  "[[autodoc]] TFForcedEOSTokenLogitsProcessor\n- __call__\n\n[[autodoc]] TFForceTokensLogitsProcessor\n- __call__\n\n[[autodoc]] TFLogitsProcessor\n- __call__\n\n[[autodoc]] TFLogitsProcessorList\n- __call__\n\n[[autodoc]] TFLogitsWarper\n- __call__\n\n[[autodoc]] TFMinLengthLogitsProcessor\n- __call__\n\n[[autodoc]] TFNoBadWordsLogitsProcessor\n- __call__\n\n[[autodoc]] TFNoRepeatNGramLogitsProcessor\n- __call__\n\n[[autodoc]] TFRepetitionPenaltyLogitsProcessor\n- __call__\n\n[[autodoc]] TFSuppressTokensAtBeginLogitsProcessor\n- __call__\n\n[[autodoc]] TFSuppressTokensLogitsProcessor\n- __call__\n\n[[autodoc]] TFTemperatureLogitsWarper\n- __call__\n\n[[autodoc]] TFTopKLogitsWarper\n- __call__\n\n[[autodoc]] TFTopPLogitsWarper\n- __call__\n\n### FLAX\n\n[[autodoc]] FlaxForcedBOSTokenLogitsProcessor\n- __call__\n\n[[autodoc]] FlaxForcedEOSTokenLogitsProcessor\n- __call__\n\n[[autodoc]] FlaxForceTokensLogitsProcessor\n- __call__\n\n[[autodoc]] FlaxLogitsProcessor\n- __call__\n\n[[autodoc]] FlaxLogitsProcessorList\n- __call__\n\n[[autodoc]] FlaxLogitsWarper\n- __call__\n\n[[autodoc]] FlaxMinLengthLogitsProcessor\n- __call__\n\n[[autodoc]] FlaxSuppressTokensAtBeginLogitsProcessor\n- __call__\n\n[[autodoc]] FlaxSuppressTokensLogitsProcessor",
  "- __call__\n\n[[autodoc]] FlaxTemperatureLogitsWarper\n- __call__\n\n[[autodoc]] FlaxTopKLogitsWarper\n- __call__\n\n[[autodoc]] FlaxTopPLogitsWarper\n- __call__\n\n[[autodoc]] FlaxWhisperTimeStampLogitsProcessor\n- __call__\n\n## StoppingCriteria\n\nA [`StoppingCriteria`] can be used to change when to stop generation (other than EOS token). Please note that this is exclusively available to our PyTorch implementations.\n\n[[autodoc]] StoppingCriteria\n- __call__\n\n[[autodoc]] StoppingCriteriaList\n- __call__\n\n[[autodoc]] MaxLengthCriteria\n- __call__\n\n[[autodoc]] MaxTimeCriteria\n- __call__\n\n[[autodoc]] StopStringCriteria\n- __call__\n\n[[autodoc]] EosTokenCriteria\n- __call__\n\n## Constraints\n\nA [`Constraint`] can be used to force the generation to include specific tokens or sequences in the output. Please note that this is exclusively available to our PyTorch implementations.\n\n[[autodoc]] Constraint\n\n[[autodoc]] PhrasalConstraint\n\n[[autodoc]] DisjunctiveConstraint\n\n[[autodoc]] ConstraintListState\n\n## BeamSearch\n\n[[autodoc]] BeamScorer\n- process\n- finalize\n\n[[autodoc]] BeamSearchScorer\n- process\n- finalize\n\n[[autodoc]] ConstrainedBeamSearchScorer\n- process\n- finalize\n\n## Streamers\n\n[[autodoc]] TextStreamer",
  "[[autodoc]] TextIteratorStreamer\n\n[[autodoc]] AsyncTextIteratorStreamer\n\n## Caches\n\n[[autodoc]] Cache\n- update\n\n[[autodoc]] CacheConfig\n- update\n\n[[autodoc]] QuantizedCacheConfig\n- validate\n\n[[autodoc]] DynamicCache\n- update\n- get_seq_length\n- reorder_cache\n- to_legacy_cache\n- from_legacy_cache\n\n[[autodoc]] QuantizedCache\n- update\n- get_seq_length\n\n[[autodoc]] QuantoQuantizedCache\n\n[[autodoc]] HQQQuantizedCache\n\n[[autodoc]] SinkCache\n- update\n- get_seq_length\n- reorder_cache\n\n[[autodoc]] OffloadedCache\n- update\n- prefetch_layer\n- evict_previous_layer\n\n[[autodoc]] StaticCache\n- update\n- get_seq_length\n- reset\n\n[[autodoc]] OffloadedStaticCache\n- update\n- get_seq_length\n- reset\n\n[[autodoc]] HybridCache\n- update\n- get_seq_length\n- reset\n\n[[autodoc]] SlidingWindowCache\n- update\n- reset\n\n[[autodoc]] EncoderDecoderCache\n- get_seq_length\n- to_legacy_cache\n- from_legacy_cache\n- reset\n- reorder_cache\n\n[[autodoc]] MambaCache\n- update_conv_state\n- update_ssm_state\n- reset\n\n## Watermark Utils\n\n[[autodoc]] WatermarkingConfig\n- __call__\n\n[[autodoc]] WatermarkDetector\n- __call__\n\n[[autodoc]] BayesianDetectorConfig\n\n[[autodoc]] BayesianDetectorModel\n- forward",
  "[[autodoc]] SynthIDTextWatermarkingConfig\n\n[[autodoc]] SynthIDTextWatermarkDetector\n- __call__\n\n## Compile Utils\n\n[[autodoc]] CompileConfig\n- __call__",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Time Series Utilities\n\nThis page lists all the utility functions and classes that can be used for Time Series based models.\n\nMost of those are only useful if you are studying the code of the time series models or you wish to add to the collection of distributional output classes.\n\n## Distributional Output\n\n[[autodoc]] time_series_utils.NormalOutput\n\n[[autodoc]] time_series_utils.StudentTOutput",
  "[[autodoc]] time_series_utils.NegativeBinomialOutput",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Utilities for Tokenizers\n\nThis page lists all the utility functions used by the tokenizers, mainly the class\n[`~tokenization_utils_base.PreTrainedTokenizerBase`] that implements the common methods between\n[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] and the mixin\n[`~tokenization_utils_base.SpecialTokensMixin`].\n\nMost of those are only useful if you are studying the code of the tokenizers in the library.\n\n## PreTrainedTokenizerBase",
  "[[autodoc]] tokenization_utils_base.PreTrainedTokenizerBase\n- __call__\n- all\n\n## SpecialTokensMixin\n\n[[autodoc]] tokenization_utils_base.SpecialTokensMixin\n\n## Enums and namedtuples\n\n[[autodoc]] tokenization_utils_base.TruncationStrategy\n\n[[autodoc]] tokenization_utils_base.CharSpan\n\n[[autodoc]] tokenization_utils_base.TokenSpan",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Utilities for Image Processors\n\nThis page lists all the utility functions used by the image processors, mainly the functional\ntransformations used to process the images.\n\nMost of those are only useful if you are studying the code of the image processors in the library.\n\n## Image Transformations\n\n[[autodoc]] image_transforms.center_crop\n\n[[autodoc]] image_transforms.center_to_corners_format",
  "[[autodoc]] image_transforms.corners_to_center_format\n\n[[autodoc]] image_transforms.id_to_rgb\n\n[[autodoc]] image_transforms.normalize\n\n[[autodoc]] image_transforms.pad\n\n[[autodoc]] image_transforms.rgb_to_id\n\n[[autodoc]] image_transforms.rescale\n\n[[autodoc]] image_transforms.resize\n\n[[autodoc]] image_transforms.to_pil_image\n\n## ImageProcessingMixin\n\n[[autodoc]] image_processing_utils.ImageProcessingMixin",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Utilities for Trainer\n\nThis page lists all the utility functions used by [`Trainer`].\n\nMost of those are only useful if you are studying the code of the Trainer in the library.\n\n## Utilities\n\n[[autodoc]] EvalPrediction\n\n[[autodoc]] IntervalStrategy\n\n[[autodoc]] enable_full_determinism\n\n[[autodoc]] set_seed\n\n[[autodoc]] torch_distributed_zero_first\n\n## Callbacks internals\n\n[[autodoc]] trainer_callback.CallbackHandler",
  "## Distributed Evaluation\n\n[[autodoc]] trainer_pt_utils.DistributedTensorGatherer\n\n## Trainer Argument Parser\n\n[[autodoc]] HfArgumentParser\n\n## Debug Utilities\n\n[[autodoc]] debug_utils.DebugUnderflowOverflow",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Utilities for pipelines\n\nThis page lists all the utility functions the library provides for pipelines.\n\nMost of those are only useful if you are studying the code of the models in the library.\n\n\n## Argument handling\n\n[[autodoc]] pipelines.ArgumentHandler\n\n[[autodoc]] pipelines.ZeroShotClassificationArgumentHandler\n\n[[autodoc]] pipelines.QuestionAnsweringArgumentHandler\n\n## Data format\n\n[[autodoc]] pipelines.PipelineDataFormat",
  "[[autodoc]] pipelines.CsvPipelineDataFormat\n\n[[autodoc]] pipelines.JsonPipelineDataFormat\n\n[[autodoc]] pipelines.PipedPipelineDataFormat\n\n## Utilities\n\n[[autodoc]] pipelines.PipelineException",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# AWQ\n\n[Activation-aware Weight Quantization (AWQ)](https://hf.co/papers/2306.00978) preserves a small fraction of the weights that are important for LLM performance to compress a model to 4-bits with minimal performance degradation.",
  "There are several libraries for quantizing models with the AWQ algorithm, such as [llm-awq](https://github.com/mit-han-lab/llm-awq), [autoawq](https://github.com/casper-hansen/AutoAWQ) or [optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc). Transformers supports loading models quantized with the llm-awq and autoawq libraries. This guide will show you how to load models quantized with autoawq, but the process is similar for llm-awq quantized models.\n\nRun the command below to install autoawq\n\n```bash\npip install autoawq\n```\n> [!WARNING]\n> AutoAWQ downgrades Transformers to version 4.47.1. If you want to do inference with AutoAWQ, you may need to reinstall your Transformers' version after installing AutoAWQ.\n\nIdentify an AWQ-quantized model by checking the `quant_method` key in the models [config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json) file.\n\n```json\n{\n\"_name_or_path\": \"/workspace/process/huggingfaceh4_zephyr-7b-alpha/source\",\n\"architectures\": [\n\"MistralForCausalLM\"\n],\n...\n...\n...\n\"quantization_config\": {\n\"quant_method\": \"awq\",\n\"zero_point\": true,\n\"group_size\": 128,\n\"bits\": 4,\n\"version\": \"gemm\"\n}\n}\n```",
  "Load the AWQ-quantized model with [`~PreTrainedModel.from_pretrained`]. This automatically sets the other weights to fp16 by default for performance reasons. Use the `torch_dtype` parameter to load these other weights in a different format.\n\nIf the model is loaded on the CPU, use the `device_map` parameter to move it to a GPU.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"TheBloke/zephyr-7B-alpha-AWQ\",\ntorch_dtype=torch.float32,\ndevice_map=\"cuda:0\"\n)\n```\n\nUse `attn_implementation` to enable [FlashAttention2](../perf_infer_gpu_one#flashattention-2) to further accelerate inference.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"TheBloke/zephyr-7B-alpha-AWQ\",\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"cuda:0\"\n)\n```\n\n## Fused modules\n\nFused modules offer improved accuracy and performance. They are supported out-of-the-box for AWQ modules for [Llama](https://huggingface.co/meta-llama) and [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1) architectures, but you can also fuse AWQ modules for unsupported architectures.",
  "> [!WARNING]\n> Fused modules cannot be combined with other optimization techniques such as FlashAttention2.\n\n<hfoptions id=\"fuse\">\n<hfoption id=\"supported architectures\">\n\nCreate an [`AwqConfig`] and set the parameters `fuse_max_seq_len` and `do_fuse=True` to enable fused modules. The `fuse_max_seq_len` parameter is the total sequence length and it should include the context length and the expected generation length. Set it to a larger value to be safe.\n\nThe example below fuses the AWQ modules of the [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ) model.\n\n```python\nimport torch\nfrom transformers import AwqConfig, AutoModelForCausalLM\n\nquantization_config = AwqConfig(\nbits=4,\nfuse_max_seq_len=512,\ndo_fuse=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"TheBloke/Mistral-7B-OpenOrca-AWQ\",\nquantization_config=quantization_config\n).to(0)\n```\n\nThe [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ) model was benchmarked with `batch_size=1` with and without fused modules.\n\n<figcaption class=\"text-center text-gray-500 text-lg\">Unfused module</figcaption>",
  "|   Batch Size |   Prefill Length |   Decode Length |   Prefill tokens/s |   Decode tokens/s | Memory (VRAM)   |\n|-------------:|-----------------:|----------------:|-------------------:|------------------:|:----------------|\n|            1 |               32 |              32 |            60.0984 |           38.4537 | 4.50 GB (5.68%) |\n|            1 |               64 |              64 |          1333.67   |           31.6604 | 4.50 GB (5.68%) |\n|            1 |              128 |             128 |          2434.06   |           31.6272 | 4.50 GB (5.68%) |\n|            1 |              256 |             256 |          3072.26   |           38.1731 | 4.50 GB (5.68%) |\n|            1 |              512 |             512 |          3184.74   |           31.6819 | 4.59 GB (5.80%) |\n|            1 |             1024 |            1024 |          3148.18   |           36.8031 | 4.81 GB (6.07%) |\n|            1 |             2048 |            2048 |          2927.33   |           35.2676 | 5.73 GB (7.23%) |\n\n<figcaption class=\"text-center text-gray-500 text-lg\">Fused module</figcaption>",
  "|   Batch Size |   Prefill Length |   Decode Length |   Prefill tokens/s |   Decode tokens/s | Memory (VRAM)   |\n|-------------:|-----------------:|----------------:|-------------------:|------------------:|:----------------|\n|            1 |               32 |              32 |            81.4899 |           80.2569 | 4.00 GB (5.05%) |\n|            1 |               64 |              64 |          1756.1    |          106.26   | 4.00 GB (5.05%) |\n|            1 |              128 |             128 |          2479.32   |          105.631  | 4.00 GB (5.06%) |\n|            1 |              256 |             256 |          1813.6    |           85.7485 | 4.01 GB (5.06%) |\n|            1 |              512 |             512 |          2848.9    |           97.701  | 4.11 GB (5.19%) |\n|            1 |             1024 |            1024 |          3044.35   |           87.7323 | 4.41 GB (5.57%) |\n|            1 |             2048 |            2048 |          2715.11   |           89.4709 | 5.57 GB (7.04%) |\n\nThe speed and throughput of fused and unfused modules were also tested with the [optimum-benchmark](https://github.com/huggingface/optimum-benchmark) library.",
  "<div class=\"flex gap-4\">\n<div>\n<img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/fused_forward_memory_plot.png\" alt=\"generate throughput per batch size\" />\n<figcaption class=\"mt-2 text-center text-sm text-gray-500\">forward peak memory/batch size</figcaption>\n</div>\n<div>\n<img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/fused_generate_throughput_plot.png\" alt=\"forward latency per batch size\" />\n<figcaption class=\"mt-2 text-center text-sm text-gray-500\">generate throughput/batch size</figcaption>\n</div>\n</div>\n\n</hfoption>\n<hfoption id=\"unsupported architectures\">\n\nFor architectures that don't support fused modules, create an [`AwqConfig`] and define a custom fusing mapping in `modules_to_fuse` to determine which modules need to be fused.\n\nThe example below fuses the AWQ modules of the [TheBloke/Yi-34B-AWQ](https://huggingface.co/TheBloke/Yi-34B-AWQ) model.\n\n```python\nimport torch\nfrom transformers import AwqConfig, AutoModelForCausalLM\n\nquantization_config = AwqConfig(\nbits=4,\nfuse_max_seq_len=512,\nmodules_to_fuse={",
  "\"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n\"layernorm\": [\"ln1\", \"ln2\", \"norm\"],\n\"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n\"use_alibi\": False,\n\"num_attention_heads\": 56,\n\"num_key_value_heads\": 8,\n\"hidden_size\": 7168\n}\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"TheBloke/Yi-34B-AWQ\",\nquantization_config=quantization_config\n).to(0)\n```\n\nThe parameter `modules_to_fuse` should include the following keys.\n\n- `\"attention\"`: The names of the attention layers to fuse in the following order: query, key, value and output projection layer. If you don't want to fuse these layers, pass an empty list.\n- `\"layernorm\"`: The names of all the LayerNorm layers you want to replace with a custom fused LayerNorm. If you don't want to fuse these layers, pass an empty list.\n- `\"mlp\"`: The names of the MLP layers you want to fuse into a single MLP layer in the order: (gate (dense, layer, post-attention) / up / down layers).\n- `\"use_alibi\"`: If your model uses ALiBi positional embedding.\n- `\"num_attention_heads\"`: The number of attention heads.\n- `\"num_key_value_heads\"`: The number of key value heads that should be used to implement Grouped Query Attention (GQA).",
  "| parameter value | attention |\n|---|---|\n| `num_key_value_heads=num_attention_heads` | Multi-Head Attention |\n| `num_key_value_heads=1` | Multi-Query Attention |\n| `num_key_value_heads=...` | Grouped Query Attention |\n\n- `\"hidden_size\"`: The dimension of the hidden representations.\n\n</hfoption>\n</hfoptions>\n\n## ExLlamaV2\n\n[ExLlamaV2](https://github.com/turboderp/exllamav2) kernels support faster prefill and decoding. Run the command below to install the latest version of autoawq with ExLlamaV2 support.\n\n```bash\npip install git+https://github.com/casper-hansen/AutoAWQ.git\n```\n\nSet `version=\"exllama\"` in [`AwqConfig`] to enable ExLlamaV2 kernels.\n\n> [!TIP]\n> ExLlamaV2 is supported on AMD GPUs.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n\nquantization_config = AwqConfig(version=\"exllama\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\",\nquantization_config=quantization_config,\ndevice_map=\"auto\",\n)\n```\n\n## CPU",
  "[Intel Extension for PyTorch (IPEX)](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/) is designed to enable performance optimizations on Intel hardware. Run the command below to install the latest version of autoawq with IPEX support.\n\n```bash\npip install intel-extension-for-pytorch # for IPEX-GPU refer to https://intel.github.io/intel-extension-for-pytorch/xpu/2.5.10+xpu/\npip install git+https://github.com/casper-hansen/AutoAWQ.git\n```\n\nSet `version=\"ipex\"` in [`AwqConfig`] to enable ExLlamaV2 kernels.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n\ndevice = \"cpu\" # set to \"xpu\" for Intel GPU\nquantization_config = AwqConfig(version=\"ipex\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\",\nquantization_config=quantization_config,\ndevice_map=device,\n)\n```\n\n## Resources\n\nRun the AWQ demo [notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY#scrollTo=Wwsg6nCwoThm) for more examples of how to quantize a model, push a quantized model to the Hub, and more.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# EETQ",
  "The [Easy & Efficient Quantization for Transformers (EETQ)](https://github.com/NetEase-FuXi/EETQ) library supports int8 weight-only per-channel quantization for NVIDIA GPUs. It uses high-performance GEMM and GEMV kernels from [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) and [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). The attention layer is optimized with [FlashAttention2](https://github.com/Dao-AILab/flash-attention). No calibration dataset is required, and the model doesn't need to be pre-quantized. Accuracy degradation is negligible owing to the per-channel quantization.\n\nEETQ further supports fine-tuning with [PEFT](https://huggingface.co/docs/peft).\n\nInstall EETQ from the [release page](https://github.com/NetEase-FuXi/EETQ/releases) or [source code](https://github.com/NetEase-FuXi/EETQ). CUDA 11.4+ is required for EETQ.\n\n<hfoptions id=\"install\">\n<hfoption id=\"release page\">\n\n```bash\npip install --no-cache-dir https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.0/EETQ-1.0.0+cu121+torch2.1.2-cp310-cp310-linux_x86_64.whl\n```\n\n</hfoption>\n<hfoption id=\"source code\">\n\n```bash\ngit clone https://github.com/NetEase-FuXi/EETQ.git\ncd EETQ/",
  "git submodule update --init --recursive\npip install .\n```\n\n</hfoption>\n</hfoptions>\n\nQuantize a model on-the-fly by defining the quantization data type in [`EetqConfig`].\n\n```py\nfrom transformers import AutoModelForCausalLM, EetqConfig\n\nquantization_config = EetqConfig(\"int8\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-3.1-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\n```\n\nSave the quantized model with [`~PreTrainedModel.save_pretrained`] so it can be reused again with [`~PreTrainedModel.from_pretrained`].\n\n```py\nquant_path = \"/path/to/save/quantized/model\"\nmodel.save_pretrained(quant_path)\nmodel = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# bitsandbytes\n\n[bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) features the LLM.int8 and QLoRA quantization to enable accessible large language model inference and training.",
  "[LLM.int8()](https://hf.co/papers/2208.07339) is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\n\nQLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training.\n\n> **Note:** For a user-friendly quantization experience, you can use the `bitsandbytes` [community space](https://huggingface.co/spaces/bnb-community/bnb-my-repo).\n\n\nRun the command below to install bitsandbytes.\n\n```bash\npip install --upgrade transformers accelerate bitsandbytes\n```\n\nQuantize a model by passing a [`BitsAndBytesConfig`] to [`~PreTrainedModel.from_pretrained`]. This works for any model in any modality, as long as it supports [Accelerate](https://huggingface.co/docs/accelerate/index) and contains [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n\n<hfoptions id=\"bnb\">",
  "<hfoption id=\"8-bit\">\n\nQuantizing a model in 8-bit halves the memory-usage, and for large models, set `device_map=\"auto\"` to efficiently distribute the weights across all available GPUs.\n\n```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n\"bigscience/bloom-1b7\",\nquantization_config=quantization_config\n)\n```\n\nBy default, all other modules such as [torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) are set to the default torch dtype. You can change the data type of these modules with the `torch_dtype` parameter. Setting `torch_dtype=\"auto\"` loads the model in the data type defined in a model's `config.json` file.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n\"facebook/opt-350m\",\nquantization_config=quantization_config,\ntorch_dtype=\"auto\"\n)\nmodel_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n```",
  "Once a model is quantized to 8-bit, you can't push the quantized weights to the Hub unless you're using the latest version of Transformers and bitsandbytes. If you have the latest versions, then you can push the 8-bit model to the Hub with [`~PreTrainedModel.push_to_hub`]. The quantization config.json file is pushed first, followed by the quantized model weights.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"bigscience/bloom-560m\",\nquantization_config=quantization_config\n)\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n\nmodel.push_to_hub(\"bloom-560m-8bit\")\n```\n\n</hfoption>\n<hfoption id=\"4-bit\">\n\nQuantizing a model in 4-bit reduces your memory-usage by 4x, and for large models, set `device_map=\"auto\"` to efficiently distribute the weights across all available GPUs.\n\n```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n\"bigscience/bloom-1b7\",\nquantization_config=quantization_config\n)",
  "```\n\nBy default, all other modules such as [torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) are converted to `torch.float16`. You can change the data type of these modules with the `torch_dtype` parameter.. Setting `torch_dtype=\"auto\"` loads the model in the data type defined in a model's `config.json` file.\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n\"facebook/opt-350m\",\nquantization_config=quantization_config,\ntorch_dtype=\"auto\"\n)\nmodel_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n```\n\nMake sure you have the latest bitsandbytes version so you can serialize 4-bit models and push them to the Hub with [`~PreTrainedModel.push_to_hub`]. Use [`~PreTrainedModel.save_pretrained`] to save the 4-bit model locally.\n\n</hfoption>\n</hfoptions>\n\n> [!WARNING]\n> 8 and 4-bit training is only supported for training *extra* parameters.\n\nCheck your memory footprint with `get_memory_footprint`.\n\n```py\nprint(model.get_memory_footprint())\n```",
  "Load quantized models with [`~PreTrainedModel.from_pretrained`] without a `quantization_config`.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\n```\n\n## LLM.int8\n\nThis section explores some of the specific features of 8-bit quantization, such as offloading, outlier thresholds, skipping module conversion, and finetuning.\n\n### Offloading\n\n8-bit models can offload weights between the CPU and GPU to fit very large models into memory. The weights dispatched to the CPU are stored in **float32** and aren't converted to 8-bit. For example, enable offloading for [bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7) through [`BitsAndBytesConfig`].\n\n```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n```\n\nDesign a custom device map to fit everything on your GPU except for the `lm_head`, which is dispatched to the CPU.\n\n```py\ndevice_map = {\n\"transformer.word_embeddings\": 0,\n\"transformer.word_embeddings_layernorm\": 0,\n\"lm_head\": \"cpu\",\n\"transformer.h\": 0,",
  "\"transformer.ln_f\": 0,\n}\n```\n\nNow load your model with the custom `device_map` and `quantization_config`.\n\n```py\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n\"bigscience/bloom-1b7\",\ntorch_dtype=\"auto\",\ndevice_map=device_map,\nquantization_config=quantization_config,\n)\n```\n\n### Outlier threshold\n\nAn \"outlier\" is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed ([-3.5, 3.5]), this distribution can be very different for large models ([-60, 6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that, there is a significant performance penalty. A good default threshold value is 6, but a lower threshold may be needed for more unstable models (small models or finetuning).\n\nTo find the best threshold for your model, experiment with the `llm_int8_threshold` parameter in [`BitsAndBytesConfig`]. For example, setting the threshold to `0.0` significantly speeds up inference at the potential cost of some accuracy loss.\n\n```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\nquantization_config = BitsAndBytesConfig(",
  "llm_int8_threshold=0.0,\nllm_int8_enable_fp32_cpu_offload=True\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=\"auto\",\ndevice_map=device_map,\nquantization_config=quantization_config,\n)\n```\n\n### Skip module conversion\n\nFor some models, like [Jukebox](model_doc/jukebox), you don't need to quantize every module to 8-bit because it can actually cause instability. With Jukebox, there are several `lm_head` modules that should be skipped using the `llm_int8_skip_modules` parameter in [`BitsAndBytesConfig`].\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\nquantization_config = BitsAndBytesConfig(\nllm_int8_skip_modules=[\"lm_head\"],\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config,\n)\n```\n\n### Finetuning",
  "The [PEFT](https://github.com/huggingface/peft) library supports fine-tuning large models like [flan-t5-large](https://huggingface.co/google/flan-t5-large) and [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b) with 8-bit quantization. You don't need to pass the `device_map` parameter for training because it automatically loads your model on a GPU. However, you can still customize the device map with the `device_map` parameter (`device_map=\"auto\"` should only be used for inference).\n\n## QLoRA\n\nThis section explores some of the specific features of 4-bit quantization, such as changing the compute data type, the Normal Float 4 (NF4) data type, and nested quantization.\n\n### Compute data type\n\nChange the data type from float32 (the default value) to bf16 in [`BitsAndBytesConfig`] to speedup computation.\n\n```py\nimport torch\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n```\n\n### Normal Float 4 (NF4)",
  "NF4 is a 4-bit data type from the [QLoRA](https://hf.co/papers/2305.14314) paper, adapted for weights initialized from a normal distribution. You should use NF4 for training 4-bit base models.\n\n```py\nfrom transformers import BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\n)\n\nmodel_nf4 = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", quantization_config=nf4_config)\n```\n\nFor inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype` and `torch_dtype` values.\n\n### Nested quantization\n\nNested quantization can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter. For example, with nested quantization, you can finetune a [Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b) model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of 1, and enable gradient accumulation with 4 steps.\n\n```py\nfrom transformers import BitsAndBytesConfig",
  "double_quant_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_use_double_quant=True,\n)\n\nmodel_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", torch_dtype=\"auto\", quantization_config=double_quant_config)\n```\n\n## Dequantizing bitsandbytes models\n\nOnce quantized, you can [`~PreTrainedModel.dequantize`] a model to the original precision but this may result in some quality loss. Make sure you have enough GPU memory to fit the dequantized model.\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", BitsAndBytesConfig(load_in_4bit=True))\nmodel.dequantize()\n```\n\n## Resources\n\nLearn more about the details of 8-bit quantization in [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration).",
  "Try 4-bit quantization in this [notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf) and learn more about it's details in [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# torchao\n\n[torchao](https://github.com/pytorch/ao) is a PyTorch architecture optimization library with support for custom high performance data types, quantization, and sparsity. It is composable with native PyTorch features such as [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for even faster inference and training.\n\nInstall torchao with the following command.\n\n```bash",
  "# Updating 🤗 Transformers to the latest version, as the example script below uses the new auto compilation\npip install --upgrade torch torchao transformers\n```\n\ntorchao supports many quantization types for different data types (int4, float8, weight only, etc.).\nStarting with version 0.10.0, torchao provides enhanced flexibility through the `AOBaseConfig` API, allowing for more customized quantization configurations.\nAnd full access to the techniques offered in the torchao library.\n\nYou can manually choose the quantization types and settings or automatically select the quantization types.\n\n<hfoptions id=\"torchao\">\n<hfoption id=\"manual\">\n\n\nCreate a [`TorchAoConfig`] and specify the quantization type and `group_size` of the weights to quantize. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method.\n\n> [!TIP]\n> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`. This is only available in torchao 0.8.0+.\n\nIn torchao 0.10.0+, you can use the more flexible `AOBaseConfig` approach instead of string identifiers:\n\n```py",
  "import torch\nfrom transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\nfrom torchao.quantization import Int4WeightOnlyConfig\n\n# Using AOBaseConfig instance (torchao >= 0.10.0)\nquant_config = Int4WeightOnlyConfig(group_size=128)\nquantization_config = TorchAoConfig(quant_type=quant_config)\n\n# Load and quantize the model\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\noutput = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\n## Available Quantization Schemes\n\nTorchAO provides a variety of quantization configurations:\n\n- `Int4WeightOnlyConfig`\n- `Int8WeightOnlyConfig`\n- `Int8DynamicActivationInt8WeightConfig`\n- `Float8WeightOnlyConfig`",
  "Each configuration can be further customized with parameters such as `group_size`, `scheme`, and `layout` to optimize for specific hardware and model architectures.\n\nFor a complete list of available configurations, see our [quantization API documentation](https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py).\n\n> **⚠️ DEPRECATION WARNING**\n>\n> Starting with version 0.10.0, the string-based API for quantization configuration (e.g., `TorchAoConfig(\"int4_weight_only\", group_size=128)`) is **deprecated** and will be removed in a future release.\n>\n> Please use the new `AOBaseConfig`-based approach instead:\n>\n> ```python\n> # Old way (deprecated)\n> quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n>\n> # New way (recommended)\n> from torchao.quantization import Int4WeightOnlyConfig\n> quant_config = Int4WeightOnlyConfig(group_size=128)\n> quantization_config = TorchAoConfig(quant_type=quant_config)\n> ```\n>\n> The new API offers greater flexibility, better type safety, and access to the full range of features available in torchao.\n>\n> ## Migration Guide\n>\n> Here's how to migrate from common string identifiers to their `AOBaseConfig` equivalents:\n>",
  "> | Old String API | New `AOBaseConfig` API |\n> |----------------|------------------------|\n> | `\"int4_weight_only\"` | `Int4WeightOnlyConfig()` |\n> | `\"int8_weight_only\"` | `Int8WeightOnlyConfig()` |\n> | `\"int8_dynamic_activation_int8_weight\"` | `Int8DynamicActivationInt8WeightConfig()` |\n>\n> All configuration objects accept parameters for customization (e.g., `group_size`, `scheme`, `layout`).\n\n\nBelow is the API for for torchao < `0.9.0`\n\n```py\nimport torch\nfrom transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n\nquantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\noutput = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")",
  "print(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nRun the code below to benchmark the quantized models performance.\n\n```py\nfrom torch._inductor.utils import do_bench_using_profiling\nfrom typing import Callable\n\ndef benchmark_fn(func: Callable, *args, **kwargs) -> float:\n\"\"\"Thin wrapper around do_bench_using_profiling\"\"\"\nno_args = lambda: func(*args, **kwargs)\ntime = do_bench_using_profiling(no_args)\nreturn time * 1e3\n\nMAX_NEW_TOKENS = 1000\nprint(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n\nbf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\noutput = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\nprint(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n```\n\n> [!TIP]\n> For best performance, you can use recommended settings by calling `torchao.quantization.utils.recommended_inductor_config_setter()`\n\n</hfoption>\n<hfoption id=\"automatic\">",
  "The [autoquant](https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) API automatically chooses a quantization type for quantizable layers (`nn.Linear`) by micro-benchmarking on input type and shape and compiling a single linear layer.\n\nCreate a [`TorchAoConfig`] and set to `\"autoquant\"`. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method. Finally, call `finalize_autoquant` on the quantized model to finalize the quantization and log the input shapes.\n\n> [!TIP]\n> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`. This is only available in torchao 0.8.0+.\n\n```py\nimport torch\nfrom transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n\nquantization_config = TorchAoConfig(\"autoquant\", min_sqnr=None)\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")",
  "input_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\noutput = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n# explicitly call `finalize_autoquant` (may be refactored and removed in the future)\nquantized_model.finalize_autoquant()\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nRun the code below to benchmark the quantized models performance.\n\n```py\nfrom torch._inductor.utils import do_bench_using_profiling\nfrom typing import Callable\n\ndef benchmark_fn(func: Callable, *args, **kwargs) -> float:\n\"\"\"Thin wrapper around do_bench_using_profiling\"\"\"\nno_args = lambda: func(*args, **kwargs)\ntime = do_bench_using_profiling(no_args)\nreturn time * 1e3\n\nMAX_NEW_TOKENS = 1000\nprint(\"autoquantized model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n\nbf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)",
  "output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\nprint(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n```\n\n</hfoption>\n</hfoptions>\n\n## Serialization\n\ntorchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor) for maximum flexibility in supporting new quantized torch.Tensor formats. [Safetensors](https://huggingface.co/docs/safetensors/en/index) serialization and deserialization does not work with torchao.\n\nTo avoid arbitrary user code execution, torchao sets `weights_only=True` in [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to ensure only tensors are loaded. Any known user functions can be whitelisted with [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals).\n\n```py\n# don't serialize model with Safetensors\noutput_dir = \"llama3-8b-int4wo-128\"\nquantized_model.save_pretrained(\"llama3-8b-int4wo-128\", safe_serialization=False)\n```\n\n## Resources",
  "For a better sense of expected performance, view the [benchmarks](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks) for various models with CUDA and XPU backends.\n\nRefer to [Other Available Quantization Techniques](https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques) for more examples and documentation.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPTQ",
  "The [GPTQModel](https://github.com/ModelCloud/GPTQModel) and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) implements the GPTQ algorithm, a post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes the error. These weights are quantized to int4, but they're restored to fp16 on the fly during inference. This can save memory usage by 4x because the int4 weights are dequantized in a fused kernel rather than a GPU's global memory. Inference is also faster because a lower bitwidth takes less time to communicate.\n\n> [!WARNING]\n> AutoGPTQ is likely to be deprecated in the future due to lack of continued support for new models and features. See the [GPTQModel](#gptqmodel) section for more details.\n\nInstall Accelerate, Transformers and Optimum first.\n\n```bash\npip install --upgrade accelerate optimum transformers\n```\n\nThen run the command below to install a GPTQ library.\n\n<hfoptions id=\"install\">\n<hfoption id=\"GPTQmodel\">\n\n```bash\npip install gptqmodel --no-build-isolation\n```\n\n</hfoption>\n<hfoption id=\"AutoGPTQ\">\n\n```bash\npip install auto-gptq --no-build-isolation\n```\n\n</hfoption>\n</hfoptions>",
  "Create a [`GPTQConfig`] class and set the number of bits to quantize to, a dataset to calbrate the weights for quantization, and a tokenizer to prepare the dataset.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\ngptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)\n```\n\nYou can pass your own dataset as a list of strings, but it is highly recommended to use the same dataset from the GPTQ paper.\n\n```py\ndataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\ngptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)\n```\n\nLoad a model to quantize and pass [`GPTQConfig`] to [`~AutoModelForCausalLM.from_pretrained`]. Set `device_map=\"auto\"` to automatically offload the model to a CPU to help fit the model in memory, and allow the model modules to be moved between the CPU and GPU for quantization.\n\n```py\nquantized_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\", quantization_config=gptq_config)\n```",
  "If you're running out of memory because a dataset is too large (disk offloading is not supported), try passing the `max_memory` parameter to allocate the amount of memory to use on your device (GPU and CPU).\n\n```py\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"facebook/opt-125m\",\ndevice_map=\"auto\",\nmax_memory={0: \"30GiB\", 1: \"46GiB\", \"cpu\": \"30GiB\"},\nquantization_config=gptq_config\n)\n```\n\n> [!WARNING]\n> Depending on your hardware, it can take some time to quantize a model from scratch. It can take ~5 minutes to quantize the [facebook/opt-350m](https://huggingface.co/facebook/opt-350m) model on a free-tier Google Colab GPU, but it'll take ~4 hours to quantize a 175B parameter model on a NVIDIA A100. Before you quantize a model, it is a good idea to check the Hub if a GPTQ-quantized version of the model already exists.\n\nOnce a model is quantized, you can use [`~PreTrainedModel.push_to_hub`] to push the model and tokenizer to the Hub where it can be easily shared and accessed. This saves the [`GPTQConfig`].\n\n```py\nquantized_model.push_to_hub(\"opt-125m-gptq\")\ntokenizer.push_to_hub(\"opt-125m-gptq\")\n```",
  "[`~PreTrainedModel.save_pretrained`] saves a quantized model locally. If the model was quantized with the `device_map` parameter, make sure to move the entire model to a GPU or CPU before saving it. The example below saves the model on a CPU.\n\n```py\nquantized_model.save_pretrained(\"opt-125m-gptq\")\ntokenizer.save_pretrained(\"opt-125m-gptq\")\n\n# if quantized with device_map set\nquantized_model.to(\"cpu\")\nquantized_model.save_pretrained(\"opt-125m-gptq\")\n```\n\nReload a quantized model with [`~PreTrainedModel.from_pretrained`], and set `device_map=\"auto\"` to automatically distribute the model on all available GPUs to load the model faster without using more memory than needed.\n\n```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")\n```\n\n## Marlin",
  "[Marlin](https://github.com/IST-DASLab/marlin) is a 4-bit only CUDA GPTQ kernel, highly optimized for the NVIDIA A100 GPU (Ampere) architecture. Loading, dequantization, and execution of post-dequantized weights are highly parallelized, offering a substantial inference improvement versus the original CUDA GPTQ kernel. Marlin is only available for quantized inference and does not support model quantization.\n\nMarlin inference can be activated with the `backend` parameter in [`GPTQConfig`].\n\n```py\n\nfrom transformers import AutoModelForCausalLM, GPTQConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\", quantization_config=GPTQConfig(bits=4, backend=\"marlin\"))\n```\n\n## ExLlama\n\n> [!WARNING]\n> Only 4-bit models are supported, and we recommend deactivating the ExLlama kernels if you're finetuning a quantized model with PEFT.",
  "[ExLlama](https://github.com/turboderp/exllama) is a Python/C++/CUDA implementation of the [Llama](model_doc/llama) model that is designed for faster inference with 4-bit GPTQ weights (check out these [benchmarks](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)). The ExLlama kernel is activated by default when you create a [`GPTQConfig`] object.\n\nTo boost inference speed even further, use the [ExLlamaV2](https://github.com/turboderp/exllamav2) kernels by configuring the `exllama_config` parameter in [`GPTQConfig`].\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, GPTQConfig\n\ngptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\nmodel = AutoModelForCausalLM.from_pretrained(\n\"{your_username}/opt-125m-gptq\",\ndevice_map=\"auto\",\nquantization_config=gptq_config\n)\n```\n\nThe ExLlama kernels are only supported when the entire model is on the GPU. If you're doing inference on a CPU with AutoGPTQ 0.4.2+, disable the ExLlama kernel in [`GPTQConfig`]. This overwrites the attributes related to the ExLlama kernels in the quantization config of the `config.json` file.\n\n```py\nimport torch",
  "from transformers import AutoModelForCausalLM, GPTQConfig\n\ngptq_config = GPTQConfig(bits=4, use_exllama=False)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"{your_username}/opt-125m-gptq\",\ndevice_map=\"cpu\",\nquantization_config=gptq_config\n)\n```\n\n## GPTQModel\n\nIt is recommended to use GPTQModel, originally a maintained fork of AutoGPTQ, because it has since diverged from AutoGTPQ with some significant features. GPTQModel has faster quantization, lower memory usage, and more accurate default quantization.\n\nGPTQModel provides asymmetric quantization which can potentially lower quantization errors compared to symmetric quantization. It is not backward compatible with AutoGPTQ, and not all kernels (Marlin) support asymmetric quantization.\n\nGPTQModel also has broader support for the latest LLM models, multimodal models (Qwen2-VL and Ovis1.6-VL), platforms (Linux, macOS, Windows 11), and hardware (AMD ROCm, Apple Silicon, Intel/AMD CPUs, and Intel Datacenter Max/Arc GPUs, etc.).\n\nThe Marlin kernels are also updated for A100 GPUs and other kernels are updated to include auto-padding for legacy models and models with non-uniform in/out-features.\n\n## Resources",
  "Run the GPTQ quantization with PEFT [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing) for a hands-on experience, and read [Making LLMs lighter with AutoGPTQ and transformers](https://huggingface.co/blog/gptq-integration) to learn more about the AutoGPTQ integration.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Overview",
  "Quantization lowers the memory requirements of loading and using a model by storing the weights in a lower precision while trying to preserve as much accuracy as possible. Weights are typically stored in full-precision (fp32) floating point representations, but half-precision (fp16 or bf16) are increasingly popular data types given the large size of models today. Some quantization methods can reduce the precision even further to integer representations, like int8 or int4.\n\nTransformers supports many quantization methods, each with their pros and cons, so you can pick the best one for your specific use case. Some methods require calibration for greater accuracy and extreme compression (1-2 bits), while other methods work out of the box with on-the-fly quantization.\n\nUse the Space below to help you pick a quantization method depending on your hardware and number of bits to quantize to.",
  "| Quantization Method                           | On the fly quantization | CPU             | CUDA GPU | ROCm GPU  | Metal (Apple Silicon)              | Intel GPU       | Torch compile() | Bits          | PEFT Fine Tuning | Serializable with 🤗Transformers | 🤗Transformers Support  | Link to library                             |\n|-----------------------------------------------|----------------------|-----------------|----------|-----------|------------------------------------|-----------------|-----------------|---------------|------------------|-----------------------------|-------------------------|---------------------------------------------|\n| [AQLM](./aqlm)                             | 🔴                   | 🟢              |     🟢     | 🔴        | 🔴                                 | 🔴              | 🟢              | 1/2         | 🟢               | 🟢                          | 🟢                      | https://github.com/Vahe1994/AQLM            |",
  "| [AWQ](./awq)                               | 🔴                   | 🟢              | 🟢        | 🟢        | 🔴                                 | 🟢              | ?               | 4             | 🟢               | 🟢                          | 🟢                      | https://github.com/casper-hansen/AutoAWQ    |\n| [bitsandbytes](./bitsandbytes)             | 🟢                   | 🟡 |     🟢     | 🟡 | 🔴                    | 🟡 | 🔴 | 4/8         | 🟢               | 🟢                          | 🟢                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n| [compressed-tensors](./compressed_tensors) | 🔴                   | 🟢              |     🟢     | 🟢        | 🔴                                 | 🔴              | 🔴              | 1/8         | 🟢               | 🟢                          | 🟢                      | https://github.com/neuralmagic/compressed-tensors |",
  "| [EETQ](./eetq)                             | 🟢                   | 🔴              | 🟢        | 🔴        | 🔴                                 | 🔴              | ?               | 8             | 🟢               | 🟢                          | 🟢                      | https://github.com/NetEase-FuXi/EETQ        |\n| [GGUF / GGML (llama.cpp)](../gguf)         | 🟢                   | 🟢              | 🟢        | 🔴        | 🟢                                 | 🔴              | 🔴              | 1/8         | 🔴               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n| [GPTQModel](./gptq)                        | 🔴                   | 🟢 | 🟢        | 🟢        | 🟢                                 | 🟢 | 🔴              | 2/3/4/8 | 🟢               | 🟢                          | 🟢                      | https://github.com/ModelCloud/GPTQModel        |",
  "| [AutoGPTQ](./gptq)                         | 🔴                   | 🔴              | 🟢        | 🟢        | 🔴                                 | 🔴              | 🔴              | 2/3/4/8 | 🟢               | 🟢                          | 🟢                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n| [HIGGS](./higgs)                           | 🟢                   | 🔴              | 🟢        | 🔴        | 🔴                                 | 🔴              | 🟢              | 2/4         | 🔴               | 🟢                          | 🟢                      | https://github.com/HanGuo97/flute           |\n| [HQQ](./hqq)                               | 🟢                   | 🟢              | 🟢        | 🔴        | 🔴                                 | 🔴              | 🟢              | 1/8         | 🟢               | 🔴                          | 🟢                      | https://github.com/mobiusml/hqq/            |",
  "| [optimum-quanto](./quanto)                 | 🟢                   | 🟢              | 🟢        | 🔴        | 🟢                                 | 🔴              | 🟢              | 2/4/8     | 🔴               | 🔴                          | 🟢                      | https://github.com/huggingface/optimum-quanto       |\n| [FBGEMM_FP8](./fbgemm_fp8)                 | 🟢                   | 🔴              | 🟢        | 🔴        | 🔴                                 | 🔴              | 🔴              | 8             | 🔴               | 🟢                          | 🟢                      | https://github.com/pytorch/FBGEMM       |\n| [torchao](./torchao)                       | 🟢                   | 🟢               | 🟢        | 🔴        | 🟡 | 🔴              |                 | 4/8         |                  | 🟢🔴                        | 🟢                      | https://github.com/pytorch/ao       |",
  "| [VPTQ](./vptq)                             | 🔴                   | 🔴              |     🟢     | 🟡        | 🔴                                 | 🔴              | 🟢              | 1/8         | 🔴               | 🟢                          | 🟢                      | https://github.com/microsoft/VPTQ            |\n| [FINEGRAINED_FP8](./finegrained_fp8)                 | 🟢                   | 🔴              | 🟢        | 🔴        | 🔴                                 | 🔴              | 🔴              | 8             | 🔴               | 🟢                          | 🟢                      |        |\n| [SpQR](./spqr)                          | 🔴                       |  🔴   | 🟢        | 🔴              |    🔴    | 🔴         |         🟢              | 3              |              🔴                     | 🟢           | 🟢                      | https://github.com/Vahe1994/SpQR/       |\n| [Quark](./quark.md)                           | 🔴                       | 🟢 | 🟢      | 🟢      | 🟢                   | 🟢       | ?               | 2/4/6/8/9/16 | 🔴                | 🔴                               | 🟢                       | https://quark.docs.amd.com/latest/                      |\n\n## Resources",
  "If you are new to quantization, we recommend checking out these beginner-friendly quantization courses in collaboration with DeepLearning.AI.\n\n* [Quantization Fundamentals with Hugging Face](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)\n* [Quantization in Depth](https://www.deeplearning.ai/short-courses/quantization-in-depth)\n\n## User-Friendly Quantization Tools\n\nIf you are looking for a user-friendly quantization experience, you can use the following community spaces and notebooks:\n\n* [Bitsandbytes Space](https://huggingface.co/spaces/bnb-community/bnb-my-repo)\n* [GGUF Space](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n* [MLX Space](https://huggingface.co/spaces/mlx-community/mlx-my-repo)\n* [AuoQuant Notebook](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing#scrollTo=ZC9Nsr9u5WhN)",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FBGEMM\n\n[FBGEMM (Facebook GEneral Matrix Multiplication)](https://github.com/pytorch/FBGEMM) is a low-precision matrix multiplication library for small batch sizes and support for accuracy-loss minimizing techniques such as row-wise quantization and outlier-aware quantization. With FBGEMM, quantize a models weights to 8-bits/channel and the activations to 8-bits/token (also known as fp8 or w8a8).\n\n> [!TIP]",
  "> You need a GPU with [compute capability 9+](https://developer.nvidia.com/cuda-gpus#collapseOne) like a H100.\n\nInstall the FBGEMM_GPU package with the command below to ensure you have the latest version.\n\n```bash\npip install --upgrade accelerate fbgemm-gpu torch\n```\n\nIf you're having installation issues, try installing the [nightly release](https://pytorch.org/FBGEMM/fbgemm_gpu-development/InstallationInstructions.html#fbgemm-gpu-install-libraries:~:text=found%20here.-,Install%20the%20FBGEMM_GPU%20Package,-Install%20through%20PyTorch).\n\nCreate a [`FbgemmFp8Config`] and pass it to [`~PreTrainedModel.from_pretrained`] to quantize a model to fp8.\n\n```py\nfrom transformers import FbgemmFp8Config, AutoModelForCausalLM\n\nquantization_config = FbgemmFp8Config()\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\n```\n\n[`~PreTrainedModel.save_pretrained`] and [`~PreTrainedModel.from_pretrained`] enable saving and loading a quantized model.\n\n```py\nquant_path = \"/path/to/save/quantized/model\"\nmodel.save_pretrained(quant_path)",
  "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")\n```\n\n## Resources\n\nRead the [Open-sourcing FBGEMM for state-of-the-art server-side inference](https://engineering.fb.com/2018/11/07/ml-applications/fbgemm/) blog post for more details on FBGEMM.",
  "<!--Copyright 2025 Advanced Micro Devices, Inc. and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Quark\n\n[Quark](https://quark.docs.amd.com/latest/) is a deep learning quantization toolkit designed to be agnostic to specific data types, algorithms, and hardware. Different pre-processing strategies, algorithms and data-types can be combined in Quark.",
  "The PyTorch support integrated through 🤗 Transformers primarily targets AMD CPUs and GPUs, and is primarily meant to be used for evaluation purposes. For example, it is possible to use [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) with 🤗 Transformers backend and evaluate a wide range of models quantized through Quark seamlessly.\n\nUsers interested in Quark can refer to its [documentation](https://quark.docs.amd.com/latest/) to get started quantizing models and using them in supported open-source libraries!\n\nAlthough Quark has its own checkpoint / [configuration format](https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV-Quark-test/blob/main/config.json#L26), the library also supports producing models with a serialization layout compliant with other quantization/runtime implementations ([AutoAWQ](https://huggingface.co/docs/transformers/quantization/awq), [native fp8 in 🤗 Transformers](https://huggingface.co/docs/transformers/quantization/finegrained_fp8)).\n\nTo be able to load Quark quantized models in Transformers, the library first needs to be installed:\n\n```bash\npip install amd-quark\n```\n\n## Support matrix",
  "Models quantized through Quark support a large range of features, that can be combined together. All quantized models independently of their configuration can seamlessly be reloaded through `PretrainedModel.from_pretrained`.\n\nThe table below shows a few features supported by Quark:\n\n| **Feature**                     | **Supported subset in Quark**                                                                             |   |\n|---------------------------------|-----------------------------------------------------------------------------------------------------------|---|\n| Data types                      | int8, int4, int2, bfloat16, float16, fp8_e5m2, fp8_e4m3, fp6_e3m2, fp6_e2m3, fp4, OCP MX, MX6, MX9, bfp16 |   |\n| Pre-quantization transformation | SmoothQuant, QuaRot, SpinQuant, AWQ                                                                       |   |\n| Quantization algorithm          | GPTQ                                                                                                      |   |\n| Supported operators             | ``nn.Linear``, ``nn.Conv2d``, ``nn.ConvTranspose2d``, ``nn.Embedding``, ``nn.EmbeddingBag``               |   |",
  "| Granularity                     | per-tensor, per-channel, per-block, per-layer, per-layer type                                             |   |\n| KV cache                        | fp8                                                                                                       |   |\n| Activation calibration          | MinMax / Percentile / MSE                                                                                 |   |\n| Quantization strategy           | weight-only, static, dynamic, with or without output quantization                                         |   |\n\n## Models on Hugging Face Hub\n\nPublic models using Quark native serialization can be found at https://huggingface.co/models?other=quark.\n\nAlthough Quark also supports [models using `quant_method=\"fp8\"`](https://huggingface.co/models?other=fp8) and [models using `quant_method=\"awq\"`](https://huggingface.co/models?other=awq), Transformers loads these models rather through [AutoAWQ](https://huggingface.co/docs/transformers/quantization/awq) or uses the [native fp8 support in 🤗 Transformers](https://huggingface.co/docs/transformers/quantization/finegrained_fp8).\n\n## Using Quark models in Transformers",
  "Here is an example of how one can load a Quark model in Transformers:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"EmbeddedLLM/Llama-3.1-8B-Instruct-w_fp8_per_channel_sym\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\nmodel = model.to(\"cuda\")\n\nprint(model.model.layers[0].self_attn.q_proj)\n# QParamsLinear(\n#   (weight_quantizer): ScaledRealQuantizer()\n#   (input_quantizer): ScaledRealQuantizer()\n#   (output_quantizer): ScaledRealQuantizer()\n# )\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninp = tokenizer(\"Where is a good place to cycle around Tokyo?\", return_tensors=\"pt\")\ninp = inp.to(\"cuda\")\n\nres = model.generate(**inp, min_new_tokens=50, max_new_tokens=100)\n\nprint(tokenizer.batch_decode(res)[0])\n# <|begin_of_text|>Where is a good place to cycle around Tokyo? There are several places in Tokyo that are suitable for cycling, depending on your skill level and interests. Here are a few suggestions:\n# 1. Yoyogi Park: This park is a popular spot for cycling and has a wide, flat path that's perfect for beginners. You can also visit the Meiji Shrine, a famous Shinto shrine located in the park.",
  "# 2. Imperial Palace East Garden: This beautiful garden has a large, flat path that's perfect for cycling. You can also visit the\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Optimum\n\n[Optimum](https://huggingface.co/docs/optimum/index) is an optimization library that supports quantization for Intel, Furiousa, ONNX Runtime, GPTQ, and lower-level PyTorch quantization functions. It is designed to enhance performance for specific hardware - Intel CPUs/HPUs, AMD GPUs, Furiousa NPUs, etc. - and model accelerators like ONNX Runtime.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Optimum Quanto\n\n[Quanto](https://github.com/huggingface/optimum-quanto) is a PyTorch quantization backend for [Optimum](https://huggingface.co/docs/optimum/index). It features linear quantization for weights (float8, int8, int4, int2) with accuracy very similar to full-precision models. Quanto is compatible with any model modality and device, making it simple to use regardless of hardware.",
  "Quanto is also compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for faster generation.\n\nInstall Quanto with the following command.\n\n```bash\npip install optimum-quanto accelerate transformers\n```\n\nQuantize a model by creating a [`QuantoConfig`] and specifying the `weights` parameter to quantize to. This works for any model in any modality as long as it contains [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n\n> [!TIP]\n> The Transformers integration only supports weight quantization. Use the Quanto library directly if you need activation quantization, calibration, or QAT.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n\nquant_config = QuantoConfig(weights=\"int8\")\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-3.1-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quant_config\n)\n```\n\n## torch.compile\n\nWrap a Quanto model with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for faster generation.\n\n```py\nimport torch",
  "from transformers import AutoModelForSpeechSeq2Seq, QuantoConfig\n\nquant_config = QuantoConfig(weights=\"int8\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\"openai/whisper-large-v2\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quant_config\n)\n\nmodel = torch.compile(model)\n```\n\n## Resources\n\nRead the [Quanto: a PyTorch quantization backend for Optimum](https://huggingface.co/blog/quanto-introduction) blog post to learn more about the library design and benchmarks.\n\nFor more hands-on examples, take a look at the Quanto [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Contribute",
  "Transformers supports many quantization methods such as QLoRA, GPTQ, LLM.int8, and AWQ. However, there are still many more quantization approaches that haven't been integrated yet. To make adding and using these quantization methods with Transformers easier, use the [`~quantizers.HfQuantizer`] class.  [`~quantizers.HfQuantizer`] is designed to be an internal helper class for adding a quantization method instead of something applied to every PyTorch module.\n\nThis guide will show you how to integrate a new quantization method with [`~quantizers.HfQuantizer`].\n\n## Requirements\n\nBefore integrating a new quantization method into Transformers, ensure the method meets the following requirements. Only quantization methods that can be run with PyTorch modules are supported.\n\n- The quantization method is available through a Python package that is pip-installable (it is also fine if you can only install the package from source). Ideally, pre-compiled kernels are included in the pip package.\n- The method can run on commonly-used hardware (CPU, GPU, etc.).",
  "- The method is wrapped in a [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) ([`~bitsandbytes.nn.Linear8bitLt`], [`~bitsandbytes.nn.Linear4bit`]), and the quantized linear layer should have the following definition.\n\n```py\nclass Linear4bit(nn.Module):\ndef __init__(self, ...):\n...\n\ndef forward(self, x):\nreturn my_4bit_kernel(x, self.weight, self.bias)\n```\n\nThis way, Transformers models are easily quantized by replacing instances of [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with a target class.\n\n- The quantization method should be serializable. You can save the quantized weights locally or push them to the Hub.\n- Make sure the package containing the quantization kernels/primitive is stable (no frequent breaking changes).\n\nSome quantization methods may require \"pre-quantizing\" the model through data calibration (AWQ). In this case, we prefer to only support inference in Transformers and let the third-party library maintained by the ML community deal handle the model quantization itself.\n\n## Create new HFQuantizer class",
  "1. Create a new quantization config class inside [src/transformers/utils/quantization_config.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/utils/quantization_config.py). Add the new quantization config to the [_import_structure](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py#L1088) inside Transformers' [src/transformers/__init__.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py) file.\n\n2. Create a new file inside [src/transformers/quantizers/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers) named `quantizer_your_method.py`, and make it inherit from [`~quantizers.HfQuantizer]. Make sure to add the new quantizer and quantization config in the quantization auto-mapping in [src/transformers/quantizers/auto.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers/auto.py).",
  "3. Define the following class attributes and property methods for your quantization method.\n\n- `requires_calibration`: Whether the quantization method requires a data calibration process. If set to `True`, you can only support inference (with quantized weights) and not inference and quantization.\n- `required_packages`: A list of strings of the required packages to use the quantized weights. You might need to define some new utility methods such as `is_auto_awq_available` in [transformers/src/utils/import_utils.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/utils/import_utils.py).",
  "- `requires_parameters_quantization`: Only required if your quantization method requires extra attention to the underlying [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) object. For example, bitsandbytes uses [`~bitsandbytes.nn.Params4bit`] and [`~bitsandbytes.nn.Int8Params`], which requires some extra attention when quantizing the model. Most of the recent quantization method packs int2 and int4 weights inside [torch.uint8](https://pytorch.org/docs/stable/tensors.html) weights, so this flag should not be really required (set to `False` by default).\n- `is_serializable`: A property method to determine whether the method is serializable or not.\n- `is_trainable`:  A property method to determine whether you can fine-tune models on top of the quantization method (with or without PEFT approaches).\n\n4. Write the `validate_environment` and `update_torch_dtype` methods. These methods are called before creating the quantized model to ensure users use the right configuration. Refer to other quantizers for an example of it is implemented.",
  "5. Write the `_process_model_before_weight_loading` method. In Transformers, the quantized models are initialized first on the `\"meta\"` device before loading the weights. This means the `_process_model_before_weight_loading` method takes care of manipulating the model skeleton to replace some modules ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) with the target modules (quantization modules).\n\nYou can define module replacement logic or any other utility method by creating a new file in [transformers/src/integrations/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/integrations) and exposing the relevant methods in that folder's `__init__.py` file. The best starting point would be to have a look at another quantization method such as [quantizer_awq.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers/quantizer_awq.py).\n\n6. Write the `_process_model_after_weight_loading` method. This method enables implementing additional features that require manipulating the model after loading the weights.",
  "7. Document everything! Make sure your quantization method is documented by adding a new file under `docs/source/en/quantization`.\n\n8. You should add tests by adding the package in our nightly Dockerfile inside `docker/transformers-quantization-latest-gpu` and then adding a new test file in `tests/quantization/xxx`. Feel free to check out existing quantization methods to see how it is implemented.",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SpQR\n\nThe [SpQR]((https://hf.co/papers/2306.03078)) quantization algorithm involves a 16x16 tiled bi-level group 3-bit quantization structure with sparse outliers.\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/spqr-diagram.png\">\n</div>\n\n> [!TIP]\n> To quantize a model with SpQR, refer to the [Vahe1994/SpQR](https://github.com/Vahe1994/SpQR) repository.",
  "Load a SpQR-quantized model with [`~PreTrainedModel.from_pretrained`].\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\",\ntorch_dtype=torch.half,\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\")\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# HQQ\n\n[Half-Quadratic Quantization (HQQ)](https://github.com/mobiusml/hqq/) supports fast on-the-fly quantization for 8, 4, 3, 2, and even 1-bits. It doesn't require calibration data, and it is compatible with any model modality (LLMs, vision, etc.).",
  "HQQ further supports fine-tuning with [PEFT](https://huggingface.co/docs/peft) and is fully compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for even faster inference and training.\n\nInstall HQQ with the following command to get the latest version and to build its corresponding CUDA kernels.\n\n```bash\npip install hqq\n```\n\nYou can choose to either replace all the linear layers in a model with the same quantization config or dedicate a specific quantization config for specific linear layers.\n\n<hfoptions id=\"hqq\">\n<hfoption id=\"replace all layers\">\n\nQuantize a model by creating a [`HqqConfig`] and specifying the `nbits` and `group_size` to replace for all the linear layers ([torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) of the model.\n\n``` py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n\nquant_config = HqqConfig(nbits=8, group_size=64)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-3.1-8B\",\ntorch_dtype=torch.float16,\ndevice_map=\"cuda\",\nquantization_config=quant_config\n)\n```\n\n</hfoption>\n<hfoption id=\"specific layers only\">",
  "Quantize a model by creating a dictionary specifying the `nbits` and `group_size` for the linear layers to quantize. Pass them to [`HqqConfig`] and set which layers to quantize with the config. This approach is especially useful for quantizing mixture-of-experts (MoEs) because they are less affected ly lower quantization settings.\n\n``` py\nq4_config = {'nbits':4, 'group_size':64}\nq3_config = {'nbits':3, 'group_size':32}\nquant_config  = HqqConfig(dynamic_config={\n'self_attn.q_proj':q4_config,\n'self_attn.k_proj':q4_config,\n'self_attn.v_proj':q4_config,\n'self_attn.o_proj':q4_config,\n\n'mlp.gate_proj':q3_config,\n'mlp.up_proj'  :q3_config,\n'mlp.down_proj':q3_config,\n})\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-3.1-8B\",\ntorch_dtype=torch.float16,\ndevice_map=\"cuda\",\nquantization_config=quant_config\n)\n```\n\n</hfoption>\n</hfoptions>\n\n## Backends\n\nHQQ supports various backends, including pure PyTorch and custom dequantization CUDA kernels. These backends are suitable for older GPUs and PEFT/QLoRA training.\n\n```py\nfrom hqq.core.quantize import *\n\nHQQLinear.set_backend(HQQBackend.PYTORCH)\n```",
  "For faster inference, HQQ supports 4-bit fused kernels (torchao and Marlin) after a model is quantized. These can reach up to 200 tokens/sec on a single 4090. The example below demonstrates enabling the torchao_int4 backend.\n\n```py\nfrom hqq.utils.patching import prepare_for_inference\n\nprepare_for_inference(\"model\", backend=\"torchao_int4\")\n```\n\nRefer to the [Backend](https://github.com/mobiusml/hqq/#backend) guide for more details.\n\n## Resources\n\nRead the [Half-Quadratic Quantization of Large Machine Learning Models](https://mobiusml.github.io/hqq_blog/) blog post for more details about HQQ.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# compressed-tensors\n\n[compressed-tensors](https://github.com/neuralmagic/compressed-tensors) extends [safetensors](https://github.com/huggingface/safetensors) files to compressed tensor data types to provide a unified checkpoint format for storing and loading various quantization and sparsity formats such dense, int-quantized (int8), float-quantized (fp8), and pack-quantized (int4 or int8 weight-quantized packed into int32).",
  "compressed-tensors supports fine-tuning with [PEFT](https://huggingface.co/docs/peft) and includes the following features as well.\n\n- fp8, int4, int8 weight and activation precisions.\n- Quantization scales and zero-points strategies for [tensor, channel, group, block, token](https://github.com/neuralmagic/compressed-tensors/blob/83b2e7a969d70606421a76b9a3d112646077c8de/src/compressed_tensors/quantization/quant_args.py#L43-L52).\n- Dynamic per-token activation quantization (or any static strategy).\n- Weight sparsity (unstructured or semi-structured like 2:4) can be composed with quantization for extreme compression.\n- Quantization of arbitrary modules, not just [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) modules.\n- Targeted support for specific modules by name or class.\n\nInstall compressed-tensors from [PyPI](https://pypi.org/project/compressed-tensors) to get the latest stable release (recommended) or install it from source to get the latest features.\n\n<hfoptions id=\"install\">\n<hfoption id=\"PyPI\">\n\n```bash\npip install compressed-tensors\n```\n\n</hfoption>\n<hfoption id=\"source code\">\n\n```bash\ngit clone https://github.com/neuralmagic/compressed-tensors",
  "cd compressed-tensors\npip install -e .\n```\n\n</hfoption>\n</hfoptions>\n\nSearch using the compressed-tensors [tag](https://huggingface.co/models?other=compressed-tensors) to find a compatible model on the Hugging Face Hub.\n\nOnly models that have already been quantized can be loaded at the moment, and once a model is loaded, it cannot be saved. To quantize a model into the compressed-tensors format, see [llm-compressor](https://github.com/vllm-project/llm-compressor). Alternatively, models can be created independently and serizlied with a compressed-tensors config.\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nct_model = AutoModelForCausalLM.from_pretrained(\"nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf\", device_map=\"auto\")\n\n# measure memory usage\nmem_params = sum([param.nelement()*param.element_size() for param in ct_model.parameters()])\nprint(f\"{mem_params/2**30:.4f} GB\")\n# 8.4575 GB\n```\n\n## Model checkpoint\n\ncompressed-tensor models are defined through its configuration entry. The following example is taken from the [nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf/blob/main/config.json) `config.json` file.",
  "There are a lot of entries to allow for flexible expression both during and after compression, but the entries for loading and inference can be simplified to focus on just a few key entries.\n\n```yaml\n\"quantization_config\": {\n\"config_groups\": {\n\"group_0\": {\n\"input_activations\": {\n\"num_bits\": 8,\n\"strategy\": \"tensor\",\n\"type\": \"float\"\n},\n\"targets\": [\"Linear\"],\n\"weights\": {\n\"num_bits\": 8,\n\"strategy\": \"tensor\",\n\"type\": \"float\"\n}\n}\n},\n\"format\": \"naive-quantized\",\n\"ignore\": [\"lm_head\"],\n\"quant_method\": \"compressed-tensors\",\n\"quantization_status\": \"frozen\"\n},\n```\n\nThe config file specifies the quantization of a config group (`group_0`), which includes weight and activation quantization to fp8 with a static per-tensor strategy. The `lm_head` module is unquantized as shown in the `ignore` key.\n\nFor a more detailed look at the model weights, use the [safetensors viewer](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf?show_file_info=model.safetensors.index.json) on the model card to see the quantized weights, input scale, and weight scale for all [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) modules.\n\n| Tensors | Shape |\tPrecision |",
  "| ------- | ----- | --------- |\nmodel.layers.0.input_layernorm.weight\t| [4 096]\t| BF16\nmodel.layers.0.mlp.down_proj.input_scale\t| [1]\t| BF16\nmodel.layers.0.mlp.down_proj.weight\t| [4 096, 14 336] |\tF8_E4M3\nmodel.layers.0.mlp.down_proj.weight_scale |\t[1]\t| BF16\nmodel.layers.0.mlp.gate_proj.input_scale |\t[1]\t| BF16\nmodel.layers.0.mlp.gate_proj.weight\t| [14 336, 4 096]\t| F8_E4M3\nmodel.layers.0.mlp.gate_proj.weight_scale\t| [1] |\tBF16\nmodel.layers.0.mlp.up_proj.input_scale|\t[1]\t|BF16\nmodel.layers.0.mlp.up_proj.weight |\t[14 336, 4 096]\t| F8_E4M3\nmodel.layers.0.mlp.up_proj.weight_scale | [1]\t| BF16\nmodel.layers.0.post_attention_layernorm.weight |\t[4 096]\t|BF16\nmodel.layers.0.self_attn.k_proj.input_scale |\t[1]\t|  BF16\nmodel.layers.0.self_attn.k_proj.weight |\t[1 024, 4 096]|\tF8_E4M3\nmodel.layers.0.self_attn.k_proj.weight_scale |[1]\t| BF16\nmodel.layers.0.self_attn.o_proj.input_scale\t| [1]\t| BF16\nmodel.layers.0.self_attn.o_proj.weight | [4 096, 4 096]\t| F8_E4M3\nmodel.layers.0.self_attn.o_proj.weight_scale | [1]\t| BF16\nmodel.layers.0.self_attn.q_proj.input_scale\t| [1]\t| BF16\nmodel.layers.0.self_attn.q_proj.weight | [4 096, 4 096]\t| F8_E4M3",
  "model.layers.0.self_attn.q_proj.weight_scale |\t[1] | BF16\nmodel.layers.0.self_attn.v_proj.input_scale\t| [1] | BF16\nmodel.layers.0.self_attn.v_proj.weight |\t[1 024, 4 096]\t| F8_E4M3\nmodel.layers.0.self_attn.v_proj.weight_scale |\t[1] |\tBF16\n\nWhen loading a compressed-tensors model with the [`~quantizers.HFQuantizer`] integration, all the [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) modules specified in the quantization config are replaced by [CompressedLinear](https://github.com/neuralmagic/compressed-tensors/blob/975cb223b19fcac2b98a4271d17668462d4d6e1d/src/compressed_tensors/linear/compressed_linear.py#L30) modules that manage the compressed weights and forward pass for inference. The `lm_head` module is still kept as an unquantized nn.Linear module.\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nct_model = AutoModelForCausalLM.from_pretrained(\"nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf\")\nprint(ct_model)\n\"\"\"\nLlamaForCausalLM(\n(model): LlamaModel(\n(embed_tokens): Embedding(128256, 4096)\n(layers): ModuleList(\n(0-31): 32 x LlamaDecoderLayer(\n(self_attn): LlamaSdpaAttention(\n(q_proj): CompressedLinear(",
  "in_features=4096, out_features=4096, bias=False\n(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(k_proj): CompressedLinear(\nin_features=4096, out_features=1024, bias=False\n(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(v_proj): CompressedLinear(\nin_features=4096, out_features=1024, bias=False\n(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(o_proj): CompressedLinear(\nin_features=4096, out_features=4096, bias=False\n(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(rotary_emb): LlamaRotaryEmbedding()\n)\n(mlp): LlamaMLP(\n(gate_proj): CompressedLinear(\nin_features=4096, out_features=14336, bias=False\n(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(up_proj): CompressedLinear(\nin_features=4096, out_features=14336, bias=False\n(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(down_proj): CompressedLinear(\nin_features=14336, out_features=4096, bias=False",
  "(input_observer): MovingAverageMinMaxObserver()\n(weight_observer): MovingAverageMinMaxObserver()\n)\n(act_fn): SiLU()\n)\n(input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n(post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n)\n)\n(norm): LlamaRMSNorm((4096,), eps=1e-05)\n(rotary_emb): LlamaRotaryEmbedding()\n)\n(lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)\n\"\"\"\n```",
  "<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fine-grained FP8\n\nFine-grained FP8 quantization quantizes the weights and activations to fp8.\n\n- The weights are quantized to 8-bits for each 2D block (`weight_block_size=(128, 128)`).\n- The activations are quantized to 8-bits for each group per token. The group value matches the weights in the input channel (128 by default).\n\nFP8 quantization enables support for [DeepSeek-V3](https://hf.co/papers/2412.19437) and DeepSeek-R1.",
  "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/b7b3b34bf826a6423ea82ffc57ecac80c46c3c76/transformers/quantization/quantization_deepseek.png\">\n</div>\n\n> [!TIP]\n> You need a GPU with Compute Capability>=9 (H100), and install a PyTorch version compatible with the CUDA version of your GPU.\n\nInstall Accelerate and upgrade to the latest version of PyTorch.\n\n```bash\npip install --upgrade accelerate torch\n```\n\nCreate a [`FineGrainedFP8Config`] class and pass it to [`~PreTrainedModel.from_pretrained`] to quantize it. The weights are loaded in full precision (`torch.float32`) by default regardless of the actual data type the weights are stored in. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a models `config.json` file to automatically load the most memory-optiomal data type.\n\n```py\nfrom transformers import FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\nquantization_config = FineGrainedFP8Config()\nquantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)",
  "tokenizer = AutoTokenizer.from_pretrained(model_name)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nUse [`~PreTrainedModel.save_pretrained`] to save the quantized model and reload it with [`~PreTrainedModel.from_pretrained`].\n\n```py\nquant_path = \"/path/to/save/quantized/model\"\nmodel.save_pretrained(quant_path)\nmodel = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")\n```",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BitNet\n\n[BitNet](https://arxiv.org/abs/2402.17764) replaces traditional linear layers in Multi-Head Attention and feed-forward networks with specialized BitLinear layers. The BitLinear layers quantize the weights using ternary precision (with values of -1, 0, and 1) and quantize the activations to 8-bit precision.\n\n<figure style=\"text-align: center;\">",
  "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png\" alt=\"Alt Text\" />\n<figcaption>The architecture of BitNet with BitLinear layers.</figcaption>\n</figure>\n\nBitNet models can't be quantized on the fly. They need to be quantized during pretraining or fine-tuning because it is a Quantization-Aware Training (QAT) technique. During training, the weights are quantized to ternary values with symmetric per tensor quantization.\n\n1. Compute the average of the absolute values of the weight matrix and use as a scale.\n2. Divide the weights by the scale, round the values, constrain them between -1 and 1, and rescale them to continue in full precision.\n3. Activations are quantized to a specified bit-width (8-bit) using [absmax](https://arxiv.org/pdf/2208.07339) quantization (symmetric per channel quantization). This involves scaling the activations into a range of [−128,127].",
  "Refer to this [PR](https://github.com/huggingface/nanotron/pull/180) to pretrain or fine-tune a 1.58-bit model with [Nanotron](https://github.com/huggingface/nanotron). For fine-tuning, convert a model from the Hugging Face to Nanotron format. Find the conversion steps in this [PR](https://github.com/huggingface/nanotron/pull/174).\n\nLoad a BitNet quantized model with [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import AutoModelForCausalLM\npath = \"/path/to/model\"\nmodel = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\")\n```\n\n## Kernels\n\n`@torch.compile` is used to unpack the weights and perform the forward pass. It’s very straightforward to implement and delivers significant speed improvements. Additional optimized kernels will be integrated in future versions.\n\n## Resources\n\nRead [Fine-tuning LLMs to 1.58bit: extreme quantization made easy](https://huggingface.co/blog/1_58_llm_extreme_quantization) to learn more about how BitNet models are trained and fine-tuned.",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# VPTQ",
  "[Vector Post-Training Quantization (VPTQ)](https://github.com/microsoft/VPTQ) is a Post-Training Quantization (PTQ) method that leverages vector quantization to quantize LLMs at an extremely low bit-width (<2-bit). VPTQ can compress a 70B, even a 405B model, to 1-2 bits without retraining and still maintain a high-degree of accuracy. It is a lightweight quantization algorithm that takes ~17 hours to quantize a 405B model. VPTQ features agile quantization inference with low decoding overhead and high throughput and Time To First Token (TTFT).\n\nRun the command below to install VPTQ which provides efficient kernels for inference on NVIDIA and AMD GPUs.\n\n```bash\npip install vptq\n```\n\nThe [VPTQ-community](https://huggingface.co/VPTQ-community) provides a collection of VPTQ-quantized models. The model name contains information about its bitwidth (excluding cookbook, parameter, and padding overhead). Consider the [Meta-Llama-3.1-70B-Instruct-v8-k65536-256-woft] model as an example.\n\n- The model name is Meta-Llama-3.1-70B-Instruct.\n- The number of centroids is given by 65536 (2^16).\n- The number of residual centroids is given by 256 (2^8).",
  "The equivalent bit-width calculation is given by the following.\n\n- index: log2(65536) = 16 / 8 = 2-bits\n- residual index: log2(256) = 8 / 8 = 1-bit\n- total bit-width: 2 + 1 = 3-bits\n\nFrom here, estimate the model size by multiplying 70B * 3-bits / 8-bits/byte for a total of 26.25GB.\n\nLoad a VPTQ quantized model with [`~PreTrainedModel.from_pretrained`].\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n```\n\nTo quantize your own model, refer to the [VPTQ Quantization Algorithm Tutorial](https://github.com/microsoft/VPTQ/blob/algorithm/algorithm.md) tutorial.\n\n## Benchmarks\n\nVPTQ achieves better accuracy and higher throughput with lower quantization overhead across models of different sizes. The following experimental results are for reference only; VPTQ can achieve better outcomes under reasonable parameters, especially in terms of model accuracy and inference speed.\n\n| Model       | bitwidth | W2↓  | C4↓  | AvgQA↑ | tok/s↑ | mem(GB) | cost/h↓ |",
  "| ----------- | -------- | ---- | ---- | ------ | ------ | ------- | ------- |\n| LLaMA-2 7B  | 2.02     | 6.13 | 8.07 | 58.2   | 39.9   | 2.28    | 2       |\n|             | 2.26     | 5.95 | 7.87 | 59.4   | 35.7   | 2.48    | 3.1     |\n| LLaMA-2 13B | 2.02     | 5.32 | 7.15 | 62.4   | 26.9   | 4.03    | 3.2     |\n|             | 2.18     | 5.28 | 7.04 | 63.1   | 18.5   | 4.31    | 3.6     |\n| LLaMA-2 70B | 2.07     | 3.93 | 5.72 | 68.6   | 9.7    | 19.54   | 19      |\n|             | 2.11     | 3.92 | 5.71 | 68.7   | 9.7    | 20.01   | 19      |\n\n## Resources\n\nSee an example demo of VPTQ on the VPTQ Online Demo [Space](https://huggingface.co/spaces/microsoft/VPTQ) or try running the VPTQ inference [notebook](https://colab.research.google.com/github/microsoft/VPTQ/blob/main/notebooks/vptq_example.ipynb).\n\nFor more information, read the VPTQ [paper](https://arxiv.org/pdf/2409.17066).",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# HIGGS\n\n[HIGGS](https://arxiv.org/abs/2411.17525) is a zero-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and state-of-the-art performance.",
  "Runtime support for HIGGS is implemented through the [FLUTE](https://github.com/HanGuo97/flute) library. Only the 70B and 405B variants of Llama 3 and Llama 3.0, and the 8B and 27B variants of Gemma 2 are currently supported. HIGGS also doesn't support quantized training and backward passes in general at the moment.\n\nRun the command below to install FLUTE.\n\n<hfoptions id=\"install\">\n<hfoption id=\"CUDA 12.1\">\n\n```bash\npip install flute-kernel\n```\n\n</hfoption>\n<hfoption id=\"CUDA 11.8\">\n\n```bash\npip install flute-kernel -i https://flute-ai.github.io/whl/cu12.4\n```\n\n</hfoption>\n</hfoptions>\n\nCreate a [`HiggsConfig`] with the number of bits to quantize a model to.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b-it\",\nquantization_config=HiggsConfig(bits=4),\ndevice_map=\"auto\",\n)\n```\n\n> [!TIP]\n> Find models pre-quantized with HIGGS in the official ISTA-DASLab [collection](https://huggingface.co/collections/ISTA-DASLab/higgs-675308e432fd56b7f6dab94e).\n\n## torch.compile\n\nHIGGS is fully compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).",
  "```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b-it\",\nquantization_config=HiggsConfig(bits=4),\ndevice_map=\"auto\",\n)\n\nmodel = torch.compile(model)\n```\n\nRefer to the table below for a benchmark of forward passes/sec for Llama-3.1-8B-Instruct on a RTX4090.\n\n| Batch Size | BF16 (with `torch.compile`) | HIGGS 4bit (without `torch.compile`) | HIGGS 4bit (with `torch.compile`) |\n|------------|-----------------------------|----------------------------------|-----------------------------------|\n| 1          | 59                          | 41                               | 124                               |\n| 4          | 57                          | 42                               | 123                               |\n| 16         | 56                          | 41                               | 120                               |",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# AQLM\n\nAdditive Quantization of Language Models ([AQLM](https://arxiv.org/abs/2401.06118)) quantizes multiple weights together and takes advantage of interdependencies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes.",
  "AQLM also supports fine-tuning with [LoRA](https://huggingface.co/docs/peft/package_reference/lora) with the [PEFT](https://huggingface.co/docs/peft) library, and is fully compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for even faster inference and training.\n\nRun the command below to install the AQLM library with kernel support for both GPU and CPU inference and training. AQLM only works with Python 3.10+.\n\n```bash\npip install aqlm[gpu,cpu]\n```\n\nLoad an AQLM-quantized model with [`~PreTrainedModel.from_pretrained`].\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n```\n\n## Configurations\n\nAQLM quantization setups vary mainly in the number of codebooks used, as well as codebook sizes in bits. The most popular setups and supported inference kernels are shown below.\n\n| Kernel | Number of codebooks | Codebook size, bits | Notation | Accuracy | Speedup     | Fast GPU inference | Fast CPU inference |",
  "|---|---------------------|---------------------|----------|-------------|-------------|--------------------|--------------------|\n| Triton | K                   | N                  | KxN     | -        | Up to ~0.7x | ✅                  | ❌                  |\n| CUDA | 1                   | 16                  | 1x16     | Best        | Up to ~1.3x | ✅                  | ❌                  |\n| CUDA | 2                   | 8                   | 2x8      | OK          | Up to ~3.0x | ✅                  | ❌                  |\n| Numba | K                   | 8                   | Kx8      | Good        | Up to ~4.0x | ❌                  | ✅                  |\n\n## Resources\n\nRun the AQLM demo [notebook](https://colab.research.google.com/drive/1-xZmBRXT5Fm3Ghn4Mwa2KRypORXb855X?usp=sharing) for more examples of how to quantize a model, push a quantized model to the Hub, and more.\n\nFor more example demo notebooks, visit the AQLM [repository](https://github.com/Vahe1994/AQLM).",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Agents & Tools\n\n<Tip warning={true}>\n\nTransformers Agents is an experimental API which is subject to change at any time. Results returned by the agents\ncan vary as the APIs or underlying models are prone to change.\n\n</Tip>\n\nTo learn more about agents and tools make sure to read the [introductory guide](../transformers_agents). This page\ncontains the API docs for the underlying classes.\n\n## Agents",
  "We provide two types of agents, based on the main [`Agent`] class:\n- [`CodeAgent`] acts in one shot, generating code to solve the task, then executes it at once.\n- [`ReactAgent`] acts step by step, each step consisting of one thought, then one tool call and execution. It has two classes:\n- [`ReactJsonAgent`] writes its tool calls in JSON.\n- [`ReactCodeAgent`] writes its tool calls in Python code.\n\n### Agent\n\n[[autodoc]] Agent\n\n### CodeAgent\n\n[[autodoc]] CodeAgent\n\n### React agents\n\n[[autodoc]] ReactAgent\n\n[[autodoc]] ReactJsonAgent\n\n[[autodoc]] ReactCodeAgent\n\n### ManagedAgent\n\n[[autodoc]] ManagedAgent\n\n## Tools\n\n### load_tool\n\n[[autodoc]] load_tool\n\n### tool\n\n[[autodoc]] tool\n\n### Tool\n\n[[autodoc]] Tool\n\n### Toolbox\n\n[[autodoc]] Toolbox\n\n### PipelineTool\n\n[[autodoc]] PipelineTool\n\n### launch_gradio_demo\n\n[[autodoc]] launch_gradio_demo\n\n### stream_to_gradio\n\n[[autodoc]] stream_to_gradio\n\n### ToolCollection\n\n[[autodoc]] ToolCollection\n\n## Engines\n\nYou're free to create and use your own engines to be usable by the Agents framework.\nThese engines have the following specification:",
  "1. Follow the [messages format](../chat_templating.md) for its input (`List[Dict[str, str]]`) and return a string.\n2. Stop generating outputs *before* the sequences passed in the argument `stop_sequences`\n\n### TransformersEngine\n\nFor convenience, we have added a `TransformersEngine` that implements the points above, taking a pre-initialized `Pipeline` as input.\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine\n\n>>> model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n>>> model = AutoModelForCausalLM.from_pretrained(model_name)\n\n>>> pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n>>> engine = TransformersEngine(pipe)\n>>> engine([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"])\n\n\"What a \"\n```\n\n[[autodoc]] TransformersEngine\n\n### HfApiEngine\n\nThe `HfApiEngine` is an engine that wraps an [HF Inference API](https://huggingface.co/docs/api-inference/index) client for the execution of the LLM.\n\n```python\n>>> from transformers import HfApiEngine\n\n>>> messages = [\n...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},",
  "...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n...   {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n... ]\n\n>>> HfApiEngine()(messages, stop_sequences=[\"conversation\"])\n\n\"That's very kind of you to say! It's always nice to have a relaxed \"\n```\n\n[[autodoc]] HfApiEngine\n\n\n## Agent Types\n\nAgents can handle any type of object in-between tools; tools, being completely multimodal, can accept and return\ntext, image, audio, video, among other types. In order to increase compatibility between tools, as well as to\ncorrectly render these returns in ipython (jupyter, colab, ipython notebooks, ...), we implement wrapper classes\naround these types.\n\nThe wrapped objects should continue behaving as initially; a text object should still behave as a string, an image\nobject should still behave as a `PIL.Image`.\n\nThese types have three specific purposes:\n\n- Calling `to_raw` on the type should return the underlying object\n- Calling `to_string` on the type should return the object as a string: that can be the string in case of an `AgentText`\nbut will be the path of the serialized version of the object in other instances",
  "- Displaying it in an ipython kernel should display the object correctly\n\n### AgentText\n\n[[autodoc]] transformers.agents.agent_types.AgentText\n\n### AgentImage\n\n[[autodoc]] transformers.agents.agent_types.AgentImage\n\n### AgentAudio\n\n[[autodoc]] transformers.agents.agent_types.AgentAudio",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Feature Extractor\n\nA feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors.\n\n\n## FeatureExtractionMixin",
  "[[autodoc]] feature_extraction_utils.FeatureExtractionMixin\n- from_pretrained\n- save_pretrained\n\n## SequenceFeatureExtractor\n\n[[autodoc]] SequenceFeatureExtractor\n- pad\n\n## BatchFeature\n\n[[autodoc]] BatchFeature\n\n## ImageFeatureExtractionMixin\n\n[[autodoc]] image_utils.ImageFeatureExtractionMixin",
  "<!--Copyright (c) Meta Platforms, Inc. and affiliates.\nAll rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n\n# ExecuTorch\n\n[`ExecuTorch`](https://github.com/pytorch/executorch) is an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers. It is part of the PyTorch ecosystem and supports the deployment of PyTorch models with a focus on portability, productivity, and performance.",
  "ExecuTorch introduces well defined entry points to perform model, device, and/or use-case specific optimizations such as backend delegation, user-defined compiler transformations, memory planning, and more. The first step in preparing a PyTorch model for execution on an edge device using ExecuTorch is to export the model. This is achieved through the use of a PyTorch API called [`torch.export`](https://pytorch.org/docs/stable/export.html).\n\n\n## ExecuTorch Integration\n\nAn integration point is being developed to ensure that 🤗 Transformers can be exported using `torch.export`. The goal of this integration is not only to enable export but also to ensure that the exported artifact can be further lowered and optimized to run efficiently in `ExecuTorch`, particularly for mobile and edge use cases.\n\n[[autodoc]] TorchExportableModuleWithStaticCache\n- forward\n\n[[autodoc]] convert_and_export_with_cache",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Generation\n\nEach framework has a generate method for text generation implemented in their respective `GenerationMixin` class:\n\n- PyTorch [`~generation.GenerationMixin.generate`] is implemented in [`~generation.GenerationMixin`].\n- TensorFlow [`~generation.TFGenerationMixin.generate`] is implemented in [`~generation.TFGenerationMixin`].",
  "- Flax/JAX [`~generation.FlaxGenerationMixin.generate`] is implemented in [`~generation.FlaxGenerationMixin`].\n\nRegardless of your framework of choice, you can parameterize the generate method with a [`~generation.GenerationConfig`]\nclass instance. Please refer to this class for the complete list of generation parameters, which control the behavior\nof the generation method.\n\nTo learn how to inspect a model's generation configuration, what are the defaults, how to change the parameters ad hoc,\nand how to create and save a customized generation configuration, refer to the\n[text generation strategies guide](../generation_strategies). The guide also explains how to use related features,\nlike token streaming.\n\n## GenerationConfig\n\n[[autodoc]] generation.GenerationConfig\n- from_pretrained\n- from_model_config\n- save_pretrained\n- update\n- validate\n- get_generation_mode\n\n## GenerationMixin\n\n[[autodoc]] GenerationMixin\n- generate\n- compute_transition_scores\n\n## TFGenerationMixin\n\n[[autodoc]] TFGenerationMixin\n- generate\n- compute_transition_scores\n\n## FlaxGenerationMixin\n\n[[autodoc]] FlaxGenerationMixin\n- generate",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Tokenizer\n\nA tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most\nof the tokenizers are available in two flavors: a full python implementation and a \"Fast\" implementation based on the\nRust library [🤗 Tokenizers](https://github.com/huggingface/tokenizers). The \"Fast\" implementations allows:\n\n1. a significant speed-up in particular when doing batched tokenization and",
  "2. additional methods to map between the original string (character and words) and the token space (e.g. getting the\nindex of the token comprising a given character or the span of characters corresponding to a given token).\n\nThe base classes [`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`]\nimplement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and\n\"Fast\" tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library\n(downloaded from HuggingFace's AWS S3 repository). They both rely on\n[`~tokenization_utils_base.PreTrainedTokenizerBase`] that contains the common methods, and\n[`~tokenization_utils_base.SpecialTokensMixin`].\n\n[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] thus implement the main\nmethods for using all the tokenizers:\n\n- Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and\nencoding/decoding (i.e., tokenizing and converting to integers).\n- Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece...).",
  "- Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the\ntokenizer for easy access and making sure they are not split during tokenization.\n\n[`BatchEncoding`] holds the output of the\n[`~tokenization_utils_base.PreTrainedTokenizerBase`]'s encoding methods (`__call__`,\n`encode_plus` and `batch_encode_plus`) and is derived from a Python dictionary. When the tokenizer is a pure python\ntokenizer, this class behaves just like a standard python dictionary and holds the various model inputs computed by\nthese methods (`input_ids`, `attention_mask`...). When the tokenizer is a \"Fast\" tokenizer (i.e., backed by\nHuggingFace [tokenizers library](https://github.com/huggingface/tokenizers)), this class provides in addition\nseveral advanced alignment methods which can be used to map between the original string (character and words) and the\ntoken space (e.g., getting the index of the token comprising a given character or the span of characters corresponding\nto a given token).\n\n\n# Multimodal Tokenizer\n\nApart from that each tokenizer can be a \"multimodal\" tokenizer which means that the tokenizer will hold all relevant special tokens",
  "as part of tokenizer attributes for easier access. For example, if the tokenizer is loaded from a vision-language model like LLaVA, you will\nbe able to access `tokenizer.image_token_id` to obtain the special image token used as a placeholder.\n\nTo enable extra special tokens for any type of tokenizer, you have to add the following lines and save the tokenizer. Extra special tokens do not\nhave to be modality related and can ne anything that the model often needs access to. In the below code, tokenizer at `output_dir` will have direct access\nto three more special tokens.\n\n```python\nvision_tokenizer = AutoTokenizer.from_pretrained(\n\"llava-hf/llava-1.5-7b-hf\",\nextra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n)\nprint(vision_tokenizer.image_token, vision_tokenizer.image_token_id)\n(\"<image>\", 32000)\n```\n\n## PreTrainedTokenizer\n\n[[autodoc]] PreTrainedTokenizer\n- __call__\n- add_tokens\n- add_special_tokens\n- apply_chat_template\n- batch_decode\n- decode\n- encode\n- push_to_hub\n- all\n\n## PreTrainedTokenizerFast",
  "The [`PreTrainedTokenizerFast`] depend on the [tokenizers](https://huggingface.co/docs/tokenizers) library. The tokenizers obtained from the 🤗 tokenizers library can be\nloaded very simply into 🤗 transformers. Take a look at the [Using tokenizers from 🤗 tokenizers](../fast_tokenizers) page to understand how this is done.\n\n[[autodoc]] PreTrainedTokenizerFast\n- __call__\n- add_tokens\n- add_special_tokens\n- apply_chat_template\n- batch_decode\n- decode\n- encode\n- push_to_hub\n- all\n\n## BatchEncoding\n\n[[autodoc]] BatchEncoding",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Optimization\n\nThe `.optimization` module provides:\n\n- an optimizer with weight decay fixed that can be used to fine-tuned models, and\n- several schedules in the form of schedule objects that inherit from `_LRSchedule`:\n- a gradient accumulation class to accumulate the gradients of multiple batches\n\n\n## AdaFactor (PyTorch)\n\n[[autodoc]] Adafactor\n\n## AdamWeightDecay (TensorFlow)\n\n[[autodoc]] AdamWeightDecay\n\n[[autodoc]] create_optimizer",
  "## Schedules\n\n### Learning Rate Schedules (PyTorch)\n\n[[autodoc]] SchedulerType\n\n[[autodoc]] get_scheduler\n\n[[autodoc]] get_constant_schedule\n\n[[autodoc]] get_constant_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png\"/>\n\n[[autodoc]] get_cosine_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png\"/>\n\n[[autodoc]] get_cosine_with_hard_restarts_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png\"/>\n\n[[autodoc]] get_linear_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png\"/>\n\n[[autodoc]] get_polynomial_decay_schedule_with_warmup\n\n[[autodoc]] get_inverse_sqrt_schedule\n\n[[autodoc]] get_wsd_schedule\n\n### Warmup (TensorFlow)\n\n[[autodoc]] WarmUp\n\n## Gradient Strategies\n\n### GradientAccumulator (TensorFlow)\n\n[[autodoc]] GradientAccumulator",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Models\n\nThe base classes [`PreTrainedModel`], [`TFPreTrainedModel`], and\n[`FlaxPreTrainedModel`] implement the common methods for loading/saving a model either from a local\nfile or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's AWS\nS3 repository).\n\n[`PreTrainedModel`] and [`TFPreTrainedModel`] also implement a few methods which\nare common among all the models to:",
  "- resize the input token embeddings when new tokens are added to the vocabulary\n- prune the attention heads of the model.\n\nThe other methods that are common to each model are defined in [`~modeling_utils.ModuleUtilsMixin`]\n(for the PyTorch models) and [`~modeling_tf_utils.TFModuleUtilsMixin`] (for the TensorFlow models) or\nfor text generation, [`~generation.GenerationMixin`] (for the PyTorch models),\n[`~generation.TFGenerationMixin`] (for the TensorFlow models) and\n[`~generation.FlaxGenerationMixin`] (for the Flax/JAX models).\n\n\n## PreTrainedModel\n\n[[autodoc]] PreTrainedModel\n- push_to_hub\n- all\n\nCustom models should also include a `_supports_assign_param_buffer`, which determines if superfast init can apply\non the particular model. Signs that your model needs this are if `test_save_and_load_from_pretrained` fails. If so,\nset this to `False`.\n\n## ModuleUtilsMixin\n\n[[autodoc]] modeling_utils.ModuleUtilsMixin\n\n## TFPreTrainedModel\n\n[[autodoc]] TFPreTrainedModel\n- push_to_hub\n- all\n\n## TFModelUtilsMixin\n\n[[autodoc]] modeling_tf_utils.TFModelUtilsMixin\n\n## FlaxPreTrainedModel\n\n[[autodoc]] FlaxPreTrainedModel\n- push_to_hub\n- all\n\n## Pushing to the Hub\n\n[[autodoc]] utils.PushToHubMixin",
  "## Sharded checkpoints\n\n[[autodoc]] modeling_utils.load_sharded_checkpoint",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Pipelines\n\nThe pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of\nthe complex code from the library, offering a simple API dedicated to several tasks, including Named Entity\nRecognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the\n[task summary](../task_summary) for examples of use.",
  "There are two categories of pipeline abstractions to be aware about:\n\n- The [`pipeline`] which is the most powerful object encapsulating all other pipelines.\n- Task-specific pipelines are available for [audio](#audio), [computer vision](#computer-vision), [natural language processing](#natural-language-processing), and [multimodal](#multimodal) tasks.\n\n## The pipeline abstraction\n\nThe *pipeline* abstraction is a wrapper around all the other available pipelines. It is instantiated as any other\npipeline but can provide additional quality of life.\n\nSimple call on one item:\n\n```python\n>>> pipe = pipeline(\"text-classification\")\n>>> pipe(\"This restaurant is awesome\")\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n```\n\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\nthe hub already defines it:\n\n```python\n>>> pipe = pipeline(model=\"FacebookAI/roberta-large-mnli\")\n>>> pipe(\"This restaurant is awesome\")\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\n```\n\nTo call a pipeline on many items, you can call it with a *list*.\n\n```python\n>>> pipe = pipeline(\"text-classification\")",
  ">>> pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\n{'label': 'NEGATIVE', 'score': 0.9996669292449951}]\n```\n\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\nGPU. If it doesn't don't hesitate to create an issue.\n\n```python\nimport datasets\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\nprint(out)\n# {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n# {\"text\": ....}\n# ....\n```\n\nFor ease of use, a generator is also possible:\n\n\n```python",
  "from transformers import pipeline\n\npipe = pipeline(\"text-classification\")\n\n\ndef data():\nwhile True:\n# This could come from a dataset, a database, a queue or HTTP request\n# in a server\n# Caveat: because this is iterative, you cannot use `num_workers > 1` variable\n# to use multiple threads to preprocess data. You can still have 1 thread that\n# does the preprocessing while the main runs the big inference\nyield \"This is a test\"\n\n\nfor out in pipe(data()):\nprint(out)\n# {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n# {\"text\": ....}\n# ....\n```\n\n[[autodoc]] pipeline\n\n## Pipeline batching\n\nAll pipelines can use batching. This will work\nwhenever the pipeline uses its streaming ability (so when passing lists or `Dataset` or `generator`).\n\n```python\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nimport datasets\n\ndataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\npipe = pipeline(\"text-classification\", device=0)\nfor out in pipe(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\nprint(out)\n# [{'label': 'POSITIVE', 'score': 0.9998743534088135}]",
  "# Exactly the same output as before, but the content are passed\n# as batches to the model\n```\n\n<Tip warning={true}>\n\nHowever, this is not automatically a win for performance. It can be either a 10x speedup or 5x slowdown depending\non hardware, data and the actual model being used.\n\nExample where it's mostly a speedup:\n\n</Tip>\n\n```python\nfrom transformers import pipeline\nfrom torch.utils.data import Dataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"text-classification\", device=0)\n\n\nclass MyDataset(Dataset):\ndef __len__(self):\nreturn 5000\n\ndef __getitem__(self, i):\nreturn \"This is a test\"\n\n\ndataset = MyDataset()\n\nfor batch_size in [1, 8, 64, 256]:\nprint(\"-\" * 30)\nprint(f\"Streaming batch_size={batch_size}\")\nfor out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\npass\n```\n\n```\n# On GTX 970\n------------------------------\nStreaming no batching\n100%|██████████████████████████████████████████████████████████████████████| 5000/5000 [00:26<00:00, 187.52it/s]\n------------------------------\nStreaming batch_size=8\n100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:04<00:00, 1205.95it/s]\n------------------------------",
  "Streaming batch_size=64\n100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2478.24it/s]\n------------------------------\nStreaming batch_size=256\n100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 2554.43it/s]\n(diminishing returns, saturated the GPU)\n```\n\nExample where it's most a slowdown:\n\n```python\nclass MyDataset(Dataset):\ndef __len__(self):\nreturn 5000\n\ndef __getitem__(self, i):\nif i % 64 == 0:\nn = 100\nelse:\nn = 1\nreturn \"This is a test\" * n\n```\n\nThis is a occasional very long sentence compared to the other. In that case, the **whole** batch will need to be 400\ntokens long, so the whole batch will be [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on\nbigger batches, the program simply crashes.\n\n\n```\n------------------------------\nStreaming no batching\n100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 183.69it/s]\n------------------------------\nStreaming batch_size=8\n100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 265.74it/s]\n------------------------------",
  "Streaming batch_size=64\n100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [00:26<00:00, 37.80it/s]\n------------------------------\nStreaming batch_size=256\n0%|                                                                                 | 0/1000 [00:00<?, ?it/s]\nTraceback (most recent call last):\nFile \"/home/nicolas/src/transformers/test.py\", line 42, in <module>\nfor out in tqdm(pipe(dataset, batch_size=256), total=len(dataset)):\n....\nq = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\nRuntimeError: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 3.95 GiB total capacity; 1.72 GiB already allocated; 354.88 MiB free; 2.46 GiB reserved in total by PyTorch)\n```\n\nThere are no good (general) solutions for this problem, and your mileage may vary depending on your use cases. Rule of\nthumb:\n\nFor users, a rule of thumb is:\n\n- **Measure performance on your load, with your hardware. Measure, measure, and keep measuring. Real numbers are the\nonly way to go.**\n- If you are latency constrained (live product doing inference), don't batch.\n- If you are using CPU, don't batch.",
  "- If you are using throughput (you want to run your model on a bunch of static data), on GPU, then:\n\n- If you have no clue about the size of the sequence_length (\"natural\" data), by default don't batch, measure and\ntry tentatively to add it, add OOM checks to recover when it will fail (and it will at some point if you don't\ncontrol the sequence_length.)\n- If your sequence_length is super regular, then batching is more likely to be VERY interesting, measure and push\nit until you get OOMs.\n- The larger the GPU the more likely batching is going to be more interesting\n- As soon as you enable batching, make sure you can handle OOMs nicely.\n\n## Pipeline chunk batching\n\n`zero-shot-classification` and `question-answering` are slightly specific in the sense, that a single input might yield\nmultiple forward pass of a model. Under normal circumstances, this would yield issues with `batch_size` argument.\n\nIn order to circumvent this issue, both of these pipelines are a bit specific, they are `ChunkPipeline` instead of\nregular `Pipeline`. In short:\n\n\n```python\npreprocessed = pipe.preprocess(inputs)\nmodel_outputs = pipe.forward(preprocessed)\noutputs = pipe.postprocess(model_outputs)\n```",
  "Now becomes:\n\n\n```python\nall_model_outputs = []\nfor preprocessed in pipe.preprocess(inputs):\nmodel_outputs = pipe.forward(preprocessed)\nall_model_outputs.append(model_outputs)\noutputs = pipe.postprocess(all_model_outputs)\n```\n\nThis should be very transparent to your code because the pipelines are used in\nthe same way.\n\nThis is a simplified view, since the pipeline can handle automatically the batch to ! Meaning you don't have to care\nabout how many forward passes you inputs are actually going to trigger, you can optimize the `batch_size`\nindependently of the inputs. The caveats from the previous section still apply.\n\n## Pipeline FP16 inference\nModels can be run in FP16 which can be significantly faster on GPU while saving memory. Most models will not suffer noticeable performance loss from this. The larger the model, the less likely that it will.\n\nTo enable FP16 inference, you can simply pass `torch_dtype=torch.float16` or `torch_dtype='float16'` to the pipeline constructor. Note that this only works for models with a PyTorch backend. Your inputs will be converted to FP16 internally.\n\n## Pipeline custom code\n\nIf you want to override a specific pipeline.",
  "Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\ncases, so `transformers` could maybe support your use case.\n\n\nIf you want to try simply you can:\n\n- Subclass your pipeline of choice\n\n```python\nclass MyPipeline(TextClassificationPipeline):\ndef postprocess():\n# Your code goes here\nscores = scores * 100\n# And here\n\n\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\n# or if you use *pipeline* function, then:\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\n```\n\nThat should enable you to do all the custom code you want.\n\n\n## Implementing a pipeline\n\n[Implementing a new pipeline](../add_new_pipeline)\n\n## Audio\n\nPipelines available for audio tasks include the following.\n\n### AudioClassificationPipeline\n\n[[autodoc]] AudioClassificationPipeline\n- __call__\n- all\n\n### AutomaticSpeechRecognitionPipeline\n\n[[autodoc]] AutomaticSpeechRecognitionPipeline\n- __call__\n- all\n\n### TextToAudioPipeline\n\n[[autodoc]] TextToAudioPipeline\n- __call__\n- all\n\n\n### ZeroShotAudioClassificationPipeline\n\n[[autodoc]] ZeroShotAudioClassificationPipeline\n- __call__\n- all\n\n## Computer vision",
  "Pipelines available for computer vision tasks include the following.\n\n### DepthEstimationPipeline\n[[autodoc]] DepthEstimationPipeline\n- __call__\n- all\n\n### ImageClassificationPipeline\n\n[[autodoc]] ImageClassificationPipeline\n- __call__\n- all\n\n### ImageSegmentationPipeline\n\n[[autodoc]] ImageSegmentationPipeline\n- __call__\n- all\n\n### ImageToImagePipeline\n\n[[autodoc]] ImageToImagePipeline\n- __call__\n- all\n\n### ObjectDetectionPipeline\n\n[[autodoc]] ObjectDetectionPipeline\n- __call__\n- all\n\n### VideoClassificationPipeline\n\n[[autodoc]] VideoClassificationPipeline\n- __call__\n- all\n\n### ZeroShotImageClassificationPipeline\n\n[[autodoc]] ZeroShotImageClassificationPipeline\n- __call__\n- all\n\n### ZeroShotObjectDetectionPipeline\n\n[[autodoc]] ZeroShotObjectDetectionPipeline\n- __call__\n- all\n\n## Natural Language Processing\n\nPipelines available for natural language processing tasks include the following.\n\n### FillMaskPipeline\n\n[[autodoc]] FillMaskPipeline\n- __call__\n- all\n\n### QuestionAnsweringPipeline\n\n[[autodoc]] QuestionAnsweringPipeline\n- __call__\n- all\n\n### SummarizationPipeline\n\n[[autodoc]] SummarizationPipeline\n- __call__\n- all\n\n### TableQuestionAnsweringPipeline",
  "[[autodoc]] TableQuestionAnsweringPipeline\n- __call__\n\n### TextClassificationPipeline\n\n[[autodoc]] TextClassificationPipeline\n- __call__\n- all\n\n### TextGenerationPipeline\n\n[[autodoc]] TextGenerationPipeline\n- __call__\n- all\n\n### Text2TextGenerationPipeline\n\n[[autodoc]] Text2TextGenerationPipeline\n- __call__\n- all\n\n### TokenClassificationPipeline\n\n[[autodoc]] TokenClassificationPipeline\n- __call__\n- all\n\n### TranslationPipeline\n\n[[autodoc]] TranslationPipeline\n- __call__\n- all\n\n### ZeroShotClassificationPipeline\n\n[[autodoc]] ZeroShotClassificationPipeline\n- __call__\n- all\n\n## Multimodal\n\nPipelines available for multimodal tasks include the following.\n\n### DocumentQuestionAnsweringPipeline\n\n[[autodoc]] DocumentQuestionAnsweringPipeline\n- __call__\n- all\n\n### FeatureExtractionPipeline\n\n[[autodoc]] FeatureExtractionPipeline\n- __call__\n- all\n\n### ImageFeatureExtractionPipeline\n\n[[autodoc]] ImageFeatureExtractionPipeline\n- __call__\n- all\n\n### ImageToTextPipeline\n\n[[autodoc]] ImageToTextPipeline\n- __call__\n- all\n\n### ImageTextToTextPipeline\n\n[[autodoc]] ImageTextToTextPipeline\n- __call__\n- all\n\n### MaskGenerationPipeline\n\n[[autodoc]] MaskGenerationPipeline\n- __call__\n- all",
  "### VisualQuestionAnsweringPipeline\n\n[[autodoc]] VisualQuestionAnsweringPipeline\n- __call__\n- all\n\n## Parent class: `Pipeline`\n\n[[autodoc]] Pipeline",
  "<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Keras callbacks\n\nWhen training a Transformers model with Keras, there are some library-specific callbacks available to automate common\ntasks:\n\n## KerasMetricCallback\n\n[[autodoc]] KerasMetricCallback\n\n## PushToHubCallback\n\n[[autodoc]] PushToHubCallback",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Model outputs\n\nAll models have outputs that are instances of subclasses of [`~utils.ModelOutput`]. Those are\ndata structures containing all the information returned by the model, but that can also be used as tuples or\ndictionaries.\n\nLet's see how this looks in an example:\n\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")",
  "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nlabels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\noutputs = model(**inputs, labels=labels)\n```\n\nThe `outputs` object is a [`~modeling_outputs.SequenceClassifierOutput`], as we can see in the\ndocumentation of that class below, it means it has an optional `loss`, a `logits`, an optional `hidden_states` and\nan optional `attentions` attribute. Here we have the `loss` since we passed along `labels`, but we don't have\n`hidden_states` and `attentions` because we didn't pass `output_hidden_states=True` or\n`output_attentions=True`.\n\n<Tip>\n\nWhen passing `output_hidden_states=True` you may expect the `outputs.hidden_states[-1]` to match `outputs.last_hidden_state` exactly.\nHowever, this is not always the case. Some models apply normalization or subsequent process to the last hidden state when it's returned.\n\n</Tip>\n\n\nYou can access each attribute as you would usually do, and if that attribute has not been returned by the model, you",
  "will get `None`. Here for instance `outputs.loss` is the loss computed by the model, and `outputs.attentions` is\n`None`.\n\nWhen considering our `outputs` object as tuple, it only considers the attributes that don't have `None` values.\nHere for instance, it has two elements, `loss` then `logits`, so\n\n```python\noutputs[:2]\n```\n\nwill return the tuple `(outputs.loss, outputs.logits)` for instance.\n\nWhen considering our `outputs` object as dictionary, it only considers the attributes that don't have `None`\nvalues. Here for instance, it has two keys that are `loss` and `logits`.\n\nWe document here the generic model outputs that are used by more than one model type. Specific output types are\ndocumented on their corresponding model page.\n\n## ModelOutput\n\n[[autodoc]] utils.ModelOutput\n- to_tuple\n\n## BaseModelOutput\n\n[[autodoc]] modeling_outputs.BaseModelOutput\n\n## BaseModelOutputWithPooling\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPooling\n\n## BaseModelOutputWithCrossAttentions\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithCrossAttentions\n\n## BaseModelOutputWithPoolingAndCrossAttentions\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",
  "## BaseModelOutputWithPast\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPast\n\n## BaseModelOutputWithPastAndCrossAttentions\n\n[[autodoc]] modeling_outputs.BaseModelOutputWithPastAndCrossAttentions\n\n## Seq2SeqModelOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqModelOutput\n\n## CausalLMOutput\n\n[[autodoc]] modeling_outputs.CausalLMOutput\n\n## CausalLMOutputWithCrossAttentions\n\n[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions\n\n## CausalLMOutputWithPast\n\n[[autodoc]] modeling_outputs.CausalLMOutputWithPast\n\n## MaskedLMOutput\n\n[[autodoc]] modeling_outputs.MaskedLMOutput\n\n## Seq2SeqLMOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqLMOutput\n\n## NextSentencePredictorOutput\n\n[[autodoc]] modeling_outputs.NextSentencePredictorOutput\n\n## SequenceClassifierOutput\n\n[[autodoc]] modeling_outputs.SequenceClassifierOutput\n\n## Seq2SeqSequenceClassifierOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqSequenceClassifierOutput\n\n## MultipleChoiceModelOutput\n\n[[autodoc]] modeling_outputs.MultipleChoiceModelOutput\n\n## TokenClassifierOutput\n\n[[autodoc]] modeling_outputs.TokenClassifierOutput\n\n## QuestionAnsweringModelOutput\n\n[[autodoc]] modeling_outputs.QuestionAnsweringModelOutput",
  "## Seq2SeqQuestionAnsweringModelOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqQuestionAnsweringModelOutput\n\n## Seq2SeqSpectrogramOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqSpectrogramOutput\n\n## SemanticSegmenterOutput\n\n[[autodoc]] modeling_outputs.SemanticSegmenterOutput\n\n## ImageClassifierOutput\n\n[[autodoc]] modeling_outputs.ImageClassifierOutput\n\n## ImageClassifierOutputWithNoAttention\n\n[[autodoc]] modeling_outputs.ImageClassifierOutputWithNoAttention\n\n## DepthEstimatorOutput\n\n[[autodoc]] modeling_outputs.DepthEstimatorOutput\n\n## Wav2Vec2BaseModelOutput\n\n[[autodoc]] modeling_outputs.Wav2Vec2BaseModelOutput\n\n## XVectorOutput\n\n[[autodoc]] modeling_outputs.XVectorOutput\n\n## Seq2SeqTSModelOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqTSModelOutput\n\n## Seq2SeqTSPredictionOutput\n\n[[autodoc]] modeling_outputs.Seq2SeqTSPredictionOutput\n\n## SampleTSPredictionOutput\n\n[[autodoc]] modeling_outputs.SampleTSPredictionOutput\n\n## TFBaseModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutput\n\n## TFBaseModelOutputWithPooling\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPooling\n\n## TFBaseModelOutputWithPoolingAndCrossAttentions",
  "[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions\n\n## TFBaseModelOutputWithPast\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast\n\n## TFBaseModelOutputWithPastAndCrossAttentions\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions\n\n## TFSeq2SeqModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqModelOutput\n\n## TFCausalLMOutput\n\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutput\n\n## TFCausalLMOutputWithCrossAttentions\n\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions\n\n## TFCausalLMOutputWithPast\n\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithPast\n\n## TFMaskedLMOutput\n\n[[autodoc]] modeling_tf_outputs.TFMaskedLMOutput\n\n## TFSeq2SeqLMOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqLMOutput\n\n## TFNextSentencePredictorOutput\n\n[[autodoc]] modeling_tf_outputs.TFNextSentencePredictorOutput\n\n## TFSequenceClassifierOutput\n\n[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutput\n\n## TFSeq2SeqSequenceClassifierOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput\n\n## TFMultipleChoiceModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFMultipleChoiceModelOutput\n\n## TFTokenClassifierOutput",
  "[[autodoc]] modeling_tf_outputs.TFTokenClassifierOutput\n\n## TFQuestionAnsweringModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFQuestionAnsweringModelOutput\n\n## TFSeq2SeqQuestionAnsweringModelOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput\n\n## FlaxBaseModelOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutput\n\n## FlaxBaseModelOutputWithPast\n\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPast\n\n## FlaxBaseModelOutputWithPooling\n\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPooling\n\n## FlaxBaseModelOutputWithPastAndCrossAttentions\n\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions\n\n## FlaxSeq2SeqModelOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqModelOutput\n\n## FlaxCausalLMOutputWithCrossAttentions\n\n[[autodoc]] modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions\n\n## FlaxMaskedLMOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxMaskedLMOutput\n\n## FlaxSeq2SeqLMOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqLMOutput\n\n## FlaxNextSentencePredictorOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxNextSentencePredictorOutput\n\n## FlaxSequenceClassifierOutput",
  "[[autodoc]] modeling_flax_outputs.FlaxSequenceClassifierOutput\n\n## FlaxSeq2SeqSequenceClassifierOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput\n\n## FlaxMultipleChoiceModelOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxMultipleChoiceModelOutput\n\n## FlaxTokenClassifierOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxTokenClassifierOutput\n\n## FlaxQuestionAnsweringModelOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxQuestionAnsweringModelOutput\n\n## FlaxSeq2SeqQuestionAnsweringModelOutput\n\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Processors\n\nProcessors can mean two different things in the Transformers library:\n- the objects that pre-process inputs for multi-modal models such as [Wav2Vec2](../model_doc/wav2vec2) (speech and text)\nor [CLIP](../model_doc/clip) (text and vision)\n- deprecated objects that were used in older versions of the library to preprocess data for GLUE or SQUAD.\n\n## Multi-modal processors",
  "Any multi-modal model will require an object to encode or decode the data that groups several modalities (among text,\nvision and audio). This is handled by objects called processors, which group together two or more processing objects\nsuch as tokenizers (for the text modality), image processors (for vision) and feature extractors (for audio).\n\nThose processors inherit from the following base class that implements the saving and loading functionality:\n\n[[autodoc]] ProcessorMixin\n\n## Deprecated processors\n\nAll processors follow the same architecture which is that of the\n[`~data.processors.utils.DataProcessor`]. The processor returns a list of\n[`~data.processors.utils.InputExample`]. These\n[`~data.processors.utils.InputExample`] can be converted to\n[`~data.processors.utils.InputFeatures`] in order to be fed to the model.\n\n[[autodoc]] data.processors.utils.DataProcessor\n\n[[autodoc]] data.processors.utils.InputExample\n\n[[autodoc]] data.processors.utils.InputFeatures\n\n## GLUE\n\n[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) is a benchmark that evaluates the",
  "performance of models across a diverse set of existing NLU tasks. It was released together with the paper [GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding](https://openreview.net/pdf?id=rJ4km2R5t7)\n\nThis library hosts a total of 10 processors for the following tasks: MRPC, MNLI, MNLI (mismatched), CoLA, SST2, STSB,\nQQP, QNLI, RTE and WNLI.\n\nThose processors are:\n\n- [`~data.processors.utils.MrpcProcessor`]\n- [`~data.processors.utils.MnliProcessor`]\n- [`~data.processors.utils.MnliMismatchedProcessor`]\n- [`~data.processors.utils.Sst2Processor`]\n- [`~data.processors.utils.StsbProcessor`]\n- [`~data.processors.utils.QqpProcessor`]\n- [`~data.processors.utils.QnliProcessor`]\n- [`~data.processors.utils.RteProcessor`]\n- [`~data.processors.utils.WnliProcessor`]\n\nAdditionally, the following method can be used to load values from a data file and convert them to a list of\n[`~data.processors.utils.InputExample`].\n\n[[autodoc]] data.processors.glue.glue_convert_examples_to_features\n\n\n## XNLI\n\n[The Cross-Lingual NLI Corpus (XNLI)](https://www.nyu.edu/projects/bowman/xnli/) is a benchmark that evaluates the",
  "quality of cross-lingual text representations. XNLI is crowd-sourced dataset based on [*MultiNLI*](http://www.nyu.edu/projects/bowman/multinli/): pairs of text are labeled with textual entailment annotations for 15\ndifferent languages (including both high-resource language such as English and low-resource languages such as Swahili).\n\nIt was released together with the paper [XNLI: Evaluating Cross-lingual Sentence Representations](https://arxiv.org/abs/1809.05053)\n\nThis library hosts the processor to load the XNLI data:\n\n- [`~data.processors.utils.XnliProcessor`]\n\nPlease note that since the gold labels are available on the test set, evaluation is performed on the test set.\n\nAn example using these processors is given in the [run_xnli.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_xnli.py) script.\n\n\n## SQuAD\n\n[The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer//) is a benchmark that\nevaluates the performance of models on question answering. Two versions are available, v1.1 and v2.0. The first version",
  "(v1.1) was released together with the paper [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250). The second version (v2.0) was released alongside the paper [Know What You Don't\nKnow: Unanswerable Questions for SQuAD](https://arxiv.org/abs/1806.03822).\n\nThis library hosts a processor for each of the two versions:\n\n### Processors\n\nThose processors are:\n\n- [`~data.processors.utils.SquadV1Processor`]\n- [`~data.processors.utils.SquadV2Processor`]\n\nThey both inherit from the abstract class [`~data.processors.utils.SquadProcessor`]\n\n[[autodoc]] data.processors.squad.SquadProcessor\n- all\n\nAdditionally, the following method can be used to convert SQuAD examples into\n[`~data.processors.utils.SquadFeatures`] that can be used as model inputs.\n\n[[autodoc]] data.processors.squad.squad_convert_examples_to_features\n\n\nThese processors as well as the aforementioned method can be used with files containing the data as well as with the\n*tensorflow_datasets* package. Examples are given below.\n\n\n### Example usage\n\nHere is an example using the processors as well as the conversion method using data files:\n\n```python\n# Loading a V2 processor",
  "processor = SquadV2Processor()\nexamples = processor.get_dev_examples(squad_v2_data_dir)\n\n# Loading a V1 processor\nprocessor = SquadV1Processor()\nexamples = processor.get_dev_examples(squad_v1_data_dir)\n\nfeatures = squad_convert_examples_to_features(\nexamples=examples,\ntokenizer=tokenizer,\nmax_seq_length=max_seq_length,\ndoc_stride=args.doc_stride,\nmax_query_length=max_query_length,\nis_training=not evaluate,\n)\n```\n\nUsing *tensorflow_datasets* is as easy as using a data file:\n\n```python\n# tensorflow_datasets only handle Squad V1.\ntfds_examples = tfds.load(\"squad\")\nexamples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n\nfeatures = squad_convert_examples_to_features(\nexamples=examples,\ntokenizer=tokenizer,\nmax_seq_length=max_seq_length,\ndoc_stride=args.doc_stride,\nmax_query_length=max_query_length,\nis_training=not evaluate,\n)\n```\n\nAnother example using these processors is given in the [run_squad.py](https://github.com/huggingface/transformers/tree/main/examples/legacy/question-answering/run_squad.py) script.",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Trainer",
  "The [`Trainer`] class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/), [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html) for PyTorch. [`Trainer`] goes hand-in-hand with the [`TrainingArguments`] class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API.\n\n[`Seq2SeqTrainer`] and [`Seq2SeqTrainingArguments`] inherit from the [`Trainer`] and [`TrainingArguments`] classes and they're adapted for training models for sequence-to-sequence tasks such as summarization or translation.\n\n<Tip warning={true}>\n\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surprising behaviors\nwhen used with other models. When using it with your own model, make sure:\n\n- your model always return tuples or subclasses of [`~utils.ModelOutput`]\n- your model can compute the loss if a `labels` argument is provided and that loss is returned as the first\nelement of the tuple (if your model returns tuples)",
  "- your model can accept multiple label arguments (use `label_names` in [`TrainingArguments`] to indicate their name to the [`Trainer`]) but none of them should be named `\"label\"`\n\n</Tip>\n\n## Trainer[[api-reference]]\n\n[[autodoc]] Trainer\n- all\n\n## Seq2SeqTrainer\n\n[[autodoc]] Seq2SeqTrainer\n- evaluate\n- predict\n\n## TrainingArguments\n\n[[autodoc]] TrainingArguments\n- all\n\n## Seq2SeqTrainingArguments\n\n[[autodoc]] Seq2SeqTrainingArguments\n- all",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Data Collator\n\nData collators are objects that will form a batch by using a list of dataset elements as input. These elements are of\nthe same type as the elements of `train_dataset` or `eval_dataset`.\n\nTo be able to build batches, data collators may apply some processing (like padding). Some of them (like\n[`DataCollatorForLanguageModeling`]) also apply some random data augmentation (like random masking)\non the formed batch.",
  "Examples of use can be found in the [example scripts](../examples) or [example notebooks](../notebooks).\n\n\n## Default data collator\n\n[[autodoc]] data.data_collator.default_data_collator\n\n## DefaultDataCollator\n\n[[autodoc]] data.data_collator.DefaultDataCollator\n\n## DataCollatorWithPadding\n\n[[autodoc]] data.data_collator.DataCollatorWithPadding\n\n## DataCollatorForTokenClassification\n\n[[autodoc]] data.data_collator.DataCollatorForTokenClassification\n\n## DataCollatorForSeq2Seq\n\n[[autodoc]] data.data_collator.DataCollatorForSeq2Seq\n\n## DataCollatorForLanguageModeling\n\n[[autodoc]] data.data_collator.DataCollatorForLanguageModeling\n- numpy_mask_tokens\n- tf_mask_tokens\n- torch_mask_tokens\n\n## DataCollatorForWholeWordMask\n\n[[autodoc]] data.data_collator.DataCollatorForWholeWordMask\n- numpy_mask_tokens\n- tf_mask_tokens\n- torch_mask_tokens\n\n## DataCollatorForPermutationLanguageModeling\n\n[[autodoc]] data.data_collator.DataCollatorForPermutationLanguageModeling\n- numpy_mask_tokens\n- tf_mask_tokens\n- torch_mask_tokens\n\n## DataCollatorWithFlattening\n\n[[autodoc]] data.data_collator.DataCollatorWithFlattening\n\n# DataCollatorForMultipleChoice",
  "[[autodoc]] data.data_collator.DataCollatorForMultipleChoice",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# DeepSpeed",
  "[DeepSpeed](https://github.com/deepspeedai/DeepSpeed), powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the [`Trainer`] class and most of the setup is automatically taken care of for you.\n\nHowever, if you want to use DeepSpeed without the [`Trainer`], Transformers provides a [`HfDeepSpeedConfig`] class.\n\n<Tip>\n\nLearn more about using DeepSpeed with [`Trainer`] in the [DeepSpeed](../deepspeed) guide.\n\n</Tip>\n\n## HfDeepSpeedConfig\n\n[[autodoc]] integrations.HfDeepSpeedConfig\n- all",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Configuration\n\nThe base class [`PretrainedConfig`] implements the common methods for loading/saving a configuration\neither from a local file or directory, or from a pretrained model configuration provided by the library (downloaded\nfrom HuggingFace's AWS S3 repository).\n\nEach derived config class implements model specific attributes. Common attributes present in all config classes are:",
  "`hidden_size`, `num_attention_heads`, and `num_hidden_layers`. Text models further implement:\n`vocab_size`.\n\n\n## PretrainedConfig\n\n[[autodoc]] PretrainedConfig\n- push_to_hub\n- all",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Logging\n\n🤗 Transformers has a centralized logging system, so that you can setup the verbosity of the library easily.\n\nCurrently the default verbosity of the library is `WARNING`.\n\nTo change the level of verbosity, just use one of the direct setters. For instance, here is how to change the verbosity\nto the INFO level.\n\n```python\nimport transformers\n\ntransformers.logging.set_verbosity_info()\n```",
  "You can also use the environment variable `TRANSFORMERS_VERBOSITY` to override the default verbosity. You can set it\nto one of the following: `debug`, `info`, `warning`, `error`, `critical`, `fatal`. For example:\n\n```bash\nTRANSFORMERS_VERBOSITY=error ./myprogram.py\n```\n\nAdditionally, some `warnings` can be disabled by setting the environment variable\n`TRANSFORMERS_NO_ADVISORY_WARNINGS` to a true value, like *1*. This will disable any warning that is logged using\n[`logger.warning_advice`]. For example:\n\n```bash\nTRANSFORMERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py\n```\n\nHere is an example of how to use the same logger as the library in your own module or script:\n\n```python\nfrom transformers.utils import logging\n\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"transformers\")\nlogger.info(\"INFO\")\nlogger.warning(\"WARN\")\n```\n\n\nAll the methods of this logging module are documented below, the main ones are\n[`logging.get_verbosity`] to get the current level of verbosity in the logger and\n[`logging.set_verbosity`] to set the verbosity to the level of your choice. In order (from the least",
  "verbose to the most verbose), those levels (with their corresponding int values in parenthesis) are:\n\n- `transformers.logging.CRITICAL` or `transformers.logging.FATAL` (int value, 50): only report the most\ncritical errors.\n- `transformers.logging.ERROR` (int value, 40): only report errors.\n- `transformers.logging.WARNING` or `transformers.logging.WARN` (int value, 30): only reports error and\nwarnings. This is the default level used by the library.\n- `transformers.logging.INFO` (int value, 20): reports error, warnings and basic information.\n- `transformers.logging.DEBUG` (int value, 10): report all information.\n\nBy default, `tqdm` progress bars will be displayed during model download. [`logging.disable_progress_bar`] and [`logging.enable_progress_bar`] can be used to suppress or unsuppress this behavior.\n\n## `logging` vs `warnings`\n\nPython has two logging systems that are often used in conjunction: `logging`, which is explained above, and `warnings`,\nwhich allows further classification of warnings in specific buckets, e.g., `FutureWarning` for a feature or path\nthat has already been deprecated and `DeprecationWarning` to indicate an upcoming deprecation.",
  "We use both in the `transformers` library. We leverage and adapt `logging`'s `captureWarnings` method to allow\nmanagement of these warning messages by the verbosity setters above.\n\nWhat does that mean for developers of the library? We should respect the following heuristics:\n- `warnings` should be favored for developers of the library and libraries dependent on `transformers`\n- `logging` should be used for end-users of the library using it in every-day projects\n\nSee reference of the `captureWarnings` method below.\n\n[[autodoc]] logging.captureWarnings\n\n## Base setters\n\n[[autodoc]] logging.set_verbosity_error\n\n[[autodoc]] logging.set_verbosity_warning\n\n[[autodoc]] logging.set_verbosity_info\n\n[[autodoc]] logging.set_verbosity_debug\n\n## Other functions\n\n[[autodoc]] logging.get_verbosity\n\n[[autodoc]] logging.set_verbosity\n\n[[autodoc]] logging.get_logger\n\n[[autodoc]] logging.enable_default_handler\n\n[[autodoc]] logging.disable_default_handler\n\n[[autodoc]] logging.enable_explicit_format\n\n[[autodoc]] logging.reset_format\n\n[[autodoc]] logging.enable_progress_bar\n\n[[autodoc]] logging.disable_progress_bar",
  "<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Image Processor\n\nAn image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.",
  "Fast image processors are available for a few models and more will be added in the future. They are based on the [torchvision](https://pytorch.org/vision/stable/index.html) library and provide a significant speed-up, especially when processing on GPU.\nThey have the same API as the base image processors and can be used as drop-in replacements.\nTo use a fast image processor, you need to install the `torchvision` library, and set the `use_fast` argument to `True` when instantiating the image processor:\n\n```python\nfrom transformers import AutoImageProcessor\n\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", use_fast=True)\n```\nNote that `use_fast` will be set to `True` by default in a future release.\n\nWhen using a fast image processor, you can also set the `device` argument to specify the device on which the processing should be done. By default, the processing is done on the same device as the inputs if the inputs are tensors, or on the CPU otherwise.\n\n```python\nfrom torchvision.io import read_image\nfrom transformers import DetrImageProcessorFast\n\nimages = read_image(\"image.jpg\")\nprocessor = DetrImageProcessorFast.from_pretrained(\"facebook/detr-resnet-50\")",
  "images_processed = processor(images, return_tensors=\"pt\", device=\"cuda\")\n```\n\nHere are some speed comparisons between the base and fast image processors for the `DETR` and `RT-DETR` models, and how they impact overall inference time:\n\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_padded.png\" />\n</div>\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_batched_compiled.png\" />\n</div>\n\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_single.png\" />\n</div>\n<div class=\"flex\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_batched.png\" />\n</div>\n\nThese benchmarks were run on an [AWS EC2 g5.2xlarge instance](https://aws.amazon.com/ec2/instance-types/g5/), utilizing an NVIDIA A10G Tensor Core GPU.\n\n\n## ImageProcessingMixin",
  "[[autodoc]] image_processing_utils.ImageProcessingMixin\n- from_pretrained\n- save_pretrained\n\n## BatchFeature\n\n[[autodoc]] BatchFeature\n\n## BaseImageProcessor\n\n[[autodoc]] image_processing_utils.BaseImageProcessor\n\n\n## BaseImageProcessorFast\n\n[[autodoc]] image_processing_utils_fast.BaseImageProcessorFast",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Callbacks\n\nCallbacks are objects that can customize the behavior of the training loop in the PyTorch\n[`Trainer`] (this feature is not yet implemented in TensorFlow) that can inspect the training loop\nstate (for progress reporting, logging on TensorBoard or other ML platforms...) and take decisions (like early\nstopping).\n\nCallbacks are \"read only\" pieces of code, apart from the [`TrainerControl`] object they return, they",
  "cannot change anything in the training loop. For customizations that require changes in the training loop, you should\nsubclass [`Trainer`] and override the methods you need (see [trainer](trainer) for examples).\n\nBy default, `TrainingArguments.report_to` is set to `\"all\"`, so a [`Trainer`] will use the following callbacks.\n\n- [`DefaultFlowCallback`] which handles the default behavior for logging, saving and evaluation.\n- [`PrinterCallback`] or [`ProgressCallback`] to display progress and print the\nlogs (the first one is used if you deactivate tqdm through the [`TrainingArguments`], otherwise\nit's the second one).\n- [`~integrations.TensorBoardCallback`] if tensorboard is accessible (either through PyTorch >= 1.4\nor tensorboardX).\n- [`~integrations.WandbCallback`] if [wandb](https://www.wandb.com/) is installed.\n- [`~integrations.CometCallback`] if [comet_ml](https://www.comet.com/site/) is installed.\n- [`~integrations.MLflowCallback`] if [mlflow](https://www.mlflow.org/) is installed.\n- [`~integrations.NeptuneCallback`] if [neptune](https://neptune.ai/) is installed.\n- [`~integrations.AzureMLCallback`] if [azureml-sdk](https://pypi.org/project/azureml-sdk/) is\ninstalled.",
  "- [`~integrations.CodeCarbonCallback`] if [codecarbon](https://pypi.org/project/codecarbon/) is\ninstalled.\n- [`~integrations.ClearMLCallback`] if [clearml](https://github.com/allegroai/clearml) is installed.\n- [`~integrations.DagsHubCallback`] if [dagshub](https://dagshub.com/) is installed.\n- [`~integrations.FlyteCallback`] if [flyte](https://flyte.org/) is installed.\n- [`~integrations.DVCLiveCallback`] if [dvclive](https://dvc.org/doc/dvclive) is installed.\n- [`~integrations.SwanLabCallback`] if [swanlab](http://swanlab.cn/) is installed.\n\nIf a package is installed but you don't wish to use the accompanying integration, you can change `TrainingArguments.report_to` to a list of just those integrations you want to use (e.g. `[\"azure_ml\", \"wandb\"]`).\n\nThe main class that implements callbacks is [`TrainerCallback`]. It gets the\n[`TrainingArguments`] used to instantiate the [`Trainer`], can access that\nTrainer's internal state via [`TrainerState`], and can take some actions on the training loop via\n[`TrainerControl`].\n\n\n## Available Callbacks\n\nHere is the list of the available [`TrainerCallback`] in the library:\n\n[[autodoc]] integrations.CometCallback\n- setup",
  "[[autodoc]] DefaultFlowCallback\n\n[[autodoc]] PrinterCallback\n\n[[autodoc]] ProgressCallback\n\n[[autodoc]] EarlyStoppingCallback\n\n[[autodoc]] integrations.TensorBoardCallback\n\n[[autodoc]] integrations.WandbCallback\n- setup\n\n[[autodoc]] integrations.MLflowCallback\n- setup\n\n[[autodoc]] integrations.AzureMLCallback\n\n[[autodoc]] integrations.CodeCarbonCallback\n\n[[autodoc]] integrations.NeptuneCallback\n\n[[autodoc]] integrations.ClearMLCallback\n\n[[autodoc]] integrations.DagsHubCallback\n\n[[autodoc]] integrations.FlyteCallback\n\n[[autodoc]] integrations.DVCLiveCallback\n- setup\n\n[[autodoc]] integrations.SwanLabCallback\n- setup\n\n## TrainerCallback\n\n[[autodoc]] TrainerCallback\n\nHere is an example of how to register a custom callback with the PyTorch [`Trainer`]:\n\n```python\nclass MyCallback(TrainerCallback):\n\"A callback that prints a message at the beginning of training\"\n\ndef on_train_begin(self, args, state, control, **kwargs):\nprint(\"Starting training\")\n\n\ntrainer = Trainer(\nmodel,\nargs,\ntrain_dataset=train_dataset,\neval_dataset=eval_dataset,\ncallbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())\n)\n```",
  "Another way to register a callback is to call `trainer.add_callback()` as follows:\n\n```python\ntrainer = Trainer(...)\ntrainer.add_callback(MyCallback)\n# Alternatively, we can pass an instance of the callback class\ntrainer.add_callback(MyCallback())\n```\n\n## TrainerState\n\n[[autodoc]] TrainerState\n\n## TrainerControl\n\n[[autodoc]] TrainerControl",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Backbone\n\nA backbone is a model used for feature extraction for higher level computer vision tasks such as object detection and image classification. Transformers provides an [`AutoBackbone`] class for initializing a Transformers backbone from pretrained model weights, and two utility classes:",
  "* [`~utils.BackboneMixin`] enables initializing a backbone from Transformers or [timm](https://hf.co/docs/timm/index) and includes functions for returning the output features and indices.\n* [`~utils.BackboneConfigMixin`] sets the output features and indices of the backbone configuration.\n\n[timm](https://hf.co/docs/timm/index) models are loaded with the [`TimmBackbone`] and [`TimmBackboneConfig`] classes.\n\nBackbones are supported for the following models:\n\n* [BEiT](../model_doc/beit)\n* [BiT](../model_doc/bit)\n* [ConvNext](../model_doc/convnext)\n* [ConvNextV2](../model_doc/convnextv2)\n* [DiNAT](../model_doc/dinat)\n* [DINOV2](../model_doc/dinov2)\n* [FocalNet](../model_doc/focalnet)\n* [MaskFormer](../model_doc/maskformer)\n* [NAT](../model_doc/nat)\n* [ResNet](../model_doc/resnet)\n* [Swin Transformer](../model_doc/swin)\n* [Swin Transformer v2](../model_doc/swinv2)\n* [ViTDet](../model_doc/vitdet)\n\n## AutoBackbone\n\n[[autodoc]] AutoBackbone\n\n## BackboneMixin\n\n[[autodoc]] utils.BackboneMixin\n\n## BackboneConfigMixin\n\n[[autodoc]] utils.BackboneConfigMixin\n\n## TimmBackbone\n\n[[autodoc]] models.timm_backbone.TimmBackbone\n\n## TimmBackboneConfig\n\n[[autodoc]] models.timm_backbone.TimmBackboneConfig",
  "",
  "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Quantization\n\nQuantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn't be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes.",
  "Quantization techniques that aren't supported in Transformers can be added with the [`HfQuantizer`] class.\n\n<Tip>\n\nLearn how to quantize models in the [Quantization](../quantization) guide.\n\n</Tip>\n\n## QuantoConfig\n\n[[autodoc]] QuantoConfig\n\n## AqlmConfig\n\n[[autodoc]] AqlmConfig\n\n## VptqConfig\n\n[[autodoc]] VptqConfig\n\n## AwqConfig\n\n[[autodoc]] AwqConfig\n\n## EetqConfig\n[[autodoc]] EetqConfig\n\n## GPTQConfig\n\n[[autodoc]] GPTQConfig\n\n## BitsAndBytesConfig\n\n[[autodoc]] BitsAndBytesConfig\n\n## HfQuantizer\n\n[[autodoc]] quantizers.base.HfQuantizer\n\n## HiggsConfig\n\n[[autodoc]] HiggsConfig\n\n## HqqConfig\n\n[[autodoc]] HqqConfig\n\n## FbgemmFp8Config\n\n[[autodoc]] FbgemmFp8Config\n\n## CompressedTensorsConfig\n\n[[autodoc]] CompressedTensorsConfig\n\n## TorchAoConfig\n\n[[autodoc]] TorchAoConfig\n\n## BitNetConfig\n\n[[autodoc]] BitNetConfig\n\n## SpQRConfig\n\n[[autodoc]] SpQRConfig\n\n## FineGrainedFP8Config\n\n[[autodoc]] FineGrainedFP8Config\n\n## QuarkConfig\n\n[[autodoc]] QuarkConfig",
  "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Exporting 🤗 Transformers models to ONNX\n\n🤗 Transformers provides a `transformers.onnx` package that enables you to\nconvert model checkpoints to an ONNX graph by leveraging configuration objects.\n\nSee the [guide](../serialization) on exporting 🤗 Transformers models for more\ndetails.\n\n## ONNX Configurations\n\nWe provide three abstract classes that you should inherit from, depending on the\ntype of model architecture you wish to export:",
  "* Encoder-based models inherit from [`~onnx.config.OnnxConfig`]\n* Decoder-based models inherit from [`~onnx.config.OnnxConfigWithPast`]\n* Encoder-decoder models inherit from [`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n\n### OnnxConfig\n\n[[autodoc]] onnx.config.OnnxConfig\n\n### OnnxConfigWithPast\n\n[[autodoc]] onnx.config.OnnxConfigWithPast\n\n### OnnxSeq2SeqConfigWithPast\n\n[[autodoc]] onnx.config.OnnxSeq2SeqConfigWithPast\n\n## ONNX Features\n\nEach ONNX configuration is associated with a set of _features_ that enable you\nto export models for different types of topologies or tasks.\n\n### FeaturesManager\n\n[[autodoc]] onnx.features.FeaturesManager",
  "<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# PEFT\n\nThe [`~integrations.PeftAdapterMixin`] provides functions from the [PEFT](https://huggingface.co/docs/peft/index) library for managing adapters with Transformers. This mixin currently supports LoRA, IA3, and AdaLora. Prefix tuning methods (prompt tuning, prompt learning) aren't supported because they can't be injected into a torch module.\n\n[[autodoc]] integrations.PeftAdapterMixin\n- load_adapter\n- add_adapter\n- set_adapter",
  "- disable_adapters\n- enable_adapters\n- active_adapters\n- get_adapter_state_dict"
]